# Testing Patterns

**Analysis Date:** 2026-01-24

## Test Framework

**Runner:**
- None configured (no pytest, unittest, or jest)
- Manual testing via standalone scripts

**Assertion Library:**
- None (no test framework)

**Run Commands:**
```bash
# No automated tests
# Manual testing via debug scripts:
python3 test_news_filter.py        # Test LLM filtering logic
python3 test_news_analyzer.py      # Test analyzer with real data
```

## Test File Organization

**Location:**
- Root directory (not separated into `tests/` folder)
- Files: `test_news_filter.py`, `test_news_analyzer.py`

**Naming:**
- Pattern: `test_*.py` prefix
- No formal test suite structure

**Structure:**
```
ai-news-aggregator/
├── test_news_filter.py          # Debug script for LLM filter
├── test_news_analyzer.py        # Debug script for analyzer
└── agents/                       # No tests in source directories
```

## Test Structure

**Suite Organization:**
```python
# Example from test_news_analyzer.py
async def main():
    # Setup
    from agents.base import CollectedItem
    from agents.analyzers.news_analyzer import NewsAnalyzer

    # Load test data
    with open(raw_file, 'r') as f:
        data = json.load(f)
    items = [CollectedItem(**item) for item in data['items']]

    # Run operation
    analyzer = NewsAnalyzer(async_client=async_client)
    report = await analyzer.analyze(items)

    # Manual verification (print results)
    print(f"Analyzed items: {len(report.all_items)}")
    print(f"Top items: {len(report.top_items)}")
```

**Patterns:**
- Manual verification via print statements
- No assertions or automated checks
- Tests load real data from `data/raw/` directory
- Tests run against live LLM APIs (not mocked)

## Mocking

**Framework:** None

**Patterns:**
- No mocking used
- Tests run against live APIs and real data
- Environment variables control which endpoints are hit

**What to Mock:**
- Not applicable (no mocking framework)

**What NOT to Mock:**
- Not applicable (no mocking framework)

## Fixtures and Factories

**Test Data:**
```python
# Example from test_news_filter.py
def load_news_items():
    """Load real collected data from disk."""
    with open('./data/raw/news_2026-01-02.json', 'r') as f:
        data = json.load(f)
    return data['items']
```

**Location:**
- Test data stored in `data/raw/` directory
- Files generated by actual pipeline runs (e.g., `news_2026-01-02.json`)
- No synthetic test fixtures

## Coverage

**Requirements:** None enforced

**View Coverage:**
```bash
# No coverage tooling configured
```

## Test Types

**Unit Tests:**
- Not implemented
- No isolated component testing

**Integration Tests:**
- Partial: debug scripts test end-to-end flows
- `test_news_analyzer.py` runs full analyzer pipeline with real data
- No formal integration test suite

**E2E Tests:**
- Not used
- Manual testing of full pipeline via `run_pipeline.py`

## Common Patterns

**Async Testing:**
```python
# Pattern from test_news_analyzer.py
async def main():
    # Initialize async components
    async_client = AsyncAnthropicClient()
    analyzer = NewsAnalyzer(async_client=async_client)

    # Run async operation
    report = await analyzer.analyze(items)

    # Verify results manually
    print(f"Results: {len(report.all_items)}")

if __name__ == '__main__':
    asyncio.run(main())
```

**Error Testing:**
```python
# Pattern from test_news_filter.py
try:
    response = await client.call_with_thinking(
        messages=[{"role": "user", "content": prompt}],
        budget_tokens=ThinkingLevel.QUICK
    )
    parsed = parse_json_response(response.content)
    logger.info(f"Parsed result: {parsed}")
except Exception as e:
    logger.error(f"Error: {e}")
    import traceback
    traceback.print_exc()
    return {}
```

## Testing Philosophy

**Current Approach:**
- No automated test suite
- Manual verification via debug scripts
- Real API calls with live data
- Cost tracking monitored during manual testing
- User runs tests themselves (per project docs: "Testing: The user always runs tests themselves")

**Debug Scripts Purpose:**
- Iterate on LLM prompts without full pipeline runs
- Verify analyzer logic with real collected data
- Compare different prompt strategies (e.g., `FILTER_PROMPTS` dict in `test_news_filter.py`)
- Track token usage and costs during development

## Frontend Testing

**Framework:** None configured

**TypeScript:**
- `svelte-check` used for type checking only (via `npm run check`)
- No unit tests for components or services
- No Jest, Vitest, or Testing Library

**Run Commands:**
```bash
cd frontend
npm run check              # TypeScript type checking
npm run check:watch        # Watch mode for type checking
```

## Quality Assurance Approach

**Manual Testing:**
- Run full pipeline with `python3 run_pipeline.py`
- Inspect generated JSON in `web/data/`
- Verify frontend rendering with `npm run dev`
- Check cost reports in `data/processed/cost_report_*.json`

**Validation:**
- Type checking via TypeScript strict mode
- Schema validation implicit (dataclass constructors, TypeScript interfaces)
- No formal validation framework

**Monitoring:**
- Cost tracking via `CostTracker` class (`agents/cost_tracker.py`)
- Collection status tracking per source
- Logs reviewed for warnings/errors

---

*Testing analysis: 2026-01-24*
