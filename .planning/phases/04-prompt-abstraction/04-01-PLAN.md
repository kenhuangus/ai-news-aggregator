---
phase: 04-prompt-abstraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/config/loader.py
  - agents/config/schema.py
  - agents/config/prompts.py
  - agents/config/__init__.py
autonomous: true

must_haves:
  truths:
    - "Runtime variables (${var}) resolve from context dict"
    - "Environment variables (${env:VAR}) still resolve from os.environ"
    - "Missing variable raises ValueError with clear path indication"
    - "PromptConfig validates YAML structure at load time"
    - "PromptAccessor provides typed access to prompts"
  artifacts:
    - path: "agents/config/loader.py"
      provides: "resolve_variables() function for runtime context"
      contains: "resolve_variables"
    - path: "agents/config/schema.py"
      provides: "PromptConfig Pydantic model"
      contains: "class PromptConfig"
    - path: "agents/config/prompts.py"
      provides: "PromptAccessor class and load_prompts function"
      exports: ["PromptAccessor", "load_prompts"]
  key_links:
    - from: "agents/config/prompts.py"
      to: "agents/config/loader.py"
      via: "imports load_yaml_with_env"
      pattern: "from .loader import"
    - from: "agents/config/prompts.py"
      to: "agents/config/schema.py"
      via: "imports PromptConfig"
      pattern: "from .schema import PromptConfig"
---

<objective>
Create the infrastructure for loading prompts from YAML with runtime variable substitution.

Purpose: Enable prompts to be externalized to config files while supporting both environment variables and runtime context variables (like ${date}, ${category}).

Output: Extended loader with variable resolution, Pydantic schema for prompts, and accessor class for typed prompt retrieval.
</objective>

<execution_context>
@/Users/ryand/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ryand/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-prompt-abstraction/04-CONTEXT.md
@.planning/phases/04-prompt-abstraction/04-RESEARCH.md
@agents/config/loader.py
@agents/config/schema.py
@agents/config/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add runtime variable resolution to loader</name>
  <files>agents/config/loader.py</files>
  <action>
Add a `resolve_variables()` function that extends the existing env var interpolation pattern to support runtime context variables:

1. Create new regex pattern: `VAR_PATTERN = re.compile(r'\$\{(env:)?([A-Za-z_][A-Za-z0-9_]*)\}')`
   - Matches `${VAR}` for runtime variables
   - Matches `${env:VAR}` for explicit environment variables

2. Implement `resolve_variables(value, context, allow_missing=False, path="")`:
   - Recursively processes strings, dicts, and lists (same pattern as _resolve_env_vars)
   - For `${env:VAR}`: looks up in os.environ
   - For `${VAR}`: looks up in context dict
   - If variable not found and not allow_missing: raises ValueError with path info
   - If allow_missing: leaves unresolved variables as-is

3. Keep existing `_resolve_env_vars()` and `ENV_VAR_PATTERN` unchanged for backwards compatibility

4. Add exports to module (resolve_variables will be used by prompts.py)

Pattern from RESEARCH.md:
```python
def resolve_variables(
    value: Any,
    context: Dict[str, str],
    allow_missing: bool = False,
    path: str = ""
) -> Any:
```
  </action>
  <verify>
Add doctest or simple test at end of file:
```python
if __name__ == "__main__":
    # Quick test
    result = resolve_variables("Hello ${name}!", {"name": "World"})
    assert result == "Hello World!", f"Got: {result}"
    print("resolve_variables test passed")
```
Run: `python3 -c "from agents.config.loader import resolve_variables; print(resolve_variables('${x} + ${y}', {'x': '1', 'y': '2'}))"`
  </verify>
  <done>resolve_variables() handles both ${var} and ${env:VAR} patterns correctly</done>
</task>

<task type="auto">
  <name>Task 2: Create PromptConfig schema</name>
  <files>agents/config/schema.py</files>
  <action>
Add Pydantic models for prompt configuration:

1. `AnalyzerPrompts` - prompts for a single analyzer:
   - batch_analysis: str (required)
   - ranking: str (required)
   - filter: Optional[str] = None (only news has this)
   - combined_analysis: Optional[str] = None (small batch optimization)
   - analysis: Optional[str] = None (legacy, keep for reference)

2. `GatheringPrompts` - prompts for gathering phase:
   - link_relevance: str (link follower decision)

3. `OrchestrationPrompts` - prompts for orchestration:
   - topic_detection: str
   - executive_summary: str

4. `PostProcessingPrompts` - prompts for post-processing:
   - link_enrichment: str
   - ecosystem_enrichment: str

5. `PromptConfig` - root config:
   - gathering: GatheringPrompts
   - analysis: Dict[str, AnalyzerPrompts] (keyed by category: news, research, social, reddit)
   - orchestration: OrchestrationPrompts
   - post_processing: PostProcessingPrompts
   - model_config = {"extra": "ignore"}

Use Field() with min_length validation on required prompts to catch empty strings.
  </action>
  <verify>
```python
python3 -c "
from agents.config.schema import PromptConfig
import json
# Test minimal valid config
config = PromptConfig.model_validate({
    'gathering': {'link_relevance': 'test prompt'},
    'analysis': {
        'news': {'batch_analysis': 'test', 'ranking': 'test'},
        'research': {'batch_analysis': 'test', 'ranking': 'test'},
        'social': {'batch_analysis': 'test', 'ranking': 'test'},
        'reddit': {'batch_analysis': 'test', 'ranking': 'test'}
    },
    'orchestration': {'topic_detection': 'test', 'executive_summary': 'test'},
    'post_processing': {'link_enrichment': 'test', 'ecosystem_enrichment': 'test'}
})
print('PromptConfig validation passed')
"
```
  </verify>
  <done>PromptConfig and nested models validate prompt YAML structure</done>
</task>

<task type="auto">
  <name>Task 3: Create PromptAccessor and load_prompts</name>
  <files>agents/config/prompts.py, agents/config/__init__.py</files>
  <action>
Create `agents/config/prompts.py`:

1. `load_prompts(config_dir: str) -> PromptConfig`:
   - Load config/prompts.yaml using load_yaml_with_env (env vars only at load time)
   - Validate with PromptConfig.model_validate()
   - Exit with helpful error if file missing or invalid (same pattern as load_config)

2. `PromptAccessor` class:
   - `__init__(self, config: PromptConfig)`
   - `get_analyzer_prompt(category, prompt_type, context) -> str`:
     - Gets prompt from config.analysis[category].{prompt_type}
     - Calls resolve_variables() with context
     - Raises ValueError for unknown category/prompt_type
   - `get_gathering_prompt(prompt_type, context) -> str`
   - `get_orchestration_prompt(prompt_type, context) -> str`
   - `get_post_processing_prompt(prompt_type, context) -> str`

Update `agents/config/__init__.py`:
- Add exports: PromptConfig, load_prompts, PromptAccessor

Follow the error handling pattern from load_config():
- sys.exit(1) with logger.error for missing file
- ValidationError handling with field-by-field error messages
  </action>
  <verify>
```python
python3 -c "
from agents.config.prompts import load_prompts, PromptAccessor
# This will fail since prompts.yaml doesn't exist yet, but import should work
print('prompts.py imports successfully')
"
```
  </verify>
  <done>PromptAccessor provides typed prompt access with runtime variable resolution</done>
</task>

</tasks>

<verification>
1. Module imports work:
   ```bash
   python3 -c "from agents.config import PromptConfig, load_prompts, PromptAccessor; print('Imports OK')"
   ```

2. Variable resolution works:
   ```bash
   python3 -c "
   from agents.config.loader import resolve_variables
   import os
   os.environ['TEST_VAR'] = 'from_env'
   result = resolve_variables('runtime=\${name}, env=\${env:TEST_VAR}', {'name': 'from_context'})
   assert 'from_context' in result and 'from_env' in result, f'Got: {result}'
   print('Variable resolution OK')
   "
   ```

3. Schema validation catches errors:
   ```bash
   python3 -c "
   from agents.config.schema import PromptConfig
   try:
       PromptConfig.model_validate({'gathering': {}})
       print('FAIL: Should have raised validation error')
   except Exception as e:
       print(f'Validation correctly rejects invalid config: {type(e).__name__}')
   "
   ```
</verification>

<success_criteria>
- resolve_variables() supports both ${var} and ${env:VAR} patterns
- PromptConfig validates all prompt structure requirements
- PromptAccessor provides typed access to prompts
- All new code follows existing patterns in loader.py and schema.py
- Backwards compatibility maintained (existing _resolve_env_vars unchanged)
</success_criteria>

<output>
After completion, create `.planning/phases/04-prompt-abstraction/04-01-SUMMARY.md`
</output>
