---
phase: 04-prompt-abstraction
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - config/prompts.yaml
autonomous: true

# NOTE: This plan covers PRM-05 ("Create config/prompts.yaml.example").
# Per user decision in 04-CONTEXT.md: "config/prompts.yaml ships in the repo with
# all working prompts (not an example file)" - the prompts.yaml file serves as both
# the working config AND the reference example with documented variables.

must_haves:
  truths:
    - "All 18 prompts are externalized to config/prompts.yaml"
    - "Prompts use literal block scalar (|) for multiline preservation"
    - "Variable placeholders use ${var} syntax documented in comments"
    - "YAML parses without errors"
    - "Schema validation passes for the file"
    - "Prompt content matches source files (not placeholder text)"
  artifacts:
    - path: "config/prompts.yaml"
      provides: "All LLM prompts for the pipeline (serves as both config and example)"
      min_lines: 500
      contains: "gathering:"
  key_links:
    - from: "config/prompts.yaml"
      to: "agents/config/schema.py"
      via: "Structure matches PromptConfig"
      pattern: "analysis:|orchestration:|post_processing:"
---

<objective>
Create config/prompts.yaml with all LLM prompts extracted from the codebase.

Purpose: Externalize all prompts to a single YAML file that users can customize without editing Python code. This file serves as both the working configuration AND the reference example (per PRM-05), with documented variables for each prompt section.

Output: Complete prompts.yaml file organized by pipeline phase (gathering, analysis, orchestration, post_processing).
</objective>

<execution_context>
@/Users/ryand/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ryand/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-prompt-abstraction/04-CONTEXT.md
@.planning/phases/04-prompt-abstraction/04-RESEARCH.md
@.planning/phases/04-prompt-abstraction/04-01-SUMMARY.md
@agents/analyzers/news_analyzer.py
@agents/analyzers/research_analyzer.py
@agents/analyzers/social_analyzer.py
@agents/analyzers/reddit_analyzer.py
@agents/orchestrator.py
@agents/link_enricher.py
@agents/gatherers/link_follower.py
@agents/ecosystem_context.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create config/prompts.yaml with all prompts</name>
  <files>config/prompts.yaml</files>
  <!-- NOTE: This task extracts 18 prompts from 8 source files. While this is more
       files than typical, the work is mechanical (copy prompt strings with variable
       conversion) and results in a single output file. The task is intentionally
       kept as one unit since it's a single Write operation producing one artifact. -->
  <action>
Create config/prompts.yaml with ALL prompts extracted from the codebase:

**File structure:**
```yaml
# =============================================================================
# AI News Aggregator - LLM Prompts Configuration
# =============================================================================
# This file contains all LLM prompts used in the pipeline.
# Edit prompts here to customize analysis behavior without changing code.
#
# Variable syntax:
#   ${var}      - Runtime variable (resolved when prompt is used)
#   ${env:VAR}  - Environment variable (resolved at load time)
#
# Use YAML literal blocks (|) to preserve formatting in multiline prompts.
# =============================================================================

gathering:
  # Link follower - decides which URLs to fetch
  # Available: ${url}, ${post_context}
  link_relevance: |
    [Extract from link_follower.py should_follow_link()]

analysis:
  news:
    # Available: ${batch_index}, ${total_batches}, ${items_context}
    batch_analysis: |
      [Extract BATCH_ANALYSIS_PROMPT from news_analyzer.py]

    # Available: ${items_context}, ${example_id}
    filter: |
      [Extract FILTER_PROMPT from news_analyzer.py]

    # Available: ${count}, ${items_context}
    combined_analysis: |
      [Extract COMBINED_ANALYSIS_PROMPT from news_analyzer.py]

    # Available: ${items_context}
    analysis: |
      [Extract ANALYSIS_PROMPT from news_analyzer.py - legacy]

    # Available: ${analysis_summary}
    ranking: |
      [Extract RANKING_PROMPT from news_analyzer.py]

  research:
    # Available: ${batch_index}, ${total_batches}, ${items_context}
    batch_analysis: |
      [Extract from research_analyzer.py]

    # ... etc for all research prompts

  social:
    # ... all social prompts

  reddit:
    # ... all reddit prompts

orchestration:
  # Available: ${context}
  topic_detection: |
    [Extract from orchestrator.py _detect_cross_category_topics()]

  # Available: ${context}
  executive_summary: |
    [Extract from orchestrator.py _generate_executive_summary()]

post_processing:
  # Available: ${date}, ${items_json}, ${text}
  link_enrichment: |
    [Extract from link_enricher.py _enrich_text()]

  # Available: ${report_date}, ${coverage_date}, ${existing_models}, ${news_items}
  ecosystem_enrichment: |
    [Extract ENRICHMENT_PROMPT from ecosystem_context.py]
```

**Critical extraction notes:**

1. **EXACT extraction** - Copy prompts EXACTLY from source files, preserving:
   - All formatting (newlines, indentation)
   - JSON examples
   - Bullet points
   - PRIORITIZE/DEPRIORITIZE sections

2. **Variable conversion** - Convert Python format strings:
   - `{batch_index}` becomes `${batch_index}`
   - `{{` and `}}` in JSON examples stay as literal `{` and `}`
   - Double braces in prompts (for JSON templates) must remain as `{{` `}}`

3. **Read source files** to extract prompts:
   - news_analyzer.py: BATCH_ANALYSIS_PROMPT, FILTER_PROMPT, COMBINED_ANALYSIS_PROMPT, ANALYSIS_PROMPT, RANKING_PROMPT
   - research_analyzer.py: BATCH_ANALYSIS_PROMPT, ANALYSIS_PROMPT, RANKING_PROMPT
   - social_analyzer.py: BATCH_ANALYSIS_PROMPT, ANALYSIS_PROMPT, RANKING_PROMPT
   - reddit_analyzer.py: BATCH_ANALYSIS_PROMPT, ANALYSIS_PROMPT, RANKING_PROMPT
   - orchestrator.py: inline in _detect_cross_category_topics(), _generate_executive_summary()
   - link_enricher.py: inline in _enrich_text()
   - link_follower.py: inline in should_follow_link()
   - ecosystem_context.py: ENRICHMENT_PROMPT

4. **Document available variables** in comments above each prompt section
  </action>
  <verify>
1. YAML parses:
```bash
python3 -c "import yaml; yaml.safe_load(open('config/prompts.yaml')); print('YAML valid')"
```

2. Schema validates:
```bash
python3 -c "
from agents.config.prompts import load_prompts
config = load_prompts('./config')
print(f'Loaded {len(config.analysis)} analyzers')
print('Schema validation passed')
"
```

3. Check prompt count:
```bash
grep -c "^\s*[a-z_]*:" config/prompts.yaml | head -1
# Should be ~25+ keys (prompts + sections)
```
  </verify>
  <done>All 18 prompts extracted to config/prompts.yaml with proper structure</done>
</task>

</tasks>

<verification>
1. File exists and parses:
   ```bash
   test -f config/prompts.yaml && python3 -c "import yaml; yaml.safe_load(open('config/prompts.yaml')); print('OK')"
   ```

2. Schema validation:
   ```bash
   python3 -c "from agents.config.prompts import load_prompts; load_prompts('./config'); print('Schema OK')"
   ```

3. All sections present:
   ```bash
   for section in gathering analysis orchestration post_processing; do
     grep -q "^${section}:" config/prompts.yaml && echo "$section: present" || echo "$section: MISSING"
   done
   ```

4. All analyzers present:
   ```bash
   for cat in news research social reddit; do
     grep -q "^  ${cat}:" config/prompts.yaml && echo "$cat: present" || echo "$cat: MISSING"
   done
   ```

5. Prompts have content (not empty):
   ```bash
   python3 -c "
   from agents.config.prompts import load_prompts
   config = load_prompts('./config')
   assert len(config.analysis['news'].batch_analysis) > 100, 'news.batch_analysis too short'
   assert len(config.orchestration.topic_detection) > 100, 'topic_detection too short'
   print('Prompts have content')
   "
   ```

6. Content spot-check (verify prompts match source files):
   ```bash
   python3 -c "
   import yaml

   # Load prompts.yaml
   with open('config/prompts.yaml') as f:
       prompts = yaml.safe_load(f)

   # Spot-check: news batch_analysis should contain 'AI news analyst' phrase from source
   assert 'AI news analyst' in prompts['analysis']['news']['batch_analysis'], \
       'news.batch_analysis missing expected phrase from source'

   # Spot-check: topic_detection should contain 'cross-category' phrase
   assert 'cross-category' in prompts['orchestration']['topic_detection'].lower(), \
       'topic_detection missing expected phrase from source'

   # Spot-check: link_enrichment should mention 'link enrichment'
   assert 'link' in prompts['post_processing']['link_enrichment'].lower(), \
       'link_enrichment missing expected phrase from source'

   # Spot-check: ecosystem_enrichment should mention 'model' or 'release'
   eco = prompts['post_processing']['ecosystem_enrichment'].lower()
   assert 'model' in eco or 'release' in eco, \
       'ecosystem_enrichment missing expected phrase from source'

   print('Content spot-check passed - prompts match source files')
   "
   ```
</verification>

<success_criteria>
- config/prompts.yaml contains all 18 prompts from the codebase
- Prompts are organized by pipeline phase (gathering, analysis, orchestration, post_processing)
- YAML literal block scalars (|) preserve multiline formatting
- Variable placeholders converted from Python {var} to ${var} syntax
- Available variables documented in comments for each prompt
- File validates against PromptConfig schema
</success_criteria>

<output>
After completion, create `.planning/phases/04-prompt-abstraction/04-02-SUMMARY.md`
</output>
