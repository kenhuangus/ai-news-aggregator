---
phase: 02-llm-provider-support
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/llm_client.py
autonomous: true

must_haves:
  truths:
    - "User can connect to Anthropic API directly using their API key"
    - "User can connect to any OpenAI-compatible proxy using their API key"
    - "User gets quality analysis from extended thinking when available"
    - "User gets clear guidance if extended thinking is unavailable in their setup"
  artifacts:
    - path: "agents/llm_client.py"
      provides: "Mode-based auth switching and thinking validation"
      contains: "class ApiKeyAuth"
  key_links:
    - from: "from_config()"
      to: "__init__(mode=...)"
      via: "passes config.mode parameter"
      pattern: "mode=config\\.mode"
    - from: "call_with_thinking()"
      to: "RuntimeError"
      via: "validates thinking blocks present"
      pattern: "thinking_blocks.*RuntimeError"
---

<objective>
Implement mode-based authentication switching and extended thinking validation for the LLM client.

Purpose: Enable users to connect to either direct Anthropic API (x-api-key header) or OpenAI-compatible proxies (Bearer token) based on their configuration mode, while ensuring extended thinking always works correctly.

Output: Modified `agents/llm_client.py` with:
- `ApiKeyAuth` class for x-api-key header authentication
- Mode-aware constructor that selects appropriate auth
- Updated `from_config()` to pass mode from configuration
- Thinking block validation in `call_with_thinking()`

Note: LLM-03 (configurable model name per mode) is already satisfied by the existing `model` parameter in LLMProviderConfig. Users specify any model name they want for their mode, and it's used as-is. No additional task needed.
</objective>

<execution_context>
@/Users/ryand/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ryand/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-llm-provider-support/02-RESEARCH.md
@agents/llm_client.py
@agents/config/schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add mode-based authentication switching</name>
  <files>agents/llm_client.py</files>
  <action>
    1. Add `ApiKeyAuth(httpx.Auth)` class below the existing `BearerAuth` class:
       - `__init__(self, api_key: str)` stores the key
       - `auth_flow(self, request)` adds `x-api-key` header with the key value
       - This mirrors the existing BearerAuth pattern

    2. Modify `AnthropicClient.__init__()`:
       - Add `mode: str = "anthropic"` parameter after existing parameters
       - Store as `self.mode = mode`
       - Add conditional auth selection BEFORE creating httpx.Client:
         ```python
         if self.mode == "anthropic":
             auth = ApiKeyAuth(self.api_key)
         elif self.mode == "openai-compatible":
             auth = BearerAuth(self.api_key)
         else:
             raise ValueError(f"Unknown mode: {self.mode}")
         ```
       - Pass `auth=auth` to the httpx.Client constructor
       - Update log message to include mode

    3. Update `AnthropicClient.from_config()`:
       - Pass `mode=config.mode` to the constructor

    4. Repeat steps 2-3 for `AsyncAnthropicClient`:
       - Same mode parameter and auth selection logic
       - Same from_config() update

    Note: The existing `BearerAuth` class stays for openai-compatible mode.
  </action>
  <verify>
    Run Python to verify the code is syntactically correct:
    ```bash
    cd /Users/ryand/Code/AATF/ai-news-aggregator && python3 -c "from agents.llm_client import AnthropicClient, AsyncAnthropicClient, ApiKeyAuth, BearerAuth; print('Import OK')"
    ```

    Verify ApiKeyAuth class exists:
    ```bash
    grep -q "class ApiKeyAuth" agents/llm_client.py && echo "ApiKeyAuth class exists"
    ```

    Verify mode parameter in constructors:
    ```bash
    grep -q 'mode.*:.*str.*=.*"anthropic"' agents/llm_client.py && echo "Mode parameter exists"
    ```

    Verify conditional auth selection logic:
    ```bash
    grep -q 'if self.mode == "anthropic"' agents/llm_client.py && grep -q "ApiKeyAuth" agents/llm_client.py && echo "Conditional auth logic exists"
    ```
  </verify>
  <done>
    - `ApiKeyAuth` class exists and adds x-api-key header
    - Both client classes accept `mode` parameter
    - Auth selection works based on mode value
    - `from_config()` passes mode from config
  </done>
</task>

<task type="auto">
  <name>Task 2: Add extended thinking validation</name>
  <files>agents/llm_client.py</files>
  <action>
    1. In `AnthropicClient.call_with_thinking()`, after extracting thinking_blocks and text_blocks, add validation:
       ```python
       # Validate thinking blocks are present when expected
       if budget_tokens > 0 and not thinking_blocks:
           error_msg = (
               f"Extended thinking requested (budget_tokens={budget_tokens}) but no thinking "
               f"blocks returned. This is required for quality analysis.\n\n"
           )
           if self.mode == "openai-compatible":
               error_msg += (
                   f"You are using openai-compatible mode with base_url={self.base_url}. "
                   f"If using LiteLLM, ensure you're using the Anthropic passthrough endpoint "
                   f"(e.g., http://proxy:4000/anthropic) not the OpenAI chat/completions endpoint. "
                   f"See: https://docs.litellm.ai/docs/pass_through/anthropic_completion"
               )
           else:
               error_msg += (
                   f"Check that the model '{self.model}' supports extended thinking "
                   f"and that the API endpoint is responding correctly."
               )
           raise RuntimeError(error_msg)
       ```

    2. Add the same validation to `AsyncAnthropicClient.call_with_thinking()` with identical logic.

    Why fail instead of warn: Per project decisions, extended thinking is essential for quality analysis in this pipeline. Degrading to no-thinking mode would produce inferior results.
  </action>
  <verify>
    Verify the validation code is correct by checking the method exists and has RuntimeError:
    ```bash
    cd /Users/ryand/Code/AATF/ai-news-aggregator && grep -A 20 "budget_tokens > 0 and not thinking_blocks" agents/llm_client.py
    ```

    Verify openai-compatible mode error message exists:
    ```bash
    grep -q "openai-compatible mode with base_url" agents/llm_client.py && echo "OpenAI-compatible error message exists"
    ```

    Verify anthropic mode error message exists:
    ```bash
    grep -q "Check that the model" agents/llm_client.py && echo "Anthropic mode error message exists"
    ```
  </verify>
  <done>
    - `call_with_thinking()` raises RuntimeError if budget_tokens > 0 but no thinking blocks
    - Error message includes mode-specific troubleshooting guidance
    - Both sync and async clients have the validation
  </done>
</task>

</tasks>

<verification>
1. Code compiles and imports correctly:
   ```bash
   python3 -c "from agents.llm_client import AnthropicClient, AsyncAnthropicClient, ApiKeyAuth, BearerAuth"
   ```

2. ApiKeyAuth class exists and has correct structure:
   ```bash
   grep -A 10 "class ApiKeyAuth" agents/llm_client.py
   ```

3. Mode parameter is accepted by constructors:
   ```bash
   grep -B 2 -A 2 "mode.*=.*anthropic" agents/llm_client.py
   ```

4. from_config passes mode:
   ```bash
   grep "mode=config.mode" agents/llm_client.py
   ```

5. Thinking validation exists:
   ```bash
   grep "budget_tokens > 0 and not thinking_blocks" agents/llm_client.py
   ```
</verification>

<success_criteria>
- ApiKeyAuth class exists and adds x-api-key header
- BearerAuth class still exists and adds Bearer token header
- AnthropicClient and AsyncAnthropicClient accept mode parameter
- Mode defaults to "anthropic" for backwards compatibility
- Auth class is selected based on mode value
- from_config() passes mode from LLMProviderConfig
- call_with_thinking() fails with RuntimeError if no thinking blocks returned
- Error message provides mode-specific troubleshooting guidance
</success_criteria>

<output>
After completion, create `.planning/phases/02-llm-provider-support/02-01-SUMMARY.md`
</output>
