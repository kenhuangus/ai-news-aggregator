---
phase: 01-configuration-infrastructure
plan: 04
type: execute
wave: 4
depends_on: ["01-01"]
files_modified:
  - agents/config/schema.py
  - config/providers.yaml.example
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "LLMProviderConfig has mode field with values 'anthropic' or 'openai-compatible'"
    - "ImageProviderConfig has mode field with values 'native' or 'openai-compatible'"
    - "ImageProviderConfig requires endpoint when mode is 'openai-compatible'"
    - "ImageProviderConfig uses correct model defaults per mode (gemini-3-pro-image-preview for native)"
    - "providers.yaml.example documents both modes with clear examples"
  artifacts:
    - path: "agents/config/schema.py"
      provides: "Updated Pydantic models with mode fields"
      contains: "mode: Literal"
    - path: "config/providers.yaml.example"
      provides: "Documentation of dual mode configuration"
      contains: "mode:"
  key_links:
    - from: "agents/config/schema.py"
      to: "typing.Literal"
      via: "mode field type"
      pattern: "Literal\\[.*anthropic.*openai-compatible"
---

<objective>
Add mode fields to LLM and Image provider configs to support dual API modes:
- LLM: Direct Anthropic API vs OpenAI-compatible proxy
- Image: Native google-genai SDK vs OpenAI-compatible REST API

This closes a gap from 01-01 where the schema was created without mode support needed for Phase 2/3 requirements (LLM-01/02, IMG-01/02).
</objective>

<execution_context>
@/Users/ryand/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ryand/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/01-configuration-infrastructure/01-01-SUMMARY.md
@agents/config/schema.py
@config/providers.yaml.example

## API Mode Details

### LLM Modes
| Mode | Auth Header | Extended Thinking | Use Case |
|------|-------------|-------------------|----------|
| `anthropic` | `x-api-key: {key}` | Full support | Direct Anthropic API |
| `openai-compatible` | `Authorization: Bearer {key}` | Warn only | LiteLLM, proxies |

### Image Modes
| Mode | API Style | Model Name | Endpoint |
|------|-----------|------------|----------|
| `native` | google-genai SDK `generate_content()` | `gemini-3-pro-image-preview` | Not needed (SDK handles) |
| `openai-compatible` | REST chat/completions | `gemini-3-pro-image` (or proxy alias) | Required |
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add mode field to LLMProviderConfig</name>
  <files>agents/config/schema.py</files>
  <action>
Update `agents/config/schema.py` to add mode field to LLMProviderConfig.

1. Update imports at top of file:
```python
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import Optional, Literal
```

2. Add mode field to LLMProviderConfig (add as first field after docstring):
```python
class LLMProviderConfig(BaseModel):
    """Configuration for LLM provider.

    Supports two modes:
    - anthropic: Direct Anthropic API with x-api-key header authentication
    - openai-compatible: OpenAI-compatible proxy with Bearer token authentication

    Attributes:
        mode: API mode - 'anthropic' for direct API, 'openai-compatible' for proxies
        api_key: API key for authentication
        base_url: API base URL (should not include /v1 suffix)
        model: Model identifier
        timeout: Request timeout in seconds (1-600)
    """
    mode: Literal["anthropic", "openai-compatible"] = Field(
        default="anthropic",
        description="API mode: 'anthropic' for direct API, 'openai-compatible' for proxies"
    )
    api_key: str = Field(..., description="API key for authentication")
    base_url: str = Field(..., description="API base URL (no /v1 suffix)")
    model: str = Field(default="claude-opus-4-5-20251101", description="Model identifier")
    timeout: float = Field(default=300.0, ge=1.0, le=600.0, description="Request timeout in seconds")
```

Keep existing field_validators unchanged.
  </action>
  <verify>
```bash
cd /Users/ryand/Code/AATF/ai-news-aggregator
python3 -c "
from agents.config.schema import LLMProviderConfig

# Test default mode
cfg = LLMProviderConfig(api_key='test', base_url='https://api.anthropic.com')
assert cfg.mode == 'anthropic', f'Expected anthropic, got {cfg.mode}'
print('Default mode: OK')

# Test explicit modes
cfg_anthropic = LLMProviderConfig(mode='anthropic', api_key='test', base_url='https://api.anthropic.com')
cfg_openai = LLMProviderConfig(mode='openai-compatible', api_key='test', base_url='https://proxy.example.com')
print('Explicit modes: OK')

# Test invalid mode rejected
try:
    LLMProviderConfig(mode='invalid', api_key='test', base_url='https://api.anthropic.com')
    print('ERROR: Should reject invalid mode')
except Exception:
    print('Invalid mode rejected: OK')
"
```
  </verify>
  <done>LLMProviderConfig has mode field with anthropic/openai-compatible options</done>
</task>

<task type="auto">
  <name>Task 2: Add mode field to ImageProviderConfig with endpoint validation</name>
  <files>agents/config/schema.py</files>
  <action>
Update ImageProviderConfig to add mode field and conditional endpoint validation.

```python
class ImageProviderConfig(BaseModel):
    """Configuration for image generation provider (optional).

    Supports two modes:
    - native: Uses google-genai SDK directly (recommended for Google API keys)
    - openai-compatible: Uses REST chat/completions format (for LiteLLM proxies)

    Attributes:
        mode: API mode - 'native' for google-genai SDK, 'openai-compatible' for REST
        api_key: API key for image generation service
        endpoint: API endpoint URL (required for openai-compatible mode only)
        model: Model name for image generation
    """
    mode: Literal["native", "openai-compatible"] = Field(
        default="native",
        description="API mode: 'native' for google-genai SDK, 'openai-compatible' for REST"
    )
    api_key: str = Field(..., description="API key for image generation")
    endpoint: Optional[str] = Field(
        default=None,
        description="API endpoint URL (required for openai-compatible mode)"
    )
    model: str = Field(
        default="gemini-3-pro-image-preview",
        description="Model name for image generation"
    )

    @field_validator('api_key')
    @classmethod
    def validate_api_key(cls, v: str) -> str:
        """Validate image API key is configured and resolved."""
        if not v or v == "your-image-api-key":
            raise ValueError(
                "Image API key not configured. "
                "Set a valid key or remove image section from config."
            )
        if v.startswith('${') and v.endswith('}'):
            raise ValueError(
                f"Environment variable {v} was not resolved. "
                "Check that the variable is set."
            )
        return v

    @model_validator(mode='after')
    def validate_endpoint_for_mode(self) -> 'ImageProviderConfig':
        """Validate endpoint is provided for openai-compatible mode."""
        if self.mode == "openai-compatible" and not self.endpoint:
            raise ValueError(
                "endpoint is required when mode is 'openai-compatible'. "
                "Provide your proxy's image generation endpoint URL."
            )
        return self
```

Note: Default model is now `gemini-3-pro-image-preview` (native Google API model name). Users with openai-compatible proxies will override this with their proxy's expected model name (e.g., `gemini-3-pro-image`).
  </action>
  <verify>
```bash
cd /Users/ryand/Code/AATF/ai-news-aggregator
python3 -c "
from agents.config.schema import ImageProviderConfig

# Test native mode (no endpoint needed)
cfg = ImageProviderConfig(api_key='test-key')
assert cfg.mode == 'native', f'Expected native, got {cfg.mode}'
assert cfg.model == 'gemini-3-pro-image-preview', f'Wrong model: {cfg.model}'
print('Native mode defaults: OK')

# Test openai-compatible mode requires endpoint
try:
    ImageProviderConfig(mode='openai-compatible', api_key='test-key')
    print('ERROR: Should require endpoint for openai-compatible mode')
except Exception as e:
    assert 'endpoint is required' in str(e), f'Wrong error: {e}'
    print('Endpoint validation: OK')

# Test openai-compatible with endpoint works
cfg = ImageProviderConfig(
    mode='openai-compatible',
    api_key='test-key',
    endpoint='https://proxy.example.com/v1/chat/completions',
    model='gemini-3-pro-image'
)
assert cfg.endpoint is not None
print('OpenAI-compatible mode: OK')
"
```
  </verify>
  <done>ImageProviderConfig has mode field with native/openai-compatible options and endpoint validation</done>
</task>

<task type="auto">
  <name>Task 3: Update providers.yaml.example with dual mode documentation</name>
  <files>config/providers.yaml.example</files>
  <action>
Rewrite `config/providers.yaml.example` to clearly document both modes for LLM and Image providers.

```yaml
# Provider Configuration for AI News Aggregator
# Copy this file to providers.yaml and fill in your values
#
# Environment variable interpolation:
#   You can reference env vars with ${VAR} syntax:
#     api_key: "${ANTHROPIC_API_KEY}"
#   Direct values are preferred for clarity.

# =============================================================================
# LLM Provider (required)
# =============================================================================
llm:
  # Mode: "anthropic" or "openai-compatible"
  #
  # anthropic (default):
  #   - Direct Anthropic API access
  #   - Uses x-api-key header authentication
  #   - Full extended thinking support (QUICK/STANDARD/DEEP/ULTRATHINK)
  #
  # openai-compatible:
  #   - For LiteLLM, AWS Bedrock proxies, or other OpenAI-compatible endpoints
  #   - Uses Bearer token authentication
  #   - Extended thinking will log warnings (proxy may not support it)
  #
  mode: "anthropic"

  # API key for authentication
  api_key: "your-api-key-here"

  # API base URL (no /v1 suffix)
  # - Direct Anthropic: https://api.anthropic.com
  # - OpenAI-compatible proxy: your proxy URL
  base_url: "https://api.anthropic.com"

  # Model identifier
  # - Direct Anthropic: claude-opus-4-5-20251101, claude-sonnet-4-20250514, etc.
  # - Proxy: whatever model name your proxy expects
  model: "claude-opus-4-5-20251101"

  # Request timeout in seconds (1-600)
  timeout: 300

# =============================================================================
# Image Provider (optional - hero image generation)
# =============================================================================
# Uncomment and configure to enable hero images.
# Remove or leave commented to skip hero image generation.
#
# image:
#   # Mode: "native" or "openai-compatible"
#   #
#   # native (default):
#   #   - Uses google-genai SDK directly
#   #   - Requires Google AI API key
#   #   - Model: gemini-3-pro-image-preview
#   #   - No endpoint needed (SDK handles it)
#   #
#   # openai-compatible:
#   #   - Uses REST chat/completions format
#   #   - For LiteLLM or other proxies that wrap Gemini
#   #   - Requires endpoint URL
#   #   - Model name depends on your proxy (e.g., gemini-3-pro-image)
#   #
#   mode: "native"
#
#   # API key (Google AI API key for native mode)
#   api_key: "your-google-api-key"
#
#   # Endpoint (required for openai-compatible mode only)
#   # endpoint: "https://your-proxy.example.com/v1/chat/completions"
#
#   # Model name
#   # - native: gemini-3-pro-image-preview
#   # - openai-compatible: depends on your proxy (e.g., gemini-3-pro-image)
#   model: "gemini-3-pro-image-preview"

# =============================================================================
# Example: OpenAI-compatible proxy configuration
# =============================================================================
# If you're using a LiteLLM proxy or similar, your config might look like:
#
# llm:
#   mode: "openai-compatible"
#   api_key: "${PROXY_API_KEY}"
#   base_url: "https://your-litellm-proxy.example.com"
#   model: "claude-opus-4-5"  # Your proxy's model alias
#   timeout: 300
#
# image:
#   mode: "openai-compatible"
#   api_key: "${PROXY_API_KEY}"
#   endpoint: "https://your-litellm-proxy.example.com/v1/chat/completions"
#   model: "gemini-3-pro-image"  # Your proxy's model alias
```
  </action>
  <verify>
```bash
cd /Users/ryand/Code/AATF/ai-news-aggregator
# Verify example file has mode documentation
grep -q 'mode: "anthropic"' config/providers.yaml.example && echo "LLM mode documented: OK"
grep -q 'mode: "native"' config/providers.yaml.example && echo "Image mode documented: OK"
grep -q 'openai-compatible' config/providers.yaml.example && echo "OpenAI-compatible documented: OK"
grep -q 'gemini-3-pro-image-preview' config/providers.yaml.example && echo "Native model name: OK"
```
  </verify>
  <done>providers.yaml.example documents both modes with clear examples</done>
</task>

</tasks>

<verification>
```bash
cd /Users/ryand/Code/AATF/ai-news-aggregator

# 1. Verify schema imports
python3 -c "from agents.config.schema import LLMProviderConfig, ImageProviderConfig; print('Schema imports: OK')"

# 2. Verify LLM mode field
python3 -c "
from agents.config.schema import LLMProviderConfig
import inspect
sig = inspect.signature(LLMProviderConfig)
assert 'mode' in sig.parameters, 'mode field missing'
print('LLM mode field: OK')
"

# 3. Verify Image mode field
python3 -c "
from agents.config.schema import ImageProviderConfig
import inspect
sig = inspect.signature(ImageProviderConfig)
assert 'mode' in sig.parameters, 'mode field missing'
print('Image mode field: OK')
"

# 4. Verify endpoint validation for openai-compatible mode
python3 -c "
from agents.config.schema import ImageProviderConfig
from pydantic import ValidationError

# Should fail without endpoint
try:
    ImageProviderConfig(mode='openai-compatible', api_key='test')
    print('ERROR: Should require endpoint')
except ValidationError as e:
    print('Endpoint validation: OK')

# Should pass with endpoint
cfg = ImageProviderConfig(mode='openai-compatible', api_key='test', endpoint='https://example.com')
print('Endpoint with mode: OK')
"

# 5. Verify native mode model default
python3 -c "
from agents.config.schema import ImageProviderConfig
cfg = ImageProviderConfig(api_key='test')
assert cfg.model == 'gemini-3-pro-image-preview', f'Wrong default: {cfg.model}'
print('Native model default: OK')
"

# 6. Verify example file structure
python3 -c "
import yaml
with open('config/providers.yaml.example') as f:
    # Just check it parses (comments are stripped)
    content = f.read()
    assert 'mode:' in content, 'mode not in example'
    assert 'anthropic' in content, 'anthropic not in example'
    assert 'native' in content, 'native not in example'
    assert 'openai-compatible' in content, 'openai-compatible not in example'
print('Example file structure: OK')
"

echo "All verifications passed"
```
</verification>

<success_criteria>
- LLMProviderConfig has mode field with Literal["anthropic", "openai-compatible"]
- ImageProviderConfig has mode field with Literal["native", "openai-compatible"]
- ImageProviderConfig validates endpoint is required for openai-compatible mode
- ImageProviderConfig default model is gemini-3-pro-image-preview (native Google API)
- providers.yaml.example documents both modes with clear examples
- All verification commands pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-configuration-infrastructure/01-04-SUMMARY.md`
</output>
