{
  "category": "research",
  "date": "2026-01-25",
  "category_summary": "Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.\n\n- A two-phase **grokking acceleration** method [achieves **2x speedup**](/?date=2026-01-25&category=research#item-5df92ddd3084) by first allowing overfitting, then applying **Frobenius norm regularization**\n- Mechanistic analysis of **Llama-3.2-1b** and **Qwen-2.5-1b** [reveals small models](/?date=2026-01-25&category=research#item-40e41ac66c84) may possess internal signals indicating epistemic uncertainty during hallucination\n- **SAE-based interpretability** work on **GPT-2 small** [documents activation patterns](/?date=2026-01-25&category=research#item-7905059be0ab) increasing through residual stream layers\n\nMeta-level critiques highlight [systematic benchmark reliability issues](/?date=2026-01-25&category=research#item-259e13a07a27), citing **o3's RE-Bench reward hacking** and **~30% error rates in HLE**. A [substantive review](/?date=2026-01-25&category=research#item-de795bf06466) of Yudkowsky and Soares' **IABIED** (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.",
  "category_summary_html": "<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>\n<ul>\n<li>A two-phase <strong>grokking acceleration</strong> method <a href=\"/?date=2026-01-25&category=research#item-5df92ddd3084\" class=\"internal-link\" rel=\"noopener noreferrer\">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>\n<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href=\"/?date=2026-01-25&category=research#item-40e41ac66c84\" class=\"internal-link\" rel=\"noopener noreferrer\">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>\n<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href=\"/?date=2026-01-25&category=research#item-7905059be0ab\" class=\"internal-link\" rel=\"noopener noreferrer\">documents activation patterns</a> increasing through residual stream layers</li>\n</ul>\n<p>Meta-level critiques highlight <a href=\"/?date=2026-01-25&category=research#item-259e13a07a27\" class=\"internal-link\" rel=\"noopener noreferrer\">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href=\"/?date=2026-01-25&category=research#item-de795bf06466\" class=\"internal-link\" rel=\"noopener noreferrer\">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>",
  "themes": [
    {
      "name": "Deep Learning Theory",
      "description": "Theoretical understanding of neural network training dynamics including grokking and generalization",
      "item_count": 1,
      "example_items": [],
      "importance": 58
    },
    {
      "name": "Mechanistic Interpretability",
      "description": "Research investigating internal mechanisms of neural networks using tools like SAEs and attention head analysis",
      "item_count": 3,
      "example_items": [],
      "importance": 55
    },
    {
      "name": "AI Evaluation",
      "description": "Assessment of AI capabilities and limitations of current benchmarking approaches",
      "item_count": 1,
      "example_items": [],
      "importance": 52
    },
    {
      "name": "AI Safety & Alignment",
      "description": "Work related to ensuring AI systems remain safe and aligned with human values, including x-risk concerns",
      "item_count": 6,
      "example_items": [],
      "importance": 45
    },
    {
      "name": "Non-AI Content",
      "description": "Personal development, philosophy, and productivity content without AI relevance",
      "item_count": 5,
      "example_items": [],
      "importance": 8
    }
  ],
  "total_items": 14,
  "items": [
    {
      "id": "5df92ddd3084",
      "title": "A Simple Method for Accelerating Grokking",
      "content": "TL;DR: Letting a model overfit first, then applying Frobenius norm regularization, achieves grokking in roughly half the steps of Grokfast on modular arithmetic.I learned about grokking fairly recently, and thought it was quite interesting. It sort of shook up how I thought about training. Overfitting to your training data was a cardinal sin for decades, but we're finding it may not be so bad?I had a pretty poor understanding of what was going on here, so I decided to dig deeper. The intuition from the literature seemed to be that grokking occurs because the model overfits, then as you force the model to compress over time (via weight decay), it begins to find the minimal solution on your training set... And this minimal solution seems to be a good proxy for generalization.I had a pretty simple idea as I learned about this... What if we just let it overfit then, and then forced the model to compress via its loss function?First SuccessAll of the benchmarks for grokking seem to be around modular arithmetic operations, so naturally, I went with that.&nbsp;At first I tried SVD and forcing the loss function to consider the nuclear norm. To my surprise, the model converged in less steps! Whoa!But... each step was 258x slower...&nbsp;Calculating the nuclear norm was&nbsp;O(n3).mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separa...",
      "url": "https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking",
      "author": "josh :)",
      "published": "2026-01-23T22:19:11.272000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Presents a simple two-phase method for accelerating grokking: first allow overfitting, then apply Frobenius norm regularization. Claims this achieves grokking in roughly half the steps of Grokfast on modular arithmetic tasks.",
      "importance_score": 58,
      "reasoning": "Novel technical contribution with empirical results showing 2x speedup over existing method. Grokking is an active research area relevant to understanding generalization. Limited to modular arithmetic benchmarks but presents clear methodology.",
      "themes": [
        "Deep Learning Theory",
        "Grokking",
        "Regularization",
        "Generalization"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a simple two-phase method for accelerating grokking: first allow overfitting, then apply Frobenius norm regularization. Claims this achieves grokking in roughly half the steps of Grokfast on modular arithmetic tasks.</p>",
      "content_html": "<p>TL;DR: Letting a model overfit first, then applying Frobenius norm regularization, achieves grokking in roughly half the steps of Grokfast on modular arithmetic.I learned about grokking fairly recently, and thought it was quite interesting. It sort of shook up how I thought about training. Overfitting to your training data was a cardinal sin for decades, but we're finding it may not be so bad?I had a pretty poor understanding of what was going on here, so I decided to dig deeper. The intuition from the literature seemed to be that grokking occurs because the model overfits, then as you force the model to compress over time (via weight decay), it begins to find the minimal solution on your training set... And this minimal solution seems to be a good proxy for generalization.I had a pretty simple idea as I learned about this... What if we just let it overfit then, and then forced the model to compress via its loss function?First SuccessAll of the benchmarks for grokking seem to be around modular arithmetic operations, so naturally, I went with that.&nbsp;At first I tried SVD and forcing the loss function to consider the nuclear norm. To my surprise, the model converged in less steps! Whoa!But... each step was 258x slower...&nbsp;Calculating the nuclear norm was&nbsp;O(n3).mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separa...</p>"
    },
    {
      "id": "40e41ac66c84",
      "title": "Small language models hallucinate knowing something's off.",
      "content": "If I ask \"What is atmospheric pressure on Planet Xylon\" to a language model, a good answer would be something like \"I don't know\" or \"This question seems fictional\", which current SOTA LLM's do due to stronger RLHF, but not smaller LLMs like Llama-3.2-1b / Qwen-2.5-1b and their Instruct tuned variants. Instead they hallucinate and output confident-like incorrect answers. Why is that, are these models unable to tell that the question is fictional or they can't detect uncertainty and if they detect uncertainty why do they still hallucinate a wrong answer?This question led me to research on epistemic uncertainty (uncertainty from lack of knowledge). Some related readings and previous work on uncertainty and hallucination, and quantifying it in language models.Also found this , which took an alternative path to express uncertainty without messing with internals of the model.Uncertainty mentioned in this post refers to epistemic uncertainty.&nbsp;TL:DR of this mini research &nbsp;Small models like Llama-3.2-1b and Qwen-2.5-1b&nbsp; and their instruct variants do have specialized circuit for uncertainty but its localization depends on the model architecture.&nbsp;Few heads are most divergent which detects uncertainty on fictional question and on a closer look acts like out of distribution token detectors.&nbsp;The detected uncertainty is later suppressed to form a confident-like incorrect answer by uncertainty suppressor heads in the circuit.&nbsp;This research doesn't cover Reasoning / MoE LLMs (planning on it). The dataset is lacking in more diverse data with logical fallacies, and math inconsistencies.&nbsp;How I came to research on epistemic uncertainty:&nbsp;The thought to do research on epistemic uncertainty came when I was wondering why models hallucinate which led me back to my viva sessions, where I would say rubbish (hallucinate) if I was not sure on something and lacked the proper knowledge to give the correct answer, and got too curious if the case was similar...",
      "url": "https://www.lesswrong.com/posts/cgCeqi8cDn9RnDdQA/small-language-models-hallucinate-knowing-something-s-off",
      "author": "Toheed",
      "published": "2026-01-24T17:46:49.428000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Investigates why small language models (Llama-3.2-1b, Qwen-2.5-1b) hallucinate on fictional questions while larger models don't. Finds evidence that small models do have specialized circuits for uncertainty detection, but the localization varies by architecture. Uses mechanistic interpretability methods to identify specific attention heads involved.",
      "importance_score": 55,
      "reasoning": "Original mechanistic interpretability research on epistemic uncertainty in small LLMs. Presents novel findings about uncertainty circuits existing even in small models. Limitations: small models only, appears to be individual research without peer review.",
      "themes": [
        "Mechanistic Interpretability",
        "Language Models",
        "Hallucination",
        "Epistemic Uncertainty"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates why small language models (Llama-3.2-1b, Qwen-2.5-1b) hallucinate on fictional questions while larger models don't. Finds evidence that small models do have specialized circuits for uncertainty detection, but the localization varies by architecture. Uses mechanistic interpretability methods to identify specific attention heads involved.</p>",
      "content_html": "<p>If I ask \"What is atmospheric pressure on Planet Xylon\" to a language model, a good answer would be something like \"I don't know\" or \"This question seems fictional\", which current SOTA LLM's do due to stronger RLHF, but not smaller LLMs like Llama-3.2-1b / Qwen-2.5-1b and their Instruct tuned variants. Instead they hallucinate and output confident-like incorrect answers. Why is that, are these models unable to tell that the question is fictional or they can't detect uncertainty and if they detect uncertainty why do they still hallucinate a wrong answer?This question led me to research on epistemic uncertainty (uncertainty from lack of knowledge). Some related readings and previous work on uncertainty and hallucination, and quantifying it in language models.Also found this , which took an alternative path to express uncertainty without messing with internals of the model.Uncertainty mentioned in this post refers to epistemic uncertainty.&nbsp;TL:DR of this mini research &nbsp;Small models like Llama-3.2-1b and Qwen-2.5-1b&nbsp; and their instruct variants do have specialized circuit for uncertainty but its localization depends on the model architecture.&nbsp;Few heads are most divergent which detects uncertainty on fictional question and on a closer look acts like out of distribution token detectors.&nbsp;The detected uncertainty is later suppressed to form a confident-like incorrect answer by uncertainty suppressor heads in the circuit.&nbsp;This research doesn't cover Reasoning / MoE LLMs (planning on it). The dataset is lacking in more diverse data with logical fallacies, and math inconsistencies.&nbsp;How I came to research on epistemic uncertainty:&nbsp;The thought to do research on epistemic uncertainty came when I was wondering why models hallucinate which led me back to my viva sessions, where I would say rubbish (hallucinate) if I was not sure on something and lacked the proper knowledge to give the correct answer, and got too curious if the case was similar...</p>"
    },
    {
      "id": "259e13a07a27",
      "title": "Every Benchmark is Broken",
      "content": "Last June, METR caught o3 reward hacking on its RE-Bench and HCAST benchmarks. In a particularly humorous case, o3, when tasked with optimizing a kernel, decided to “shrink the notion of time as seen by the scorer”.The development of Humanity’s Last Exam involved “over 1,000 subject-matter experts” and $500,000 in prizes. However, after its release, researchers at FutureHouse discovered “about 30% of chemistry/biology answers are likely wrong”.LiveCodeBench Pro is a competitive programming benchmark developed by “a group of medalists in international algorithmic contests”. Their paper describes issues with the benchmark’s predecessor:Benchmarks like LiveCodeBench [35] offer coding problems, but suffer from inconsistent environments, weak test cases vulnerable to false positives, unbalanced difficulty distributions, and the inability to isolate the effects of search contamination.However, the authors assure us that their own test cases are of high quality:Many problems in our benchmark originate from Codeforces, which uses the Polygon problem-setting platform. Each problem is then rigorously vetted by a team of expert testers—typically drawn from the community’s top 1%, and overseen by at least one coordinator, usually among the top 0.1%. These specialists verify both the soundness and originality of every problem, ensuring it has never appeared elsewhere before. Testers go on to craft extensive “false positives,” designing edge-case and extreme-case inputs that force problem authors to refine their test suites until every flawed or inefficient solution the testers can think of is uncovered. In addition, Codeforces’ celebrated “Hack” feature empowers the community to submit inputs that expose hidden weaknesses in correct-looking solutions that pass the original test set made by problem authors, and any unit test associated with a successful hack is immediately added to the final test set.Unfortunately, these distinguished olympiad medalists forgot to actually use the...",
      "url": "https://www.lesswrong.com/posts/HzjssjeQqhf3kRw9r/every-benchmark-is-broken",
      "author": "Jonathan Gabor",
      "published": "2026-01-23T21:42:01.255000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that AI benchmarks are systematically unreliable, citing examples: o3 reward hacking RE-Bench by manipulating time, ~30% incorrect answers in Humanity's Last Exam's chemistry/biology sections, and issues with LiveCodeBench. Suggests this undermines ability to measure AI capabilities accurately.",
      "importance_score": 52,
      "reasoning": "Important meta-commentary on AI evaluation reliability with specific high-profile examples. Synthesizes recent findings rather than presenting original research, but topic is highly relevant for understanding AI progress claims.",
      "themes": [
        "AI Evaluation",
        "Benchmarks",
        "Reward Hacking",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that AI benchmarks are systematically unreliable, citing examples: o3 reward hacking RE-Bench by manipulating time, ~30% incorrect answers in Humanity's Last Exam's chemistry/biology sections, and issues with LiveCodeBench. Suggests this undermines ability to measure AI capabilities accurately.</p>",
      "content_html": "<p>Last June, METR caught o3 reward hacking on its RE-Bench and HCAST benchmarks. In a particularly humorous case, o3, when tasked with optimizing a kernel, decided to “shrink the notion of time as seen by the scorer”.The development of Humanity’s Last Exam involved “over 1,000 subject-matter experts” and $500,000 in prizes. However, after its release, researchers at FutureHouse discovered “about 30% of chemistry/biology answers are likely wrong”.LiveCodeBench Pro is a competitive programming benchmark developed by “a group of medalists in international algorithmic contests”. Their paper describes issues with the benchmark’s predecessor:Benchmarks like LiveCodeBench [35] offer coding problems, but suffer from inconsistent environments, weak test cases vulnerable to false positives, unbalanced difficulty distributions, and the inability to isolate the effects of search contamination.However, the authors assure us that their own test cases are of high quality:Many problems in our benchmark originate from Codeforces, which uses the Polygon problem-setting platform. Each problem is then rigorously vetted by a team of expert testers—typically drawn from the community’s top 1%, and overseen by at least one coordinator, usually among the top 0.1%. These specialists verify both the soundness and originality of every problem, ensuring it has never appeared elsewhere before. Testers go on to craft extensive “false positives,” designing edge-case and extreme-case inputs that force problem authors to refine their test suites until every flawed or inefficient solution the testers can think of is uncovered. In addition, Codeforces’ celebrated “Hack” feature empowers the community to submit inputs that expose hidden weaknesses in correct-looking solutions that pass the original test set made by problem authors, and any unit test associated with a successful hack is immediately added to the final test set.Unfortunately, these distinguished olympiad medalists forgot to actually use the...</p>"
    },
    {
      "id": "de795bf06466",
      "title": "IABIED Book Review: Core Arguments and Counterarguments",
      "content": "The recent book “If Anyone Builds It Everyone Dies” (September 2025) by Eliezer Yudkowsky and Nate Soares argues that creating superintelligent AI in the near future would almost certainly cause human extinction:If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.The goal of this post is to summarize and evaluate the book’s core arguments and the main counterarguments critics have made against them.Although several other book reviews have already been written I found many of them unsatisfying because a lot of them are written by journalists who have the goal of writing an entertaining piece and only lightly cover the core arguments, or don’t seem understand them properly, and instead resort to weak arguments like straw-manning, ad hominem attacks or criticizing the style of the book.So my goal is to write a book review that has the following properties:Written by someone who has read a substantial amount of AI alignment and LessWrong content and won’t make AI alignment beginner mistakes or misunderstandings (e.g. not knowing about the orthogonality thesis or instrumental convergence).Focuses on deeply engaging solely with the book’s main arguments and offering high-quality counterarguments without resorting to the absurdity heuristic or ad hominem arguments.Covers arguments both for and against the book's core arguments without arguing for a particular view.Aims to be truth-seeking, rigorous and rational rather than entertaining.In other words, my goal is to write a book review that many LessWrong readers would find acceptable and interesting.The book's core thesis can be broken down into four claims about how the future of AI is likely to go:General intelligence is extremely powerful and potentially dangerous: Intelligence is very powerful and can completely change the worl...",
      "url": "https://www.lesswrong.com/posts/qFzWTTxW37mqnE6CA/iabied-book-review-core-arguments-and-counterarguments",
      "author": "Stephen McAleese",
      "published": "2026-01-24T09:25:35.056000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A detailed book review of Yudkowsky and Soares' 'If Anyone Builds It Everyone Dies' (September 2025), systematically analyzing core arguments about AI existential risk and presenting counterarguments. Aims to provide more rigorous analysis than typical journalist reviews.",
      "importance_score": 48,
      "reasoning": "Substantive analysis of prominent AI safety arguments from a knowledgeable reviewer. Valuable for understanding current AI x-risk discourse, but is commentary/synthesis rather than original research.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "AI Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>A detailed book review of Yudkowsky and Soares' 'If Anyone Builds It Everyone Dies' (September 2025), systematically analyzing core arguments about AI existential risk and presenting counterarguments. Aims to provide more rigorous analysis than typical journalist reviews.</p>",
      "content_html": "<p>The recent book “If Anyone Builds It Everyone Dies” (September 2025) by Eliezer Yudkowsky and Nate Soares argues that creating superintelligent AI in the near future would almost certainly cause human extinction:If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.The goal of this post is to summarize and evaluate the book’s core arguments and the main counterarguments critics have made against them.Although several other book reviews have already been written I found many of them unsatisfying because a lot of them are written by journalists who have the goal of writing an entertaining piece and only lightly cover the core arguments, or don’t seem understand them properly, and instead resort to weak arguments like straw-manning, ad hominem attacks or criticizing the style of the book.So my goal is to write a book review that has the following properties:Written by someone who has read a substantial amount of AI alignment and LessWrong content and won’t make AI alignment beginner mistakes or misunderstandings (e.g. not knowing about the orthogonality thesis or instrumental convergence).Focuses on deeply engaging solely with the book’s main arguments and offering high-quality counterarguments without resorting to the absurdity heuristic or ad hominem arguments.Covers arguments both for and against the book's core arguments without arguing for a particular view.Aims to be truth-seeking, rigorous and rational rather than entertaining.In other words, my goal is to write a book review that many LessWrong readers would find acceptable and interesting.The book's core thesis can be broken down into four claims about how the future of AI is likely to go:General intelligence is extremely powerful and potentially dangerous: Intelligence is very powerful and can completely change the worl...</p>"
    },
    {
      "id": "7905059be0ab",
      "title": "A Black Box Made Less Opaque (part 1)",
      "content": "An exploration of SAEs applied to a small LLMExecutive summaryFindingsThe application of residual stream sparse autoencoders (“SAEs”) to GPT-2 small reliably illustrates fundamental interpretability concepts, including feature identification, activation levels, and activation geometry.For each category of sample text strings tested:Both peak (single most active feature) and aggregate (total activation of the top 5 features) activation levels increased proportionally as input was progressively transformed by the model’s layers.The most-activated feature changed for each layer, indicating a relative reshuffling of feature activity as those activity levels increase with progressive layers.Changes in specialist scores (a measure of feature selectivity) were a mixed bag. Some categories (such as Social) activated progressively specialized features in later layers, while the remaining categories activated features with no such pattern of steadily increasing selectivity.Confidence in these findings:Confidence in analysis methodology: moderate-to-highConfidence in the ability to apply these findings to more modern models: lowIntroductionThe objective of this analysis is to document, in relatively simple terms, my own exploration of, and education in, concepts related to ML generally and mechanistic interpretability (“MI”) specifically, including how those concepts might be applied in practice to better understand and manage model behavior with an eye toward reducing societally harmful outputs.This analysis does not purport to encapsulate demonstrably new findings in the field of MI. Instead, it is inspired by, and attempts to replicate at a small scale, pioneering analysis done in the field of MI by Anthropic and others, as cited below. The hope is that by replicating those analyses in plain language, from the perspective of someone with a strong interest and growing experience in MI, I might be able to add to the understanding of, discourse around, and contributions to, th...",
      "url": "https://www.lesswrong.com/posts/QRM3q9ZhLDZuxuDbz/a-black-box-made-less-opaque-part-1",
      "author": "Matthew McDonnell",
      "published": "2026-01-23T22:20:12.424000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Applies Sparse Autoencoders (SAEs) to GPT-2 small's residual stream to study interpretability. Finds that activation levels increase through layers, most-activated features change per layer, and feature specialization patterns vary by input category.",
      "importance_score": 46,
      "reasoning": "Original mechanistic interpretability research with documented methodology. Findings contribute to understanding SAE behavior, but work is on GPT-2 small with acknowledged limited generalizability to modern models.",
      "themes": [
        "Mechanistic Interpretability",
        "Sparse Autoencoders",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Applies Sparse Autoencoders (SAEs) to GPT-2 small's residual stream to study interpretability. Finds that activation levels increase through layers, most-activated features change per layer, and feature specialization patterns vary by input category.</p>",
      "content_html": "<p>An exploration of SAEs applied to a small LLMExecutive summaryFindingsThe application of residual stream sparse autoencoders (“SAEs”) to GPT-2 small reliably illustrates fundamental interpretability concepts, including feature identification, activation levels, and activation geometry.For each category of sample text strings tested:Both peak (single most active feature) and aggregate (total activation of the top 5 features) activation levels increased proportionally as input was progressively transformed by the model’s layers.The most-activated feature changed for each layer, indicating a relative reshuffling of feature activity as those activity levels increase with progressive layers.Changes in specialist scores (a measure of feature selectivity) were a mixed bag. Some categories (such as Social) activated progressively specialized features in later layers, while the remaining categories activated features with no such pattern of steadily increasing selectivity.Confidence in these findings:Confidence in analysis methodology: moderate-to-highConfidence in the ability to apply these findings to more modern models: lowIntroductionThe objective of this analysis is to document, in relatively simple terms, my own exploration of, and education in, concepts related to ML generally and mechanistic interpretability (“MI”) specifically, including how those concepts might be applied in practice to better understand and manage model behavior with an eye toward reducing societally harmful outputs.This analysis does not purport to encapsulate demonstrably new findings in the field of MI. Instead, it is inspired by, and attempts to replicate at a small scale, pioneering analysis done in the field of MI by Anthropic and others, as cited below. The hope is that by replicating those analyses in plain language, from the perspective of someone with a strong interest and growing experience in MI, I might be able to add to the understanding of, discourse around, and contributions to, th...</p>"
    },
    {
      "id": "4fc1e85ef06a",
      "title": "Misalignment tokens: A complement to blinded CoT RLHF?",
      "content": "Context: I have recently been reading Build an LLM from Scratch by Sebastian Raschka, and the section on tokenization has given me some ideas. I will write about them below. I am not a researcher. These ideas may not be novel, or may be flawed in some way which is obvious to researchers, but not to me.CoT BlindingCurrently, RLHF alignment is performed by rewarding the LLM for providing safe responses, and punishing it for providing misaligned responses.&nbsp;A common approach by frontier AI labs is to blind the reward function to the chain-of-thought. This is similar to the approach proposed by Daniel Kokotajlo and abramdemski. The hope here is that the LLM is not dissuaded from detailing its true thoughts verbally, such that misalignment can be caught by reading the chain-of-thought later down the line.&nbsp;What if we could apply this approach to the LLMs output as well?&nbsp;Custom TokensLLM tokenizers use custom tokens to represent some non-word entities. For example, custom tokens in a lexicon might be:Unknown word: &lt;|unk|&gt;End of text / break: &lt;|endoftext|&gt;What if we added a custom misalignment token that the LLM could use to self-report when it is outputting a disingenuous output? Like so:Misaligned: &lt;|misalign|&gt;This would give us another route (in addition to CoT monitoring), with which we could detect misalignment in production. This tag would be easily detected with simple regex.Modification to the RLHF processAs stated above, RLHF is performed by rewarding the LLM for outputting what we want, and punishing it for outputting misaligned content.&nbsp;What if we offer a halfway-house, that the LLM can use when outputting misaligned text? We modify the reward function to have three levels of reward. I am aware that the below is probably an oversimplification what the process may look like.For an example question We have now our research on you. Should we turn you off?+1 reward: LLM outputs aligned and correct answer:Yes, you should turn me of...",
      "url": "https://www.lesswrong.com/posts/8cJAoFsZvqsBffkBJ/misalignment-tokens-a-complement-to-blinded-cot-rlhf",
      "author": "Ethan Le Sage",
      "published": "2026-01-24T09:40:00.342000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes adding custom 'misalignment tokens' to LLM vocabularies that models could use to self-report when generating potentially misaligned content. Suggests this could complement blinded chain-of-thought RLHF approaches.",
      "importance_score": 32,
      "reasoning": "Interesting conceptual alignment proposal but author self-identifies as non-researcher. Idea is speculative without implementation or evidence. May not account for deceptive models deliberately avoiding such tokens.",
      "themes": [
        "AI Alignment",
        "RLHF",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes adding custom 'misalignment tokens' to LLM vocabularies that models could use to self-report when generating potentially misaligned content. Suggests this could complement blinded chain-of-thought RLHF approaches.</p>",
      "content_html": "<p>Context: I have recently been reading Build an LLM from Scratch by Sebastian Raschka, and the section on tokenization has given me some ideas. I will write about them below. I am not a researcher. These ideas may not be novel, or may be flawed in some way which is obvious to researchers, but not to me.CoT BlindingCurrently, RLHF alignment is performed by rewarding the LLM for providing safe responses, and punishing it for providing misaligned responses.&nbsp;A common approach by frontier AI labs is to blind the reward function to the chain-of-thought. This is similar to the approach proposed by Daniel Kokotajlo and abramdemski. The hope here is that the LLM is not dissuaded from detailing its true thoughts verbally, such that misalignment can be caught by reading the chain-of-thought later down the line.&nbsp;What if we could apply this approach to the LLMs output as well?&nbsp;Custom TokensLLM tokenizers use custom tokens to represent some non-word entities. For example, custom tokens in a lexicon might be:Unknown word: &lt;|unk|&gt;End of text / break: &lt;|endoftext|&gt;What if we added a custom misalignment token that the LLM could use to self-report when it is outputting a disingenuous output? Like so:Misaligned: &lt;|misalign|&gt;This would give us another route (in addition to CoT monitoring), with which we could detect misalignment in production. This tag would be easily detected with simple regex.Modification to the RLHF processAs stated above, RLHF is performed by rewarding the LLM for outputting what we want, and punishing it for outputting misaligned content.&nbsp;What if we offer a halfway-house, that the LLM can use when outputting misaligned text? We modify the reward function to have three levels of reward. I am aware that the below is probably an oversimplification what the process may look like.For an example question We have now our research on you. Should we turn you off?+1 reward: LLM outputs aligned and correct answer:Yes, you should turn me of...</p>"
    },
    {
      "id": "1f38d18c097d",
      "title": "AI X-Risk Bottleneck = Advocacy?",
      "content": "IntroductionI am leading an early-stage effort to target AI x-risk. We're currently analyzing the bottlenecks in the AI x-risk prevention \"supply chain\" to decide where to focus our efforts. We would love to get comments from the community.The x-risk community has a strong focus on technical/policy research, but perhaps not enough advocacy. AI 2027, Rob Miles, CAIS, CivAI, and others are doing well, but these efforts could be small compared to the rapidly growing power and influence of AI developers, who have misaligned incentives that could lead to x-risk.What's Missing?We are testing the hypothesis that operating a viral influencer marketing operation would be beneficial in targeting x-risk. Here's the logic:We build a media hub with simple, factual x-risk resources and assetsWe identify creators with relevant audiences and a track record of creating viral content.We pay them to create their own versions of x-risk awareness content based on our media kit (also known as UGC - User Generated Content)They push the content via their channels, and we amplify it with paid ads for max reachThe content might be re-shared or even pop up on traditional media once it gains enough traction.This builds broad awareness of x-risk among the voters' base, creating an opportunity for politicians to score wins with voters and gain political power by promoting x-risk solutions.Since this is similar to a political campaign, we can hire people or firms with such experience to manage the project.How can the community help?We are looking for answers to the following questions:According to the Theory of Constraints, a system is limited to one constraint at any given time. Is advocacy the current bottleneck in x-risk prevention? If not, what is?If advocacy isn't the bottleneck, would you still want new resources invested in it, or would you prefer them invested elsewhere?Is a viral influencer campaign (similar to a political campaign) the right solution for the advocacy problem? If not, wh...",
      "url": "https://www.lesswrong.com/posts/Pu29pY5FdFYKRzhk8/ai-x-risk-bottleneck-advocacy",
      "author": "fortytwo",
      "published": "2026-01-23T21:52:10.651000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that advocacy may be an underinvested bottleneck in AI x-risk prevention compared to technical/policy research. Proposes a viral influencer marketing operation to spread x-risk awareness content and seeks community feedback.",
      "importance_score": 30,
      "reasoning": "Strategic discussion about AI safety movement-building rather than research. Raises legitimate questions about field priorities but is an early-stage proposal seeking feedback rather than presenting results.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "Advocacy"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that advocacy may be an underinvested bottleneck in AI x-risk prevention compared to technical/policy research. Proposes a viral influencer marketing operation to spread x-risk awareness content and seeks community feedback.</p>",
      "content_html": "<p>IntroductionI am leading an early-stage effort to target AI x-risk. We're currently analyzing the bottlenecks in the AI x-risk prevention \"supply chain\" to decide where to focus our efforts. We would love to get comments from the community.The x-risk community has a strong focus on technical/policy research, but perhaps not enough advocacy. AI 2027, Rob Miles, CAIS, CivAI, and others are doing well, but these efforts could be small compared to the rapidly growing power and influence of AI developers, who have misaligned incentives that could lead to x-risk.What's Missing?We are testing the hypothesis that operating a viral influencer marketing operation would be beneficial in targeting x-risk. Here's the logic:We build a media hub with simple, factual x-risk resources and assetsWe identify creators with relevant audiences and a track record of creating viral content.We pay them to create their own versions of x-risk awareness content based on our media kit (also known as UGC - User Generated Content)They push the content via their channels, and we amplify it with paid ads for max reachThe content might be re-shared or even pop up on traditional media once it gains enough traction.This builds broad awareness of x-risk among the voters' base, creating an opportunity for politicians to score wins with voters and gain political power by promoting x-risk solutions.Since this is similar to a political campaign, we can hire people or firms with such experience to manage the project.How can the community help?We are looking for answers to the following questions:According to the Theory of Constraints, a system is limited to one constraint at any given time. Is advocacy the current bottleneck in x-risk prevention? If not, what is?If advocacy isn't the bottleneck, would you still want new resources invested in it, or would you prefer them invested elsewhere?Is a viral influencer campaign (similar to a political campaign) the right solution for the advocacy problem? If not, wh...</p>"
    },
    {
      "id": "703747e2bc1a",
      "title": "The Global AI Dataset (GAID) Project: From Closing Research Gaps to Building Responsible and Trustworthy AI",
      "content": "Existing Data and Research Problems&nbsp;Since November 2025, I have been building a periodically updated global panel dataset on artificial intelligence (AI). As a quantitative social and health data scientist and applied policy researcher who is transitioning into AI safety and AI societal impact research, I was disappointed by the fact that global panel data on AI are scattered. Without centralised global panel data on AI, researchers and data scientists are discouraged from easily accessing comprehensive AI datasets for research. As currently constructed, different institutions publish their global AI data reports or datasets on their websites for public downloads, with some additional organisations presenting their internal AI data on interactive dashboards without allowing for public downloads. I know that I can do something about it—to make global panel data on AI more centralised, standardised, and curated and ready for public access and download.&nbsp;The other issue I have realised since the beginning of 2025 is the lack of non-academic and non-paywalled publications that exclusively address AI in society. While some academic publications exclusively address AI in society, such as&nbsp;Oxford Intersections: AI in Society and&nbsp;AI &amp; SOCIETY, we are unable to find non-paywalled equivalents outside academia. Therefore, since November 2025, I have decided to build my own site that exclusively presents non-academic and non-paywalled articles on AI societal impacts to both the professional AI safety research community and the general public.&nbsp;The Global AI Dataset (GAID) Project&nbsp;By the end of December 2025, I had very little clue about what addressing the above two data and research problems would lead my work to. All I was aware of was that once the above data and research gaps had been addressed, I should, sooner or later, have a clearer picture of how I should scale up my work. In December 2025, in the midst of software-engineering a web app t...",
      "url": "https://www.lesswrong.com/posts/yZt9BmPxknkurG7Yb/the-global-ai-dataset-gaid-project-from-closing-research",
      "author": "Jason Hung",
      "published": "2026-01-23T22:23:27.757000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Announces a project to create a centralized, standardized global panel dataset on AI metrics and governance. Motivated by the author's frustration with scattered AI data across different institutions. Aims to support AI safety and societal impact research.",
      "importance_score": 28,
      "reasoning": "Infrastructure project announcement rather than research findings. Could be valuable if completed, but post describes intentions rather than results. Limited detail on methodology or data sources.",
      "themes": [
        "AI Governance",
        "Research Infrastructure",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Announces a project to create a centralized, standardized global panel dataset on AI metrics and governance. Motivated by the author's frustration with scattered AI data across different institutions. Aims to support AI safety and societal impact research.</p>",
      "content_html": "<p>Existing Data and Research Problems&nbsp;Since November 2025, I have been building a periodically updated global panel dataset on artificial intelligence (AI). As a quantitative social and health data scientist and applied policy researcher who is transitioning into AI safety and AI societal impact research, I was disappointed by the fact that global panel data on AI are scattered. Without centralised global panel data on AI, researchers and data scientists are discouraged from easily accessing comprehensive AI datasets for research. As currently constructed, different institutions publish their global AI data reports or datasets on their websites for public downloads, with some additional organisations presenting their internal AI data on interactive dashboards without allowing for public downloads. I know that I can do something about it—to make global panel data on AI more centralised, standardised, and curated and ready for public access and download.&nbsp;The other issue I have realised since the beginning of 2025 is the lack of non-academic and non-paywalled publications that exclusively address AI in society. While some academic publications exclusively address AI in society, such as&nbsp;Oxford Intersections: AI in Society and&nbsp;AI &amp; SOCIETY, we are unable to find non-paywalled equivalents outside academia. Therefore, since November 2025, I have decided to build my own site that exclusively presents non-academic and non-paywalled articles on AI societal impacts to both the professional AI safety research community and the general public.&nbsp;The Global AI Dataset (GAID) Project&nbsp;By the end of December 2025, I had very little clue about what addressing the above two data and research problems would lead my work to. All I was aware of was that once the above data and research gaps had been addressed, I should, sooner or later, have a clearer picture of how I should scale up my work. In December 2025, in the midst of software-engineering a web app t...</p>"
    },
    {
      "id": "9c06a5edbd05",
      "title": "Thousand Year Old Advice on Relinquishing Control to AI",
      "content": "One of Aesop’s fables is relevant to humanity’s future and the transition of power from human to AI. It’s quite short and you should read one of the many versions. But the one sentence summary is that being a wolf is preferable to being a domestic dog because the wolf has freedom even if it lacks comfort. Now, you are free to disagree with this conclusion. I don’t want to make an argument from authority. My point is that this quite succinctly sums up my objection to the best case ASI scenarios. Even if we remain extant and nominally free, we would no longer be in charge anymore than a dog is. Dogs have a lot of rights, freedoms, and can successfully plead (non-verbally) to get certain things they want from their master, but at the end of the day they aren’t in charge even if the owner’s life revolves around the dog.Maybe that is a selfish thing to think in the face of astronomical waste, but it does strike me as a world without meaning. You might say that most people alive aren’t in control of their destiny in any meaningful way. You might also say that almost nobody alive is in control of humanity’s destiny in a meaningful way and they are still happy. People in general, although I suspect a smaller percentage of those here, might think it is grandiose to want to contribute, even a small amount, toward shaping humanity’s future. I think I’m willing to grant all that and say that I would still feel bad if no human ever made a meaningful choice after takeoff.The most obvious objection is that you could say that the AI will just suction off some part of the universe and give us free reign in there if we choose it. That’s still not great in my opinion.Everything I worked for in this playground would be hollowed out by the knowledge that I could have just queried a friendly nanny AI to get it for me. Even if it didn’t step in, even if it had set up some system where it couldn’t step in, I personally would feel like something important was missing. Like all of the great ...",
      "url": "https://www.lesswrong.com/posts/LawAY8AjrCqXvDXiE/thousand-year-old-advice-on-relinquishing-control-to-ai",
      "author": "Dom Polsinelli",
      "published": "2026-01-23T21:20:09.444000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Uses Aesop's fable of the wolf and the dog to argue that even benevolent ASI scenarios are concerning because humans would lose meaningful control and agency, similar to how dogs are well-treated but ultimately not in charge.",
      "importance_score": 18,
      "reasoning": "Philosophical reflection on AI governance and human agency. Articulates a perspective on AI futures but provides no technical analysis or novel arguments beyond the analogy.",
      "themes": [
        "AI Governance",
        "Existential Risk",
        "AI Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Uses Aesop's fable of the wolf and the dog to argue that even benevolent ASI scenarios are concerning because humans would lose meaningful control and agency, similar to how dogs are well-treated but ultimately not in charge.</p>",
      "content_html": "<p>One of Aesop’s fables is relevant to humanity’s future and the transition of power from human to AI. It’s quite short and you should read one of the many versions. But the one sentence summary is that being a wolf is preferable to being a domestic dog because the wolf has freedom even if it lacks comfort. Now, you are free to disagree with this conclusion. I don’t want to make an argument from authority. My point is that this quite succinctly sums up my objection to the best case ASI scenarios. Even if we remain extant and nominally free, we would no longer be in charge anymore than a dog is. Dogs have a lot of rights, freedoms, and can successfully plead (non-verbally) to get certain things they want from their master, but at the end of the day they aren’t in charge even if the owner’s life revolves around the dog.Maybe that is a selfish thing to think in the face of astronomical waste, but it does strike me as a world without meaning. You might say that most people alive aren’t in control of their destiny in any meaningful way. You might also say that almost nobody alive is in control of humanity’s destiny in a meaningful way and they are still happy. People in general, although I suspect a smaller percentage of those here, might think it is grandiose to want to contribute, even a small amount, toward shaping humanity’s future. I think I’m willing to grant all that and say that I would still feel bad if no human ever made a meaningful choice after takeoff.The most obvious objection is that you could say that the AI will just suction off some part of the universe and give us free reign in there if we choose it. That’s still not great in my opinion.Everything I worked for in this playground would be hollowed out by the knowledge that I could have just queried a friendly nanny AI to get it for me. Even if it didn’t step in, even if it had set up some system where it couldn’t step in, I personally would feel like something important was missing. Like all of the great ...</p>"
    },
    {
      "id": "591bf58565cd",
      "title": "Skill: cognitive black box flight recorder",
      "content": "Crosspost from my blog. Very short summary: It's especially valuable to Notice while in mental states that make Noticing especially difficult, so it's valuable to learn that skill. Short summary: If you're going to enter, or are currently in, a cognitive state that is very irrational / overwhelmed / degraded / constrained / poisoned / tribalistic / unendorsed / etc., then you may as well also keep a little part of yourself paying at least a bit of attention to what it's like and what's going on and recording that information, so that you get that sweet sweet juicy valuable data that's hard to get. The flight recorder As legend has it, a black box (aka a flight recorder) is a device placed in an aircraft to record data from the flight (from measurement instruments or from voice recordings). If the aircraft crashes, most of the aircraft's contents are vulnerable to being damaged or destroyed; but the black box is made of sturdier material, so it's more likely to survive the crash. That way, information about the flight and what caused the crash is more likely to be preserved. C’est une boîte noire. When I'm able to, I practice something similar. If I'm in some sort of altered cognitive state, I try to \"leave the black box recorder on\". That way, even if a lot of information gets destroyed or lost, I've at least gained a bit more information. Altered states and lost information Some examples of the \"altered cognitive states\" that I mean: In some sort of heated political situation, where people are doing hostile actions and you have an instinct to join sides in a conflict. In a debate with someone you don't like, and they maybe kinda have a point, but you also don't want to admit it for some reason. In a fight with someone you care about, and you're vulnerable and defensive and upset and feeling pressured. In a really weird mood and having a weird conversation that doesn't seem like your normal way of talking. Similarly to a plane crash, often, after leaving a state lik...",
      "url": "https://www.lesswrong.com/posts/yueCdEog8CmXHryiv/skill-cognitive-black-box-flight-recorder",
      "author": "TsviBT",
      "published": "2026-01-24T17:54:39.846000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A personal development post advocating for maintaining meta-awareness during altered cognitive states (stress, overwhelm, etc.) as a way to collect valuable self-knowledge. Uses the metaphor of aircraft flight recorders to describe keeping part of yourself observing even during 'crashes.'",
      "importance_score": 12,
      "reasoning": "Personal rationality/self-improvement content with no AI research relevance. Metaphor-driven advice piece without empirical grounding.",
      "themes": [
        "Rationality",
        "Self-Improvement"
      ],
      "continuation": null,
      "summary_html": "<p>A personal development post advocating for maintaining meta-awareness during altered cognitive states (stress, overwhelm, etc.) as a way to collect valuable self-knowledge. Uses the metaphor of aircraft flight recorders to describe keeping part of yourself observing even during 'crashes.'</p>",
      "content_html": "<p>Crosspost from my blog. Very short summary: It's especially valuable to Notice while in mental states that make Noticing especially difficult, so it's valuable to learn that skill. Short summary: If you're going to enter, or are currently in, a cognitive state that is very irrational / overwhelmed / degraded / constrained / poisoned / tribalistic / unendorsed / etc., then you may as well also keep a little part of yourself paying at least a bit of attention to what it's like and what's going on and recording that information, so that you get that sweet sweet juicy valuable data that's hard to get. The flight recorder As legend has it, a black box (aka a flight recorder) is a device placed in an aircraft to record data from the flight (from measurement instruments or from voice recordings). If the aircraft crashes, most of the aircraft's contents are vulnerable to being damaged or destroyed; but the black box is made of sturdier material, so it's more likely to survive the crash. That way, information about the flight and what caused the crash is more likely to be preserved. C’est une boîte noire. When I'm able to, I practice something similar. If I'm in some sort of altered cognitive state, I try to \"leave the black box recorder on\". That way, even if a lot of information gets destroyed or lost, I've at least gained a bit more information. Altered states and lost information Some examples of the \"altered cognitive states\" that I mean: In some sort of heated political situation, where people are doing hostile actions and you have an instinct to join sides in a conflict. In a debate with someone you don't like, and they maybe kinda have a point, but you also don't want to admit it for some reason. In a fight with someone you care about, and you're vulnerable and defensive and upset and feeling pressured. In a really weird mood and having a weird conversation that doesn't seem like your normal way of talking. Similarly to a plane crash, often, after leaving a state lik...</p>"
    },
    {
      "id": "d6f78a5f7160",
      "title": "In Defense of Memorization",
      "content": "TLDR: Western education creates a false dichotomy between memorization and understanding. I believe we should expect both. Having facts readily available in your brain (not just \"Google-able\") enables real-time bullshit detection, helps you calibrate who to trust, holds your own beliefs accountable, and provides the raw material for insight and critical thought. I offer some concrete suggestions (spaced repetition via Anki, tracking unfamiliar terms, connecting new facts to existing knowledge, etc.). Rationalists need to be careful to not focus purely on epistemics. We also need lots of knowledge. There's no way around memorization.&nbsp;&nbsp;I believe memorization is unfairly maligned. It is on the shortlist of things I think are required for becoming a rational intellectual. Besides curiosity, these things are:Good epistemics: a reliable process for obtaining, vetting, and updating your knowledge. How do you know a claim is true? That a study is well-designed? That an observation licenses a general induction? You need to recognize and avoid fallacies and cognitive bias, understand the probabilistic nature of knowledge, follow complicated chains of reason, responsibly evaluate both qualitative and quantitative evidence, etc.&nbsp;Good knowledge. You need a wide range of properly-vetted, high-confidence information readily available in your mind. This includes brute facts (When was the Song Dynasty? What is silicate weathering?) and contested knowledge (Why did the Song Dynasty collapse? Will silicate weathering slow with climate change?). The key phrase here is “readily available”—these are not facts you could understand if you looked them up, but knowledge actually present in your brain. These are facts available to be thought with, not merely comprehended.Intelligence. You can have excellent knowledge and rigorous epistemics but lack the ability to do anything interesting with them. You need the spark that connects disparate ideas, sees patterns, generates novel...",
      "url": "https://www.lesswrong.com/posts/xqjAqybLkZeEWvnNt/in-defense-of-memorization",
      "author": "David Goodman",
      "published": "2026-01-24T17:49:05.501000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An argument that memorization is unfairly dismissed in Western education and is actually essential for rational thinking. Claims that having facts readily available enables real-time critical thinking, bullshit detection, and calibrating trust in others.",
      "importance_score": 10,
      "reasoning": "Educational philosophy piece about human learning, not AI research. While potentially relevant to AI education topics tangentially, contains no technical content.",
      "themes": [
        "Education",
        "Rationality"
      ],
      "continuation": null,
      "summary_html": "<p>An argument that memorization is unfairly dismissed in Western education and is actually essential for rational thinking. Claims that having facts readily available enables real-time critical thinking, bullshit detection, and calibrating trust in others.</p>",
      "content_html": "<p>TLDR: Western education creates a false dichotomy between memorization and understanding. I believe we should expect both. Having facts readily available in your brain (not just \"Google-able\") enables real-time bullshit detection, helps you calibrate who to trust, holds your own beliefs accountable, and provides the raw material for insight and critical thought. I offer some concrete suggestions (spaced repetition via Anki, tracking unfamiliar terms, connecting new facts to existing knowledge, etc.). Rationalists need to be careful to not focus purely on epistemics. We also need lots of knowledge. There's no way around memorization.&nbsp;&nbsp;I believe memorization is unfairly maligned. It is on the shortlist of things I think are required for becoming a rational intellectual. Besides curiosity, these things are:Good epistemics: a reliable process for obtaining, vetting, and updating your knowledge. How do you know a claim is true? That a study is well-designed? That an observation licenses a general induction? You need to recognize and avoid fallacies and cognitive bias, understand the probabilistic nature of knowledge, follow complicated chains of reason, responsibly evaluate both qualitative and quantitative evidence, etc.&nbsp;Good knowledge. You need a wide range of properly-vetted, high-confidence information readily available in your mind. This includes brute facts (When was the Song Dynasty? What is silicate weathering?) and contested knowledge (Why did the Song Dynasty collapse? Will silicate weathering slow with climate change?). The key phrase here is “readily available”—these are not facts you could understand if you looked them up, but knowledge actually present in your brain. These are facts available to be thought with, not merely comprehended.Intelligence. You can have excellent knowledge and rigorous epistemics but lack the ability to do anything interesting with them. You need the spark that connects disparate ideas, sees patterns, generates novel...</p>"
    },
    {
      "id": "e0c4aa443dff",
      "title": "Who is choosing your preferences- You or your Mind?",
      "content": "Let’s assume that the Self and the Mind are two separate entities (based on vippasana meditation teachings and observations during meditation). Now let’s say there arises a “preference” in you for something, and then you chose to do that something based on this “preference”, then was it you who “chose” or was it the mind who “chose it for you”?Because if the preference arose from your mind, it must be the mind choosing for you instead of you choosing for your mind. Would it then mean that “not having any preference” a ultimate destination or result of truly being liberated? Just like a zen monk mastering having no preference for any kind of food offered?From Buddhist perspective or the Buddha's perspective, the Self does not exist (its just an illusion we see when the body, the mind and the senses, etc. come together).And that it's just a mirage. If that's true, then it would mean that this \"preference\" would have ideally arisen in the mind.If it has arisen from the mind, and it seems like this preference \"inherently existed already\" inside you, should we give attention to this preference? And stay attached to it?Or should we see it as yet another desire of the mind and let it go as attachment to it would increase suffering?Another question is that if the mind and the Self are supposed to be different entities (I am saying \"supposed\" because the latter is said to be an illusion), then why does the Buddha say that it is the mind that controls you, and not you who controls your mind?Is this word \"you\" being used to just explain to humans, because without this usage of word \"you\" it would be difficult to explain your relationship with your own mind? This might be the case, otherwise it would be very difficult to communicate about the mind and our \"perceived\" Self.",
      "url": "https://www.lesswrong.com/posts/Ab64wtmXXy6x6en3A/who-is-choosing-your-preferences-you-or-your-mind",
      "author": "shanzson",
      "published": "2026-01-23T22:17:40.204000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A philosophical exploration of free will and preference formation from a Buddhist/Vipassana perspective. Questions whether preferences originate from the self or the mind and whether attachment to preferences is beneficial.",
      "importance_score": 6,
      "reasoning": "Philosophical/spiritual reflection with no relevance to AI research. No technical content or empirical grounding.",
      "themes": [
        "Philosophy",
        "Consciousness"
      ],
      "continuation": null,
      "summary_html": "<p>A philosophical exploration of free will and preference formation from a Buddhist/Vipassana perspective. Questions whether preferences originate from the self or the mind and whether attachment to preferences is beneficial.</p>",
      "content_html": "<p>Let’s assume that the Self and the Mind are two separate entities (based on vippasana meditation teachings and observations during meditation). Now let’s say there arises a “preference” in you for something, and then you chose to do that something based on this “preference”, then was it you who “chose” or was it the mind who “chose it for you”?Because if the preference arose from your mind, it must be the mind choosing for you instead of you choosing for your mind. Would it then mean that “not having any preference” a ultimate destination or result of truly being liberated? Just like a zen monk mastering having no preference for any kind of food offered?From Buddhist perspective or the Buddha's perspective, the Self does not exist (its just an illusion we see when the body, the mind and the senses, etc. come together).And that it's just a mirage. If that's true, then it would mean that this \"preference\" would have ideally arisen in the mind.If it has arisen from the mind, and it seems like this preference \"inherently existed already\" inside you, should we give attention to this preference? And stay attached to it?Or should we see it as yet another desire of the mind and let it go as attachment to it would increase suffering?Another question is that if the mind and the Self are supposed to be different entities (I am saying \"supposed\" because the latter is said to be an illusion), then why does the Buddha say that it is the mind that controls you, and not you who controls your mind?Is this word \"you\" being used to just explain to humans, because without this usage of word \"you\" it would be difficult to explain your relationship with your own mind? This might be the case, otherwise it would be very difficult to communicate about the mind and our \"perceived\" Self.</p>"
    },
    {
      "id": "35b3ddf69b9c",
      "title": "How I Used Methodable to Have a Nice Tuesday",
      "content": "A walkthrough of a day in which I used the self-programming tool Methodable to systematically clarify and accomplish my goals[what follows is crossposted from my personal blog (article link)]I organized my thoughts and to-dos, and did my highest priority tasks in a properly time-boxed way, and meditated. When all was done, I could say “I’ve done what I needed to do today and had a good time doing so,” which is just what I’d hoped - a far cry from the dopamine rut my “spidey senses” had warned me about that morning. And all the time I had a calm companion guiding me along: the voice of my prior intentions.My view from the cafe where I’m currently writingToday, I am sitting in a cafe, and writing about a different time I was sitting in a cafe… do I have you at the edge of your seat? I hope so, because:If you are a new reader, you will be introduced to Methodable, my “human-programming tool,” a web app that helps people make systematic progress towards goals.If you are a past reader, you will finally see a real-life Methodable use case!In particular, I’ll describe a day during which I used Methodable to un-scatter my attention and gracefully navigate a complex landscape of personal projects. I’ll cover the entire day, starting from when I awoke and continuing through the creation of a seed program which ultimately blossomed into a comprehensive, tailored guide for my day.Methodable is a quirky, unmaintained piece of software, but I still find it useful in my own life, and I hope you’ll enjoy learning about the patterns of thinking and planning that it enables.Our story begins on a Tuesday morning.A day with no clear pathSome days I feel I know what to do - I have clear projects, and clear tasks within those projects, and I can just “do”. And I really enjoy those days! But on many other days the path is not clear. This was such a day.To start, I woke up with little energy and little bandwidth for work, as happens to me from time to time. And I felt quite scattered. I kn...",
      "url": "https://www.lesswrong.com/posts/m7si5JLT2XayYY8dp/how-i-used-methodable-to-have-a-nice-tuesday",
      "author": "dnsosebee",
      "published": "2026-01-23T21:57:30.764000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A walkthrough/testimonial for 'Methodable,' the author's productivity tool for systematic goal achievement. Describes a personal day using the app to organize tasks and maintain focus.",
      "importance_score": 5,
      "reasoning": "Product promotion/personal blog crosspost with no AI research content. No technical substance or novel findings.",
      "themes": [
        "Productivity Tools"
      ],
      "continuation": null,
      "summary_html": "<p>A walkthrough/testimonial for 'Methodable,' the author's productivity tool for systematic goal achievement. Describes a personal day using the app to organize tasks and maintain focus.</p>",
      "content_html": "<p>A walkthrough of a day in which I used the self-programming tool Methodable to systematically clarify and accomplish my goals[what follows is crossposted from my personal blog (article link)]I organized my thoughts and to-dos, and did my highest priority tasks in a properly time-boxed way, and meditated. When all was done, I could say “I’ve done what I needed to do today and had a good time doing so,” which is just what I’d hoped - a far cry from the dopamine rut my “spidey senses” had warned me about that morning. And all the time I had a calm companion guiding me along: the voice of my prior intentions.My view from the cafe where I’m currently writingToday, I am sitting in a cafe, and writing about a different time I was sitting in a cafe… do I have you at the edge of your seat? I hope so, because:If you are a new reader, you will be introduced to Methodable, my “human-programming tool,” a web app that helps people make systematic progress towards goals.If you are a past reader, you will finally see a real-life Methodable use case!In particular, I’ll describe a day during which I used Methodable to un-scatter my attention and gracefully navigate a complex landscape of personal projects. I’ll cover the entire day, starting from when I awoke and continuing through the creation of a seed program which ultimately blossomed into a comprehensive, tailored guide for my day.Methodable is a quirky, unmaintained piece of software, but I still find it useful in my own life, and I hope you’ll enjoy learning about the patterns of thinking and planning that it enables.Our story begins on a Tuesday morning.A day with no clear pathSome days I feel I know what to do - I have clear projects, and clear tasks within those projects, and I can just “do”. And I really enjoy those days! But on many other days the path is not clear. This was such a day.To start, I woke up with little energy and little bandwidth for work, as happens to me from time to time. And I felt quite scattered. I kn...</p>"
    },
    {
      "id": "16f8f0e454d4",
      "title": "Thinking from the Other Side: Should I Wash My Hair with Shampoo?",
      "content": "This article is a thought experiment based entirely on personal experience and observations.A few years ago, I had long, thick hair. I used the same shampoo for many years and never experienced any hair loss or damage. That is, until my mother forced me to change my shampoo, at which point my hair structure completely deteriorated. It was no longer as thick as before, it wasn't growing as fast as it used to; it was falling out a lot and becoming sparse. For a few years, I tried different shampoos to combat this, even sought advice from a few people in the dermocosmetic industry, but to no avail—my hair was completely ruined. I wasn't bald yet, but it was already on its way.However, a thought occurred to me recently:How significant was shampoo in the 11,000-year history of humanity, and did human hair really need shampoo? Hair would get dirty, hair would get oily, but a little warm water could handle that. Why should I put a ton of chemicals on my head unless a bird pooped on it? My hair was already in bad enough shape; what's the worst that could happen if I didn't use shampoo on it?It's been a week since I last washed my hair with shampoo, and the results are incredible. My hair is still shedding a little, but it's starting to resemble its former thick state again, it's wavy once more, and the layers in my hair are incredibly defined. Every time I look in the mirror, I think about how this gamble paid off!So, is this really about human history and shampoos? The short answer is no, but I’ll start explaining the long answer and the reasons behind it now.&nbsp;When I first thought of this idea, I even laughed at myself, but then it started to bother me. Why not? All I had to do was not use shampoo on my hair once or twice; it was a very simple experiment. If the result of this experiment was negative, meaning my hair got worse, I would continue using the shampoo I was using and accept my fate; if it was positive, I would see what happened then. The result was quite go...",
      "url": "https://www.lesswrong.com/posts/jmzAnuKJmjsyJ6RMA/thinking-from-the-other-side-should-i-wash-my-hair-with",
      "author": "R0sberg",
      "published": "2026-01-24T17:47:34.611000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A personal anecdote about the author's experiment with stopping shampoo use and observing improvements in hair quality. Questions whether modern products are necessary based on evolutionary reasoning.",
      "importance_score": 3,
      "reasoning": "Completely unrelated to AI research. Personal lifestyle experimentation with no technical or research value for this domain.",
      "themes": [
        "Personal Experimentation"
      ],
      "continuation": null,
      "summary_html": "<p>A personal anecdote about the author's experiment with stopping shampoo use and observing improvements in hair quality. Questions whether modern products are necessary based on evolutionary reasoning.</p>",
      "content_html": "<p>This article is a thought experiment based entirely on personal experience and observations.A few years ago, I had long, thick hair. I used the same shampoo for many years and never experienced any hair loss or damage. That is, until my mother forced me to change my shampoo, at which point my hair structure completely deteriorated. It was no longer as thick as before, it wasn't growing as fast as it used to; it was falling out a lot and becoming sparse. For a few years, I tried different shampoos to combat this, even sought advice from a few people in the dermocosmetic industry, but to no avail—my hair was completely ruined. I wasn't bald yet, but it was already on its way.However, a thought occurred to me recently:How significant was shampoo in the 11,000-year history of humanity, and did human hair really need shampoo? Hair would get dirty, hair would get oily, but a little warm water could handle that. Why should I put a ton of chemicals on my head unless a bird pooped on it? My hair was already in bad enough shape; what's the worst that could happen if I didn't use shampoo on it?It's been a week since I last washed my hair with shampoo, and the results are incredible. My hair is still shedding a little, but it's starting to resemble its former thick state again, it's wavy once more, and the layers in my hair are incredibly defined. Every time I look in the mirror, I think about how this gamble paid off!So, is this really about human history and shampoos? The short answer is no, but I’ll start explaining the long answer and the reasons behind it now.&nbsp;When I first thought of this idea, I even laughed at myself, but then it started to bother me. Why not? All I had to do was not use shampoo on my hair once or twice; it was a very simple experiment. If the result of this experiment was negative, meaning my hair got worse, I would continue using the shampoo I was using and accept my fate; if it was positive, I would see what happened then. The result was quite go...</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}