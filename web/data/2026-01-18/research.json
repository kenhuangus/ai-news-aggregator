{
  "category": "research",
  "date": "2026-01-18",
  "category_summary": "An unusually light day for AI research, with only two substantive original contributions. The standout is a novel **AI-assisted policy analysis** using **Claude Sonnet 4.5** with web search to [systematically catalog](/?date=2026-01-18&category=research#item-d473a553750e) every US congressperson's public AGI positions—producing actionable governance data.\n\n- A philosophical piece on AI safety [argues that **flourishing-focused interventions**](/?date=2026-01-18&category=research#item-32842710cab1) may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing\n- **AISC** [project update](/?date=2026-01-18&category=research#item-516cf5c335c1) on 'Understanding Trust' references an **IQA paper** output from Spring 2025 cohort work\n- **MATS Summer 2026** [applications closing](/?date=2026-01-18&category=research#item-b1120185b7d9) January 18th—relevant for safety talent pipeline but not research itself\n\nRemaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.",
  "category_summary_html": "<p>An unusually light day for AI research, with only two substantive original contributions. The standout is a novel <strong>AI-assisted policy analysis</strong> using <strong>Claude Sonnet 4.5</strong> with web search to <a href=\"/?date=2026-01-18&category=research#item-d473a553750e\" class=\"internal-link\" rel=\"noopener noreferrer\">systematically catalog</a> every US congressperson's public AGI positions—producing actionable governance data.</p>\n<ul>\n<li>A philosophical piece on AI safety <a href=\"/?date=2026-01-18&category=research#item-32842710cab1\" class=\"internal-link\" rel=\"noopener noreferrer\">argues that <strong>flourishing-focused interventions</strong></a> may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing</li>\n<li><strong>AISC</strong> <a href=\"/?date=2026-01-18&category=research#item-516cf5c335c1\" class=\"internal-link\" rel=\"noopener noreferrer\">project update</a> on 'Understanding Trust' references an <strong>IQA paper</strong> output from Spring 2025 cohort work</li>\n<li><strong>MATS Summer 2026</strong> <a href=\"/?date=2026-01-18&category=research#item-b1120185b7d9\" class=\"internal-link\" rel=\"noopener noreferrer\">applications closing</a> January 18th—relevant for safety talent pipeline but not research itself</li>\n</ul>\n<p>Remaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.</p>",
  "themes": [
    {
      "name": "AI Policy & Governance",
      "description": "Analysis of political and regulatory landscape for AI development",
      "item_count": 1,
      "example_items": [],
      "importance": 58
    },
    {
      "name": "AI Safety & Alignment",
      "description": "Content related to AI safety research, existential risk from AI, and alignment community activities",
      "item_count": 4,
      "example_items": [],
      "importance": 45
    },
    {
      "name": "Non-AI Topics",
      "description": "Economics, neuroscience, and general life advice unrelated to AI research",
      "item_count": 4,
      "example_items": [],
      "importance": 12
    }
  ],
  "total_items": 9,
  "items": [
    {
      "id": "d473a553750e",
      "title": "What Washington Says About AGI",
      "content": "I spent a few hundred dollars on Anthropic API credits and let Claude individually research every current US congressperson's position on AI. This is a summary of my findings.Disclaimer: Summarizing people's beliefs is hard and inherently subjective and noisy. Likewise, US politicians change their opinions on things constantly so it's hard to know what's up-to-date. Also, I vibe-coded a lot of this.MethodologyI used Claude Sonnet 4.5 with web search to research every congressperson's public statements on AI, then used GPT-4o to score each politician on how \"AGI-pilled\" they are, how concerned they are about existential risk, and how focused they are on US-China AI competition. I plotted these scores against GovTrack ideology data to search for any partisan splits.I. AGI awareness is not partisan and not widespreadFew members of Congress have public statements taking AGI seriously. For those that do, the difference is not in political ideology. If we simply plot the AGI-pilled score vs the ideology score, we observe no obvious partisan split.There are 151 congresspeople who Claude could not find substantial quotes about AI from. These members are not included on this plot or any of the plots which follow.&nbsp;II. Existential risk is partisan at the tailsWhen you change the scoring prompt to ask how much a congressperson's statements reflect a concern about existential risk, the plot looks different. Note that the scoring prompt here emphasizes \"A politician who is most XRisk-pilled is someone who thinks AI is a risk to humanity -- not just the US.\" This separates x-risk concerns from fears related to US-China relations.This graph looks mostly like noise but it does show that the majority of the most x-risk pilled politicians are Democrats.[1]&nbsp;This is troubling. Politics is a mind-killer and if AI Safety becomes partisan, productive debate will be even more difficult than it currently is.III. Both parties are fixated on ChinaSome congresspeople have made up thei...",
      "url": "https://www.lesswrong.com/posts/WLdcvAcoFZv9enR37/what-washington-says-about-agi",
      "author": "zroe1",
      "published": "2026-01-17T00:43:46.426000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Original research using Claude Sonnet 4.5 with web search to systematically analyze every US congressperson's public statements on AI. Findings show AGI awareness is not partisan but is rare, with x-risk concerns and US-China competition focus also mapped against ideology.",
      "importance_score": 58,
      "reasoning": "Novel methodology applying AI tools to policy research. Produces actionable data on AI governance landscape. Relevant to understanding political dynamics around AI regulation. Limitations in methodology (vibe-coded, model hallucination risks) noted but still valuable.",
      "themes": [
        "AI Policy",
        "AI Governance",
        "US Politics",
        "AI-Assisted Research"
      ],
      "continuation": null,
      "summary_html": "<p>Original research using Claude Sonnet 4.5 with web search to systematically analyze every US congressperson's public statements on AI. Findings show AGI awareness is not partisan but is rare, with x-risk concerns and US-China competition focus also mapped against ideology.</p>",
      "content_html": "<p>I spent a few hundred dollars on Anthropic API credits and let Claude individually research every current US congressperson's position on AI. This is a summary of my findings.Disclaimer: Summarizing people's beliefs is hard and inherently subjective and noisy. Likewise, US politicians change their opinions on things constantly so it's hard to know what's up-to-date. Also, I vibe-coded a lot of this.MethodologyI used Claude Sonnet 4.5 with web search to research every congressperson's public statements on AI, then used GPT-4o to score each politician on how \"AGI-pilled\" they are, how concerned they are about existential risk, and how focused they are on US-China AI competition. I plotted these scores against GovTrack ideology data to search for any partisan splits.I. AGI awareness is not partisan and not widespreadFew members of Congress have public statements taking AGI seriously. For those that do, the difference is not in political ideology. If we simply plot the AGI-pilled score vs the ideology score, we observe no obvious partisan split.There are 151 congresspeople who Claude could not find substantial quotes about AI from. These members are not included on this plot or any of the plots which follow.&nbsp;II. Existential risk is partisan at the tailsWhen you change the scoring prompt to ask how much a congressperson's statements reflect a concern about existential risk, the plot looks different. Note that the scoring prompt here emphasizes \"A politician who is most XRisk-pilled is someone who thinks AI is a risk to humanity -- not just the US.\" This separates x-risk concerns from fears related to US-China relations.This graph looks mostly like noise but it does show that the majority of the most x-risk pilled politicians are Democrats.[1]&nbsp;This is troubling. Politics is a mind-killer and if AI Safety becomes partisan, productive debate will be even more difficult than it currently is.III. Both parties are fixated on ChinaSome congresspeople have made up thei...</p>"
    },
    {
      "id": "32842710cab1",
      "title": "Focusing on Flourishing Even When Survival is Unlikely (I)",
      "content": "1. The CaseYou've probably heard something like this before:If we survive this century, the expected value of the future is massive.If we don't survive, the expected value is near zero.Therefore, the value of an intervention is approximately proportional to how much it increases the chance of survival.You won't go badly wrong following the conclusion, but (3) doesn't actually follow from (1) and (2). That's because interventions might vary in how they affect the expected value of the future conditional on survival.[1]Will MacAskill makes roughly this argument in Better Futures (August 2025). See the diagram below: survival-focused interventions target the red rectangle, flourishing-focused interventions target the blue. But the blue rectangle might be much larger than the red rectangle -- if x-risk is 20% then even the best survival intervention can increase EV by at most 1.2x, whereas a flourishing intervention could increase EV by 5x or 5000x.But is x-risk only 20%? MacAskill thinks so,[2]&nbsp;but his argument applies even if extinction is very likely — say 99% — so long as there are interventions that increase flourishing by +100x. That's the scenario I want to discuss here in this post.My conclusion is:When survival is unlikely, focusing on flourishing becomes qualitatively different in ways that cut against focusing on flourishing.There are various strategies for focusing on flourishing when survival is unlikely, but none are attractive.&nbsp;Overall, when survival is unlikely, we shouldn't focus on flourishing. Even if you buy that the best possible futures are hundreds of times better than the middling futures.2. ChallengesRecall that flourishing is the expected value conditional on survival. That is, flourishing-focused interventions target the survival posterior, consisting of the green and blue rectangles. Consequentially, if survival is likely then the survival posterior consists of ordinary futures, but if survival is unlikely then the survival posterio...",
      "url": "https://www.lesswrong.com/posts/cjGALjyJEvenyP9pD/focusing-on-flourishing-even-when-survival-is-unlikely-i",
      "author": "Cleo Nardo",
      "published": "2026-01-17T13:47:59.025000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical argument that even when existential risk is very high (e.g., 99%), interventions focused on improving the quality of post-survival futures ('flourishing') may have higher expected value than survival-focused interventions. References Will MacAskill's August 2025 work 'Better Futures.'",
      "importance_score": 42,
      "reasoning": "Novel framing for AI safety prioritization with mathematical structure. Challenges common assumptions about x-risk intervention value calculations. Philosophical rather than technical, but has implications for AI safety resource allocation.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "Effective Altruism",
        "Philosophy"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical argument that even when existential risk is very high (e.g., 99%), interventions focused on improving the quality of post-survival futures ('flourishing') may have higher expected value than survival-focused interventions. References Will MacAskill's August 2025 work 'Better Futures.'</p>",
      "content_html": "<p>1. The CaseYou've probably heard something like this before:If we survive this century, the expected value of the future is massive.If we don't survive, the expected value is near zero.Therefore, the value of an intervention is approximately proportional to how much it increases the chance of survival.You won't go badly wrong following the conclusion, but (3) doesn't actually follow from (1) and (2). That's because interventions might vary in how they affect the expected value of the future conditional on survival.[1]Will MacAskill makes roughly this argument in Better Futures (August 2025). See the diagram below: survival-focused interventions target the red rectangle, flourishing-focused interventions target the blue. But the blue rectangle might be much larger than the red rectangle -- if x-risk is 20% then even the best survival intervention can increase EV by at most 1.2x, whereas a flourishing intervention could increase EV by 5x or 5000x.But is x-risk only 20%? MacAskill thinks so,[2]&nbsp;but his argument applies even if extinction is very likely — say 99% — so long as there are interventions that increase flourishing by +100x. That's the scenario I want to discuss here in this post.My conclusion is:When survival is unlikely, focusing on flourishing becomes qualitatively different in ways that cut against focusing on flourishing.There are various strategies for focusing on flourishing when survival is unlikely, but none are attractive.&nbsp;Overall, when survival is unlikely, we shouldn't focus on flourishing. Even if you buy that the best possible futures are hundreds of times better than the middling futures.2. ChallengesRecall that flourishing is the expected value conditional on survival. That is, flourishing-focused interventions target the survival posterior, consisting of the green and blue rectangles. Consequentially, if survival is likely then the survival posterior consists of ordinary futures, but if survival is unlikely then the survival posterio...</p>"
    },
    {
      "id": "b1120185b7d9",
      "title": "Applying to MATS: What the Program Is Like, and Who It’s For",
      "content": "Application deadline: Three days remaining! MATS Summer 2026 applications close this Sunday, January 18, 2026 AOE. We've shortened the application this year. Most people finish in 1–2 hours, and we'll get back to applicants about first stage results by the end of January. Visit our website for details: matsprogram.org/apply.TL;DR:&nbsp;This post is a follow-up to our shorter announcement that MATS Summer 2026 applications are open. It's intended for people who are considering applying and want a clearer sense of what the program is actually like, how mentorship and research support work in practice, and whether MATS is likely to be a good fit.What MATS is trying to doMATS aims to find and train talented individuals for what we see as one of the world's most urgent and talent-constrained problems: reducing risks from unaligned AI. We believe ambitious people from a wide range of backgrounds can meaningfully contribute to this work. Our program provides the mentorship, funding, training, and community to make that happen.Since late 2021, MATS has supported over 500 researchers working with more than 100 mentors from organizations like Anthropic, Google DeepMind, OpenAI, UK AISI, GovAI, METR, Apollo, RAND, AI Futures Project, Redwood Research, and more. Fellows have collectively co-authored 160+ research papers, with 7,800+ citations and an organizational h-index of 40.Fellows have contributed to research agendas like:Sparse auto-encoders for AI interpretabilityActivation/representation engineeringEmergent misalignmentEvaluating situational awarenessInoculation promptingDevelopmental interpretabilityComputational mechanicsGradient routingApproximately 80% of alumni now work directly in AI safety/security, and around 10% have gone on to co-found AI safety organizations or research teams. These 30+ initiatives include Apollo Research,&nbsp;Atla AI,&nbsp;Timaeus, Simplex,&nbsp;Leap Labs,&nbsp;Theorem Labs,&nbsp;Workshop Labs, and Watertight AIWhat fellows receiveThe initi...",
      "url": "https://www.lesswrong.com/posts/GJWgXZ3jjYfkfzKut/applying-to-mats-what-the-program-is-like-and-who-it-s-for",
      "author": "Raj Thimmiah",
      "published": "2026-01-16T19:25:16.002000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Detailed information about the MATS (ML Alignment Theory Scholars) Summer 2026 program with applications closing January 18, 2026. Describes program structure, mentorship model, and success metrics for prospective AI safety researchers.",
      "importance_score": 30,
      "reasoning": "Important community resource for AI safety talent pipeline. Not research itself but relevant to field-building. MATS has supported 500+ researchers and connects to major labs (Anthropic, DeepMind, OpenAI).",
      "themes": [
        "AI Safety",
        "Research Training",
        "Field Building"
      ],
      "continuation": null,
      "summary_html": "<p>Detailed information about the MATS (ML Alignment Theory Scholars) Summer 2026 program with applications closing January 18, 2026. Describes program structure, mentorship model, and success metrics for prospective AI safety researchers.</p>",
      "content_html": "<p>Application deadline: Three days remaining! MATS Summer 2026 applications close this Sunday, January 18, 2026 AOE. We've shortened the application this year. Most people finish in 1–2 hours, and we'll get back to applicants about first stage results by the end of January. Visit our website for details: matsprogram.org/apply.TL;DR:&nbsp;This post is a follow-up to our shorter announcement that MATS Summer 2026 applications are open. It's intended for people who are considering applying and want a clearer sense of what the program is actually like, how mentorship and research support work in practice, and whether MATS is likely to be a good fit.What MATS is trying to doMATS aims to find and train talented individuals for what we see as one of the world's most urgent and talent-constrained problems: reducing risks from unaligned AI. We believe ambitious people from a wide range of backgrounds can meaningfully contribute to this work. Our program provides the mentorship, funding, training, and community to make that happen.Since late 2021, MATS has supported over 500 researchers working with more than 100 mentors from organizations like Anthropic, Google DeepMind, OpenAI, UK AISI, GovAI, METR, Apollo, RAND, AI Futures Project, Redwood Research, and more. Fellows have collectively co-authored 160+ research papers, with 7,800+ citations and an organizational h-index of 40.Fellows have contributed to research agendas like:Sparse auto-encoders for AI interpretabilityActivation/representation engineeringEmergent misalignmentEvaluating situational awarenessInoculation promptingDevelopmental interpretabilityComputational mechanicsGradient routingApproximately 80% of alumni now work directly in AI safety/security, and around 10% have gone on to co-found AI safety organizations or research teams. These 30+ initiatives include Apollo Research,&nbsp;Atla AI,&nbsp;Timaeus, Simplex,&nbsp;Leap Labs,&nbsp;Theorem Labs,&nbsp;Workshop Labs, and Watertight AIWhat fellows receiveThe initi...</p>"
    },
    {
      "id": "516cf5c335c1",
      "title": "Understanding Trust: Project Update",
      "content": "This is a brief note on what I did with my funding in 2025, and my plans for 2026, written primarily because Manifund nudged me for an update on my project.I ran my AISC project (which I announced here) with four mentees in Spring 2025: Norman Hsia, Hanna Gabor, Paul Rapoport, and Roman Malov. A few other people attended the weekly meetings as well, and those regular meetings have continued (they are joinable -- pm me if interested). Norman and Paul ended up as coauthors of my ILIAD 2024 paper Understanding Trust, which had been drafted in 2024, so served as both an input and an output of the AISC project.I recorded most of the meetings involved in the project, as one of the hopeful outputs was publicly posted videos explaining the research agenda. I've proven to be bad at this side of things: I don't like listening to myself talk, so I found it difficult to edit or even to review edits done by others. I'm finally uploading the videos with minimal AI-orchestrated edits. Playlist here. At the time of publication, there's only two, but more coming very soon. If you are OK with the almost-unedited presentation style, it should be a good resource to get a very in-depth view on my thinking about AI safety and decision theory; a thorough snapshot of my thinking as of spring 2025.In 2025, I obtained funding for 2025 as well as 2026. (My total financial runway is longer than this, but 2025 and 2026 have been funded by grants/donations which compensated me for my continued research at specific price points.) I'm opening up my Manifund project for funding for 2027, for those who feel so inclined.In addition to publishing the ILIAD 2024 paper, I also published an ILIAD 2025 paper: Communication &amp; Trust. I consider it to be an incremental improvement: the ILIAD 2024 treated self-modifying actions as a distinct class with known effects which work with certainty. The ILIAD 2025 paper treated all actions as having some subjective chance of disrupting the agent's computation.&n...",
      "url": "https://www.lesswrong.com/posts/yig4LeEfpkFfiWpk2/understanding-trust-project-update",
      "author": "abramdemski",
      "published": "2026-01-17T16:19:09.213000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Project update from abramdemski on his AISC (AI Safety Camp) project about 'Understanding Trust,' which ran in Spring 2025 with four mentees. The project produced a co-authored ILIAD 2024 paper and recorded videos explaining the research agenda are being uploaded.",
      "importance_score": 28,
      "reasoning": "Administrative update rather than substantive research content. References actual research output (ILIAD paper) but doesn't present technical details. Useful for those tracking alignment research community activities but low standalone research value.",
      "themes": [
        "AI Safety",
        "Alignment Research",
        "Research Community"
      ],
      "continuation": null,
      "summary_html": "<p>Project update from abramdemski on his AISC (AI Safety Camp) project about 'Understanding Trust,' which ran in Spring 2025 with four mentees. The project produced a co-authored ILIAD 2024 paper and recorded videos explaining the research agenda are being uploaded.</p>",
      "content_html": "<p>This is a brief note on what I did with my funding in 2025, and my plans for 2026, written primarily because Manifund nudged me for an update on my project.I ran my AISC project (which I announced here) with four mentees in Spring 2025: Norman Hsia, Hanna Gabor, Paul Rapoport, and Roman Malov. A few other people attended the weekly meetings as well, and those regular meetings have continued (they are joinable -- pm me if interested). Norman and Paul ended up as coauthors of my ILIAD 2024 paper Understanding Trust, which had been drafted in 2024, so served as both an input and an output of the AISC project.I recorded most of the meetings involved in the project, as one of the hopeful outputs was publicly posted videos explaining the research agenda. I've proven to be bad at this side of things: I don't like listening to myself talk, so I found it difficult to edit or even to review edits done by others. I'm finally uploading the videos with minimal AI-orchestrated edits. Playlist here. At the time of publication, there's only two, but more coming very soon. If you are OK with the almost-unedited presentation style, it should be a good resource to get a very in-depth view on my thinking about AI safety and decision theory; a thorough snapshot of my thinking as of spring 2025.In 2025, I obtained funding for 2025 as well as 2026. (My total financial runway is longer than this, but 2025 and 2026 have been funded by grants/donations which compensated me for my continued research at specific price points.) I'm opening up my Manifund project for funding for 2027, for those who feel so inclined.In addition to publishing the ILIAD 2024 paper, I also published an ILIAD 2025 paper: Communication &amp; Trust. I consider it to be an incremental improvement: the ILIAD 2024 treated self-modifying actions as a distinct class with known effects which work with certainty. The ILIAD 2025 paper treated all actions as having some subjective chance of disrupting the agent's computation.&amp;n...</p>"
    },
    {
      "id": "d61a988bfca9",
      "title": "Turning Down the Overthinking: How Cathodal Brain Stimulation Could Transform Stuttering Therapy",
      "content": "The cruelest irony of stuttering is that trying harder to speak fluently makes it worse. Not trying harder in the sense of practice or effort, but trying harder in the sense of conscious attention to speech mechanics. When someone who stutters focuses intently on controlling their words, analyzing their breathing, and monitoring their mouth movements, their speech doesn't improve. It deteriorates.This is the reinvestment hypothesis in action: explicit, conscious control actively interferes with skills that should be automatic. A pianist who thinks too carefully about finger placement plays worse. An athlete who consciously monitors their form chokes under pressure. And a person who stutters, desperately focusing on each syllable, finds their speech becoming more fragmented, not less.For the 70 million people worldwide who stutter, this creates a devastating trap. They know their speech is broken. They focus intensely on fixing it. And that very focus makes the problem worse.What if we could temporarily turn off that interference? What if we could create a neural state where the overthinking stops, where the brain's executive control systems step aside and let procedural motor learning do its work? And what if we could do this precisely during speech practice, when the brain is trying to encode new, fluent motor patterns?TDCS Shows Promise, But We're Targeting the Wrong MechanismBrain stimulation for stuttering isn't a new idea. Over the past seven years, researchers have tested transcranial direct current stimulation (tDCS) in adults who stutter, with mixed but encouraging results.The landmark study came from Oxford in 2018. Chesters and colleagues ran a rigorous double-blind trial with 30 adults who stutter. The intervention was straightforward: 1 milliamp of anodal (excitatory) tDCS applied to the left inferior frontal cortex for 20 minutes, five days in a row, while participants practiced fluency-inducing speech techniques like choral reading and metronome-timed ...",
      "url": "https://www.lesswrong.com/posts/iXsbazekj4tTxP5zp/turning-down-the-overthinking-how-cathodal-brain-stimulation",
      "author": "Rudaiba",
      "published": "2026-01-17T09:54:46.826000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Neuroscience exploration of how cathodal brain stimulation might help stuttering by reducing conscious interference with automatic speech processes. Based on the 'reinvestment hypothesis' that conscious attention can impair motor skills that should be automatic.",
      "importance_score": 18,
      "reasoning": "Interesting neuroscience with potential parallels to AI learning (explicit vs procedural knowledge), but not AI research. Could have distant relevance to understanding human cognition for AI alignment purposes.",
      "themes": [
        "Neuroscience",
        "Medical Research",
        "Motor Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Neuroscience exploration of how cathodal brain stimulation might help stuttering by reducing conscious interference with automatic speech processes. Based on the 'reinvestment hypothesis' that conscious attention can impair motor skills that should be automatic.</p>",
      "content_html": "<p>The cruelest irony of stuttering is that trying harder to speak fluently makes it worse. Not trying harder in the sense of practice or effort, but trying harder in the sense of conscious attention to speech mechanics. When someone who stutters focuses intently on controlling their words, analyzing their breathing, and monitoring their mouth movements, their speech doesn't improve. It deteriorates.This is the reinvestment hypothesis in action: explicit, conscious control actively interferes with skills that should be automatic. A pianist who thinks too carefully about finger placement plays worse. An athlete who consciously monitors their form chokes under pressure. And a person who stutters, desperately focusing on each syllable, finds their speech becoming more fragmented, not less.For the 70 million people worldwide who stutter, this creates a devastating trap. They know their speech is broken. They focus intensely on fixing it. And that very focus makes the problem worse.What if we could temporarily turn off that interference? What if we could create a neural state where the overthinking stops, where the brain's executive control systems step aside and let procedural motor learning do its work? And what if we could do this precisely during speech practice, when the brain is trying to encode new, fluent motor patterns?TDCS Shows Promise, But We're Targeting the Wrong MechanismBrain stimulation for stuttering isn't a new idea. Over the past seven years, researchers have tested transcranial direct current stimulation (tDCS) in adults who stutter, with mixed but encouraging results.The landmark study came from Oxford in 2018. Chesters and colleagues ran a rigorous double-blind trial with 30 adults who stutter. The intervention was straightforward: 1 milliamp of anodal (excitatory) tDCS applied to the left inferior frontal cortex for 20 minutes, five days in a row, while participants practiced fluency-inducing speech techniques like choral reading and metronome-timed ...</p>"
    },
    {
      "id": "c9ba47395b18",
      "title": "The truth behind the 2026 J.P. Morgan Healthcare Conference",
      "content": "In 1654, a Jesuit polymath named Athanasius Kircher published Mundus Subterraneus, a comprehensive geography of the Earth’s interior. It had maps and illustrations and rivers of fire and vast subterranean oceans and air channels connecting every volcano on the planet. He wrote that “the whole Earth is not solid but everywhere gaping, and hollowed with empty rooms and spaces, and hidden burrows.”. Alongside comments like this, Athanasius identified the legendary lost island of Atlantis, pondered where one could find the remains of giants, and detailed the kinds of animals that lived in this lower world, including dragons. The book was based entirely on secondhand accounts, like travelers tales, miners reports, classical texts, so it was as comprehensive as it could’ve possibly been.But Athanasius had never been underground and neither had anyone else, not really, not in a way that mattered.Today, I am in San Francisco, the site of the 2026 J.P. Morgan Healthcare Conference, and it feels a lot like Mundus Subterraneus.There is ostensibly plenty of evidence to believe that the conference exists, that it actually occurs between January 12, 2026 to January 16, 2026 at the Westin St. Francis Hotel, 335 Powell Street, San Francisco, and that it has done so for the last forty-four years, just like everyone has told you. There is a website for it, there are articles about it, there are dozens of AI-generated posts on Linkedin about how excited people were about it. But I have never met anyone who has actually been inside the conference.I have never been approached by one, or seated next to one, or introduced to one. They do not appear in my life. They do not appear in anyone’s life that I know. I have put my boots on the ground to rectify this, and asked around, first casually and then less casually, “Do you know anyone who has attended the JPM conference?”, and then they nod, and then I refine the question to be, “No, no, like, someone who has actually been in the physical ...",
      "url": "https://www.lesswrong.com/posts/eopA4MqhrE4dkLjHX/the-truth-behind-the-2026-j-p-morgan-healthcare-conference",
      "author": "Abhishaike Mahajan",
      "published": "2026-01-17T12:28:11.623000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Satirical essay drawing parallels between Athanasius Kircher's speculative 1654 geography 'Mundus Subterraneus' and the author's experience at the 2026 J.P. Morgan Healthcare Conference. Commentary on the nature of secondhand knowledge and conferences.",
      "importance_score": 12,
      "reasoning": "Literary/satirical piece with no AI research content. Interesting writing but not relevant to AI research, safety, or policy discussions.",
      "themes": [
        "Healthcare",
        "Science Commentary"
      ],
      "continuation": null,
      "summary_html": "<p>Satirical essay drawing parallels between Athanasius Kircher's speculative 1654 geography 'Mundus Subterraneus' and the author's experience at the 2026 J.P. Morgan Healthcare Conference. Commentary on the nature of secondhand knowledge and conferences.</p>",
      "content_html": "<p>In 1654, a Jesuit polymath named Athanasius Kircher published Mundus Subterraneus, a comprehensive geography of the Earth’s interior. It had maps and illustrations and rivers of fire and vast subterranean oceans and air channels connecting every volcano on the planet. He wrote that “the whole Earth is not solid but everywhere gaping, and hollowed with empty rooms and spaces, and hidden burrows.”. Alongside comments like this, Athanasius identified the legendary lost island of Atlantis, pondered where one could find the remains of giants, and detailed the kinds of animals that lived in this lower world, including dragons. The book was based entirely on secondhand accounts, like travelers tales, miners reports, classical texts, so it was as comprehensive as it could’ve possibly been.But Athanasius had never been underground and neither had anyone else, not really, not in a way that mattered.Today, I am in San Francisco, the site of the 2026 J.P. Morgan Healthcare Conference, and it feels a lot like Mundus Subterraneus.There is ostensibly plenty of evidence to believe that the conference exists, that it actually occurs between January 12, 2026 to January 16, 2026 at the Westin St. Francis Hotel, 335 Powell Street, San Francisco, and that it has done so for the last forty-four years, just like everyone has told you. There is a website for it, there are articles about it, there are dozens of AI-generated posts on Linkedin about how excited people were about it. But I have never met anyone who has actually been inside the conference.I have never been approached by one, or seated next to one, or introduced to one. They do not appear in my life. They do not appear in anyone’s life that I know. I have put my boots on the ground to rectify this, and asked around, first casually and then less casually, “Do you know anyone who has attended the JPM conference?”, and then they nod, and then I refine the question to be, “No, no, like, someone who has actually been in the physical ...</p>"
    },
    {
      "id": "ae0e0e81f2b1",
      "title": "Lightcone is hiring a generalist, a designer, and a campus operations co-lead",
      "content": "Lightcone is hiring! We build beautiful things for truth-seeking and world-saving.&nbsp;We are hiring for three different positions: a senior designer, a campus manager, and a core team generalist. This is the first time in almost two years where we are actively hiring and trying to grow our team!&nbsp;&nbsp;Senior DesignerWhen we are at our best, I think we produce world-class design. AI 2027 was I think a great design achievement, so is much of LessWrong.com itself. I also think on a product and business level, making things beautiful and intuitive and well-crafted is crucial. I like some of Patrick Collison's thinking on this:If Stripe is a monstrously successful business, but what we make isn’t beautiful, and Stripe doesn’t embody a culture of incredibly exacting craftsmanship, I’ll be much less happy. I think the returns to both of those things in the world are really high. I think even beyond the pecuniary or financial returns, the world’s just uglier than it needs to be… One can do things well or poorly, and beauty is not a rivalrous good.My intuition is that more of Stripe’s success than one would think is downstream of the fact that people like beautiful things—and for kind of rational reasons because what does a beautiful thing tell you? Well it tells you the person who made it really cared… And so if you care about the infrastructure being holistically good, indexing on the superficial characteristics that you can actually observe is not an irrational thing to do.I want us to continue making beautiful and well-designed things. Indeed, we currently have enormous demand for making more things like AI 2027 and DecidingToWin.org, with multiple new inquiries for projects like this per month, and I think many of those opportunities could be great. I also think LessWrong itself is substantially bottlenecked on design.Now, design is a very broad category. The specific role I want to hire for is someone helping us make beautiful websites. This very likely implies ...",
      "url": "https://www.lesswrong.com/posts/Wowc8jfvyrsp4a6uk/lightcone-is-hiring-a-generalist-a-designer-and-a-campus",
      "author": "habryka",
      "published": "2026-01-16T20:47:39.061000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Job posting from Lightcone (LessWrong operators) for three positions: senior designer, campus manager, and core team generalist. Discusses organizational values around design quality and truth-seeking.",
      "importance_score": 10,
      "reasoning": "Hiring announcement, not research content. Relevant only to those interested in joining the LessWrong/rationalist community infrastructure.",
      "themes": [
        "Research Community",
        "Hiring"
      ],
      "continuation": null,
      "summary_html": "<p>Job posting from Lightcone (LessWrong operators) for three positions: senior designer, campus manager, and core team generalist. Discusses organizational values around design quality and truth-seeking.</p>",
      "content_html": "<p>Lightcone is hiring! We build beautiful things for truth-seeking and world-saving.&nbsp;We are hiring for three different positions: a senior designer, a campus manager, and a core team generalist. This is the first time in almost two years where we are actively hiring and trying to grow our team!&nbsp;&nbsp;Senior DesignerWhen we are at our best, I think we produce world-class design. AI 2027 was I think a great design achievement, so is much of LessWrong.com itself. I also think on a product and business level, making things beautiful and intuitive and well-crafted is crucial. I like some of Patrick Collison's thinking on this:If Stripe is a monstrously successful business, but what we make isn’t beautiful, and Stripe doesn’t embody a culture of incredibly exacting craftsmanship, I’ll be much less happy. I think the returns to both of those things in the world are really high. I think even beyond the pecuniary or financial returns, the world’s just uglier than it needs to be… One can do things well or poorly, and beauty is not a rivalrous good.My intuition is that more of Stripe’s success than one would think is downstream of the fact that people like beautiful things—and for kind of rational reasons because what does a beautiful thing tell you? Well it tells you the person who made it really cared… And so if you care about the infrastructure being holistically good, indexing on the superficial characteristics that you can actually observe is not an irrational thing to do.I want us to continue making beautiful and well-designed things. Indeed, we currently have enormous demand for making more things like AI 2027 and DecidingToWin.org, with multiple new inquiries for projects like this per month, and I think many of those opportunities could be great. I also think LessWrong itself is substantially bottlenecked on design.Now, design is a very broad category. The specific role I want to hire for is someone helping us make beautiful websites. This very likely implies ...</p>"
    },
    {
      "id": "91fb79db7ee3",
      "title": "Japan is a bank",
      "content": "Among developed countries, Japan has long had the highest debt/GDP ratio, currently ~232%. That seems pretty bad, and conversely has made some people say that the US debt is fine because it's still much lower than Japan's. But here are some points that might clarify the situation: First, that ratio has declined recently, from 258% in 2020. Second, the Japanese government holds a lot of stocks and foreign bonds. Its net debt/GDP is \"only\" 140%, and has declined since 2020. The US government doesn't do that. (The government of Singapore also holds a lot of assets, and Temasek is well-known as a large investment fund, but Japan is a bigger country, and despite smaller holdings per capita, its investments are much larger than Singapore's.) Meanwhile, America's federal debt/GDP ratio is ~124%. Add in state debt and it's ~127%. So the net debt/GDP of the US government isn't that different from Japan. It's still higher, but arguably the \"quality\" of that US GDP is lower, for a couple reasons: The US has a worse trade balance than Japan. It borrows more money, and has a net inflow of investment. That investment and borrowed money then circulates around and raises GDP by some multiplier, mostly by making both prices and incomes higher in the US. Japan has higher PPP/nominal GDP than the US, by a factor of ~1.6x. Arguably this is a better measure of ability to pay back debt with real stuff than nominal GDP. On the other hand, the US does have more natural resources, and the federal goverment owns a lot of land. My point is just that, while I've often seen it said that the US government debt situation is clearly better than Japan's, that's not clearly the case. By the way, another economic metric I think is interesting to compare is median and average personal wealth.",
      "url": "https://www.lesswrong.com/posts/vbXWJSKKynepq7sqY/japan-is-a-bank",
      "author": "bhauth",
      "published": "2026-01-17T11:33:04.553000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Economic analysis comparing Japan's debt/GDP ratio to the US, arguing that when accounting for government-held assets, Japan's net debt position isn't as alarming as headline figures suggest, and the US situation is more comparable than typically assumed.",
      "importance_score": 8,
      "reasoning": "Purely economic/financial analysis with no AI relevance. Well-reasoned but completely outside AI research scope.",
      "themes": [
        "Economics",
        "Finance"
      ],
      "continuation": null,
      "summary_html": "<p>Economic analysis comparing Japan's debt/GDP ratio to the US, arguing that when accounting for government-held assets, Japan's net debt position isn't as alarming as headline figures suggest, and the US situation is more comparable than typically assumed.</p>",
      "content_html": "<p>Among developed countries, Japan has long had the highest debt/GDP ratio, currently ~232%. That seems pretty bad, and conversely has made some people say that the US debt is fine because it's still much lower than Japan's. But here are some points that might clarify the situation: First, that ratio has declined recently, from 258% in 2020. Second, the Japanese government holds a lot of stocks and foreign bonds. Its net debt/GDP is \"only\" 140%, and has declined since 2020. The US government doesn't do that. (The government of Singapore also holds a lot of assets, and Temasek is well-known as a large investment fund, but Japan is a bigger country, and despite smaller holdings per capita, its investments are much larger than Singapore's.) Meanwhile, America's federal debt/GDP ratio is ~124%. Add in state debt and it's ~127%. So the net debt/GDP of the US government isn't that different from Japan. It's still higher, but arguably the \"quality\" of that US GDP is lower, for a couple reasons: The US has a worse trade balance than Japan. It borrows more money, and has a net inflow of investment. That investment and borrowed money then circulates around and raises GDP by some multiplier, mostly by making both prices and incomes higher in the US. Japan has higher PPP/nominal GDP than the US, by a factor of ~1.6x. Arguably this is a better measure of ability to pay back debt with real stuff than nominal GDP. On the other hand, the US does have more natural resources, and the federal goverment owns a lot of land. My point is just that, while I've often seen it said that the US government debt situation is clearly better than Japan's, that's not clearly the case. By the way, another economic metric I think is interesting to compare is median and average personal wealth.</p>"
    },
    {
      "id": "7cbfeb9c04a2",
      "title": "Forfeiting Ill-Gotten Gains",
      "content": "It's a holiday. The cousins are over, and the kids are having a great time. Unfortunately, that includes rampaging through the kitchen. We're trying to cook, so there's a \"no cutting through the kitchen\" rule. Imagine enforcement looks like: Kid: [dashes into kitchen, pursued by cousin] Adult: Out of the kitchen! Kid: Sorry! [Continues their path, leaving through the other door; escapes pursuit from more rule-abiding cousin] This doesn't work! The kid got what they wanted out of this interaction, and isn't going to change their behavior. Instead, I need to make it be not worth their while: Kid: [dashes into kitchen, pursued by cousin] Adult: No cutting through the kitchen! [Physically rebuffs intruder]! Kid: Sorry! [Forced to leave through the door they entered by; caught by cousin.] Other examples: Sneak candy, spit it out and forfeit dessert. Use sibling's tablet time, lose your own. Interrupt, be ignored. The general principle is that if you want to limit behavior the combination of the gains from rule-breaking and penalty from punishment need to put the kid in a worse position than if they'd never broken the rule. This isn't just a parenting thing: it's common to say that \"crime should not pay\", and many legal systems prohibit unjust enrichment. One place I'd like to see this implemented is airplane evacuation. If the safety announcements included \"In the event of an emergency evacuation, any carry-on luggage you bring will be confiscated and destroyed. You will also be fined.\" we would have more JAL 516 (379 occupants, zero deaths) and less Aeroflot 1492 or Emirates 521. Comment via: facebook, mastodon, bluesky",
      "url": "https://www.lesswrong.com/posts/pyuhYvkqX9Lzr6QWX/forfeiting-ill-gotten-gains",
      "author": "jefftk",
      "published": "2026-01-16T19:20:38.363000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Parenting principle about enforcement: when punishing rule-breaking, the combination of penalty and forfeiture of gains must leave the violator worse off than if they hadn't broken the rule. Extended briefly to crime deterrence principles.",
      "importance_score": 8,
      "reasoning": "General life advice with loose game-theoretic framing. No AI relevance. Possibly distant connection to mechanism design principles that could apply to AI systems.",
      "themes": [
        "Parenting",
        "Incentive Design"
      ],
      "continuation": null,
      "summary_html": "<p>Parenting principle about enforcement: when punishing rule-breaking, the combination of penalty and forfeiture of gains must leave the violator worse off than if they hadn't broken the rule. Extended briefly to crime deterrence principles.</p>",
      "content_html": "<p>It's a holiday. The cousins are over, and the kids are having a great time. Unfortunately, that includes rampaging through the kitchen. We're trying to cook, so there's a \"no cutting through the kitchen\" rule. Imagine enforcement looks like: Kid: [dashes into kitchen, pursued by cousin] Adult: Out of the kitchen! Kid: Sorry! [Continues their path, leaving through the other door; escapes pursuit from more rule-abiding cousin] This doesn't work! The kid got what they wanted out of this interaction, and isn't going to change their behavior. Instead, I need to make it be not worth their while: Kid: [dashes into kitchen, pursued by cousin] Adult: No cutting through the kitchen! [Physically rebuffs intruder]! Kid: Sorry! [Forced to leave through the door they entered by; caught by cousin.] Other examples: Sneak candy, spit it out and forfeit dessert. Use sibling's tablet time, lose your own. Interrupt, be ignored. The general principle is that if you want to limit behavior the combination of the gains from rule-breaking and penalty from punishment need to put the kid in a worse position than if they'd never broken the rule. This isn't just a parenting thing: it's common to say that \"crime should not pay\", and many legal systems prohibit unjust enrichment. One place I'd like to see this implemented is airplane evacuation. If the safety announcements included \"In the event of an emergency evacuation, any carry-on luggage you bring will be confiscated and destroyed. You will also be fined.\" we would have more JAL 516 (379 occupants, zero deaths) and less Aeroflot 1492 or Emirates 521. Comment via: facebook, mastodon, bluesky</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}