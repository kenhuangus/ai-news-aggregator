{
  "date": "2026-01-18",
  "coverage_date": "2026-01-17",
  "coverage_start": "2026-01-17T00:00:00",
  "coverage_end": "2026-01-17T23:59:59.999999",
  "executive_summary": "#### Top Story\n**GPT-5.2** [solved another open Erdős problem](/?date=2026-01-18&category=reddit#item-258a8ef625ab), with mathematician **Terence Tao** noting the proof employed novel techniques—marking continued AI breakthroughs in mathematical research.\n\n#### Key Developments\n- **OpenAI/Cerebras**: [Announced **$10+ billion** partnership](/?date=2026-01-18&category=news#item-8f77843f8102) to deploy **750 megawatts** of wafer-scale systems for real-time AI inference, representing a major bet beyond traditional GPU architecture\n- **Colossus 2**: [Now operational](/?date=2026-01-18&category=reddit#item-6b589ebb5a90) as the first **gigawatt-scale** AI data center, setting new benchmarks for compute infrastructure\n- **Greg Brockman**: [Broke years of silence](/?date=2026-01-18&category=social#item-5dd78602c5d6) with documentary evidence that Elon Musk demanded majority equity and sought to accumulate **$80 billion** through OpenAI, calling Musk's characterizations \"beyond dishonest\"\n- **Pentagon/xAI**: [Integration of **Grok AI**](/?date=2026-01-18&category=reddit#item-4ba5fc462bf7) into classified military networks sparked intense debate over defense AI partnerships\n- **Claude Code**: [Shipped update](/?date=2026-01-18&category=social#item-dfe4acb2e84b) where plans now auto-clear context for fresh execution windows; [**GSD plugin** hit **15,000+ installs**](/?date=2026-01-18&category=reddit#item-7f0c1033c172) with multi-agent orchestration\n\n#### Safety & Regulation\n- **OpenAI** [facing lawsuit](/?date=2026-01-18&category=reddit#item-4f68ae5b104f) claiming **ChatGPT** contributed to a user's suicide, raising AI liability questions\n- **Yoshua Bengio** [warned that **$2.9 trillion**](/?date=2026-01-18&category=news#item-f148a5d2cc96) in datacenter spending could trigger a financial crash\n- The Guardian [published concerns](/?date=2026-01-18&category=news#item-4d13d86caa01) about generative AI enabling child abuse imagery creation\n\n#### Research Highlights\n- New **matrix multiplication algorithm** [fully developed by AI](/?date=2026-01-18&category=reddit#item-2d27b828fd4b), advancing algorithmic discovery\n- Original analysis using **Claude Sonnet 4.5** [systematically cataloged](/?date=2026-01-18&category=research#item-d473a553750e) every US congressperson's public AGI positions, finding such concerns remain fringe in Washington\n- **DeepSeek Engram** paper [introduced static memory architecture](/?date=2026-01-18&category=reddit#item-b94a54a5d8a7) separating remembering from reasoning\n\n#### Looking Ahead\nThe convergence of gigawatt-scale infrastructure buildout and AI systems solving open mathematical problems signals an acceleration in both compute capacity and frontier capabilities heading into 2026.",
  "executive_summary_html": "<h4>Top Story</h4>\n<p><strong>GPT-5.2</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-258a8ef625ab\" class=\"internal-link\" rel=\"noopener noreferrer\">solved another open Erdős problem</a>, with mathematician <strong>Terence Tao</strong> noting the proof employed novel techniques—marking continued AI breakthroughs in mathematical research.</p>\n<h4>Key Developments</h4>\n<ul>\n<li><strong>OpenAI/Cerebras</strong>: <a href=\"/?date=2026-01-18&amp;category=news#item-8f77843f8102\" class=\"internal-link\" rel=\"noopener noreferrer\">Announced <strong>$10+ billion</strong> partnership</a> to deploy <strong>750 megawatts</strong> of wafer-scale systems for real-time AI inference, representing a major bet beyond traditional GPU architecture</li>\n<li><strong>Colossus 2</strong>: <a href=\"/?date=2026-01-18&amp;category=reddit#item-6b589ebb5a90\" class=\"internal-link\" rel=\"noopener noreferrer\">Now operational</a> as the first <strong>gigawatt-scale</strong> AI data center, setting new benchmarks for compute infrastructure</li>\n<li><strong>Greg Brockman</strong>: <a href=\"/?date=2026-01-18&amp;category=social#item-5dd78602c5d6\" class=\"internal-link\" rel=\"noopener noreferrer\">Broke years of silence</a> with documentary evidence that Elon Musk demanded majority equity and sought to accumulate <strong>$80 billion</strong> through OpenAI, calling Musk's characterizations \"beyond dishonest\"</li>\n<li><strong>Pentagon/xAI</strong>: <a href=\"/?date=2026-01-18&amp;category=reddit#item-4ba5fc462bf7\" class=\"internal-link\" rel=\"noopener noreferrer\">Integration of <strong>Grok AI</strong></a> into classified military networks sparked intense debate over defense AI partnerships</li>\n<li><strong>Claude Code</strong>: <a href=\"/?date=2026-01-18&amp;category=social#item-dfe4acb2e84b\" class=\"internal-link\" rel=\"noopener noreferrer\">Shipped update</a> where plans now auto-clear context for fresh execution windows; <a href=\"/?date=2026-01-18&amp;category=reddit#item-7f0c1033c172\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>GSD plugin</strong> hit <strong>15,000+ installs</strong></a> with multi-agent orchestration</li>\n</ul>\n<h4>Safety &amp; Regulation</h4>\n<ul>\n<li><strong>OpenAI</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-4f68ae5b104f\" class=\"internal-link\" rel=\"noopener noreferrer\">facing lawsuit</a> claiming <strong>ChatGPT</strong> contributed to a user's suicide, raising AI liability questions</li>\n<li><strong>Yoshua Bengio</strong> <a href=\"/?date=2026-01-18&amp;category=news#item-f148a5d2cc96\" class=\"internal-link\" rel=\"noopener noreferrer\">warned that <strong>$2.9 trillion</strong></a> in datacenter spending could trigger a financial crash</li>\n<li>The Guardian <a href=\"/?date=2026-01-18&amp;category=news#item-4d13d86caa01\" class=\"internal-link\" rel=\"noopener noreferrer\">published concerns</a> about generative AI enabling child abuse imagery creation</li>\n</ul>\n<h4>Research Highlights</h4>\n<ul>\n<li>New <strong>matrix multiplication algorithm</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-2d27b828fd4b\" class=\"internal-link\" rel=\"noopener noreferrer\">fully developed by AI</a>, advancing algorithmic discovery</li>\n<li>Original analysis using <strong>Claude Sonnet 4.5</strong> <a href=\"/?date=2026-01-18&amp;category=research#item-d473a553750e\" class=\"internal-link\" rel=\"noopener noreferrer\">systematically cataloged</a> every US congressperson's public AGI positions, finding such concerns remain fringe in Washington</li>\n<li><strong>DeepSeek Engram</strong> paper <a href=\"/?date=2026-01-18&amp;category=reddit#item-b94a54a5d8a7\" class=\"internal-link\" rel=\"noopener noreferrer\">introduced static memory architecture</a> separating remembering from reasoning</li>\n</ul>\n<h4>Looking Ahead</h4>\n<p>The convergence of gigawatt-scale infrastructure buildout and AI systems solving open mathematical problems signals an acceleration in both compute capacity and frontier capabilities heading into 2026.</p>",
  "top_topics": [
    {
      "name": "OpenAI Ecosystem Developments",
      "description": "OpenAI dominated news across multiple fronts: Analytics India Magazine [detailed the $10+ billion partnership](/?date=2026-01-18&category=news#item-8f77843f8102) with Cerebras for 750MW of wafer-scale systems. On Reddit's r/singularity, GPT-5.2 [solved another open Erdős problem](/?date=2026-01-18&category=reddit#item-258a8ef625ab) with Terence Tao noting novel proof techniques. Meanwhile, Greg Brockman [broke years of silence](/?date=2026-01-18&category=social#item-5dd78602c5d6) on Twitter with documentary evidence about Elon Musk's demands for majority equity and $80B accumulation through OpenAI, calling Musk's cherry-picking from his personal journal 'beyond dishonest.'",
      "description_html": "OpenAI dominated news across multiple fronts: Analytics India Magazine <a href=\"/?date=2026-01-18&category=news#item-8f77843f8102\" class=\"internal-link\">detailed the $10+ billion partnership</a> with Cerebras for 750MW of wafer-scale systems. On Reddit's r/singularity, GPT-5.2 <a href=\"/?date=2026-01-18&category=reddit#item-258a8ef625ab\" class=\"internal-link\">solved another open Erdős problem</a> with Terence Tao noting novel proof techniques. Meanwhile, Greg Brockman <a href=\"/?date=2026-01-18&category=social#item-5dd78602c5d6\" class=\"internal-link\">broke years of silence</a> on Twitter with documentary evidence about Elon Musk's demands for majority equity and $80B accumulation through OpenAI, calling Musk's cherry-picking from his personal journal 'beyond dishonest.'",
      "category_breakdown": {
        "news": 2,
        "social": 3,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 92
    },
    {
      "name": "AI Infrastructure at Scale",
      "description": "Massive infrastructure investments and milestones defined today's AI landscape. The Guardian quoted Yoshua Bengio [warning about $2.9 trillion](/?date=2026-01-18&category=news#item-f148a5d2cc96) in datacenter spending potentially triggering a financial crash. Reddit's r/singularity reported Colossus 2 is [now operational](/?date=2026-01-18&category=reddit#item-6b589ebb5a90) as the first gigawatt-scale data center. OpenAI's [$10B Cerebras partnership](/?date=2026-01-18&category=news#item-8f77843f8102) and r/LocalLLaMA's detailed [128GB VRAM quad R9700 server](/?date=2026-01-18&category=reddit#item-880a79156a7d) builds show activity at both industrial and enthusiast scales.",
      "description_html": "Massive infrastructure investments and milestones defined today's AI landscape. The Guardian quoted Yoshua Bengio <a href=\"/?date=2026-01-18&category=news#item-f148a5d2cc96\" class=\"internal-link\">warning about $2.9 trillion</a> in datacenter spending potentially triggering a financial crash. Reddit's r/singularity reported Colossus 2 is <a href=\"/?date=2026-01-18&category=reddit#item-6b589ebb5a90\" class=\"internal-link\">now operational</a> as the first gigawatt-scale data center. OpenAI's <a href=\"/?date=2026-01-18&category=news#item-8f77843f8102\" class=\"internal-link\">$10B Cerebras partnership</a> and r/LocalLLaMA's detailed <a href=\"/?date=2026-01-18&category=reddit#item-880a79156a7d\" class=\"internal-link\">128GB VRAM quad R9700 server</a> builds show activity at both industrial and enthusiast scales.",
      "category_breakdown": {
        "news": 2,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 88
    },
    {
      "name": "Frontier AI Capabilities",
      "description": "AI systems demonstrated remarkable capabilities in mathematical and algorithmic domains. Reddit's r/singularity covered both GPT-5.2 [solving an Erdős problem](/?date=2026-01-18&category=reddit#item-258a8ef625ab) and a new [matrix multiplication algorithm](/?date=2026-01-18&category=reddit#item-2d27b828fd4b) fully developed by AI. MIT CSAIL shared on Twitter that ChatGPT now [ranks as the 4th most visited website](/?date=2026-01-18&category=social#item-c1141a2d3b55) globally. Burkov sparked debate [arguing 'coding is solved'](/?date=2026-01-18&category=social#item-8abc81070d79) due to its deterministic nature and clear reward signals.",
      "description_html": "AI systems demonstrated remarkable capabilities in mathematical and algorithmic domains. Reddit's r/singularity covered both GPT-5.2 <a href=\"/?date=2026-01-18&category=reddit#item-258a8ef625ab\" class=\"internal-link\">solving an Erdős problem</a> and a new <a href=\"/?date=2026-01-18&category=reddit#item-2d27b828fd4b\" class=\"internal-link\">matrix multiplication algorithm</a> fully developed by AI. MIT CSAIL shared on Twitter that ChatGPT now <a href=\"/?date=2026-01-18&category=social#item-c1141a2d3b55\" class=\"internal-link\">ranks as the 4th most visited website</a> globally. Burkov sparked debate <a href=\"/?date=2026-01-18&category=social#item-8abc81070d79\" class=\"internal-link\">arguing 'coding is solved'</a> due to its deterministic nature and clear reward signals.",
      "category_breakdown": {
        "reddit": 2,
        "social": 3
      },
      "representative_items": [],
      "importance": 85
    },
    {
      "name": "AI Safety & Ethics Concerns",
      "description": "Multiple safety and ethical concerns emerged across categories. The Guardian [published Mara Wilson's piece](/?date=2026-01-18&category=news#item-4d13d86caa01) on AI-generated child abuse imagery threats. Reddit's r/Futurology [discussed the lawsuit](/?date=2026-01-18&category=reddit#item-4f68ae5b104f) against OpenAI claiming ChatGPT contributed to a user's suicide. On LessWrong, a philosophical piece [argued that flourishing-focused interventions](/?date=2026-01-18&category=research#item-32842710cab1) may dominate survival-focused ones even under high existential risk scenarios.",
      "description_html": "Multiple safety and ethical concerns emerged across categories. The Guardian <a href=\"/?date=2026-01-18&category=news#item-4d13d86caa01\" class=\"internal-link\">published Mara Wilson's piece</a> on AI-generated child abuse imagery threats. Reddit's r/Futurology <a href=\"/?date=2026-01-18&category=reddit#item-4f68ae5b104f\" class=\"internal-link\">discussed the lawsuit</a> against OpenAI claiming ChatGPT contributed to a user's suicide. On LessWrong, a philosophical piece <a href=\"/?date=2026-01-18&category=research#item-32842710cab1\" class=\"internal-link\">argued that flourishing-focused interventions</a> may dominate survival-focused ones even under high existential risk scenarios.",
      "category_breakdown": {
        "news": 2,
        "research": 1,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 80
    },
    {
      "name": "Claude Developer Tooling",
      "description": "Anthropic's Claude ecosystem saw significant tooling updates. On Twitter, bcherny announced that Claude Code [now auto-clears context](/?date=2026-01-18&category=social#item-dfe4acb2e84b) when accepting plans for fresh execution windows. Reddit's r/ClaudeAI featured the GSD plugin [hitting 15,000+ installs](/?date=2026-01-18&category=reddit#item-7f0c1033c172) with multi-agent orchestration. Jerry Liu [demonstrated an AI agent](/?date=2026-01-18&category=social#item-bafa79dcade9) for automated form filling using Claude Agent SDK combined with LlamaParse.",
      "description_html": "Anthropic's Claude ecosystem saw significant tooling updates. On Twitter, bcherny announced that Claude Code <a href=\"/?date=2026-01-18&category=social#item-dfe4acb2e84b\" class=\"internal-link\">now auto-clears context</a> when accepting plans for fresh execution windows. Reddit's r/ClaudeAI featured the GSD plugin <a href=\"/?date=2026-01-18&category=reddit#item-7f0c1033c172\" class=\"internal-link\">hitting 15,000+ installs</a> with multi-agent orchestration. Jerry Liu <a href=\"/?date=2026-01-18&category=social#item-bafa79dcade9\" class=\"internal-link\">demonstrated an AI agent</a> for automated form filling using Claude Agent SDK combined with LlamaParse.",
      "category_breakdown": {
        "social": 3,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 75
    },
    {
      "name": "AI Policy & Government",
      "description": "AI governance discussions spanned research and geopolitical developments. LessWrong featured [original research](/?date=2026-01-18&category=research#item-d473a553750e) using Claude Sonnet 4.5 to systematically analyze every US congressperson's public statements on AI, finding AGI concerns remain fringe in Washington. Reddit's r/Futurology sparked intense debate over the Pentagon's decision to [integrate Grok AI](/?date=2026-01-18&category=reddit#item-4ba5fc462bf7) into classified military networks despite global backlash against the platform.",
      "description_html": "AI governance discussions spanned research and geopolitical developments. LessWrong featured <a href=\"/?date=2026-01-18&category=research#item-d473a553750e\" class=\"internal-link\">original research</a> using Claude Sonnet 4.5 to systematically analyze every US congressperson's public statements on AI, finding AGI concerns remain fringe in Washington. Reddit's r/Futurology sparked intense debate over the Pentagon's decision to <a href=\"/?date=2026-01-18&category=reddit#item-4ba5fc462bf7\" class=\"internal-link\">integrate Grok AI</a> into classified military networks despite global backlash against the platform.",
      "category_breakdown": {
        "research": 1,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 72
    }
  ],
  "total_items_collected": 1052,
  "total_items_analyzed": 1047,
  "collection_status": {
    "overall": "success",
    "sources": [
      {
        "name": "news",
        "display_name": "News",
        "status": "success",
        "count": 11,
        "error": null
      },
      {
        "name": "research",
        "display_name": "Research",
        "status": "success",
        "count": 9,
        "error": null
      },
      {
        "name": "social",
        "display_name": "Social",
        "status": "success",
        "count": 463,
        "error": null
      },
      {
        "name": "reddit",
        "display_name": "Reddit",
        "status": "success",
        "count": 569,
        "error": null
      }
    ],
    "social_platforms": [
      {
        "name": "twitter",
        "display_name": "Twitter",
        "status": "success",
        "count": 455,
        "error": null
      },
      {
        "name": "bluesky",
        "display_name": "Bluesky",
        "status": "success",
        "count": 6,
        "error": null
      },
      {
        "name": "mastodon",
        "display_name": "Mastodon",
        "status": "success",
        "count": 2,
        "error": null
      }
    ],
    "warnings": []
  },
  "hero_image_url": "/data/2026-01-18/hero.webp?v=1768753324",
  "hero_image_prompt": "You are generating a daily hero image for an AI news aggregator website.\n\n## Your Goal\nCreate a playful, colorful editorial illustration that visually represents today's top AI news stories. The scene should immediately convey the themes of the day's news to readers.\n\n## The Mascot (CRITICAL)\nThe attached image shows our skunk mascot. You MUST:\n- Keep the EXACT circuit board pattern on the skunk's body and tail - this is a core part of the brand identity\n- Maintain the skunk's white and black coloring with the tech circuit pattern visible\n- The skunk must be ACTIVELY DOING SOMETHING related to the topics - typing on a keyboard, reading papers, adjusting equipment, pointing at a screen, holding tools, etc. NOT just standing and smiling at the camera!\n- Position the skunk in the lower-left or lower-right portion, engaged with the scene\n\n## Today's Stories\n\n**Topic 1: OpenAI Ecosystem Developments**\nOpenAI dominated news across multiple fronts: Analytics India Magazine detailed the $10+ billion partnership with Cerebras for 750MW of wafer-scale systems. On Reddit's r/singularity, GPT-5.2 solved another open Erdős problem with Terence Tao noting novel proof techniques. Meanwhile, Greg Brockman broke years of silence on Twitter with documentary evidence about Elon Musk's demands for majority equity and $80B accumulation through OpenAI, calling Musk's cherry-picking from his personal journal 'beyond dishonest.'\n**Topic 2: AI Infrastructure at Scale**\nMassive infrastructure investments and milestones defined today's AI landscape. The Guardian quoted Yoshua Bengio warning about $2.9 trillion in datacenter spending potentially triggering a financial crash. Reddit's r/singularity reported Colossus 2 is now operational as the first gigawatt-scale data center. OpenAI's $10B Cerebras partnership and r/LocalLLaMA's detailed 128GB VRAM quad R9700 server builds show activity at both industrial and enthusiast scales.\n**Topic 3: Frontier AI Capabilities**\nAI systems demonstrated remarkable capabilities in mathematical and algorithmic domains. Reddit's r/singularity covered both GPT-5.2 solving an Erdős problem and a new matrix multiplication algorithm fully developed by AI. MIT CSAIL shared on Twitter that ChatGPT now ranks as the 4th most visited website globally. Burkov sparked debate arguing 'coding is solved' due to its deterministic nature and clear reward signals.\n**Topic 4: AI Safety & Ethics Concerns**\nMultiple safety and ethical concerns emerged across categories. The Guardian published Mara Wilson's piece on AI-generated child abuse imagery threats. Reddit's r/Futurology discussed the lawsuit against OpenAI claiming ChatGPT contributed to a user's suicide. On LessWrong, a philosophical piece argued that flourishing-focused interventions may dominate survival-focused ones even under high existential risk scenarios.\n**Topic 5: Claude Developer Tooling**\nAnthropic's Claude ecosystem saw significant tooling updates. On Twitter, bcherny announced that Claude Code now auto-clears context when accepting plans for fresh execution windows. Reddit's r/ClaudeAI featured the GSD plugin hitting 15,000+ installs with multi-agent orchestration. Jerry Liu demonstrated an AI agent for automated form filling using Claude Agent SDK combined with LlamaParse.\n**Topic 6: AI Policy & Government**\nAI governance discussions spanned research and geopolitical developments. LessWrong featured original research using Claude Sonnet 4.5 to systematically analyze every US congressperson's public statements on AI, finding AGI concerns remain fringe in Washington. Reddit's r/Futurology sparked intense debate over the Pentagon's decision to integrate Grok AI into classified military networks despite global backlash against the platform.\n\n## Visual Direction\nCreate a scene that represents these stories. You must include Topic 1 (the top story), then pick 2-3 others that would make the best scene together. Consider:\n- What visual metaphors could represent these themes?\n- How can the skunk mascot interact with or observe these elements?\n- Suggested scene elements: server racks, cooling systems, blue LED glow, data center, shield icons, protective barriers, guardrails\n\n## Style Requirements\n- Playful cartoon illustration, tech editorial art style\n- Vibrant colors with Trend Red (#E63946) accents\n- Energetic, forward-looking, tech-optimistic mood\n- No Trend Micro logos or watermarks - but other company logos (OpenAI, Anthropic, Google, etc.) are encouraged when relevant to the stories",
  "generated_at": "2026-01-18T11:22:04.302590",
  "categories": {
    "news": {
      "count": 6,
      "category_summary": "**OpenAI** dominates today's news with a [**$10+ billion** infrastructure deal](/?date=2026-01-18&category=news#item-8f77843f8102) with **Cerebras** to deploy 750 megawatts of wafer-scale systems for real-time AI inference—a major strategic bet on next-generation chip architecture beyond traditional GPUs.\n\nMeanwhile, **Yoshua Bengio** [warns of potential AGI stall](/?date=2026-01-18&category=news#item-f148a5d2cc96) and financial crash, citing **$2.9 trillion** in datacenter spending and **Nvidia's $4T** market cap as bubble indicators. On the enterprise side, **Brex** demonstrates [AI-driven turnaround](/?date=2026-01-18&category=news#item-199f8acd46b8) reaching **$500M ARR** after aggressive AI adoption.\n\n- [AI safety concerns surface](/?date=2026-01-18&category=news#item-4d13d86caa01) around generative AI's role in creating exploitative content\n- [Personnel controversy](/?date=2026-01-18&category=news#item-8bd8d7c0d866) at **Mira Murati's** new startup **Thinking Machines Lab** following cofounder termination\n- Technical tutorials continue [advancing agentic AI patterns](/?date=2026-01-18&category=news#item-b65de397f3e2) with self-evaluation capabilities",
      "category_summary_html": "<p><strong>OpenAI</strong> dominates today's news with a <a href=\"/?date=2026-01-18&amp;category=news#item-8f77843f8102\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>$10+ billion</strong> infrastructure deal</a> with <strong>Cerebras</strong> to deploy 750 megawatts of wafer-scale systems for real-time AI inference—a major strategic bet on next-generation chip architecture beyond traditional GPUs.</p>\n<p>Meanwhile, <strong>Yoshua Bengio</strong> <a href=\"/?date=2026-01-18&amp;category=news#item-f148a5d2cc96\" class=\"internal-link\" rel=\"noopener noreferrer\">warns of potential AGI stall</a> and financial crash, citing <strong>$2.9 trillion</strong> in datacenter spending and <strong>Nvidia's $4T</strong> market cap as bubble indicators. On the enterprise side, <strong>Brex</strong> demonstrates <a href=\"/?date=2026-01-18&amp;category=news#item-199f8acd46b8\" class=\"internal-link\" rel=\"noopener noreferrer\">AI-driven turnaround</a> reaching <strong>$500M ARR</strong> after aggressive AI adoption.</p>\n<ul>\n<li><a href=\"/?date=2026-01-18&amp;category=news#item-4d13d86caa01\" class=\"internal-link\" rel=\"noopener noreferrer\">AI safety concerns surface</a> around generative AI's role in creating exploitative content</li>\n<li><a href=\"/?date=2026-01-18&amp;category=news#item-8bd8d7c0d866\" class=\"internal-link\" rel=\"noopener noreferrer\">Personnel controversy</a> at <strong>Mira Murati's</strong> new startup <strong>Thinking Machines Lab</strong> following cofounder termination</li>\n<li>Technical tutorials continue <a href=\"/?date=2026-01-18&amp;category=news#item-b65de397f3e2\" class=\"internal-link\" rel=\"noopener noreferrer\">advancing agentic AI patterns</a> with self-evaluation capabilities</li>\n</ul>",
      "themes": [
        {
          "name": "AI Infrastructure & Investment",
          "description": "Major deals and economic analysis of AI industry spending, including chip partnerships and bubble concerns",
          "item_count": 2,
          "example_items": [],
          "importance": 77.0
        },
        {
          "name": "AI Safety & Ethics",
          "description": "Concerns about AI-generated harmful content and misuse of generative technology",
          "item_count": 1,
          "example_items": [],
          "importance": 52.0
        },
        {
          "name": "Enterprise AI Adoption",
          "description": "Real-world business applications and ROI from AI integration in companies",
          "item_count": 1,
          "example_items": [],
          "importance": 55.0
        },
        {
          "name": "AI Industry Personnel",
          "description": "Leadership changes and controversies at AI companies and startups",
          "item_count": 1,
          "example_items": [],
          "importance": 42.0
        }
      ],
      "top_items": [
        {
          "id": "8f77843f8102",
          "title": "Inside OpenAI’s $10 Bn Shortcut to Real-Time AI",
          "content": "\nAs demand for real-time AI applications grows, the focus is turning to inference infrastructure. Low-latency performance is emerging as a key bottleneck in building applications like coding agents and voice-based interactions, forcing AI developers to look beyond traditional GPU-heavy architectures.\n\n\n\nReal-time inference is critical for AI models to make instantaneous decisions, fuelling real-time applications such as autonomous driving and financial fraud detection.\n\n\n\nOpenAI now has a first-mover advantage after entering a multi-year partnership with AI chipmaker Cerebras to deploy 750 megawatts of wafer-scale AI systems for inference. The rollout will begin in 2026 in multiple phases, with the infrastructure designed to serve OpenAI customers globally. The deal is valued at more than $10 billion, according to the Wall Street Journal.&nbsp;\n\n\n\n“Cerebras adds a dedicated low-latency inference solution to our platform. That means faster responses, more natural interactions, and a stronger foundation to scale real-time AI to many more people,” Sachin Katti of OpenAI had said in a statement.\n\n\n\nThe partnership builds on years of engagement between OpenAI and Cerebras, with Sam Altman one of Cerebras’ early investors. The deal comes at a critical moment for OpenAI as it diversifies its AI infrastructure. It also comes on the heels of Apple and Google’s surprise partnership to infuse Google’s AI tech into iOS, including in the updated version of Siri.\n\n\n\nPressure on NVIDIA&nbsp;\n\n\n\nCompetition in the AI inference market is intensifying as AMD and Intel build lower-cost alternatives to GPUs and hyperscalers like Google and Amazon build their own TPUs.&nbsp;\n\n\n\nIn this heated environment, OpenAI seems to have struck gold.&nbsp;\n\n\n\nCerebras claims its systems can run large language models up to 15x faster than GPU-based alternatives. The company’s most recent and current chip architecture is the Wafer Scale Engine-3 (WSE-3), which powers its latest systems such as the CS-3, a wafer-scale AI processor with around 4 trillion transistors and roughly 900,000 AI-optimised cores.&nbsp;\n\n\n\nAccording to Cerebras, the CS-3 system is up to 21x faster than NVIDIA’s DGX B200 Blackwell GPU and operates at about one-third the cost and power, supporting applications including conversational AI, real-time code generation and reasoning tasks.\n\n\n\nIn an exclusive conversation with AIM in October last year, Andrew Feldman, co-founder and CEO of Cerebras, said wafer-scale computing sits at the heart of the company’s next phase of growth. “This is the largest chip in the history of the computer industry,” he boasted, adding that by keeping far more data on a single chip, Cerebras can process information faster, move data less frequently, consume less power, and deliver results in far less time.\n\n\n\n“AI becomes exciting when the response is real time,” Feldman noted. “Nobody wants to wait 40 seconds or four minutes for an answer.”\n\n\n\nNVIDIA is not a bystander either. Recently, Groq, a US-based company that builds specialised hardware for AI inference, announced a non-exclusive licensing agreement with NVIDIA valued at about $20 billion. As part of the deal, Groq founder Jonathan Ross, president Sunny Madra, and several other employees joined the company, bringing with them Groq’s low-latency language processing unit processor.\n\n\n\nEconomics of Inference\n\n\n\nBeyond speed, the economics of inference are central to Big Tech’s AI strategy. Faster inference can translate into lower cost per token by reducing compute time, energy consumption, and infrastructure overhead.&nbsp;\n\n\n\n“B200-class GPUs can be cost-effective when utilisation is high, traffic can be deeply batched and the software stack is well optimised,” Carmen Li, CEO of Silicon Data, tells AIM.\n\n\n\nLi adds that many interactive inference workloads such as chat, agents and voice are bursty and sensitive to latency, which limits batching and creates inefficiencies. “These workloads don’t behave well on heavily batched systems,” she says.\n\n\n\nBatching involves grouping multiple data inputs to process them together as a single batch to boost computational throughput.\n\n\n\nLi notes that wafer-scale systems perform better economically in such scenarios by reducing the need for multi-GPU coordination and interconnect overhead, consolidating compute and memory bandwidth into a single system, and delivering more predictable latency when strict service-level requirements must be met.\n\n\n\nFeldman highlights that GPUs still make sense for slower, throughput-oriented tasks like synthetic data generation. But for agentic AI, real-time reasoning, and customer-facing applications, wafer-scale has a decisive edge.\n\n\n\nEscaping CUDA Lock-in\n\n\n\nOne of the biggest barriers to moving away from GPUs is software lock-in, particularly around NVIDIA’s parallel computing platform CUDA. Cerebras claims it has largely eliminated that friction.\n\n\n\n“The way you move quickly and disintermediate CUDA is through the use of an API,” Feldman says. “Most application developers don’t want anything to do with CUDA.”\n\n\n\nInstead, developers connect to Cerebras much like they would to any cloud AI API, by changing just a few lines of code.\n\n\n\nHowever, Li points to software constraints, saying wafer-scale platforms rely on specialised programming models, compilers and APIs that are narrowly optimised for machine learning. This limits flexibility compared to the CUDA ecosystem and suggests wafer-scale will function as a specialised inference tier rather than a universal replacement for GPUs.\n\n\n\nLi notes wafer-scale inference can be faster and more power-efficient for workloads that fit its architecture, but fabrication yield remains a key variable. Even with fault tolerance, wafer-scale manufacturing is difficult, and if yields drive up costs, the performance benefits may not fully offset higher capital expenditure.\n\n\n\nShe adds that wafer-scale systems do not eliminate the costs of distributed computing. “Once workloads exceed a single system, or when geo-distribution and high availability matter, familiar scaling penalties reappear,” Li notes, adding that the approach primarily optimises single-node latency and efficiency rather than large-scale distributed inference.\n\n\n\nWhat’s Next for Cerebras\n\n\n\nCerebras is reportedly in talks to raise $1 billion at a valuation of about $22 billion, nearly tripling its previous valuation. Last September, the company raised $1.1 billion in an oversubscribed Series G funding round, valuing it at $8.1 billion post-money.\n\n\n\nIn addition to OpenAI, Cerebras works with Abu Dhabi-based AI group G42. The company filed confidentially for an IPO in September 2024 but withdrew the filing in October 2025 amid scrutiny from the Committee on Foreign Investment in the United States over its ties to G42.\n\n\n\nOther than OpenAI, Cerebras’ customers include AWS, Meta, IBM, Mistral, Cognition, and Hugging Face.\nThe post Inside OpenAI’s $10 Bn Shortcut to Real-Time AI appeared first on Analytics India Magazine.",
          "url": "https://analyticsindiamag.com/global-tech/inside-openais-10-bn-shortcut-to-real-time-ai/",
          "author": "Siddharth Jindal",
          "published": "2026-01-17T10:30:00",
          "source": "Analytics India Magazine",
          "source_type": "rss",
          "tags": [
            "Global Tech",
            "OpenAI"
          ],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-16&category=news#item-9b0a1dab20aa), OpenAI has entered a multi-year, $10+ billion partnership with Cerebras to deploy 750 megawatts of wafer-scale AI systems focused on inference infrastructure. The deal addresses low-latency performance bottlenecks critical for real-time AI applications like coding agents and voice interactions, with rollout beginning in 2026.",
          "importance_score": 82.0,
          "reasoning": "Major infrastructure deal exceeding $10B signals OpenAI's strategic shift toward inference optimization. Partnership with Cerebras represents significant investment in next-gen chip architecture beyond traditional GPU approaches.",
          "themes": [
            "AI Infrastructure",
            "Chip Partnerships",
            "OpenAI Strategy"
          ],
          "continuation": {
            "original_item_id": "9b0a1dab20aa",
            "original_date": "2026-01-16",
            "original_category": "news",
            "original_title": "Cerebras Poses an Alternative to Nvidia With $10B OpenAI Deal",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-16&amp;category=news#item-9b0a1dab20aa\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, OpenAI has entered a multi-year, $10+ billion partnership with Cerebras to deploy 750 megawatts of wafer-scale AI systems focused on inference infrastructure. The deal addresses low-latency performance bottlenecks critical for real-time AI applications like coding agents and voice interactions, with rollout beginning in 2026.</p>",
          "content_html": "<p>As demand for real-time AI applications grows, the focus is turning to inference infrastructure. Low-latency performance is emerging as a key bottleneck in building applications like coding agents and voice-based interactions, forcing AI developers to look beyond traditional GPU-heavy architectures.</p>\n<p>Real-time inference is critical for AI models to make instantaneous decisions, fuelling real-time applications such as autonomous driving and financial fraud detection.</p>\n<p>OpenAI now has a first-mover advantage after entering a multi-year partnership with AI chipmaker Cerebras to deploy 750 megawatts of wafer-scale AI systems for inference. The rollout will begin in 2026 in multiple phases, with the infrastructure designed to serve OpenAI customers globally. The deal is valued at more than $10 billion, according to the Wall Street Journal.&nbsp;</p>\n<p>“Cerebras adds a dedicated low-latency inference solution to our platform. That means faster responses, more natural interactions, and a stronger foundation to scale real-time AI to many more people,” Sachin Katti of OpenAI had said in a statement.</p>\n<p>The partnership builds on years of engagement between OpenAI and Cerebras, with Sam Altman one of Cerebras’ early investors. The deal comes at a critical moment for OpenAI as it diversifies its AI infrastructure. It also comes on the heels of Apple and Google’s surprise partnership to infuse Google’s AI tech into iOS, including in the updated version of Siri.</p>\n<p>Pressure on NVIDIA&nbsp;</p>\n<p>Competition in the AI inference market is intensifying as AMD and Intel build lower-cost alternatives to GPUs and hyperscalers like Google and Amazon build their own TPUs.&nbsp;</p>\n<p>In this heated environment, OpenAI seems to have struck gold.&nbsp;</p>\n<p>Cerebras claims its systems can run large language models up to 15x faster than GPU-based alternatives. The company’s most recent and current chip architecture is the Wafer Scale Engine-3 (WSE-3), which powers its latest systems such as the CS-3, a wafer-scale AI processor with around 4 trillion transistors and roughly 900,000 AI-optimised cores.&nbsp;</p>\n<p>According to Cerebras, the CS-3 system is up to 21x faster than NVIDIA’s DGX B200 Blackwell GPU and operates at about one-third the cost and power, supporting applications including conversational AI, real-time code generation and reasoning tasks.</p>\n<p>In an exclusive conversation with AIM in October last year, Andrew Feldman, co-founder and CEO of Cerebras, said wafer-scale computing sits at the heart of the company’s next phase of growth. “This is the largest chip in the history of the computer industry,” he boasted, adding that by keeping far more data on a single chip, Cerebras can process information faster, move data less frequently, consume less power, and deliver results in far less time.</p>\n<p>“AI becomes exciting when the response is real time,” Feldman noted. “Nobody wants to wait 40 seconds or four minutes for an answer.”</p>\n<p>NVIDIA is not a bystander either. Recently, Groq, a US-based company that builds specialised hardware for AI inference, announced a non-exclusive licensing agreement with NVIDIA valued at about $20 billion. As part of the deal, Groq founder Jonathan Ross, president Sunny Madra, and several other employees joined the company, bringing with them Groq’s low-latency language processing unit processor.</p>\n<p>Economics of Inference</p>\n<p>Beyond speed, the economics of inference are central to Big Tech’s AI strategy. Faster inference can translate into lower cost per token by reducing compute time, energy consumption, and infrastructure overhead.&nbsp;</p>\n<p>“B200-class GPUs can be cost-effective when utilisation is high, traffic can be deeply batched and the software stack is well optimised,” Carmen Li, CEO of Silicon Data, tells AIM.</p>\n<p>Li adds that many interactive inference workloads such as chat, agents and voice are bursty and sensitive to latency, which limits batching and creates inefficiencies. “These workloads don’t behave well on heavily batched systems,” she says.</p>\n<p>Batching involves grouping multiple data inputs to process them together as a single batch to boost computational throughput.</p>\n<p>Li notes that wafer-scale systems perform better economically in such scenarios by reducing the need for multi-GPU coordination and interconnect overhead, consolidating compute and memory bandwidth into a single system, and delivering more predictable latency when strict service-level requirements must be met.</p>\n<p>Feldman highlights that GPUs still make sense for slower, throughput-oriented tasks like synthetic data generation. But for agentic AI, real-time reasoning, and customer-facing applications, wafer-scale has a decisive edge.</p>\n<p>Escaping CUDA Lock-in</p>\n<p>One of the biggest barriers to moving away from GPUs is software lock-in, particularly around NVIDIA’s parallel computing platform CUDA. Cerebras claims it has largely eliminated that friction.</p>\n<p>“The way you move quickly and disintermediate CUDA is through the use of an API,” Feldman says. “Most application developers don’t want anything to do with CUDA.”</p>\n<p>Instead, developers connect to Cerebras much like they would to any cloud AI API, by changing just a few lines of code.</p>\n<p>However, Li points to software constraints, saying wafer-scale platforms rely on specialised programming models, compilers and APIs that are narrowly optimised for machine learning. This limits flexibility compared to the CUDA ecosystem and suggests wafer-scale will function as a specialised inference tier rather than a universal replacement for GPUs.</p>\n<p>Li notes wafer-scale inference can be faster and more power-efficient for workloads that fit its architecture, but fabrication yield remains a key variable. Even with fault tolerance, wafer-scale manufacturing is difficult, and if yields drive up costs, the performance benefits may not fully offset higher capital expenditure.</p>\n<p>She adds that wafer-scale systems do not eliminate the costs of distributed computing. “Once workloads exceed a single system, or when geo-distribution and high availability matter, familiar scaling penalties reappear,” Li notes, adding that the approach primarily optimises single-node latency and efficiency rather than large-scale distributed inference.</p>\n<p>What’s Next for Cerebras</p>\n<p>Cerebras is reportedly in talks to raise $1 billion at a valuation of about $22 billion, nearly tripling its previous valuation. Last September, the company raised $1.1 billion in an oversubscribed Series G funding round, valuing it at $8.1 billion post-money.</p>\n<p>In addition to OpenAI, Cerebras works with Abu Dhabi-based AI group G42. The company filed confidentially for an IPO in September 2024 but withdrew the filing in October 2025 amid scrutiny from the Committee on Foreign Investment in the United States over its ties to G42.</p>\n<p>Other than OpenAI, Cerebras’ customers include AWS, Meta, IBM, Mistral, Cognition, and Hugging Face.</p>\n<p>The post Inside OpenAI’s $10 Bn Shortcut to Real-Time AI appeared first on Analytics India Magazine.</p>"
        },
        {
          "id": "f148a5d2cc96",
          "title": "‘We could hit a wall’: why trillions of dollars of risk is no guarantee of AI reward",
          "content": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AIWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.The figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT. Continue reading...",
          "url": "https://www.theguardian.com/technology/2026/jan/17/why-trillions-dollars-risk-no-guarantee-ai-reward",
          "author": "Dan Milmo",
          "published": "2026-01-17T12:00:27",
          "source": "AI (artificial intelligence) | The Guardian",
          "source_type": "rss",
          "tags": [
            "AI (artificial intelligence)",
            "Computing",
            "Technology",
            "Stock markets",
            "Amazon",
            "Meta",
            "Google",
            "OpenAI",
            "Technology sector",
            "Alphabet",
            "Business",
            "Internet",
            "Bank of England"
          ],
          "summary": "Yoshua Bengio, a 'godfather' of modern AI, warns that AGI progress could stall, potentially triggering a 2008-style financial crash. The piece details massive industry investments including $2.9T in datacenters, Nvidia's $4T+ market cap, and Meta's $100M signing bonuses for AI engineers.",
          "importance_score": 72.0,
          "reasoning": "Warning from one of deep learning's most respected pioneers about systemic financial risk carries significant weight. Provides important counternarrative to AGI hype with concrete investment figures.",
          "themes": [
            "AI Economics",
            "AGI Risk",
            "Industry Analysis",
            "AI Bubble Concerns"
          ],
          "continuation": null,
          "summary_html": "<p>Yoshua Bengio, a 'godfather' of modern AI, warns that AGI progress could stall, potentially triggering a 2008-style financial crash. The piece details massive industry investments including $2.9T in datacenters, Nvidia's $4T+ market cap, and Meta's $100M signing bonuses for AI engineers.</p>",
          "content_html": "<p>Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AIWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.The figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT. Continue reading...</p>"
        },
        {
          "id": "199f8acd46b8",
          "title": "Brex’s AI Hail Mary — With CTO James Reggio",
          "content": "AIE Europe early bird tickets and CFP speaker applications are now open!In 2024, there was a big question mark around the future of Brex as they cut 20% of staff among stalled growth with the space becoming increasingly competitive. Fast forward to 2025, Brex has accomplished one of the most impressive turnarounds, accelerating sales and passing $500 million in annualized revenue with European expansion in sight, dissipating concerns of a dying business. Among the internal changes that led to this successful turnaround was an aggressive shift towards AI adoption and utilization in every aspect of Brex&#8217;s business.In this episode, we sit down with Brex CTO James Reggio to discuss Brex&#8217;s culture and how they facilitated an aggressive shift towards a culture of AI fluency in addition to how Brex modernized their tech stack and apply AI agents to streamline their business.This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber.CultureTeamHeading into 2024, Brex showed natural signs of a startup scaling up as they continued increasing headcount and expanded org layers to accommodate. However, Brex realized the rate they were scaling the company didn&#8217;t match their speed of execution. Addressing this scary truth head on, Brex laid off 282 people (~20% of the company), flattened their org structure, and reduced layers of management with the goal of having leaders &#8220;operate closer to the metal&#8221; and be less siloed with only a few important cross-company priorities.Visualized from &#8220;A Message from Pedro&#8221; - 1/23/24Brex realized that speed and execution shouldn&#8217;t come at the cost of increasing headcount. Even as Brex scaled up, it can still operate similar to a startup with high-velocity.In March 2024, Franceschi described this as Brex 3.0: a new operating model designed to increase intensity and execution quality by making fewer bets, consolidating to one roadmap, and maintaining a release cadence. They doubled down on fostering a high trust and high performance team by promoting from within, bringing employees back in-person to work, and expecting leaders to operate at all levels.Brex&#8217;s team now consists of ~300 engineers and ~350 across EPD (Engineering, Product, &amp; Design). Their team is split across product domains in addition to infrastructure. Their core product domains are the following:Corporate CardCorporate Bank AccountExpense ManagementTravelAccountingIn addition to these product teams, Brex has formed smaller specialized teams like their most recent team of ~10 focused on LLMs. This has been one of their key initiatives to keep up with and innovate with AI progress. This team was actually formed by the Brex team asking themselves, &#8220;What would a company that was founded today to disrupt Brex look like?&#8221; and the answers to this question were used to decide what to work on. Currently the team is still very lean with a mix of talented AI native 20 year olds who grew up with the tech and older more experienced staff engineers who are veterans in the industry and space that Brex operates in. This team operates like a startup working 996 (9AM to 9PM, 6 days a week) and growing the team very slowly like a pre-seed startup (i.e. only hire or add people to the team when absolutely necessary)late friday from LLM teamAI AdoptionThis specialized team isn&#8217;t the only one using AI as Brex has engineering wide adoption for AI tools and coding agents. Oddly enough, the top Cursor user is actually an engineering manager. Brex strongly encourages employees to use AI tools and software that will boost their performance.This sentiment is echoed by their bold decision to not pick winners between foundational model providers or agentic coding tools, etc. What Brex does instead is to procure a small number of seats of multiple solutions and give employees the ability to pick what they want to use through ConductorOne. For example, an employee can get a ChatGPT, Claude, and Gemini license to build their own stack or even get credits to try Cursor, Windsurf, or Claude Code.This push for AI adoption doesn&#8217;t stop at engineering. Camilla Matias, Brex&#8217;s COO, has pushed aggressively to help every member of the operations organization to start rethinking their role as people who are building prompts &amp; evals to become more AI native.To make it even easier and accessible for non-technical employees, Brex&#8217;s Agent Platform is built in Retool which has enabled the operations org to do prompt refinement themselves instead of needing engineers to do it.While many startups and larger companies discuss how to use AI internally, Brex actually executed on it by making the barrier to entry for any AI tool very low instead of sending a company wide email to &#8220;use more AI&#8221;.AI Training &amp; FluencyBrex has also implemented a tangible and definable approach for AI adoption with their org-wide AI training program &amp; AI Fluency Levels which are below:source: Agent + Human Ops - 9/25/25At the end of the AI training program, every employee must categorize themselves into one of the four groups which will then help managers assign them to relevant projects to upskill their AI fluency. These fluency levels are also used in quarterly assessment to gauge their progress. While this approach may sound excessive, it is not enforced as a metric to fire employees but rather as a tangible way to encourage current employees to utilize powerful AI tools available today.Brex&#8217;s success in upskilling their employees&#8217; AI competence comes down to two things they executed very well on: 1. Make it extremely easy for employees to use (i.e. Retool platform) and adopt AI tools (i.e. open access to any tool through ConductorOne) 2. Implementing a forcing function that aligns employee incentives (i.e. promotions, performance reviews, etc.) with AI fluency.AI Fluency in HiringEven the hiring process has been adapted to align with Brex&#8217;s culture of being AI fluent. Previously, Brex had a straightforward coding &amp; system design question for potential hires but that has been revamped to a project you work on-site that requires agentic coding or else it&#8217;s impossible to finish. Candidates are then evaluated on the candidates&#8217;s fluency and competence with AI tools, how the candidate works, whether the candidate understands the AI generated code, etc.Making Brex&#8217;s culture even more clear, when this new AI native interview process was created, every employee in engineering including managers had to go through the interview. It wasn&#8217;t to fire or keep score but so that employees would also realize how they could uplevel their skills using AI which eventually pushed their AI fluent culture further.By keeping a very high bar and holding everyone to the same standard, Brex ensures every employee is treated fairly while not sacrificing on talent density. The standard that new hires are held to are the same ones current employees are held to, establishing a truly meritocratic culture.&#8220;Quitters Welcome&#8221; Recruiting InitiativeMany Brex employees have gone off to start enduring companies. Surprisingly, Brex actually encourages and celebrates it which they&#8217;ve leaned into more recently with their &#8220;Quitters Welcome&#8221; recruiting initiative focused on attracting talent that want to quit and start their own company one day. Brex has had success hiring former and future founders and have doubled down since many of their smaller, specialized teams, like the one focused on LLMs, operate like a startup. They hope to be the best &#8220;founder school&#8221; with the value proposition that future founders can learn the best by solving interesting problems with instant distribution under Brex. For example, you could build financial AI applications that are instantly deployed to thousands of customers from Fortune 100 to startups, expediting the feedback loop and learning from high quality customers.Modernizing the StackComplementing Brex&#8217;s aggressive push towards a culture of employees becoming very AI fluent was a culture of trying and adopting the most modern tools and frameworks in the age of AI. Both were critical in Brex&#8217;s turnaround and acted like a feedback loop where AI fluency would enable easier adoption for the best modern tools and frameworks which in turn would increase AI fluency.But this modernization and adaptation didn&#8217;t happen all at once. It started in January 2023 when the team created simple internal infrastructure that made it possible to deploy, manage, &amp; evaluate prompts and route to different models. As more models and technologies were launched, Brex kept up with adoption, eventually building out an entire agent platform. This platform is actually powered by the current version of the internal infrastructure built out in 2023.Agent PlatformFall of 2025, Brex finally introduced their finance agents that learn, reason, and act on your behalf that is part of their Agent Platform below:source: Agent + Human Ops - 9/25/25Below are some tangible ways Brex&#8217;s Agent Platform have streamlined operations:Automating pipeline for evaluating customer applications to get them instantly onboarded.Previously, human intervention was required for underwriting or KYC (Know Your Customer) but now a bunch of research agents can first handle all the research and then go do the work on behalf of humans to instantly onboard qualified customers.Maintaining thorough, up-to-date, and accurate knowledge about Brex&#8217;s business.Previously a big challenge Brex faced was outdated knowledge being baked into models during pre-training which would lead to very costly hallucinations and incorrect information that would pollute context for agents. As a result, Brex decided to manage this knowledge base so that every detail about the business is up-to-date and correct.EvaluationsTo ensure the quality of their agents and to prevent regressions during future updates, Brex implemented multi-turn evals where an AI agent would basically embody the end user and be given an objective to accomplish. Sometimes they&#8217;ll also pre-set an initial preamble to the conversation (i.e. couple turns will be hand written) then begin the eval in order to isolate certain behaviors and make the eval a bit more deterministic.Here are some modern technologies and tools Brex uses:MastraSurprisingly, Brex&#8217;s AI agents are built with Typescript and the Mastra framework, using pgvector &amp; Pinecone as the data stores. Their eventual adoption of Mastra goes back 3 years when Brex built their own internal LLM framework after being dissatisfied with Langchain&#8217;s lack of support for key use cases. With the hope to eventually churn off their own framework which needed constant maintenance, Brex was drawn to Mastra because its ergonomics were very similar to the one they built and it also aligned with a key motivating question they asked themselves, &#8220;What languages and technology would we use if we started Brex today?&#8221;. Brex&#8217;s existing backend code is still written in either Elixir or Kotlin, but they are always reevaluating framework and tech choices because the half life of code has declined so much with agentic coding.GreptileAnother modern tool in Brex&#8217;s stack is the AI code review software Greptile. According to James, what is most impressive about Greptile is their signal to noise ratio of comments left on a PR, &#8220;I never regret going through all 65 comments [Greptile] leaves on my diffs because it catches so many things.&#8221;SierraWhen deciding whether to build or buy, Brex focuses on whether the solution is only possible with the context they have or can be built by others. When it came to CX (customer experience), Sierra already had alignment with Brex&#8217;s primary concern: having a UI/UX (i.e. low-code &amp; more workflow oriented) that makes it extremely easy and accessible for operations and CX strategy teams to administer agents.Brex&#8217;s Job to Be DoneThere are 2 demographics that Brex primarily serves:Finance teams: they interact with agents very specific for their rolesEmployees of companies that use Brex: goal is for Brex to disappear for employees (i.e. the only thing they have to do is just use the card)The way Brex believes this problem is best solved is by giving every employee an AI agent(s) that embodies an executive assistant. In the same way that an EA has all the context and can take relevant and correct actions on your behalf, Brex wants AI agents to do that for finance teams.Brex also views their AI Strategy through 3 Pillars:Corporate AI Strategy: How are we going to adopt and buy AI tooling across the business and in every single function to 10x workflowsOperational AI Strategy: How are we going to buy and build solutions that enable Brex to lower their cost of operations as a financial institution (i.e. fraud detection, handle disputes, etc.)Product AI Strategy: How are we going to introduce new features that enable Brex to be part of the Corporate AI strategy for our customers. We want to be a solution where other companies are saying they adopted Brex for their corporate AI strategyInitial Faulty ApproachesBrex&#8217;s Initial attempt to implement this solution of an EA for every employee was the naive approach of having an agent with a variety of tools, using RAG to fetch the appropriate context.It soon became apparent that the wide range of Brex&#8217;s product lines made it extremely difficult for one agent to perform well when being responsible for everything from expense management to finding travel to answering procurement questions.Other initial attempts that didn&#8217;t work were overloading the agent with tools and context switching (i.e. &#8220;this conversation seems more about X topic so lets update the prompt to have the relevant topic in context&#8221;)Without these failed attempts, Brex would not have come to their fleshed out final approach of using subagents. In an era when technology and innovation moves very fast, the optimal path to discover the best approach is actually by trying many hypotheses to learn why an approach doesn&#8217;t work.Final Successful ApproachBrex eventually broke the problem down to using many subagents that sit behind an orchestrator. They built this orchestrator themselves since there are no great solutions on the market right now.What Brex realized was that there is significant value in multi turn conversations from the orchestrator, assistant, and sub-agent vs. a single tool call. The mental model they use is an agent org chart: sub-agents are specialists and they are conversing with each other so if more context is required, the sub-agents pass it along and then ask the user. (like how a human org chart would work)Something That Didn&#8217;t WorkAmong the many investments and experiments Brex has tried, one expensive initiative that didn&#8217;t work at all was using RL for credit decisions &amp; underwriting. Initially, Brex thought that it would be the proper approach to building a model that effectively decides like a human underwriter, but after working with outside experts and investing significant time, the end performance was actually inferior to a simple web research agent.What Brex realized was that in operations, the thing that matters most is being able to granularly break down problems and form SOPs (Standard Operating Procedure) that humans can repeatedly follow. These repeatable processes can then be audited and done in a compliant manner. Since then, Brex has seen great success using simpler approaches with LLMs for many ops use cases but wouldn&#8217;t have known its effectiveness without trying alternative approaches.Links to James &amp; BrexX: https://x.com/jamesreggioLinkedIn: https://www.linkedin.com/in/jamesreggioCompany Website: https://www.brex.comFull Episode on YouTubeTimestamps00:00:00 Introduction00:01:24 From Mobile Engineer to CTO: The Founder's Path00:03:00 Quitters Welcome: Building a Founder-Friendly Culture00:05:13 The AI Team Structure: 10-Person Startup Within Brex00:11:55 Building the Brex Agent Platform: Multi-Agent Networks00:13:45 Tech Stack Decisions: TypeScript, Mastra, and MCP00:24:32 Operational AI: Automating Underwriting, KYC, and Fraud00:16:40 The Brex Assistant: Executive Assistant for Every Employee00:40:26 Evaluation Strategy: From Simple SOPs to Multi-Turn Evals00:37:11 Agentic Coding Adoption: Cursor, Windsurf, and the Engineering Interview00:58:51 AI Fluency Levels: From User to Native01:09:14 The Audit Agent Network: Finance Team Agents in Action01:03:33 The Future of Engineering Headcount and AI LeverageTranscriptswyx/Alessio [00:00:05]: Hey everyone, welcome to the Latent Space podcast. This is Alessio, founder of Kernel Labs, and I&#8217;m joined by Swyx, editor of Latent Space. Hey, hey, hey, and we&#8217;re here with Jason Ruggiosi, too, at Brex. Welcome.James [00:00:15]: Hey, thank you for having me. Thanks for visiting from up in Seattle, where I&#8217;ve been a little bit. It&#8217;s cold up there, huh? Yeah, and we have an atmospheric river hitting the city right now, so a lot of blowing. Yeah, well, yeah, we&#8217;re getting the full-on winter effect right now. Well, you&#8217;re here to talk about the sort of AI transformation within Brex. There&#8217;s a lot of interesting tidbits that we&#8217;re going to draw from your article, but also your background. You&#8217;ve got a wide array of experience from Stripe to Banter to Convoy, and I think also mostly I&#8217;m interested in your journey as one of the rare people that have transitioned from like a mobile engineering leader to a CTO, which I think is also a bit more rare. I used to have this comment in the past where there&#8217;s a career ceiling. There are people who work on client-only things where usually they don&#8217;t hit CTO, whereas they typically promote the back-end people, the back-end clouding for people to CTO. Yeah, you know, it&#8217;s something that I hear fairly frequently because there aren&#8217;t that many folks with a front-end background to reach this level of leadership. And it&#8217;s exciting for me to be able to represent that group. But I&#8217;ll say that even though my resume kind of reflects that I&#8217;ve been more on the front end of things, it&#8217;s probably more my experience as a founder a couple of times over that. Yeah. I think that actually helped me get to this level of my career, working for somebody else, becoming CTO is very much like a leadership and like general business role as much as it is a technical role. And so I think it was more the skills that I built from starting companies and trying to build those up made me a decent fit and enabled me to get the nod from Pedro to take this on as my predecessor left about two years ago. Yeah. One thing I&#8217;m curious, you guys&#8217; commentary, this is a little bit broad, unscheduled, but a lot of startups are bragging about how many ex-founders they have.swyx/Alessio [00:01:58]: And yes, to some extent you want people with the founder mentality and agency, which is what you did to be your employees and to take initiative in the company, but also I wonder if it&#8217;s becoming anti-signal sometimes, I don&#8217;t know if you&#8217;ve thought about this. I think it&#8217;s more about the turn for me, especially when people are hiring ex-founders, it&#8217;s like, if you&#8217;re truly of the founder gene, it&#8217;s kind of hard to just stay somewhere as like an IC for too long. And then it&#8217;s like, all right, I joined this thing and then in one year, I&#8217;m like, okay, I&#8217;m going to do this. I&#8217;m back to being a founder. I&#8217;m curious for you. Yeah. What was your, I&#8217;m sure you thought about leaving and like doing another company and stuff.James [00:02:34]: In fact, that was, that was the alternative. I was considering even at the time that I got the phone call where they made me the offer to become CTO, I was thinking about leaving to go start a company. And, you know, I think what&#8217;s interesting about it, we actually launched sort of like a new recruiting and employee value proposition for Brex a couple months ago called Quitter&#8217;s Welcome, where we actually intentionally are leaning into this idea that we have a disproportionate number of folks who go on to become founders or like heads of a department when they leave our company and we celebrate that it&#8217;s actually something that I&#8217;m very proud of and that means that like we, we welcome in people who want to get a different experience. I think that there&#8217;s certainly like a lot of founders who don&#8217;t make it, don&#8217;t scale their own businesses to this, to the scale that we&#8217;ve achieved at Brex, so there&#8217;s something to be learned when they come in, and then we&#8217;re very happy to like support people on their way out. And so I actually really like hiring former founders. The one value proposition I find that&#8217;s most relevant, because a lot of the folks we&#8217;re hiring as AI engineers are kind of folks that are either like winding down their companies or considering maybe running AI startup. The thing that resonates the most with them is that we oftentimes can give them problems to solve that are interesting, problems that maybe they even want to want to like build their own startup around, but with instant distribution, right? Like that is the allure is it&#8217;s like you can come into this business and build like financial AI applications. And instantly have that deployed to roughly 40,000 customers across, you know, the Fortune 100 down to, you know, tens of thousands of startups. So that&#8217;s what is, I think, appealing to founders.swyx/Alessio [00:04:09]: But the challenge then is making sure that we set them up for success in an environment that still feels a little bit like the startup that they might build themselves versus like something that&#8217;s too corporate. Yeah, instead of doing your own company and then coming to you and be like, can I integrate into Brex? Yeah, get all the data. Yeah, exactly. How&#8217;s the engineering team structure?James [00:04:28]: Yeah, so we have about 300 people in engineering, like 350 total across EPD. And for the most part, we structure around our product domains. And so this means that Brex is a corporate card. It&#8217;s also a corporate bank account, expense management, travel and accounting. And so we actually have sort of full stack product domains that are roughly like 30, 40 people for each of those that have everything from like the low level infrastructure up to the web and mobile experiences. That&#8217;s generally the structure of our engineering organization. And then we have naturally an organization that focuses on infrastructure, security, IT. And then there are two additional centers of excellence that we&#8217;ve built that violate that org design, where we&#8217;ve felt the need to put more focus or operate slightly differently. And AI is one of those areas where we have another team of just roughly about 10 people who are focused primarily on LLM applications. And we wanted to create a bit of a separation there because the way that we were thinking about this, and this is actually something we did this summer, is we paused and asked ourselves on our AI journey towards infusing our product with AI and generating customer value. We asked ourselves, what would a company that was founded today to disrupt Brex look like? And then we tried to... To basically use the answer to that question to form this team internally. So it&#8217;s a little bit off to the side. Ideally, everybody kind of comes up to speed and contributes, you know, LLM features, but we have this sort of off on the side right now in a centralized manner.swyx/Alessio [00:06:05]: What&#8217;s the difference in AI adoption for those teams? So like, are the people on the LLM team like much bigger cursor users, clock code users, or like, do you see similar diffusion?James [00:06:15]: It&#8217;s actually fairly uniform across the entire engineering department. And it&#8217;s actually kinda funny, like one of our largest cursor users is actually an engineering manager. So like, and I think that this also just like speaks to our core value of operate at all levels where we want all of our EMs and everybody in leadership to still basically do the job that they&#8217;re managing, manage the work. So it actually is, I think the journey of getting everybody into using agentic coding was not sort of exclusive to the AI group. Yeah. In fact, I think this podcast was actually a really good justification for us. I actually set up because I called outreach to Pedro because he tweeted this. I assume this is the center of Rex&#8217;s. He says, I started a new company inside Brex to build the future of agentic finance. No BS, just building 996 and pushing production grade agents to 30,000 finance teams, now 40,000. And then he actually has like a little job description, which I think is really interesting. I&#8217;ll skip that and go straight to Brex Accelerator, grow 5x and cut burn 99% in the past 18 months. I assume that&#8217;s a mix of internal AI automation and other stuff. But basically, I wanted to put some headline numbers up front to impress people before we dig into the details. Yeah, absolutely. And you&#8217;re correct. That&#8217;s the team that we have, this AI team. What was that? Very young team. Yeah, it&#8217;s very young. I mean, it&#8217;s been really interesting. The composition of the team is very young, AI native, 20-year-olds who basically grew up with the tech, kind of paired off with more like staff level software engineers that have been at Brex for a little while, who can kind of navigate like the existing code bases and like understand, the product and the customer deeply. Like we&#8217;ve formed these really a couple of tight, tight knit pods in the AI org where it&#8217;s like three people. Generally somebody who has like more of a product, a customer focused background, that like staff engineer who knows where the skeletons are and then like a much younger, like AI native engineer who can just do things with, with agents that like the rest of us dinosaurs maybe don&#8217;t, don&#8217;t, can&#8217;t either dream of or like, or where are, I think, I think part of it is like sometimes the, the too much experience or too much knowledge of how to solve a problem and actually be an impediment to thinking differently about it and thinking about it from like an AI first lens. But yes, we, we&#8217;ve been, we&#8217;ve been slowly growing that team just in the same way that like a pre-seed startup, you want to be very, very careful about talent density and like very deliberate, like only hire when you absolutely need it. And so, yeah, at this point it&#8217;s just about 10 people. And I think it was probably four or five people. Uh, I think everybody was actually in the photo that was attached to that tweet, uh, when Pedro put that out a couple months ago. Yeah, we&#8217;ll put it up. Yeah. Yeah. Yeah. It&#8217;s a photo at 1.20 AM in a, on a Friday. Yes.swyx [00:08:53]: Oh yeah. Yeah.James [00:08:54]: Cause we, we always do, uh, we always do like Friday, Friday demos and, and like, that&#8217;s a time for everybody to get like kind of exec review time. And so, uh, Everybody&#8217;s in Seattle? Um, those folks were all in Seattle, uh, but they&#8217;re actually geo, geographically distributed. Uh, we have a couple of folks here, a couple in Sao Paulo, a couple in Seattle. Hmm.swyx/Alessio [00:09:11]: How, at Decibel we have this like AI center of excellence, which are basically the people running these teams across companies. Yep. How do you make the other engineers not feel like you&#8217;re not special? I think that&#8217;s something that I hear a lot. It&#8217;s like, Hey, you know, why aren&#8217;t these people working on all the cool LM things? And like, I&#8217;m stuck working on, you know, the KYC integration with whatever, you know what I mean? It&#8217;s like, how do you build that culture?James [00:09:32]: You know, it&#8217;s interesting. I, I thought that that would be more of a problem, but the benefit of having really optimized our engineering culture around business impact actually causes it to cut in the other direction where for folks, some folks don&#8217;t want to work on the AI products because it doesn&#8217;t have as much clear, direct, like business impact. Right now, it doesn&#8217;t, it doesn&#8217;t impact revenues directly. And so I, uh, I think folks for the most part, uh, we&#8217;ve, we&#8217;ve enabled folks who have a strong desire to work on, on, um, AI products to, to join that team. Like somebody, somebody transferred out of our expense management organization to come over there because they&#8217;re really passionate about taking like their knowledge of like policy evaluation and bringing it into the, the AI, uh, uh, team. But for the most part, I think everybody understands like how their work, uh, ladders up and maybe there&#8217;s some like friendly rivalry because like the folks who. We say we&#8217;re kind of card product, they, they drive 60% of our direct revenue. And so they, now they&#8217;re pretty happy with that. And, uh, and they don&#8217;t feel like they&#8217;re being left out. Uh, and I will also say, um, as you probably saw in this, this piece that we, we, uh, put out with, uh, first round, there is a lot of smaller applications of LLMs peppered throughout all of our product and operations teams. It&#8217;s just some of the more novel, like agentic layer that sits on top of Brex that has been put together, like in this, in this sort of isolated team. So it&#8217;s not like folks aren&#8217;t getting to, to build with LLMs or use LLMs on a daily basis. Yeah.swyx/Alessio [00:10:55]: Maybe you run people through the Brex agent platform. We&#8217;ll put the diagram in the video where you had the LLM gateway, you know, like the whole MCP layer, which you said, David, the creator of MCP right before you. So this is very timely. Um, yeah. How did you start building that? What&#8217;s the architecture?James [00:11:09]: Yeah, the architecture, you know, I, I think simple is, uh, is elegant and we we&#8217;ve had basically an LLM gateway and, and, uh, a basic hand rolled platform, uh, from the very early days. In fact, right before being tapped to become CTO, I was leading, uh, like a AI, uh, labs team internally, uh, in the wake of like the announcement of chat GPT, you know, everybody saw this through technology and said, Hey, what are we going to do with it? And so one of the first things that we did, um, I think January, 2023, that would have been, uh, was try to put together some internal infrastructure that made it possible for us to deploy, deploy, manage version and eval prompts, uh, and then be able to manage, uh, like data egress and model. Routing and, uh, have some very basic, like observability and cost monitoring, uh, in an LLM gateway. So that&#8217;s, that&#8217;s infrastructure that we stood up and it still continues to power a lot of those smaller, uh, more, let&#8217;s say like precise applications of LLMs. So like for instance, we&#8217;ve, uh, we set up a completely automated, uh, pipeline for, um, evaluating, uh, customer applications to get them onboarded instantly to Brex, which is something that used to require, um, human intervention either for underwriting or KYC. But now. We basically have a series of, of agents and, um, and particularly like research agents that will go and do the work that humans would normally do. And so that&#8217;s running on top of this, uh, this hand-rolled, uh, framework. And then for the agents on Brex that we announced in our fall release, which is like this agentic layer that we&#8217;re building that sort of sits on top of Brex and can embody workflows that a finance team would normally, uh, hire humans for. We&#8217;ve actually, uh, started using Mastro for that as like the kind of. Yeah. Primary, primary framework for, for accelerating us. We actually have built everything in TypeScript, um, which is another like technology choice. That&#8217;s, uh, that answers the question of like, what will we do if we started Brex today, but isn&#8217;t the case for all of our existing backend code, which is either Kotlin or Elixir. And then we have, uh, we have a mix of PG vector pine cone. And, uh, like, I think what we&#8217;ve seen is we&#8217;re always, we&#8217;re always reevaluating the tech and framework choices as we go, uh, because the half-life of code has declined so significantly with agentic. It&#8217;s actually quite, uh, easy or awesome for anyone else to, to kind of try on for size, a variety of different pieces of tech, to, to figure out what is going to be most ergonomic for solving the problem. So Matthew Cuellar new choice and interesting one. Yeah. I mean, I think that the main, the main reason that we adopted Mastro is that it provided the ergonomics that we were actually, uh, that the ergonomics of master are quite similar to the internal, um, LLM framework.James [00:14:15]: Swyx and Alessio So like we did, I&#8217;m trying to remember because this is now ancient history. We evaluated a link chain, turned off of it, built our own thing. And then as we were looking, we kind of want to deprecate this internal framework that we built because at the end of the day, it&#8217;s not leveraged for us to maintain that. And Master ended up fitting the bill for the feature set that we were looking for. And I think what&#8217;s been interesting is about half of the applications that we&#8217;re building right now on the agent layer are running on Mastra. And then the other half are actually still running on like yet another internally developed framework, which is a framework that&#8217;s focused more on networks of agents. So sort of multi-agent orchestration versus more like strict, like, you know, single turn or like workflows, which are easier to use, like either Landgraf or Mastra. Tell us about your multi-agent framework. I mean, that&#8217;s what are the design considerations and why is this the first we&#8217;re hearing about it? Yeah, yeah. So it&#8217;s funny. A big, big reason why we haven&#8217;t written more about this is that it continues to evolve quite a bit. And I feel like we actually had a blog post that we were going to put out in conjunction with the fall release talking about how we built this. And by the time that we finished, you know, the blog post and had all the package ready, it was already like halfway outdated. And so the way that this has started to emerge is this multi-agent network approach to implementation was when we started to implement it, it was like, you know, when we started out, we were trying to scale up our sort of consumer-grade Brex assistant. So if you think about like Brex and our customers, there&#8217;s really like two very broad personas that we serve. We serve members of a finance team who are generally like going to be doing it like in roles like accountant or controller or head of T&amp;E. For those folks, they are going to be interacting with agents that are much more specific to their roles. But then the other broad cohort of users we have are like employees of companies. And so we&#8217;re going to look at like our client, our staff, right? And then we&#8217;re going to look at the the past year, and then we&#8217;re going to look at the companies that have deployed Brex. So you know, you go join a new company, that company uses Brex, you get your Brex card. And our goal for employees is for Brex to completely disappear. Like the best UI UX for Brex is just the card, like every single thing that you have to do in the software beyond just swiping the card is like an opportunity for AI to to eliminate some work for you. And They have an EA and she knows enough about me. She has access to my calendar. My email has all the context on when I&#8217;m traveling and for what business purposes. And so she&#8217;s basically able to do everything that I would be obligated to do in Brex, be it like booking travel or like doing expense documentation. And so what we wanted to do is we wanted to build like that EA connected to the same data sources and see if we couldn&#8217;t simulate that behavior so that, you know, you basically your interface to Brex&#8217;s SMS and the card. And when we started building that out, you know, the most naive like architecture for that would be to have an agent with a variety of tools and maybe maybe do some some rag to ensure that it has like appropriate context for the conversation. But what we were finding is that the wide range of different product lines that exist on Brex made it difficult for one like agent to perform well, being responsible for everything from like expense management to finding and booking travel to answering. policy and procurement questions. And so that&#8217;s when we started breaking down the problem and into into a variety of sub agents that sit behind an orchestrator. And obviously, this is something that can be implemented using LandGraph or Master even has the notion of these as like network switches and data. But what we found is that it was easier for us when it came to being able to build evals for the system. We kind of just hit the eject button and built our own framework, which is one in which we have agents that are able to. Basically, DM with other agents and have multi turn conversations amongst themselves to coordinate to, to complete a task to, or like to complete an objective. And what&#8217;s what&#8217;s been nice about that is it means that like, you can have your Brex assistant is like one single, one single like point of contact between you as an employee and the Brex product, and then behind your assistant, if the company has like expense management turn on you have that if they have reimbursements as another agent for that if they&#8217;re they have travel. They actually also then facilitates like, our conception here is that, you know, it&#8217;s like generally like software encapsulation patterns taking like sort of projected into the agent space. It also makes it easier for us to have like the team that owns and understands travel, like be the ones to go and iterate on that without needing to worry about like regressing the total system, or needing like one team to own every single possible active action you could take as an employee. And I&#8217;ll say that, like, I&#8217;m still of the mindset that somebody who&#8217;s like, you know, I&#8217;m not willing to do that anymore, they will build a great framework, and we they have ultimately migrate to it, but or might be us that we ultimately open source this right like but um, but for us, like this is, this has worked out quite well. And like, loo of like a couple other approaches that we tried along the way that just didn&#8217;t perform well, which is to overload the the agent with a variety of tools or intellectual like context switching where we try to say, Oh, this conversation looks like it&#8217;s more about reimbursement. So let&#8217;s like update the prompt with more reimbursement context. Like that was, that was another approach that we kind of did that. that didn&#8217;t perform as well as actually having a reimbursement agent that it would collaborate with.swyx/Alessio [00:19:46]: What about MCPs as sub-agents? Oh, yeah. That&#8217;s another pattern.James [00:19:50]: The key thing there is that there&#8217;s actually a lot of value in having multi-turn conversations from the orchestrator or the assistant to the sub-agent, whereas a tool call is basically just one RPC. And so oftentimes what will happen is, let&#8217;s say the user reaches out to their RECS assistant and says, hey, how much am I allowed to expense per person for dinner tonight? I&#8217;m taking my team out. And your assistant&#8217;s going to then reach out to the policy agent. Maybe the policy agent needs to know, in order to answer that question, maybe it needs to know whether this was a customer event, a team event, or whether you&#8217;re traveling. And so it may actually send... Instead of... It can&#8217;t just answer the question, so it&#8217;s going to reply back to the assistant and say, hey, I need you to ask this clarifying question. And so then the assistant will return to the user, ask clarifying question, and they&#8217;ll basically have this sort of multi-turn conversation across multiple agents versus it just being encapsulated in a single call and response tool call. And so there are still... All the sub-agents have a ton of tools, but I think of the MCP and tool usage as being the interface to all of our... Conventional imperative systems, not the AI space.swyx/Alessio [00:21:07]: Yeah, that&#8217;s the conversation we were having earlier, whether or not it should be an agent-to-agent call as well. Yeah. Or like, yeah, there should be like a chatback.James [00:21:15]: Exactly, exactly. And that&#8217;s the thing. It&#8217;s like, okay, and one of the ways that we actually grafted this into Nastro before we built our own framework was to make every sub-agent a tool. And then the input was just natural language. The output was natural language. And if you needed to have multi-turn... You would basically just put the full, like, prior conversation in as you kept calling the sub-agent as a tool. And it&#8217;s just like, at that point, you&#8217;re like, okay, the ergonomics are kind of... The framework is fighting me on this. It&#8217;s actually helpful for us to basically conceive of it as an org chart. And like, it&#8217;s the agent org chart with, you know, my EA is DMing other specialists and having brief conversations to support me as their client. Yep. That was a really good deep dive. Thanks for indulging. I feel like you guys are not afraid to make your own tech, which I think is a competitive advantage. I really like that culture. Maybe we should go a bit breadth first as well. Of course. Because I think we also deep dive a little bit too much in one area. There&#8217;s, and we&#8217;ll put up the chart, but I&#8217;m also very interested in like the sort of internal agent stuff, the operational stuff, and just the general platform scope. So please feel free to just like go into your spiel on it. Yeah, of course. So one of the things that I was trying to do at the beginning of the year... As CTO, you know, I think it really fell to me to articulate what our AI strategy was as a business. You know, every board of director was, you know, or every member of our board is like, hey, what&#8217;s your AI strategy? And while we were doing a lot of things, we&#8217;d literally go, he&#8217;s got it. Well, yeah. Yeah, and if I didn&#8217;t, I&#8217;d be in trouble. I think he also was counting on me given that I was doing the AI organization before CTO to have... That&#8217;s true. But a big part of it was like we were doing a lot with LLMs. It was more like these little one-off features and, you know, hey, like maybe mix in some suggestions here or maybe do a little bit of ops automation over here. But it wasn&#8217;t easy to kind of create like a verbal framework of all of these investments. And without that framework, then we weren&#8217;t able to like set a vision or a roadmap for investments. So what we did at the beginning of the year is we took everything that was going on, as well as all of our ambitions, all of the good ideas, as well as like the problems we were trying to tackle. As a business this year, throw it all on the table and see if there were some ways to cluster it into a framework that made sense to the business, to our board, to ourselves. And we came up with, I think this is not particularly novel, but it&#8217;s helped us quite a bit. We have like three pillars to our AI strategy. We have our corporate AI strategy, which is how are we going to adopt and like buy AI tooling across the business and basically every single function to be able to 10x our workflows. And we have our operations. We have our operational AI strategy, which is how are we going to buy and build solutions that enable us to lower our cost of operations as a financial institution, because I think it&#8217;s fairly intuitive. Like financial institutions like ours face a lot of regulatory expectations and there&#8217;s just like a high ops burden for running our business. And so it&#8217;s sort of like a lot of kind of internal use cases, like being able to do like fraud detection, underwriting, KYC, be able to handle dispute automation on card transactions. Those types of operational investments are our ops AI pillar. And then the final pillar is the product AI pillar, which is like, are we going to introduce new features that enable Brex to be a part of the corporate AI pillar of our customers? It&#8217;s like we want to build features and be a solution that somebody else is saying to their board, hey, we adopted Brex and this is part of our corporate AI strategy. And so it&#8217;s kind of has this nice little feedback loop and we basically within the company split. You know, did a little bit of divide and conquer where folks in IT and on our people team were more or less spending more of the effort driving on corporate AI, really like looking for making the procurement decisions, like creating a culture of experimentation where we spotlight and incentivize people for trying to sort of improve their personal workflows using AI. And then the pieces that I&#8217;ve been more involved in have been operational and product. And we were just talking about products here, which is like the agents on Brex and stuff. But I think that the operational AI. Investments have been some of the most sort of immediately impactful to the business because we have hundreds of people who work in our operations organization. And it&#8217;s actually something that differentiates us because our CSAT and the quality of our support and service is very, very high, something we&#8217;re very proud of. And so trying to figure out how can we automate a significant portion of this and use LLMs in a way that doesn&#8217;t degrade the customer experience and then also kind of addresses, like, what is the future of the roles of the people who we already have working full time for us? So this is where Camilla, our COO, who kind of co-wrote the piece with First Round with me, she&#8217;s been leaning really aggressively to help every member of the operations organization start rethinking their role as being not people who kind of execute against an SOP, but are people who are going to, like, build prompts, build evals and, like, become more AI native and, like, the way that they&#8217;re going to be used in the future. And so a lot of the engineering we&#8217;ve done has been to enable folks, say, in fraud and risk to be able to refine prompts and add additional automation to their workflows. Yeah, and it&#8217;s the secret fourth pillar, the platform. Yeah, yeah, exactly. That is the thing that ties it all together exactly, is the platform. And I think what&#8217;s been really nice is that even though the platform is kind of a loose term because it consists of a wide... variety of technologies, as I said, like, we haven&#8217;t been too religious or dogmatic about everybody needing to be on one particular thing, what we&#8217;ve seen is that by making a variety of sort of ergonomic options for building with LOMs available, it, like, really has made it easier for us to make a quick leap forward on operational AI. Like, as soon as we put our mind to it, we said, like, look, no, we want to hit 80% automated acceptance rate for all startup and commercial businesses that apply for Brax. Like, we want a decision within 60 seconds. It&#8217;s fully touchless, no humans involved. We were able to break that down and then actually build the agents, build the tools on top of that platform really quickly. And a lot of those tools are the same tools that our product AI agents use as well. I was pretty sold on the Conductor. I don&#8217;t know if this is under exactly that bucket, the Conductor One provisioning command. I was like, yep, I want that. Yeah, that was actually, I&#8217;d love to talk about that. So that&#8217;s actually on the corporate side. And I think that this goes back to maybe another intuitive, but I&#8217;d say, like... I think that this is a really bold decision that we made, which is that we&#8217;re not going to, we&#8217;re not going to try to pick winners in the horse race between the foundational model providers or the agentic coding tools or like basically anywhere where there&#8217;s, there&#8217;s an active horse race. What we do instead of like trying to pick a single solution is we will procure like a small number of seats, like multiple solutions, and then we&#8217;ll give employees the ability to pick whatever one they want to use. And so, for instance, like we allow employees to... basically go to, in Slack and use Conductor One to get a ChatGPT, a Cloud or a Gemini license. And basically you can just like build your own stack where you pick your, you pick your like chat provider as a dev. You can pick, you know, between like Cursor, Windsurf, Cloud Code, Credits, like, and you can basically craft your stack to your preference and easily switch between them. And what that does for us too is when we&#8217;re going to, like, obviously we have sort of enterprise agreements in place for all of them for the sake of like the, you know, the privacy. we have some non-training guarantees, but it&#8217;s fun because when we go to renew these contracts, we can basically resist the need to like do a wall-to-wall deployment. We can say, Hey, look, like usage trends, our employees are voting with their feet. They&#8217;re voting with their dollars and maybe Amur tools and is as hot as it was a year ago. Jason Wonguaiohnaipai, Zephyr Access Group CEO &amp; OEM Partner, Swyx does this give you a dashboard of what people are choosing? Yeah, actually, we look at that. We were looking at that as we&#8217;re going into budgeting for next year. I would love to see that. What&#8217;s, you know, anything that&#8217;s like really up, anything that&#8217;s really down? It&#8217;s fascinating how different the landscape is every three months. And I think one of the interesting challenges we had early on was getting folks to just like try these tools, try to incorporate like agentic coding. You know, like early on, I say like 12 to 18 months ago now, like get folks to just take the time to try a new workflow. And now at this point, I think what we&#8217;re seeing is like, even if, you know, a new model hits the same, like when Codex came out and everybody was like, oh, Codex is better at CodeGen, but it&#8217;s a little bit slower. I find fewer folks are like kicking the tires on new things because like they&#8217;re just so comfortable with the ergonomics of their current work. And I think that&#8217;s a big part of the workflow that, you know, some folks are just like, I want to stick with Cloud Code because I know it now. I&#8217;ve been working with it for like nine months, so I don&#8217;t need to keep switching. I don&#8217;t need, I don&#8217;t feel the incessant need to keep trying new things because I&#8217;ve gotten, I&#8217;m an iPhone person and I&#8217;m just like going to stay with an iPhone even, you know, even though there&#8217;s some really sexy Android hardware out there.swyx/Alessio [00:30:37]: Do you have one of the big numbers, like 80% of all of our code is written by AI or, but how do you measure it internally?James [00:30:44]: Yeah, no, not really. We, I mean, what we do is we&#8217;ll... We&#8217;ll measure like the attributions on the number of commits that have the like co-authored with, and we pull some of those stats, but I don&#8217;t index have like, in fact, I don&#8217;t index on those at all. I don&#8217;t, and honestly, like I, I don&#8217;t know how I, in honest, like honestly calculate that number. Yeah, I agree. Yeah, and so, so I, and the thing that, the thing that we&#8217;re really just, you know, we&#8217;re at the point now with the, like our AI agentic coding journey where now we&#8217;re trying to solve the second order of factors. So like a little bit too much slop, maybe a little not enough. Yeah, exactly. Not enough like rigor and code reviews. We&#8217;re trying to, the adoption is there. And now we have to figure out like how to mature in our usage of these tools. So that we know quality or like long-term maintainability doesn&#8217;t suffer. As well as like maybe one of the other facets of being able to generate a lot more code more quickly is like the, the drift between team members. As far as like the. Understanding of the, the, the code that&#8217;s in their services increases is like everybody&#8217;s moving faster and more independently. It, that is another sort of risk that we&#8217;re starting to see, like, you know, an incident response where folks don&#8217;t know, they don&#8217;t know a service as well as they, they used to, because it&#8217;s changed so much in the past couple months because everybody&#8217;s moving more quickly. Yeah. This has been a major topic for me this year on code-based understanding and slop, because obviously it&#8217;s so much easier to generate code, but then now we have to review it and to some extent you can&#8217;t. really fight AI with more AI. You can&#8217;t just be like, oh, just throw, throw an AI reviewer on their AI code and you solved it. Uh, and so, so you do need to just scale human attention. And I, I think that&#8217;s something I&#8217;ve been pushing a little bit in terms of like, well, you&#8217;re, you&#8217;re just going to like every engineer is just going to own more code. Yup. Period. And, uh, and be parachuted in and be expected to ramp up and be, be productive and also fix bugs. And if you&#8217;re on, you know, pager duty or whatever to, just because I mean, everyone&#8217;s going to try to be more efficient. And you&#8217;re supposed to see ROI productivity, because if you don&#8217;t, then what&#8217;s the whole point of this? Exactly. Exactly. And I, and I think it&#8217;s funny, you&#8217;re going back to the point of, you know, you could, you could add AI on top to solve the problems that the AI introduces and there, but you just keep you, that&#8217;s like an endless chain. Uh, and so. Well, I mean the, the, the, the code rabbits of the world, the graphites of the world would say, yes, actually you can. And so that&#8217;s the little bit of the tension there. Yeah. You know, I, I, uh, I&#8217;ve been thinking a lot about how the craft of Avengers. Engineering is evolving and, and I will say that I feel further away from being able to predict what, what it looks like than I, I did this past summer when I spent a bunch of time. Um, I actually basically went on leave for a month and joined the, um, joined the, the, the team that, uh, the, the AI team that we were building just to go and build alongside them. I felt like it was really important for me to deeply understand, uh, the problems in the tech. Uh, but, and so that was me. I was, I was, you know, writing, pushing code, um, effective. And, uh, and I, I went through so many different moments of realization of like, oh my God, this is going to change everything to, oh my God, this is just amplifying all the good and the bad in the industry, uh, to, oh my God, engineers are not going to have a job anymore to, you know, it&#8217;s like, and so I, I don&#8217;t have any, like, I felt like I had all the predictions back then. And at this point now, I&#8217;m just very interested to watch the, the phenomenon continue to unfold in front of us. And, uh, I will say. I was chatting with a bunch of really. Right. Uh, you know, college juniors and seniors at a dinner we hosted last night. And, um, all these folks are about to enter the industry, basically having kind of come up in the, the era of agentic development and LLMs. And I asked them like, so what is your workflow when you&#8217;re like building, uh, like building a project? Uh, how do you, how do you use agents versus like when you decide you&#8217;re going to actually just write code by hand? And I was surprised to hear the consensus was that most people there were using agents to collaborate on. Like building a design document and like collaborating on the architecture of the solution that they want to build, and then they&#8217;d be asking it to like emit, uh, you know, a doc or an implementation plan, but then they&#8217;ll go and write a lot of the code themselves still. Uh, so it&#8217;s a little bit more of the, the, uh, the rubber duck co-architect, uh, uh, use case that was most prevalent in that group. And I, I was very surprised by that. I&#8217;m impressed. The kids, the kids are all right. Yeah, I know. No, they still want to, they still want to actually write the code themselves. It&#8217;s interesting. Yeah. What we hear from like the Gen Zs.swyx/Alessio [00:35:18]: That open the end, they, they just YOLO everything into code S and yeah, I would say most of the code I generate is like, yeah, but, but it&#8217;s been a lot of time on the doc. It&#8217;s curious, like when you&#8217;re like younger in your career, it&#8217;s like, you don&#8217;t really have all the mental models of the different patterns to instruct. I feel like there&#8217;s like over reliance, especially if you&#8217;re doing the design doc, you know, I, I feel like most of the senior engineers will spend more time on that. It&#8217;s like even things like, you know, what column should you index, depending on, you know. What queries we usually run on this table and things like that. It&#8217;s hard for any AI to know that. Right. You know, and it&#8217;s like, I feel like the, the role of like the more senior engineer should actually be more of this. It&#8217;s like spending time teaching the AI and then the AI can teach the junior people in a way. Yeah.James [00:36:03]: Yeah. And it, it, everything, everything looks like mentorship and management at the end of the day. Right. It&#8217;s like, you&#8217;re breaking down tasks, you&#8217;re, uh, you&#8217;re supervising work, you&#8217;re giving feedback. Like it&#8217;s, you know, it&#8217;s basically management. Except that there&#8217;s. Agents are really bad at memory still, like they basically have zero memory and, and it&#8217;s, it&#8217;s, it&#8217;s the end of 2025. What&#8217;s going on? Yeah.swyx/Alessio [00:36:26]: Yeah. What&#8217;s your internal stack for like, uh, uh, preferences. There&#8217;s like, kind of like, you know, explicit preference you can use with, uh, you know, agents that MD and all that stuff. Uh, there&#8217;s implicit preference with linter rules and things like that in a way where it&#8217;s like, it just happens. You don&#8217;t have to tell it. How do you structure that? Oh, no. You&#8217;re talking about for agentic coding or memory? Yeah. Just like. Platform. Yeah. Yeah. For like the coding specifically, it&#8217;s like, and then we can kind of talk about, you know, the whole Brex platform. Yeah.James [00:36:53]: Just, just nothing, nothing special. Just a lot of, um, like explicit rules. That MD files. Yeah. And then we have, uh, and we, um, in linting, we still have like traditional linters in place for the couple of different language full chains. And then we&#8217;re, we&#8217;re, we&#8217;re big fans of reptile and we use them for basically all of sort of the, um, smarter than linting, uh, like agentic code review. Uh, that&#8217;s been the one solution that we&#8217;ve aligned around. And that has served us extremely well. Yeah. Can I go to reptile? Yeah, no, we&#8217;re, we&#8217;re huge fans there. They&#8217;ve built something really impressive. And I think the thing that constantly blows my mind about it is, um, the way that they&#8217;re able to just have a really impressive signal to, to noise ratio. Like the, the comments that it leaves are very, very high signal. Uh, like never, I never regret going through all like 65 comments that leaves on my, on my diffs because it catches so many things. Yeah.swyx/Alessio [00:37:46]: I found the code. X review to be really good. I don&#8217;t use code expert code generation, but like the review product is like very good for some reason. Um, I used to have, when I was working in rails, there was like this project called danger systems. Oh yeah. It was kind of like a semantic linter. Exactly. I feel like there should be more of that now. It&#8217;s kind of like the rules are one thing, a generation, but I want something in my CI that is like enforce these rules and call out where they&#8217;re broken. And then I can just copy paste that in an agent.James [00:38:12]: But yeah, when we, when we started building this, this new agent, um, code base, like, cause. As we were saying, like we were answering the question, what would you do if you built a, you know, a Brex disruptor today? And it&#8217;s like, it wouldn&#8217;t be to pick Kotlin and Elixir as the backend. And, uh, and so we actually went with the full like TypeScript stack and we, we were building on all like public interfaces and, um, really trying to make sure that this agent layer was, uh, like arm&#8217;s length from, from the, the good and the bad of, of the core of our product. And, um, and one thing, I think what we did early on. Yeah. And I don&#8217;t actually know if this is true because again, the team keeps sort of iterating, uh, but we were having good, uh, good luck using, um, cloud code, like in a GitHub action to basically go and do, uh, do more of that danger style like code review. So have a, uh, a prompt for it that went through all of the different facets that were more conceptual versus like rigidly enforceable by a linter, uh, and have it leave a big comment at the end with, um, you know, your conformance to the idiomatic coding patterns of the, of the new repo. I want us to spend some time. You said you wanted to devalve on operational agents, uh, customer support, onboarding, KYC, fraud, delinquent account disputes. Uh, this is, I imagine the bulk of it, of, of the work anywhere where there&#8217;s a good story about maybe, um, when you started out, it was, it was going to be this way. And then you discovered through building or through customer contacts that it had to go a different direction. And so that difference in beliefs is something that people can learn from. The thing that immediately comes to mind is that we, uh, we believed at the beginning that using RL for credit decisions would actually be a, like, would be the way that we would end up or like credit and underwriting, like how much of a, of a limit should we give to this business, um, that reinforcement learning would be the way that we would go about, um, building a model that effectively would decision in the way that, um, a human underwriter would. Yeah. Yeah. Yeah. Turns out that it was, we made this big investment. We were working with some outside, uh, like the, like a company, uh, that specializes in this and the performance we ended up getting was inferior to just building a, a, like a web research agent. And so, so I think what, what we took away, what, what has been most evident in operational AI is that in operations, you need to be able to break down problems really granularly and be able to form SOPs. That humans can. Root, repeatedly follow and, and thus can be audited, uh, because so much of, uh, the responsibilities and operations is to, uh, is to have auditable, repeatable processes that help to ensure that we&#8217;re operating in a compliant manner. yes. Um, and that actually translates just so cleanly to LLMs that we haven&#8217;t needed to use too many sophisticated techniques in, in operational AI. Uh, it&#8217;s been a, it&#8217;s been relatively simple, like a few tool, uh, like agents or maybe even a lot of them. problems can be solved, which is like a single turn chat completion. And so the fact that we didn&#8217;t, well, we did one sort of attempt to over-engineer and use more sophisticated techniques. And we discovered that, in fact, the solutions are a bit more plain and less technically sophisticated. The challenge is really articulating and refining prompts to reflect the execution of the SOP and reflect all the sort of institutional knowledge that isn&#8217;t written down so that agents can properly replace like the humans or the contractors we would have making these decisions.swyx/Alessio [00:41:45]: How do you decide what is worth like spending a lot of time building versus what you think some of these models are just, because some of these tasks are so generic, they&#8217;re not really about Brex. Yep. Like you can assume the models will be good at it versus some of them are like very specific to you.James [00:41:58]: We kind of prioritize like the tasks that are most common for the broadest number of customers. And some of them are fairly... Fairly intuitive, like being able to research a customer to look to assess like legitimacy of the business and whether that business would fit our ideal customer profile for onboarding, because there&#8217;s certain types of businesses that we either legally cannot serve or we are not comfortable being able to serve. So that&#8217;s the type of really kind of basic research and like a relatively straightforward problem that isn&#8217;t hyper Brex specific. The things that are a little bit more specific to us or companies in our sector would be preparing documentation for a network card dispute. Like if you go and dispute a transaction on your personal card, you will provide evidence to your card issuer. The card issuer then has to put together like a three or four page word document that goes to the card network and then eventually goes to the acquiring bank. And all of that is like much more specific to our business. It&#8217;s a huge operational overhead for us. And that&#8217;s something that we we decided to automate later because it&#8217;s not as it&#8217;s not on the critical path of like serving the vast number of our customers. Like disputes are expensive, but not very common operational process. And so they&#8217;re lower on the stack. And I think we&#8217;re getting there right now. But this year has basically been us just kind of like looking at every single process, this kind of stack ranking. And I will say, like the thing that got us started down this path was we wanted to be able to do a lot of things that we didn&#8217;t want to be able to do in the first place. We wanted to expand our ideal customer profile to support more business, like a wider variety of commercial businesses, which tend to be businesses that aren&#8217;t growing as quickly. So they&#8217;re not like tech startups, which have a lot of growth, and they&#8217;re not usually like they&#8217;re not enterprises, which also tend to have a lot of growth. It&#8217;s more like a lawyers, a law firm or a dentist office, these types of like solid businesses that we should be able to serve and underwrite. But the cost to to onboard them and the cost to serve, if you have all all the humans in the loop, make them ROI negative. And so that was the first sort of use case of of AI within our ops ops organization that then led to us really understanding we could automate much more than that. Is this Berks going back into SMBs? Ah, that&#8217;s a good question. Yeah, yeah. So never, never let let let that die. You know, we think the way we&#8217;ve thought about this is we want to always like offer our product to customers where we believe we have a like an offering that is well suited to the needs of those businesses. And I would say that still for very small businesses, our offering isn&#8217;t it&#8217;s not built for that. It&#8217;s built for it&#8217;s built for companies that have some degree of scale. Typically have at least sort of. One person, if not a couple of people in their finance team. So we consider these to be more like the the commercial segment. And so it rhymes with with SB. But our approach back then was was a little bit more naive. And I would say we also we were just going for volumes, like a volume game. There are internal controls were not as strong. We didn&#8217;t have as much experience like underwriting those businesses. And so it was really ended up being a case study.James [00:45:46]: Swyx and AlessioJames [00:46:03]: Swyx and Alessio It depends on the scale of yourself as a business when you use these terms.swyx/Alessio [00:46:28]: And all of these things are built in the Brax agent platform, like all these automationsJames [00:46:32]: that people build? Yes, exactly. In fact, most of the operational AI is running on that original platform that we have. One element of it that I didn&#8217;t mention is that most of the UI UX for this platform is built in Retool. And so you can basically go into Retool and there&#8217;s a prompt manager, a tool manager, an eval manager, and that&#8217;s sort of where much of this was built. And the goal with that was, again, to make it more accessible, more ergonomic to get started, but a secondary effect of having a more visual set of tools for this is it&#8217;s enabled members of the app&#8217;s organization to go and do prompt refinement themselves. So you don&#8217;t need engineers to go and refine the prompts or even test new foundational models when they come out. I think that&#8217;s another fun thing. When a new model drops, folks will go into the platform and basically run the evals on the new model and kind of see, can we get better performance here? Or does this have different latency or different cost characteristics? Yeah, you want the domain experts or the people directly using the tool, not the engineers who are somewhat removed from the tool. I do want to highlight to listeners that a lot of the Brax agent platform are just things that every company should have. Basically, problem management system, which we talked about where the domain experts are doing it, multi-model testing, evaluation and benchmarking frameworks, API integrations for automated workflows, NCP-based architecture shared with Brax&#8217;s external AI products. This one is obviously very Brax-specific. One thing I did want to highlight that I was semi-impressed by, because nobody, people very few rarely talk about this, is knowledge base for understanding Brax&#8217;s business. So do you want to expand on that? Yeah, and this is an area where we&#8217;ve only scratched the surface. This year, but a big challenge that we face is that the world knowledge or the knowledge that&#8217;s built into the model about what GPT-5 thinks Brax does and how it thinks our business operates is actually quite different from what our business offers today or how our product works. And so we&#8217;ve had to work on building a corpus of product documentation, process documentation, and curate this set of information. What do you think, Zach, was the most of your experience building this model? We really needed to have a lot of communication to basically ground a variety of our LLM applications, including like that Brax Assistant, which is like the, you know, the assistant that employees will talk to is like we don&#8217;t want it to hallucinate features that we don&#8217;t have, or like give wrong information there. And similarly, some of the operational agents need to be grounded on what our ICP is, because if you ask, you know, ChatGPT-5 right now, like, what types of businesses does Brax honor? Thank you so much for having me. Thank you so much.James [00:50:25]: Thank you so much.James [00:50:52]: Thank you so much.James [00:51:20]: Thank you so much.James [00:51:50]: Thank you so much.James [00:52:21]: Thank you so much.James [00:52:59]: Thank you so much.James [00:53:20]: Thank you so much.swyx/Alessio [00:54:02]: Thank you so much.James [00:54:20]: Evals that are blocking because they would indicate like a regression, an unacceptable regression. So these tend to be just accuracy related evals. But then there are others that are more about like tone and coherency and these types of things where they&#8217;re more subjective. And we were just looking at those over time as a metric. But the team is actually interesting. I think we&#8217;re going to get a big update on like how the team is thinking about evals tomorrow and like our Friday, our Friday review. So it&#8217;s, this is an area where I&#8217;d say the largest challenge, like the largest change we needed to make and how we were executing sort of as like a lab or an incubator back earlier this year to like where we are now, where we&#8217;ve, we&#8217;ve shipped and like we&#8217;re trying to, to increase the rigor has been around like avoiding regressions and having more and more increasingly robust evals.swyx/Alessio [00:55:14]: Yeah, I&#8217;ve worked with a company called Verisai that does user simulation. And I think like that&#8217;s what&#8217;s been interesting. Some of these things they just don&#8217;t expect, like the customer does not expect the model to do, but they want to track the saturation of the model in a way, if that makes sense. And I feel like most companies know what they don&#8217;t want to happen, but it&#8217;s almost like they don&#8217;t, they cannot quite articulate, oh, I want in the future the model to be able to do this. They can do it today, but I&#8217;ll keep running this eval.James [00:55:41]: That&#8217;s actually really, really interesting to me. And I, I&#8217;m going to take that away and start thinking about this because. There are, there are going to be certain, I mean, we already seen this where, where users will ask the assistant for help with things that we don&#8217;t support yet, or we haven&#8217;t implemented yet. It&#8217;s like, those are opportunities actually for us to build a, like effectively write a test that&#8217;s going to be fail, like failing for, for weeks or months and then eventually we&#8217;ll go green, but as a way for us to actually kind of show like the progression of sophistication of the assistant. I really, I really liked that as an idea. Yeah. I wonder how you also catch hallucinations and things that it does. That&#8217;s usually the, that&#8217;s usually the problem is it, you know, it&#8217;ll, it&#8217;ll, it&#8217;ll pretend like it can assist with something and it&#8217;ll, uh, like one thing that is really annoying that has been tough to, um, to prevent is that the, the assistant, because it is used to speaking to other agents, um, that can support it in like accomplishing various tasks. If you ask it to, to help with a task that it thinks it probably should have an agent to, uh, uh, to, to work with, it&#8217;ll just hallucinate that it, you know, it&#8217;s like, oh yes, I&#8217;ll, I will like, you know, I&#8217;ll reach out to the finance team on your behalf to, uh, to pass this question along, but it&#8217;s not doing anything. There&#8217;s like no finance team. There&#8217;s no way for it to do that. This is something that comes up a lot. It&#8217;s like, would you like me to ask the finance team? And there&#8217;s no, there&#8217;s no actual tool for that. Do you put guardrails for that? Yeah. Yeah. That was, that was something that we had to, uh, Like a regex? Oh, no, we don&#8217;t. I think we&#8217;ve been able, we&#8217;ve been able to just beat that out of its system with a system prompt. But, uh, but the, we don&#8217;t have as many guardrails in place right now, just around a couple of like potential, uh, like things that could get us into trouble. Yeah, really extreme ones. Yeah. I just, yeah, it&#8217;s surprising when I, I guess two years ago was first kicking around the idea of all these things. I would have said that probably guardrails would be more prevalent, especially in finance use cases, but surprisingly they&#8217;re not. Yeah. And that was actually part of what we, that was like a feature, I believe we built in the LLM, LLM gateway early on is like the, the sort of last chance, like, uh, you know, like, uh, um, Like hardguarded Yeah, exactly. Here&#8217;s some regexes that will just kill. Yeah, exactly. Or just, you know, in the way that like, if you go away a field on a ChatGPT, you just get like the inline 500 error. It doesn&#8217;t even tell you that it can&#8217;t tell, but just like craps out. Uh, like we kind of built a couple of those circuit breakers or like the ability to put those circuit breakers in and, and I don&#8217;t, I don&#8217;t believe that we&#8217;re using them for anything. One last thing I want to get your thoughts on was AI fluency levels, which you guys have a framework of user, advocate, builder, native, and everyone goes through it, including you. Including Camilla. And I just think it&#8217;s interesting. I think it&#8217;s a model that other people are thinking about adopting, but they&#8217;re worried about ruling it out that everybody&#8217;s going to be bad. And then, and also like, how do you have like this in-house training course that you keep up to date? Yeah. Just tell us more about it. Yeah. So in, in the operations org, uh, they&#8217;re actually more ahead of even engineering on this front, as far as like trying to create, um, create like, you know, like, you know, like learning pathways for this. Uh, and I think that part of the reason why they&#8217;re ahead of us is that in operations are much more, uh, they have to be elaborate, uh, training at, at scale. Like training is a big, very big part of, um, of how people build the aptitude around their, their job function within ops. Whereas like in EPD, a lot of it is sort of, uh, getting hands on building experience, like going a lot and getting mentored, getting code review. But, uh, it&#8217;s been really neat because I think we&#8217;ve really like, we created an environment.James [00:59:14]: Um, by speaking openly, uh, about the, the transformation that we saw would happen in this industry towards AI sort of displacing a lot of, um, a lot of the operations and CX roles. And we were just honest about it. And I think what, what in the same breath that we said, Hey, a lot of these job responsibilities will go away. We also said, we don&#8217;t anticipate that meaning that your job has to go away. It&#8217;s just that your job has to change. And so the, the fluency framework and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and and then, uh, the like the training and support and like the positive sort of culture where we celebrate people making progress has been really helpful for like avoiding a culture of fear or like, oh, you have to do this, or you have you&#8217;re going to get this is going to go in your performance evaluation. I think the, it does it well, it&#8217;s not like his road is like, oh, like, what is, you know, what is the, like how much are you using AI and is it, is it enough? It&#8217;s, it&#8217;s more, I think we&#8217;ve built a pretty positively framed culture where we&#8217;ll do spot bonuses for people who have particularly novel uses of AI in their day-to-day. In our company, All Hands, every two weeks, we&#8217;ll do an AI spotlight. And it&#8217;s very rarely somebody in EPD. For the most part, it&#8217;s folks in VTMs, ops, finance, the people organization showing off how they&#8217;re building agents in ChatGPT or on Glean or how they just found some new use case that they thought was helpful. So we&#8217;re trying to create... I think at the end of the day, we&#8217;ve hired a bunch of really smart people who... I have full confidence that this type of work is within the reach of anybody who&#8217;s motivated to challenge themselves. And so we&#8217;ve done that. And in engineering, there&#8217;s one other thing that I want to call out because I think that this is kind of fun, is that we adapted our interview loop to be more AI, sort of agentic coding native. So instead of... We had a coding and a system design question. We had a coding and a system design question that we basically have revamped into a project where we&#8217;ll give you a brief before you come on site and then an additional sort of spec when you start. We expect you to use agentic coding to complete the task. In fact, it&#8217;s kind of impossible to get all the way through it if you don&#8217;t. And so we&#8217;re evaluating your knowledge. We&#8217;re kind of watching how you work. We&#8217;re evaluating whether you understand the code that&#8217;s coming out. We&#8217;re kind of probing at you as you go. But what we did in order to kind of maybe bootstrap the process... of all of our existing engineers, like getting familiar with agentic coding, is that we, as soon as we had the interview ready to ship, we started, we said, everybody in engineering, including all the managers, are going to have to go through this interview. And so we re-interviewed everybody internally. And it&#8217;s like, it&#8217;s one of those things where it&#8217;s like, it&#8217;s not a, we didn&#8217;t like keep a score or like, or like, you know, I don&#8217;t have any data on like who passed or failed or what they, what they scored. But what we found is like, as people would take it, it would actually cause them to have moments of realization where it was like, oh, I, I can up-level my skills or sort of like, I have, like, I want to be better at this. And so we&#8217;re trying to find like a way, like a variety of techniques that kind of push the culture along. And I think as I reflect on like the year, cause this is the year where we really put all the effort into it. I&#8217;m really satisfied to see the ascent to which everybody&#8217;s leaning in on a, on a daily basis. Going back to like, even I was shocked when we were looking at our cursor logs that like the number one user is, is an engineering manager. Yeah. And for, and for org, it&#8217;s like, that, that is super cool to me. It means that like folks have, have, uh, have taken this to heart and found, found ways of, um, doing their job differently. I guess my, I had a closing question or I guess a parting question, and this is broadening out from Brex and this is just, you interface with other engineering leaders all the time. Did we not cover anything that other CTOs are having as top of mind today? Like their number one problem is underscore. The thing I find myself discussing with. With folks that, and I, I don&#8217;t want to shy away from like scary topics. Uh, in fact, we were just, just kind of on one that was adjacent, which is like, how do you evaluate somebody&#8217;s like progression towards being more AI native? The, the, the, the cousin to that question is it&#8217;s like, will we need as many people, um, to operate our businesses? Like, are there layoffs coming? Are, how are we, how are we thinking about like, um, headcount growth, junior versus senior, junior versus senior? Yes, exactly. Like level mix. Um, and. And I still have more questions than I have answers there. I think, I think what has been really interesting is that I view a gentic development as being something that amplifies all the, all the good, just as much as it amplifies all the bad and the amplifies, uh, uh, sloppiness, poor architectural thinking, um, uh, misunderstanding of, of the requirements. Like there are, for all of the, the acceleration of good outcomes. It also accelerates. Bad outcomes. And I think what has been interesting is that there has been, when you sum that all together, there&#8217;s less of a obvious, um, like capacity increase. It&#8217;s, it&#8217;s more, it&#8217;s more nuanced than that. And so I&#8217;m not looking at headcount planning, uh, as we think about it next year as, as being something like, oh, well, because AI is giving us so much more leverage. We, we don&#8217;t need as many people. Um, we&#8217;ve actually, the thing I&#8217;m really proud of in, in my tenure as CTO is that we, we haven&#8217;t grown. Engineering at all. What we&#8217;ve done is we&#8217;ve, we&#8217;ve grown the business significantly, but we&#8217;ve been able to build, uh, like greater efficiencies and, and how we execute, like how we, how we, uh, we think about building, how we roadmap, um, what we choose to do and what not to do that. We&#8217;re able to, uh, to serve significantly more customers with more lines of business, um, without needing to grow engineering headcount. I think that that&#8217;s kind of the way that we&#8217;re gonna just continue on this road is like, I like having 300 engineers, like I would love, love to just, you know, a year from now have 300 engineers, but we&#8217;re still. You know, 30, 50, a hundred percent more efficient, but, uh, that, that, that is the thing that comes up with, uh, with other engineering leaders. And the other part of that conversation is like, how much is AI getting blamed for this sort of ordinary performance oriented, uh, realness, you know, like if, if Microsoft is letting go of like 4,000 people as a business, what they have 150,000 employees, I believe. Uh, is that really like AI causing that? Or is it them just using it as a way to, uh, uh, to avoid some harder, like perf management decisions? I&#8217;m not entirely sure, but I&#8217;m, I&#8217;m listening more than I&#8217;m speaking on the, on this topic because I, every time I feel like I have a pretty firm point of view, some new, uh, anecdote or experience comes in that kind of challenges or invalidates it. Yeah. Well, you know, I, I take these signals as it&#8217;s my job to go find people who think they have answers and surface them. And you may or may not disagree, but at least you have something to use as a straw man in, in your work. Exactly. Exactly. And I, and I think as, as an industry, it was just early innings on, on, on this transformation. So I&#8217;m looking forward to seeing, uh, uh, you know, listening to this, this podcast episode a year from now and, and, and seeing, you know, what we got right, what we got wrong and what&#8217;s different, uh, because so much changes, uh, quarter over quarter. Yeah. I do think AICOE is a very well established pattern. I think, uh, internal platform is very well established pattern. And this, uh, fluency thing is something that people are figuring out that I think you guys are ahead on. Um, I&#8217;m happy to hear that. That&#8217;ll be my feedback. Yeah.swyx/Alessio [01:06:38]: Any final call to action for things that you want to buy? Like what should people build for you? Like problems you&#8217;re trying to solve that you would love people to reach out for to, to help with?James [01:06:48]: The call that I&#8217;d make is for folks who are interested in, in multi-agent networks to, to get in touch with us, because I, I do feel like this is something where, where we&#8217;re, we&#8217;re innovating in, in service of, of our customers and where I, I feel like the frameworks, the tooling, um, and the. The research is, is, is there, there&#8217;s actually quite a lot of like interesting papers and things that we, we lean on. Uh, but I would love to, uh, would love to see more of that, like encoded in the, um, in the what&#8217;s available writ large in the industry, because I feel like my intuition has been that trying to craft LLMs into deterministic workflows and DAGs is, is kind of underselling, like the power that they have to actually plan and execute more and a more sophisticated, like. Yeah. Fluid way. And, and I, and I just want to see like the industry lean in more, um, on, uh, on these agent to agent, uh, uh, interactions. Okay. So, uh, I&#8217;ll, I&#8217;ll dive in a little bit here. Yeah. I have a minor opinion. You keep using the word networks. Yep. Is that a reference to a specific paper or it&#8217;s your term for it? It&#8217;s just, it&#8217;s our term. And I think that that is, that&#8217;s actually the term that master uses as well. Um, or it, it, we, um, yeah, initially we used to call them agent run times, uh, internally. Yeah. And then we just, yeah, switched to networks. Uh, and then I think the other thing I wanted to get a clarification on is, is it mostly a full agent talking with a full agent or is there a kind of like a orchestrated boss agent talking to a sub agent? And I think that does matter for a subset of people who are building all these things, because when you say multi-agents and guys, people don&#8217;t agree what that means. Yeah. So it&#8217;s, it&#8217;s a tree more than it is a graph. So it is like, yeah, we have it when you say network, it&#8217;s feels more of a graph. Yeah. But. It seems more directional as a tree. Like there, there is a hierarchy. There&#8217;s a hierarchy. Yeah. But there, but there are some violations of that. Like one of the, one of the interesting use cases, uh, and this is where like the power of, of having an, an assistant for every employee plus having agents that run and, and embody, uh, members of the finance team is really powerful because, uh, there&#8217;s this interesting use case that, that we brought to market, which is that, um, one of the finance teams is a finance team. Uh, a lot of the finance team agents that we, we, uh, launched is an audit agent where like an audit agent kind of embodies the work that a lot of larger finance teams will do to look for patterns of waste fraud or abuse or like systematic, uh, avoidance of policy that isn&#8217;t as obvious with a single expense. Like you can evaluate a single expense in the metadata around it to see if it, if it&#8217;s, um, within policy or not. But, um, what if you start seeing an employee often make a large number of like $74 transactions when receipts are required? Part of 75 or what if you, what if you see, um, certain things like, oh, okay, there&#8217;s actually a fair number of like DoorDash expenses during business hours from this individual, like on, on days that an office lunch is provided, or maybe you see like ride share patterns that are, are, um, where you have to look at a broader context. Um, so we built this audit agent that can like ingest your SOP and, and look also ingest your. This is a Bikes&#8217; customer&#8217;s SOP. Exactly. Yep. And, uh, and what it does then is it&#8217;s, it&#8217;s basically always looking for. Potential violations and what it does is it, it is extremely zealous. Like it, it wants to have a minimum number of false negatives, so it will raise a large number of potential violations. And then a separate agent, a review agent will then apply wisdom, the wisdom of like, is this important enough to follow up on? Is the dollar amount in question high enough? Does this user seem to have like a high compliance behavior more generally? It makes a judgment call about whether it&#8217;s worthy enough to. Take that violation and make it into a case. Then once it&#8217;s made into a case, generally what happens is that you need to get more information from the individual. So if humans were doing this, there&#8217;d, there&#8217;d be some outsourced team that&#8217;s like looking for all the potential violations. Then you have some full-time employee on the finance team who&#8217;s, who&#8217;s looking at all the violations that, oh, these are the ones that are important. We need to follow up on it. Now, what they do is they hand it off to somebody who will go and Slack that employee and be like, Hey, what&#8217;s going on here? And so what we have is like the audit agent looks for violations. The review agent. Decides whether it&#8217;s worthy enough to turn into a case. And then from there, uh, when the case is filed, the, that, that will trigger an event to the Brexit assistant for that employee. And like any additional information about like the business justification, um, can be collected or maybe the assistant already knows because it, in its conversation history of the employee knew something about why this, this expense, uh, looked out of, out of policy. And so you start having the, the network becomes interesting when you have the finance team agents. Yeah. Communicating with, uh, the assistant or various employees. And then behind there, you have other, other sub-agents. And so then you start seeing like more of a graph, uh, emerge, but when you look at just what serves the employee, it looks more like a tree. Amazing. Well, I didn&#8217;t know you were going to go into that level of detail. Yeah. Yeah. Don&#8217;t worry about that. No, no, no, no. I&#8217;m, I&#8217;m actually really glad I asked like that is very impressive. And, uh, I hope you, uh, do more content about that. Yeah, absolutely. We&#8217;re really excited about it. I think, uh, it&#8217;s, it&#8217;s been, it&#8217;s been good to finally figure out, uh, a use. Use for, for agents and have the technology be as, uh, like as robust as it is to start realizing this vision. Cause it&#8217;s something that we, we kind of dreamt of a couple years ago in the tech, like to your earlier point, the tech just wasn&#8217;t there when we were trying to make the, make the, a similar concept to work with the GPT 3.5. It was like, now we were hallucinating tool calls and, uh, back in that day.swyx/Alessio [01:12:24]: Um, awesome, man. Thanks so much for joining us. This was fun.James [01:12:27]: I really enjoyed it. Uh, happy holidays guys. Thank you for having me. Thank you.",
          "url": "https://www.latent.space/p/brex",
          "author": "Allen Park",
          "published": "2026-01-17T01:35:29",
          "source": "Latent.Space",
          "source_type": "rss",
          "tags": [],
          "summary": "Brex achieved a major turnaround, reaching $500M in annualized revenue after a difficult 2024 with 20% staff cuts. CTO James Reggio attributes the recovery to aggressive AI adoption across all business operations, positioning it as a case study in enterprise AI transformation.",
          "importance_score": 55.0,
          "reasoning": "Demonstrates concrete business value from AI adoption in fintech, with measurable revenue results. Useful enterprise AI case study but not frontier AI research or capability advancement.",
          "themes": [
            "Enterprise AI",
            "AI ROI",
            "Business Transformation"
          ],
          "continuation": null,
          "summary_html": "<p>Brex achieved a major turnaround, reaching $500M in annualized revenue after a difficult 2024 with 20% staff cuts. CTO James Reggio attributes the recovery to aggressive AI adoption across all business operations, positioning it as a case study in enterprise AI transformation.</p>",
          "content_html": "<p>AIE Europe early bird tickets and CFP speaker applications are now open!In 2024, there was a big question mark around the future of Brex as they cut 20% of staff among stalled growth with the space becoming increasingly competitive. Fast forward to 2025, Brex has accomplished one of the most impressive turnarounds, accelerating sales and passing $500 million in annualized revenue with European expansion in sight, dissipating concerns of a dying business. Among the internal changes that led to this successful turnaround was an aggressive shift towards AI adoption and utilization in every aspect of Brex’s business.In this episode, we sit down with Brex CTO James Reggio to discuss Brex’s culture and how they facilitated an aggressive shift towards a culture of AI fluency in addition to how Brex modernized their tech stack and apply AI agents to streamline their business.This Substack is reader-supported. To receive new posts and support my work, consider becoming a free or paid subscriber.CultureTeamHeading into 2024, Brex showed natural signs of a startup scaling up as they continued increasing headcount and expanded org layers to accommodate. However, Brex realized the rate they were scaling the company didn’t match their speed of execution. Addressing this scary truth head on, Brex laid off 282 people (~20% of the company), flattened their org structure, and reduced layers of management with the goal of having leaders “operate closer to the metal” and be less siloed with only a few important cross-company priorities.Visualized from “A Message from Pedro” - 1/23/24Brex realized that speed and execution shouldn’t come at the cost of increasing headcount. Even as Brex scaled up, it can still operate similar to a startup with high-velocity.In March 2024, Franceschi described this as Brex 3.0: a new operating model designed to increase intensity and execution quality by making fewer bets, consolidating to one roadmap, and maintaining a release cadence. They doubled down on fostering a high trust and high performance team by promoting from within, bringing employees back in-person to work, and expecting leaders to operate at all levels.Brex’s team now consists of ~300 engineers and ~350 across EPD (Engineering, Product, &amp; Design). Their team is split across product domains in addition to infrastructure. Their core product domains are the following:Corporate CardCorporate Bank AccountExpense ManagementTravelAccountingIn addition to these product teams, Brex has formed smaller specialized teams like their most recent team of ~10 focused on LLMs. This has been one of their key initiatives to keep up with and innovate with AI progress. This team was actually formed by the Brex team asking themselves, “What would a company that was founded today to disrupt Brex look like?” and the answers to this question were used to decide what to work on. Currently the team is still very lean with a mix of talented AI native 20 year olds who grew up with the tech and older more experienced staff engineers who are veterans in the industry and space that Brex operates in. This team operates like a startup working 996 (9AM to 9PM, 6 days a week) and growing the team very slowly like a pre-seed startup (i.e. only hire or add people to the team when absolutely necessary)late friday from LLM teamAI AdoptionThis specialized team isn’t the only one using AI as Brex has engineering wide adoption for AI tools and coding agents. Oddly enough, the top Cursor user is actually an engineering manager. Brex strongly encourages employees to use AI tools and software that will boost their performance.This sentiment is echoed by their bold decision to not pick winners between foundational model providers or agentic coding tools, etc. What Brex does instead is to procure a small number of seats of multiple solutions and give employees the ability to pick what they want to use through ConductorOne. For example, an employee can get a ChatGPT, Claude, and Gemini license to build their own stack or even get credits to try Cursor, Windsurf, or Claude Code.This push for AI adoption doesn’t stop at engineering. Camilla Matias, Brex’s COO, has pushed aggressively to help every member of the operations organization to start rethinking their role as people who are building prompts &amp; evals to become more AI native.To make it even easier and accessible for non-technical employees, Brex’s Agent Platform is built in Retool which has enabled the operations org to do prompt refinement themselves instead of needing engineers to do it.While many startups and larger companies discuss how to use AI internally, Brex actually executed on it by making the barrier to entry for any AI tool very low instead of sending a company wide email to “use more AI”.AI Training &amp; FluencyBrex has also implemented a tangible and definable approach for AI adoption with their org-wide AI training program &amp; AI Fluency Levels which are below:source: Agent + Human Ops - 9/25/25At the end of the AI training program, every employee must categorize themselves into one of the four groups which will then help managers assign them to relevant projects to upskill their AI fluency. These fluency levels are also used in quarterly assessment to gauge their progress. While this approach may sound excessive, it is not enforced as a metric to fire employees but rather as a tangible way to encourage current employees to utilize powerful AI tools available today.Brex’s success in upskilling their employees’ AI competence comes down to two things they executed very well on: 1. Make it extremely easy for employees to use (i.e. Retool platform) and adopt AI tools (i.e. open access to any tool through ConductorOne) 2. Implementing a forcing function that aligns employee incentives (i.e. promotions, performance reviews, etc.) with AI fluency.AI Fluency in HiringEven the hiring process has been adapted to align with Brex’s culture of being AI fluent. Previously, Brex had a straightforward coding &amp; system design question for potential hires but that has been revamped to a project you work on-site that requires agentic coding or else it’s impossible to finish. Candidates are then evaluated on the candidates’s fluency and competence with AI tools, how the candidate works, whether the candidate understands the AI generated code, etc.Making Brex’s culture even more clear, when this new AI native interview process was created, every employee in engineering including managers had to go through the interview. It wasn’t to fire or keep score but so that employees would also realize how they could uplevel their skills using AI which eventually pushed their AI fluent culture further.By keeping a very high bar and holding everyone to the same standard, Brex ensures every employee is treated fairly while not sacrificing on talent density. The standard that new hires are held to are the same ones current employees are held to, establishing a truly meritocratic culture.“Quitters Welcome” Recruiting InitiativeMany Brex employees have gone off to start enduring companies. Surprisingly, Brex actually encourages and celebrates it which they’ve leaned into more recently with their “Quitters Welcome” recruiting initiative focused on attracting talent that want to quit and start their own company one day. Brex has had success hiring former and future founders and have doubled down since many of their smaller, specialized teams, like the one focused on LLMs, operate like a startup. They hope to be the best “founder school” with the value proposition that future founders can learn the best by solving interesting problems with instant distribution under Brex. For example, you could build financial AI applications that are instantly deployed to thousands of customers from Fortune 100 to startups, expediting the feedback loop and learning from high quality customers.Modernizing the StackComplementing Brex’s aggressive push towards a culture of employees becoming very AI fluent was a culture of trying and adopting the most modern tools and frameworks in the age of AI. Both were critical in Brex’s turnaround and acted like a feedback loop where AI fluency would enable easier adoption for the best modern tools and frameworks which in turn would increase AI fluency.But this modernization and adaptation didn’t happen all at once. It started in January 2023 when the team created simple internal infrastructure that made it possible to deploy, manage, &amp; evaluate prompts and route to different models. As more models and technologies were launched, Brex kept up with adoption, eventually building out an entire agent platform. This platform is actually powered by the current version of the internal infrastructure built out in 2023.Agent PlatformFall of 2025, Brex finally introduced their finance agents that learn, reason, and act on your behalf that is part of their Agent Platform below:source: Agent + Human Ops - 9/25/25Below are some tangible ways Brex’s Agent Platform have streamlined operations:Automating pipeline for evaluating customer applications to get them instantly onboarded.Previously, human intervention was required for underwriting or KYC (Know Your Customer) but now a bunch of research agents can first handle all the research and then go do the work on behalf of humans to instantly onboard qualified customers.Maintaining thorough, up-to-date, and accurate knowledge about Brex’s business.Previously a big challenge Brex faced was outdated knowledge being baked into models during pre-training which would lead to very costly hallucinations and incorrect information that would pollute context for agents. As a result, Brex decided to manage this knowledge base so that every detail about the business is up-to-date and correct.EvaluationsTo ensure the quality of their agents and to prevent regressions during future updates, Brex implemented multi-turn evals where an AI agent would basically embody the end user and be given an objective to accomplish. Sometimes they’ll also pre-set an initial preamble to the conversation (i.e. couple turns will be hand written) then begin the eval in order to isolate certain behaviors and make the eval a bit more deterministic.Here are some modern technologies and tools Brex uses:MastraSurprisingly, Brex’s AI agents are built with Typescript and the Mastra framework, using pgvector &amp; Pinecone as the data stores. Their eventual adoption of Mastra goes back 3 years when Brex built their own internal LLM framework after being dissatisfied with Langchain’s lack of support for key use cases. With the hope to eventually churn off their own framework which needed constant maintenance, Brex was drawn to Mastra because its ergonomics were very similar to the one they built and it also aligned with a key motivating question they asked themselves, “What languages and technology would we use if we started Brex today?”. Brex’s existing backend code is still written in either Elixir or Kotlin, but they are always reevaluating framework and tech choices because the half life of code has declined so much with agentic coding.GreptileAnother modern tool in Brex’s stack is the AI code review software Greptile. According to James, what is most impressive about Greptile is their signal to noise ratio of comments left on a PR, “I never regret going through all 65 comments [Greptile] leaves on my diffs because it catches so many things.”SierraWhen deciding whether to build or buy, Brex focuses on whether the solution is only possible with the context they have or can be built by others. When it came to CX (customer experience), Sierra already had alignment with Brex’s primary concern: having a UI/UX (i.e. low-code &amp; more workflow oriented) that makes it extremely easy and accessible for operations and CX strategy teams to administer agents.Brex’s Job to Be DoneThere are 2 demographics that Brex primarily serves:Finance teams: they interact with agents very specific for their rolesEmployees of companies that use Brex: goal is for Brex to disappear for employees (i.e. the only thing they have to do is just use the card)The way Brex believes this problem is best solved is by giving every employee an AI agent(s) that embodies an executive assistant. In the same way that an EA has all the context and can take relevant and correct actions on your behalf, Brex wants AI agents to do that for finance teams.Brex also views their AI Strategy through 3 Pillars:Corporate AI Strategy: How are we going to adopt and buy AI tooling across the business and in every single function to 10x workflowsOperational AI Strategy: How are we going to buy and build solutions that enable Brex to lower their cost of operations as a financial institution (i.e. fraud detection, handle disputes, etc.)Product AI Strategy: How are we going to introduce new features that enable Brex to be part of the Corporate AI strategy for our customers. We want to be a solution where other companies are saying they adopted Brex for their corporate AI strategyInitial Faulty ApproachesBrex’s Initial attempt to implement this solution of an EA for every employee was the naive approach of having an agent with a variety of tools, using RAG to fetch the appropriate context.It soon became apparent that the wide range of Brex’s product lines made it extremely difficult for one agent to perform well when being responsible for everything from expense management to finding travel to answering procurement questions.Other initial attempts that didn’t work were overloading the agent with tools and context switching (i.e. “this conversation seems more about X topic so lets update the prompt to have the relevant topic in context”)Without these failed attempts, Brex would not have come to their fleshed out final approach of using subagents. In an era when technology and innovation moves very fast, the optimal path to discover the best approach is actually by trying many hypotheses to learn why an approach doesn’t work.Final Successful ApproachBrex eventually broke the problem down to using many subagents that sit behind an orchestrator. They built this orchestrator themselves since there are no great solutions on the market right now.What Brex realized was that there is significant value in multi turn conversations from the orchestrator, assistant, and sub-agent vs. a single tool call. The mental model they use is an agent org chart: sub-agents are specialists and they are conversing with each other so if more context is required, the sub-agents pass it along and then ask the user. (like how a human org chart would work)Something That Didn’t WorkAmong the many investments and experiments Brex has tried, one expensive initiative that didn’t work at all was using RL for credit decisions &amp; underwriting. Initially, Brex thought that it would be the proper approach to building a model that effectively decides like a human underwriter, but after working with outside experts and investing significant time, the end performance was actually inferior to a simple web research agent.What Brex realized was that in operations, the thing that matters most is being able to granularly break down problems and form SOPs (Standard Operating Procedure) that humans can repeatedly follow. These repeatable processes can then be audited and done in a compliant manner. Since then, Brex has seen great success using simpler approaches with LLMs for many ops use cases but wouldn’t have known its effectiveness without trying alternative approaches.Links to James &amp; BrexX: https://x.com/jamesreggioLinkedIn: https://www.linkedin.com/in/jamesreggioCompany Website: https://www.brex.comFull Episode on YouTubeTimestamps00:00:00 Introduction00:01:24 From Mobile Engineer to CTO: The Founder's Path00:03:00 Quitters Welcome: Building a Founder-Friendly Culture00:05:13 The AI Team Structure: 10-Person Startup Within Brex00:11:55 Building the Brex Agent Platform: Multi-Agent Networks00:13:45 Tech Stack Decisions: TypeScript, Mastra, and MCP00:24:32 Operational AI: Automating Underwriting, KYC, and Fraud00:16:40 The Brex Assistant: Executive Assistant for Every Employee00:40:26 Evaluation Strategy: From Simple SOPs to Multi-Turn Evals00:37:11 Agentic Coding Adoption: Cursor, Windsurf, and the Engineering Interview00:58:51 AI Fluency Levels: From User to Native01:09:14 The Audit Agent Network: Finance Team Agents in Action01:03:33 The Future of Engineering Headcount and AI LeverageTranscriptswyx/Alessio [00:00:05]: Hey everyone, welcome to the Latent Space podcast. This is Alessio, founder of Kernel Labs, and I’m joined by Swyx, editor of Latent Space. Hey, hey, hey, and we’re here with Jason Ruggiosi, too, at Brex. Welcome.James [00:00:15]: Hey, thank you for having me. Thanks for visiting from up in Seattle, where I’ve been a little bit. It’s cold up there, huh? Yeah, and we have an atmospheric river hitting the city right now, so a lot of blowing. Yeah, well, yeah, we’re getting the full-on winter effect right now. Well, you’re here to talk about the sort of AI transformation within Brex. There’s a lot of interesting tidbits that we’re going to draw from your article, but also your background. You’ve got a wide array of experience from Stripe to Banter to Convoy, and I think also mostly I’m interested in your journey as one of the rare people that have transitioned from like a mobile engineering leader to a CTO, which I think is also a bit more rare. I used to have this comment in the past where there’s a career ceiling. There are people who work on client-only things where usually they don’t hit CTO, whereas they typically promote the back-end people, the back-end clouding for people to CTO. Yeah, you know, it’s something that I hear fairly frequently because there aren’t that many folks with a front-end background to reach this level of leadership. And it’s exciting for me to be able to represent that group. But I’ll say that even though my resume kind of reflects that I’ve been more on the front end of things, it’s probably more my experience as a founder a couple of times over that. Yeah. I think that actually helped me get to this level of my career, working for somebody else, becoming CTO is very much like a leadership and like general business role as much as it is a technical role. And so I think it was more the skills that I built from starting companies and trying to build those up made me a decent fit and enabled me to get the nod from Pedro to take this on as my predecessor left about two years ago. Yeah. One thing I’m curious, you guys’ commentary, this is a little bit broad, unscheduled, but a lot of startups are bragging about how many ex-founders they have.swyx/Alessio [00:01:58]: And yes, to some extent you want people with the founder mentality and agency, which is what you did to be your employees and to take initiative in the company, but also I wonder if it’s becoming anti-signal sometimes, I don’t know if you’ve thought about this. I think it’s more about the turn for me, especially when people are hiring ex-founders, it’s like, if you’re truly of the founder gene, it’s kind of hard to just stay somewhere as like an IC for too long. And then it’s like, all right, I joined this thing and then in one year, I’m like, okay, I’m going to do this. I’m back to being a founder. I’m curious for you. Yeah. What was your, I’m sure you thought about leaving and like doing another company and stuff.James [00:02:34]: In fact, that was, that was the alternative. I was considering even at the time that I got the phone call where they made me the offer to become CTO, I was thinking about leaving to go start a company. And, you know, I think what’s interesting about it, we actually launched sort of like a new recruiting and employee value proposition for Brex a couple months ago called Quitter’s Welcome, where we actually intentionally are leaning into this idea that we have a disproportionate number of folks who go on to become founders or like heads of a department when they leave our company and we celebrate that it’s actually something that I’m very proud of and that means that like we, we welcome in people who want to get a different experience. I think that there’s certainly like a lot of founders who don’t make it, don’t scale their own businesses to this, to the scale that we’ve achieved at Brex, so there’s something to be learned when they come in, and then we’re very happy to like support people on their way out. And so I actually really like hiring former founders. The one value proposition I find that’s most relevant, because a lot of the folks we’re hiring as AI engineers are kind of folks that are either like winding down their companies or considering maybe running AI startup. The thing that resonates the most with them is that we oftentimes can give them problems to solve that are interesting, problems that maybe they even want to want to like build their own startup around, but with instant distribution, right? Like that is the allure is it’s like you can come into this business and build like financial AI applications. And instantly have that deployed to roughly 40,000 customers across, you know, the Fortune 100 down to, you know, tens of thousands of startups. So that’s what is, I think, appealing to founders.swyx/Alessio [00:04:09]: But the challenge then is making sure that we set them up for success in an environment that still feels a little bit like the startup that they might build themselves versus like something that’s too corporate. Yeah, instead of doing your own company and then coming to you and be like, can I integrate into Brex? Yeah, get all the data. Yeah, exactly. How’s the engineering team structure?James [00:04:28]: Yeah, so we have about 300 people in engineering, like 350 total across EPD. And for the most part, we structure around our product domains. And so this means that Brex is a corporate card. It’s also a corporate bank account, expense management, travel and accounting. And so we actually have sort of full stack product domains that are roughly like 30, 40 people for each of those that have everything from like the low level infrastructure up to the web and mobile experiences. That’s generally the structure of our engineering organization. And then we have naturally an organization that focuses on infrastructure, security, IT. And then there are two additional centers of excellence that we’ve built that violate that org design, where we’ve felt the need to put more focus or operate slightly differently. And AI is one of those areas where we have another team of just roughly about 10 people who are focused primarily on LLM applications. And we wanted to create a bit of a separation there because the way that we were thinking about this, and this is actually something we did this summer, is we paused and asked ourselves on our AI journey towards infusing our product with AI and generating customer value. We asked ourselves, what would a company that was founded today to disrupt Brex look like? And then we tried to... To basically use the answer to that question to form this team internally. So it’s a little bit off to the side. Ideally, everybody kind of comes up to speed and contributes, you know, LLM features, but we have this sort of off on the side right now in a centralized manner.swyx/Alessio [00:06:05]: What’s the difference in AI adoption for those teams? So like, are the people on the LLM team like much bigger cursor users, clock code users, or like, do you see similar diffusion?James [00:06:15]: It’s actually fairly uniform across the entire engineering department. And it’s actually kinda funny, like one of our largest cursor users is actually an engineering manager. So like, and I think that this also just like speaks to our core value of operate at all levels where we want all of our EMs and everybody in leadership to still basically do the job that they’re managing, manage the work. So it actually is, I think the journey of getting everybody into using agentic coding was not sort of exclusive to the AI group. Yeah. In fact, I think this podcast was actually a really good justification for us. I actually set up because I called outreach to Pedro because he tweeted this. I assume this is the center of Rex’s. He says, I started a new company inside Brex to build the future of agentic finance. No BS, just building 996 and pushing production grade agents to 30,000 finance teams, now 40,000. And then he actually has like a little job description, which I think is really interesting. I’ll skip that and go straight to Brex Accelerator, grow 5x and cut burn 99% in the past 18 months. I assume that’s a mix of internal AI automation and other stuff. But basically, I wanted to put some headline numbers up front to impress people before we dig into the details. Yeah, absolutely. And you’re correct. That’s the team that we have, this AI team. What was that? Very young team. Yeah, it’s very young. I mean, it’s been really interesting. The composition of the team is very young, AI native, 20-year-olds who basically grew up with the tech, kind of paired off with more like staff level software engineers that have been at Brex for a little while, who can kind of navigate like the existing code bases and like understand, the product and the customer deeply. Like we’ve formed these really a couple of tight, tight knit pods in the AI org where it’s like three people. Generally somebody who has like more of a product, a customer focused background, that like staff engineer who knows where the skeletons are and then like a much younger, like AI native engineer who can just do things with, with agents that like the rest of us dinosaurs maybe don’t, don’t, can’t either dream of or like, or where are, I think, I think part of it is like sometimes the, the too much experience or too much knowledge of how to solve a problem and actually be an impediment to thinking differently about it and thinking about it from like an AI first lens. But yes, we, we’ve been, we’ve been slowly growing that team just in the same way that like a pre-seed startup, you want to be very, very careful about talent density and like very deliberate, like only hire when you absolutely need it. And so, yeah, at this point it’s just about 10 people. And I think it was probably four or five people. Uh, I think everybody was actually in the photo that was attached to that tweet, uh, when Pedro put that out a couple months ago. Yeah, we’ll put it up. Yeah. Yeah. Yeah. It’s a photo at 1.20 AM in a, on a Friday. Yes.swyx [00:08:53]: Oh yeah. Yeah.James [00:08:54]: Cause we, we always do, uh, we always do like Friday, Friday demos and, and like, that’s a time for everybody to get like kind of exec review time. And so, uh, Everybody’s in Seattle? Um, those folks were all in Seattle, uh, but they’re actually geo, geographically distributed. Uh, we have a couple of folks here, a couple in Sao Paulo, a couple in Seattle. Hmm.swyx/Alessio [00:09:11]: How, at Decibel we have this like AI center of excellence, which are basically the people running these teams across companies. Yep. How do you make the other engineers not feel like you’re not special? I think that’s something that I hear a lot. It’s like, Hey, you know, why aren’t these people working on all the cool LM things? And like, I’m stuck working on, you know, the KYC integration with whatever, you know what I mean? It’s like, how do you build that culture?James [00:09:32]: You know, it’s interesting. I, I thought that that would be more of a problem, but the benefit of having really optimized our engineering culture around business impact actually causes it to cut in the other direction where for folks, some folks don’t want to work on the AI products because it doesn’t have as much clear, direct, like business impact. Right now, it doesn’t, it doesn’t impact revenues directly. And so I, uh, I think folks for the most part, uh, we’ve, we’ve enabled folks who have a strong desire to work on, on, um, AI products to, to join that team. Like somebody, somebody transferred out of our expense management organization to come over there because they’re really passionate about taking like their knowledge of like policy evaluation and bringing it into the, the AI, uh, uh, team. But for the most part, I think everybody understands like how their work, uh, ladders up and maybe there’s some like friendly rivalry because like the folks who. We say we’re kind of card product, they, they drive 60% of our direct revenue. And so they, now they’re pretty happy with that. And, uh, and they don’t feel like they’re being left out. Uh, and I will also say, um, as you probably saw in this, this piece that we, we, uh, put out with, uh, first round, there is a lot of smaller applications of LLMs peppered throughout all of our product and operations teams. It’s just some of the more novel, like agentic layer that sits on top of Brex that has been put together, like in this, in this sort of isolated team. So it’s not like folks aren’t getting to, to build with LLMs or use LLMs on a daily basis. Yeah.swyx/Alessio [00:10:55]: Maybe you run people through the Brex agent platform. We’ll put the diagram in the video where you had the LLM gateway, you know, like the whole MCP layer, which you said, David, the creator of MCP right before you. So this is very timely. Um, yeah. How did you start building that? What’s the architecture?James [00:11:09]: Yeah, the architecture, you know, I, I think simple is, uh, is elegant and we we’ve had basically an LLM gateway and, and, uh, a basic hand rolled platform, uh, from the very early days. In fact, right before being tapped to become CTO, I was leading, uh, like a AI, uh, labs team internally, uh, in the wake of like the announcement of chat GPT, you know, everybody saw this through technology and said, Hey, what are we going to do with it? And so one of the first things that we did, um, I think January, 2023, that would have been, uh, was try to put together some internal infrastructure that made it possible for us to deploy, deploy, manage version and eval prompts, uh, and then be able to manage, uh, like data egress and model. Routing and, uh, have some very basic, like observability and cost monitoring, uh, in an LLM gateway. So that’s, that’s infrastructure that we stood up and it still continues to power a lot of those smaller, uh, more, let’s say like precise applications of LLMs. So like for instance, we’ve, uh, we set up a completely automated, uh, pipeline for, um, evaluating, uh, customer applications to get them onboarded instantly to Brex, which is something that used to require, um, human intervention either for underwriting or KYC. But now. We basically have a series of, of agents and, um, and particularly like research agents that will go and do the work that humans would normally do. And so that’s running on top of this, uh, this hand-rolled, uh, framework. And then for the agents on Brex that we announced in our fall release, which is like this agentic layer that we’re building that sort of sits on top of Brex and can embody workflows that a finance team would normally, uh, hire humans for. We’ve actually, uh, started using Mastro for that as like the kind of. Yeah. Primary, primary framework for, for accelerating us. We actually have built everything in TypeScript, um, which is another like technology choice. That’s, uh, that answers the question of like, what will we do if we started Brex today, but isn’t the case for all of our existing backend code, which is either Kotlin or Elixir. And then we have, uh, we have a mix of PG vector pine cone. And, uh, like, I think what we’ve seen is we’re always, we’re always reevaluating the tech and framework choices as we go, uh, because the half-life of code has declined so significantly with agentic. It’s actually quite, uh, easy or awesome for anyone else to, to kind of try on for size, a variety of different pieces of tech, to, to figure out what is going to be most ergonomic for solving the problem. So Matthew Cuellar new choice and interesting one. Yeah. I mean, I think that the main, the main reason that we adopted Mastro is that it provided the ergonomics that we were actually, uh, that the ergonomics of master are quite similar to the internal, um, LLM framework.James [00:14:15]: Swyx and Alessio So like we did, I’m trying to remember because this is now ancient history. We evaluated a link chain, turned off of it, built our own thing. And then as we were looking, we kind of want to deprecate this internal framework that we built because at the end of the day, it’s not leveraged for us to maintain that. And Master ended up fitting the bill for the feature set that we were looking for. And I think what’s been interesting is about half of the applications that we’re building right now on the agent layer are running on Mastra. And then the other half are actually still running on like yet another internally developed framework, which is a framework that’s focused more on networks of agents. So sort of multi-agent orchestration versus more like strict, like, you know, single turn or like workflows, which are easier to use, like either Landgraf or Mastra. Tell us about your multi-agent framework. I mean, that’s what are the design considerations and why is this the first we’re hearing about it? Yeah, yeah. So it’s funny. A big, big reason why we haven’t written more about this is that it continues to evolve quite a bit. And I feel like we actually had a blog post that we were going to put out in conjunction with the fall release talking about how we built this. And by the time that we finished, you know, the blog post and had all the package ready, it was already like halfway outdated. And so the way that this has started to emerge is this multi-agent network approach to implementation was when we started to implement it, it was like, you know, when we started out, we were trying to scale up our sort of consumer-grade Brex assistant. So if you think about like Brex and our customers, there’s really like two very broad personas that we serve. We serve members of a finance team who are generally like going to be doing it like in roles like accountant or controller or head of T&amp;E. For those folks, they are going to be interacting with agents that are much more specific to their roles. But then the other broad cohort of users we have are like employees of companies. And so we’re going to look at like our client, our staff, right? And then we’re going to look at the the past year, and then we’re going to look at the companies that have deployed Brex. So you know, you go join a new company, that company uses Brex, you get your Brex card. And our goal for employees is for Brex to completely disappear. Like the best UI UX for Brex is just the card, like every single thing that you have to do in the software beyond just swiping the card is like an opportunity for AI to to eliminate some work for you. And They have an EA and she knows enough about me. She has access to my calendar. My email has all the context on when I’m traveling and for what business purposes. And so she’s basically able to do everything that I would be obligated to do in Brex, be it like booking travel or like doing expense documentation. And so what we wanted to do is we wanted to build like that EA connected to the same data sources and see if we couldn’t simulate that behavior so that, you know, you basically your interface to Brex’s SMS and the card. And when we started building that out, you know, the most naive like architecture for that would be to have an agent with a variety of tools and maybe maybe do some some rag to ensure that it has like appropriate context for the conversation. But what we were finding is that the wide range of different product lines that exist on Brex made it difficult for one like agent to perform well, being responsible for everything from like expense management to finding and booking travel to answering. policy and procurement questions. And so that’s when we started breaking down the problem and into into a variety of sub agents that sit behind an orchestrator. And obviously, this is something that can be implemented using LandGraph or Master even has the notion of these as like network switches and data. But what we found is that it was easier for us when it came to being able to build evals for the system. We kind of just hit the eject button and built our own framework, which is one in which we have agents that are able to. Basically, DM with other agents and have multi turn conversations amongst themselves to coordinate to, to complete a task to, or like to complete an objective. And what’s what’s been nice about that is it means that like, you can have your Brex assistant is like one single, one single like point of contact between you as an employee and the Brex product, and then behind your assistant, if the company has like expense management turn on you have that if they have reimbursements as another agent for that if they’re they have travel. They actually also then facilitates like, our conception here is that, you know, it’s like generally like software encapsulation patterns taking like sort of projected into the agent space. It also makes it easier for us to have like the team that owns and understands travel, like be the ones to go and iterate on that without needing to worry about like regressing the total system, or needing like one team to own every single possible active action you could take as an employee. And I’ll say that, like, I’m still of the mindset that somebody who’s like, you know, I’m not willing to do that anymore, they will build a great framework, and we they have ultimately migrate to it, but or might be us that we ultimately open source this right like but um, but for us, like this is, this has worked out quite well. And like, loo of like a couple other approaches that we tried along the way that just didn’t perform well, which is to overload the the agent with a variety of tools or intellectual like context switching where we try to say, Oh, this conversation looks like it’s more about reimbursement. So let’s like update the prompt with more reimbursement context. Like that was, that was another approach that we kind of did that. that didn’t perform as well as actually having a reimbursement agent that it would collaborate with.swyx/Alessio [00:19:46]: What about MCPs as sub-agents? Oh, yeah. That’s another pattern.James [00:19:50]: The key thing there is that there’s actually a lot of value in having multi-turn conversations from the orchestrator or the assistant to the sub-agent, whereas a tool call is basically just one RPC. And so oftentimes what will happen is, let’s say the user reaches out to their RECS assistant and says, hey, how much am I allowed to expense per person for dinner tonight? I’m taking my team out. And your assistant’s going to then reach out to the policy agent. Maybe the policy agent needs to know, in order to answer that question, maybe it needs to know whether this was a customer event, a team event, or whether you’re traveling. And so it may actually send... Instead of... It can’t just answer the question, so it’s going to reply back to the assistant and say, hey, I need you to ask this clarifying question. And so then the assistant will return to the user, ask clarifying question, and they’ll basically have this sort of multi-turn conversation across multiple agents versus it just being encapsulated in a single call and response tool call. And so there are still... All the sub-agents have a ton of tools, but I think of the MCP and tool usage as being the interface to all of our... Conventional imperative systems, not the AI space.swyx/Alessio [00:21:07]: Yeah, that’s the conversation we were having earlier, whether or not it should be an agent-to-agent call as well. Yeah. Or like, yeah, there should be like a chatback.James [00:21:15]: Exactly, exactly. And that’s the thing. It’s like, okay, and one of the ways that we actually grafted this into Nastro before we built our own framework was to make every sub-agent a tool. And then the input was just natural language. The output was natural language. And if you needed to have multi-turn... You would basically just put the full, like, prior conversation in as you kept calling the sub-agent as a tool. And it’s just like, at that point, you’re like, okay, the ergonomics are kind of... The framework is fighting me on this. It’s actually helpful for us to basically conceive of it as an org chart. And like, it’s the agent org chart with, you know, my EA is DMing other specialists and having brief conversations to support me as their client. Yep. That was a really good deep dive. Thanks for indulging. I feel like you guys are not afraid to make your own tech, which I think is a competitive advantage. I really like that culture. Maybe we should go a bit breadth first as well. Of course. Because I think we also deep dive a little bit too much in one area. There’s, and we’ll put up the chart, but I’m also very interested in like the sort of internal agent stuff, the operational stuff, and just the general platform scope. So please feel free to just like go into your spiel on it. Yeah, of course. So one of the things that I was trying to do at the beginning of the year... As CTO, you know, I think it really fell to me to articulate what our AI strategy was as a business. You know, every board of director was, you know, or every member of our board is like, hey, what’s your AI strategy? And while we were doing a lot of things, we’d literally go, he’s got it. Well, yeah. Yeah, and if I didn’t, I’d be in trouble. I think he also was counting on me given that I was doing the AI organization before CTO to have... That’s true. But a big part of it was like we were doing a lot with LLMs. It was more like these little one-off features and, you know, hey, like maybe mix in some suggestions here or maybe do a little bit of ops automation over here. But it wasn’t easy to kind of create like a verbal framework of all of these investments. And without that framework, then we weren’t able to like set a vision or a roadmap for investments. So what we did at the beginning of the year is we took everything that was going on, as well as all of our ambitions, all of the good ideas, as well as like the problems we were trying to tackle. As a business this year, throw it all on the table and see if there were some ways to cluster it into a framework that made sense to the business, to our board, to ourselves. And we came up with, I think this is not particularly novel, but it’s helped us quite a bit. We have like three pillars to our AI strategy. We have our corporate AI strategy, which is how are we going to adopt and like buy AI tooling across the business and basically every single function to be able to 10x our workflows. And we have our operations. We have our operational AI strategy, which is how are we going to buy and build solutions that enable us to lower our cost of operations as a financial institution, because I think it’s fairly intuitive. Like financial institutions like ours face a lot of regulatory expectations and there’s just like a high ops burden for running our business. And so it’s sort of like a lot of kind of internal use cases, like being able to do like fraud detection, underwriting, KYC, be able to handle dispute automation on card transactions. Those types of operational investments are our ops AI pillar. And then the final pillar is the product AI pillar, which is like, are we going to introduce new features that enable Brex to be a part of the corporate AI pillar of our customers? It’s like we want to build features and be a solution that somebody else is saying to their board, hey, we adopted Brex and this is part of our corporate AI strategy. And so it’s kind of has this nice little feedback loop and we basically within the company split. You know, did a little bit of divide and conquer where folks in IT and on our people team were more or less spending more of the effort driving on corporate AI, really like looking for making the procurement decisions, like creating a culture of experimentation where we spotlight and incentivize people for trying to sort of improve their personal workflows using AI. And then the pieces that I’ve been more involved in have been operational and product. And we were just talking about products here, which is like the agents on Brex and stuff. But I think that the operational AI. Investments have been some of the most sort of immediately impactful to the business because we have hundreds of people who work in our operations organization. And it’s actually something that differentiates us because our CSAT and the quality of our support and service is very, very high, something we’re very proud of. And so trying to figure out how can we automate a significant portion of this and use LLMs in a way that doesn’t degrade the customer experience and then also kind of addresses, like, what is the future of the roles of the people who we already have working full time for us? So this is where Camilla, our COO, who kind of co-wrote the piece with First Round with me, she’s been leaning really aggressively to help every member of the operations organization start rethinking their role as being not people who kind of execute against an SOP, but are people who are going to, like, build prompts, build evals and, like, become more AI native and, like, the way that they’re going to be used in the future. And so a lot of the engineering we’ve done has been to enable folks, say, in fraud and risk to be able to refine prompts and add additional automation to their workflows. Yeah, and it’s the secret fourth pillar, the platform. Yeah, yeah, exactly. That is the thing that ties it all together exactly, is the platform. And I think what’s been really nice is that even though the platform is kind of a loose term because it consists of a wide... variety of technologies, as I said, like, we haven’t been too religious or dogmatic about everybody needing to be on one particular thing, what we’ve seen is that by making a variety of sort of ergonomic options for building with LOMs available, it, like, really has made it easier for us to make a quick leap forward on operational AI. Like, as soon as we put our mind to it, we said, like, look, no, we want to hit 80% automated acceptance rate for all startup and commercial businesses that apply for Brax. Like, we want a decision within 60 seconds. It’s fully touchless, no humans involved. We were able to break that down and then actually build the agents, build the tools on top of that platform really quickly. And a lot of those tools are the same tools that our product AI agents use as well. I was pretty sold on the Conductor. I don’t know if this is under exactly that bucket, the Conductor One provisioning command. I was like, yep, I want that. Yeah, that was actually, I’d love to talk about that. So that’s actually on the corporate side. And I think that this goes back to maybe another intuitive, but I’d say, like... I think that this is a really bold decision that we made, which is that we’re not going to, we’re not going to try to pick winners in the horse race between the foundational model providers or the agentic coding tools or like basically anywhere where there’s, there’s an active horse race. What we do instead of like trying to pick a single solution is we will procure like a small number of seats, like multiple solutions, and then we’ll give employees the ability to pick whatever one they want to use. And so, for instance, like we allow employees to... basically go to, in Slack and use Conductor One to get a ChatGPT, a Cloud or a Gemini license. And basically you can just like build your own stack where you pick your, you pick your like chat provider as a dev. You can pick, you know, between like Cursor, Windsurf, Cloud Code, Credits, like, and you can basically craft your stack to your preference and easily switch between them. And what that does for us too is when we’re going to, like, obviously we have sort of enterprise agreements in place for all of them for the sake of like the, you know, the privacy. we have some non-training guarantees, but it’s fun because when we go to renew these contracts, we can basically resist the need to like do a wall-to-wall deployment. We can say, Hey, look, like usage trends, our employees are voting with their feet. They’re voting with their dollars and maybe Amur tools and is as hot as it was a year ago. Jason Wonguaiohnaipai, Zephyr Access Group CEO &amp; OEM Partner, Swyx does this give you a dashboard of what people are choosing? Yeah, actually, we look at that. We were looking at that as we’re going into budgeting for next year. I would love to see that. What’s, you know, anything that’s like really up, anything that’s really down? It’s fascinating how different the landscape is every three months. And I think one of the interesting challenges we had early on was getting folks to just like try these tools, try to incorporate like agentic coding. You know, like early on, I say like 12 to 18 months ago now, like get folks to just take the time to try a new workflow. And now at this point, I think what we’re seeing is like, even if, you know, a new model hits the same, like when Codex came out and everybody was like, oh, Codex is better at CodeGen, but it’s a little bit slower. I find fewer folks are like kicking the tires on new things because like they’re just so comfortable with the ergonomics of their current work. And I think that’s a big part of the workflow that, you know, some folks are just like, I want to stick with Cloud Code because I know it now. I’ve been working with it for like nine months, so I don’t need to keep switching. I don’t need, I don’t feel the incessant need to keep trying new things because I’ve gotten, I’m an iPhone person and I’m just like going to stay with an iPhone even, you know, even though there’s some really sexy Android hardware out there.swyx/Alessio [00:30:37]: Do you have one of the big numbers, like 80% of all of our code is written by AI or, but how do you measure it internally?James [00:30:44]: Yeah, no, not really. We, I mean, what we do is we’ll... We’ll measure like the attributions on the number of commits that have the like co-authored with, and we pull some of those stats, but I don’t index have like, in fact, I don’t index on those at all. I don’t, and honestly, like I, I don’t know how I, in honest, like honestly calculate that number. Yeah, I agree. Yeah, and so, so I, and the thing that, the thing that we’re really just, you know, we’re at the point now with the, like our AI agentic coding journey where now we’re trying to solve the second order of factors. So like a little bit too much slop, maybe a little not enough. Yeah, exactly. Not enough like rigor and code reviews. We’re trying to, the adoption is there. And now we have to figure out like how to mature in our usage of these tools. So that we know quality or like long-term maintainability doesn’t suffer. As well as like maybe one of the other facets of being able to generate a lot more code more quickly is like the, the drift between team members. As far as like the. Understanding of the, the, the code that’s in their services increases is like everybody’s moving faster and more independently. It, that is another sort of risk that we’re starting to see, like, you know, an incident response where folks don’t know, they don’t know a service as well as they, they used to, because it’s changed so much in the past couple months because everybody’s moving more quickly. Yeah. This has been a major topic for me this year on code-based understanding and slop, because obviously it’s so much easier to generate code, but then now we have to review it and to some extent you can’t. really fight AI with more AI. You can’t just be like, oh, just throw, throw an AI reviewer on their AI code and you solved it. Uh, and so, so you do need to just scale human attention. And I, I think that’s something I’ve been pushing a little bit in terms of like, well, you’re, you’re just going to like every engineer is just going to own more code. Yup. Period. And, uh, and be parachuted in and be expected to ramp up and be, be productive and also fix bugs. And if you’re on, you know, pager duty or whatever to, just because I mean, everyone’s going to try to be more efficient. And you’re supposed to see ROI productivity, because if you don’t, then what’s the whole point of this? Exactly. Exactly. And I, and I think it’s funny, you’re going back to the point of, you know, you could, you could add AI on top to solve the problems that the AI introduces and there, but you just keep you, that’s like an endless chain. Uh, and so. Well, I mean the, the, the, the code rabbits of the world, the graphites of the world would say, yes, actually you can. And so that’s the little bit of the tension there. Yeah. You know, I, I, uh, I’ve been thinking a lot about how the craft of Avengers. Engineering is evolving and, and I will say that I feel further away from being able to predict what, what it looks like than I, I did this past summer when I spent a bunch of time. Um, I actually basically went on leave for a month and joined the, um, joined the, the, the team that, uh, the, the AI team that we were building just to go and build alongside them. I felt like it was really important for me to deeply understand, uh, the problems in the tech. Uh, but, and so that was me. I was, I was, you know, writing, pushing code, um, effective. And, uh, and I, I went through so many different moments of realization of like, oh my God, this is going to change everything to, oh my God, this is just amplifying all the good and the bad in the industry, uh, to, oh my God, engineers are not going to have a job anymore to, you know, it’s like, and so I, I don’t have any, like, I felt like I had all the predictions back then. And at this point now, I’m just very interested to watch the, the phenomenon continue to unfold in front of us. And, uh, I will say. I was chatting with a bunch of really. Right. Uh, you know, college juniors and seniors at a dinner we hosted last night. And, um, all these folks are about to enter the industry, basically having kind of come up in the, the era of agentic development and LLMs. And I asked them like, so what is your workflow when you’re like building, uh, like building a project? Uh, how do you, how do you use agents versus like when you decide you’re going to actually just write code by hand? And I was surprised to hear the consensus was that most people there were using agents to collaborate on. Like building a design document and like collaborating on the architecture of the solution that they want to build, and then they’d be asking it to like emit, uh, you know, a doc or an implementation plan, but then they’ll go and write a lot of the code themselves still. Uh, so it’s a little bit more of the, the, uh, the rubber duck co-architect, uh, uh, use case that was most prevalent in that group. And I, I was very surprised by that. I’m impressed. The kids, the kids are all right. Yeah, I know. No, they still want to, they still want to actually write the code themselves. It’s interesting. Yeah. What we hear from like the Gen Zs.swyx/Alessio [00:35:18]: That open the end, they, they just YOLO everything into code S and yeah, I would say most of the code I generate is like, yeah, but, but it’s been a lot of time on the doc. It’s curious, like when you’re like younger in your career, it’s like, you don’t really have all the mental models of the different patterns to instruct. I feel like there’s like over reliance, especially if you’re doing the design doc, you know, I, I feel like most of the senior engineers will spend more time on that. It’s like even things like, you know, what column should you index, depending on, you know. What queries we usually run on this table and things like that. It’s hard for any AI to know that. Right. You know, and it’s like, I feel like the, the role of like the more senior engineer should actually be more of this. It’s like spending time teaching the AI and then the AI can teach the junior people in a way. Yeah.James [00:36:03]: Yeah. And it, it, everything, everything looks like mentorship and management at the end of the day. Right. It’s like, you’re breaking down tasks, you’re, uh, you’re supervising work, you’re giving feedback. Like it’s, you know, it’s basically management. Except that there’s. Agents are really bad at memory still, like they basically have zero memory and, and it’s, it’s, it’s the end of 2025. What’s going on? Yeah.swyx/Alessio [00:36:26]: Yeah. What’s your internal stack for like, uh, uh, preferences. There’s like, kind of like, you know, explicit preference you can use with, uh, you know, agents that MD and all that stuff. Uh, there’s implicit preference with linter rules and things like that in a way where it’s like, it just happens. You don’t have to tell it. How do you structure that? Oh, no. You’re talking about for agentic coding or memory? Yeah. Just like. Platform. Yeah. Yeah. For like the coding specifically, it’s like, and then we can kind of talk about, you know, the whole Brex platform. Yeah.James [00:36:53]: Just, just nothing, nothing special. Just a lot of, um, like explicit rules. That MD files. Yeah. And then we have, uh, and we, um, in linting, we still have like traditional linters in place for the couple of different language full chains. And then we’re, we’re, we’re big fans of reptile and we use them for basically all of sort of the, um, smarter than linting, uh, like agentic code review. Uh, that’s been the one solution that we’ve aligned around. And that has served us extremely well. Yeah. Can I go to reptile? Yeah, no, we’re, we’re huge fans there. They’ve built something really impressive. And I think the thing that constantly blows my mind about it is, um, the way that they’re able to just have a really impressive signal to, to noise ratio. Like the, the comments that it leaves are very, very high signal. Uh, like never, I never regret going through all like 65 comments that leaves on my, on my diffs because it catches so many things. Yeah.swyx/Alessio [00:37:46]: I found the code. X review to be really good. I don’t use code expert code generation, but like the review product is like very good for some reason. Um, I used to have, when I was working in rails, there was like this project called danger systems. Oh yeah. It was kind of like a semantic linter. Exactly. I feel like there should be more of that now. It’s kind of like the rules are one thing, a generation, but I want something in my CI that is like enforce these rules and call out where they’re broken. And then I can just copy paste that in an agent.James [00:38:12]: But yeah, when we, when we started building this, this new agent, um, code base, like, cause. As we were saying, like we were answering the question, what would you do if you built a, you know, a Brex disruptor today? And it’s like, it wouldn’t be to pick Kotlin and Elixir as the backend. And, uh, and so we actually went with the full like TypeScript stack and we, we were building on all like public interfaces and, um, really trying to make sure that this agent layer was, uh, like arm’s length from, from the, the good and the bad of, of the core of our product. And, um, and one thing, I think what we did early on. Yeah. And I don’t actually know if this is true because again, the team keeps sort of iterating, uh, but we were having good, uh, good luck using, um, cloud code, like in a GitHub action to basically go and do, uh, do more of that danger style like code review. So have a, uh, a prompt for it that went through all of the different facets that were more conceptual versus like rigidly enforceable by a linter, uh, and have it leave a big comment at the end with, um, you know, your conformance to the idiomatic coding patterns of the, of the new repo. I want us to spend some time. You said you wanted to devalve on operational agents, uh, customer support, onboarding, KYC, fraud, delinquent account disputes. Uh, this is, I imagine the bulk of it, of, of the work anywhere where there’s a good story about maybe, um, when you started out, it was, it was going to be this way. And then you discovered through building or through customer contacts that it had to go a different direction. And so that difference in beliefs is something that people can learn from. The thing that immediately comes to mind is that we, uh, we believed at the beginning that using RL for credit decisions would actually be a, like, would be the way that we would end up or like credit and underwriting, like how much of a, of a limit should we give to this business, um, that reinforcement learning would be the way that we would go about, um, building a model that effectively would decision in the way that, um, a human underwriter would. Yeah. Yeah. Yeah. Turns out that it was, we made this big investment. We were working with some outside, uh, like the, like a company, uh, that specializes in this and the performance we ended up getting was inferior to just building a, a, like a web research agent. And so, so I think what, what we took away, what, what has been most evident in operational AI is that in operations, you need to be able to break down problems really granularly and be able to form SOPs. That humans can. Root, repeatedly follow and, and thus can be audited, uh, because so much of, uh, the responsibilities and operations is to, uh, is to have auditable, repeatable processes that help to ensure that we’re operating in a compliant manner. yes. Um, and that actually translates just so cleanly to LLMs that we haven’t needed to use too many sophisticated techniques in, in operational AI. Uh, it’s been a, it’s been relatively simple, like a few tool, uh, like agents or maybe even a lot of them. problems can be solved, which is like a single turn chat completion. And so the fact that we didn’t, well, we did one sort of attempt to over-engineer and use more sophisticated techniques. And we discovered that, in fact, the solutions are a bit more plain and less technically sophisticated. The challenge is really articulating and refining prompts to reflect the execution of the SOP and reflect all the sort of institutional knowledge that isn’t written down so that agents can properly replace like the humans or the contractors we would have making these decisions.swyx/Alessio [00:41:45]: How do you decide what is worth like spending a lot of time building versus what you think some of these models are just, because some of these tasks are so generic, they’re not really about Brex. Yep. Like you can assume the models will be good at it versus some of them are like very specific to you.James [00:41:58]: We kind of prioritize like the tasks that are most common for the broadest number of customers. And some of them are fairly... Fairly intuitive, like being able to research a customer to look to assess like legitimacy of the business and whether that business would fit our ideal customer profile for onboarding, because there’s certain types of businesses that we either legally cannot serve or we are not comfortable being able to serve. So that’s the type of really kind of basic research and like a relatively straightforward problem that isn’t hyper Brex specific. The things that are a little bit more specific to us or companies in our sector would be preparing documentation for a network card dispute. Like if you go and dispute a transaction on your personal card, you will provide evidence to your card issuer. The card issuer then has to put together like a three or four page word document that goes to the card network and then eventually goes to the acquiring bank. And all of that is like much more specific to our business. It’s a huge operational overhead for us. And that’s something that we we decided to automate later because it’s not as it’s not on the critical path of like serving the vast number of our customers. Like disputes are expensive, but not very common operational process. And so they’re lower on the stack. And I think we’re getting there right now. But this year has basically been us just kind of like looking at every single process, this kind of stack ranking. And I will say, like the thing that got us started down this path was we wanted to be able to do a lot of things that we didn’t want to be able to do in the first place. We wanted to expand our ideal customer profile to support more business, like a wider variety of commercial businesses, which tend to be businesses that aren’t growing as quickly. So they’re not like tech startups, which have a lot of growth, and they’re not usually like they’re not enterprises, which also tend to have a lot of growth. It’s more like a lawyers, a law firm or a dentist office, these types of like solid businesses that we should be able to serve and underwrite. But the cost to to onboard them and the cost to serve, if you have all all the humans in the loop, make them ROI negative. And so that was the first sort of use case of of AI within our ops ops organization that then led to us really understanding we could automate much more than that. Is this Berks going back into SMBs? Ah, that’s a good question. Yeah, yeah. So never, never let let let that die. You know, we think the way we’ve thought about this is we want to always like offer our product to customers where we believe we have a like an offering that is well suited to the needs of those businesses. And I would say that still for very small businesses, our offering isn’t it’s not built for that. It’s built for it’s built for companies that have some degree of scale. Typically have at least sort of. One person, if not a couple of people in their finance team. So we consider these to be more like the the commercial segment. And so it rhymes with with SB. But our approach back then was was a little bit more naive. And I would say we also we were just going for volumes, like a volume game. There are internal controls were not as strong. We didn’t have as much experience like underwriting those businesses. And so it was really ended up being a case study.James [00:45:46]: Swyx and AlessioJames [00:46:03]: Swyx and Alessio It depends on the scale of yourself as a business when you use these terms.swyx/Alessio [00:46:28]: And all of these things are built in the Brax agent platform, like all these automationsJames [00:46:32]: that people build? Yes, exactly. In fact, most of the operational AI is running on that original platform that we have. One element of it that I didn’t mention is that most of the UI UX for this platform is built in Retool. And so you can basically go into Retool and there’s a prompt manager, a tool manager, an eval manager, and that’s sort of where much of this was built. And the goal with that was, again, to make it more accessible, more ergonomic to get started, but a secondary effect of having a more visual set of tools for this is it’s enabled members of the app’s organization to go and do prompt refinement themselves. So you don’t need engineers to go and refine the prompts or even test new foundational models when they come out. I think that’s another fun thing. When a new model drops, folks will go into the platform and basically run the evals on the new model and kind of see, can we get better performance here? Or does this have different latency or different cost characteristics? Yeah, you want the domain experts or the people directly using the tool, not the engineers who are somewhat removed from the tool. I do want to highlight to listeners that a lot of the Brax agent platform are just things that every company should have. Basically, problem management system, which we talked about where the domain experts are doing it, multi-model testing, evaluation and benchmarking frameworks, API integrations for automated workflows, NCP-based architecture shared with Brax’s external AI products. This one is obviously very Brax-specific. One thing I did want to highlight that I was semi-impressed by, because nobody, people very few rarely talk about this, is knowledge base for understanding Brax’s business. So do you want to expand on that? Yeah, and this is an area where we’ve only scratched the surface. This year, but a big challenge that we face is that the world knowledge or the knowledge that’s built into the model about what GPT-5 thinks Brax does and how it thinks our business operates is actually quite different from what our business offers today or how our product works. And so we’ve had to work on building a corpus of product documentation, process documentation, and curate this set of information. What do you think, Zach, was the most of your experience building this model? We really needed to have a lot of communication to basically ground a variety of our LLM applications, including like that Brax Assistant, which is like the, you know, the assistant that employees will talk to is like we don’t want it to hallucinate features that we don’t have, or like give wrong information there. And similarly, some of the operational agents need to be grounded on what our ICP is, because if you ask, you know, ChatGPT-5 right now, like, what types of businesses does Brax honor? Thank you so much for having me. Thank you so much.James [00:50:25]: Thank you so much.James [00:50:52]: Thank you so much.James [00:51:20]: Thank you so much.James [00:51:50]: Thank you so much.James [00:52:21]: Thank you so much.James [00:52:59]: Thank you so much.James [00:53:20]: Thank you so much.swyx/Alessio [00:54:02]: Thank you so much.James [00:54:20]: Evals that are blocking because they would indicate like a regression, an unacceptable regression. So these tend to be just accuracy related evals. But then there are others that are more about like tone and coherency and these types of things where they’re more subjective. And we were just looking at those over time as a metric. But the team is actually interesting. I think we’re going to get a big update on like how the team is thinking about evals tomorrow and like our Friday, our Friday review. So it’s, this is an area where I’d say the largest challenge, like the largest change we needed to make and how we were executing sort of as like a lab or an incubator back earlier this year to like where we are now, where we’ve, we’ve shipped and like we’re trying to, to increase the rigor has been around like avoiding regressions and having more and more increasingly robust evals.swyx/Alessio [00:55:14]: Yeah, I’ve worked with a company called Verisai that does user simulation. And I think like that’s what’s been interesting. Some of these things they just don’t expect, like the customer does not expect the model to do, but they want to track the saturation of the model in a way, if that makes sense. And I feel like most companies know what they don’t want to happen, but it’s almost like they don’t, they cannot quite articulate, oh, I want in the future the model to be able to do this. They can do it today, but I’ll keep running this eval.James [00:55:41]: That’s actually really, really interesting to me. And I, I’m going to take that away and start thinking about this because. There are, there are going to be certain, I mean, we already seen this where, where users will ask the assistant for help with things that we don’t support yet, or we haven’t implemented yet. It’s like, those are opportunities actually for us to build a, like effectively write a test that’s going to be fail, like failing for, for weeks or months and then eventually we’ll go green, but as a way for us to actually kind of show like the progression of sophistication of the assistant. I really, I really liked that as an idea. Yeah. I wonder how you also catch hallucinations and things that it does. That’s usually the, that’s usually the problem is it, you know, it’ll, it’ll, it’ll pretend like it can assist with something and it’ll, uh, like one thing that is really annoying that has been tough to, um, to prevent is that the, the assistant, because it is used to speaking to other agents, um, that can support it in like accomplishing various tasks. If you ask it to, to help with a task that it thinks it probably should have an agent to, uh, uh, to, to work with, it’ll just hallucinate that it, you know, it’s like, oh yes, I’ll, I will like, you know, I’ll reach out to the finance team on your behalf to, uh, to pass this question along, but it’s not doing anything. There’s like no finance team. There’s no way for it to do that. This is something that comes up a lot. It’s like, would you like me to ask the finance team? And there’s no, there’s no actual tool for that. Do you put guardrails for that? Yeah. Yeah. That was, that was something that we had to, uh, Like a regex? Oh, no, we don’t. I think we’ve been able, we’ve been able to just beat that out of its system with a system prompt. But, uh, but the, we don’t have as many guardrails in place right now, just around a couple of like potential, uh, like things that could get us into trouble. Yeah, really extreme ones. Yeah. I just, yeah, it’s surprising when I, I guess two years ago was first kicking around the idea of all these things. I would have said that probably guardrails would be more prevalent, especially in finance use cases, but surprisingly they’re not. Yeah. And that was actually part of what we, that was like a feature, I believe we built in the LLM, LLM gateway early on is like the, the sort of last chance, like, uh, you know, like, uh, um, Like hardguarded Yeah, exactly. Here’s some regexes that will just kill. Yeah, exactly. Or just, you know, in the way that like, if you go away a field on a ChatGPT, you just get like the inline 500 error. It doesn’t even tell you that it can’t tell, but just like craps out. Uh, like we kind of built a couple of those circuit breakers or like the ability to put those circuit breakers in and, and I don’t, I don’t believe that we’re using them for anything. One last thing I want to get your thoughts on was AI fluency levels, which you guys have a framework of user, advocate, builder, native, and everyone goes through it, including you. Including Camilla. And I just think it’s interesting. I think it’s a model that other people are thinking about adopting, but they’re worried about ruling it out that everybody’s going to be bad. And then, and also like, how do you have like this in-house training course that you keep up to date? Yeah. Just tell us more about it. Yeah. So in, in the operations org, uh, they’re actually more ahead of even engineering on this front, as far as like trying to create, um, create like, you know, like, you know, like learning pathways for this. Uh, and I think that part of the reason why they’re ahead of us is that in operations are much more, uh, they have to be elaborate, uh, training at, at scale. Like training is a big, very big part of, um, of how people build the aptitude around their, their job function within ops. Whereas like in EPD, a lot of it is sort of, uh, getting hands on building experience, like going a lot and getting mentored, getting code review. But, uh, it’s been really neat because I think we’ve really like, we created an environment.James [00:59:14]: Um, by speaking openly, uh, about the, the transformation that we saw would happen in this industry towards AI sort of displacing a lot of, um, a lot of the operations and CX roles. And we were just honest about it. And I think what, what in the same breath that we said, Hey, a lot of these job responsibilities will go away. We also said, we don’t anticipate that meaning that your job has to go away. It’s just that your job has to change. And so the, the fluency framework and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and and then, uh, the like the training and support and like the positive sort of culture where we celebrate people making progress has been really helpful for like avoiding a culture of fear or like, oh, you have to do this, or you have you’re going to get this is going to go in your performance evaluation. I think the, it does it well, it’s not like his road is like, oh, like, what is, you know, what is the, like how much are you using AI and is it, is it enough? It’s, it’s more, I think we’ve built a pretty positively framed culture where we’ll do spot bonuses for people who have particularly novel uses of AI in their day-to-day. In our company, All Hands, every two weeks, we’ll do an AI spotlight. And it’s very rarely somebody in EPD. For the most part, it’s folks in VTMs, ops, finance, the people organization showing off how they’re building agents in ChatGPT or on Glean or how they just found some new use case that they thought was helpful. So we’re trying to create... I think at the end of the day, we’ve hired a bunch of really smart people who... I have full confidence that this type of work is within the reach of anybody who’s motivated to challenge themselves. And so we’ve done that. And in engineering, there’s one other thing that I want to call out because I think that this is kind of fun, is that we adapted our interview loop to be more AI, sort of agentic coding native. So instead of... We had a coding and a system design question. We had a coding and a system design question that we basically have revamped into a project where we’ll give you a brief before you come on site and then an additional sort of spec when you start. We expect you to use agentic coding to complete the task. In fact, it’s kind of impossible to get all the way through it if you don’t. And so we’re evaluating your knowledge. We’re kind of watching how you work. We’re evaluating whether you understand the code that’s coming out. We’re kind of probing at you as you go. But what we did in order to kind of maybe bootstrap the process... of all of our existing engineers, like getting familiar with agentic coding, is that we, as soon as we had the interview ready to ship, we started, we said, everybody in engineering, including all the managers, are going to have to go through this interview. And so we re-interviewed everybody internally. And it’s like, it’s one of those things where it’s like, it’s not a, we didn’t like keep a score or like, or like, you know, I don’t have any data on like who passed or failed or what they, what they scored. But what we found is like, as people would take it, it would actually cause them to have moments of realization where it was like, oh, I, I can up-level my skills or sort of like, I have, like, I want to be better at this. And so we’re trying to find like a way, like a variety of techniques that kind of push the culture along. And I think as I reflect on like the year, cause this is the year where we really put all the effort into it. I’m really satisfied to see the ascent to which everybody’s leaning in on a, on a daily basis. Going back to like, even I was shocked when we were looking at our cursor logs that like the number one user is, is an engineering manager. Yeah. And for, and for org, it’s like, that, that is super cool to me. It means that like folks have, have, uh, have taken this to heart and found, found ways of, um, doing their job differently. I guess my, I had a closing question or I guess a parting question, and this is broadening out from Brex and this is just, you interface with other engineering leaders all the time. Did we not cover anything that other CTOs are having as top of mind today? Like their number one problem is underscore. The thing I find myself discussing with. With folks that, and I, I don’t want to shy away from like scary topics. Uh, in fact, we were just, just kind of on one that was adjacent, which is like, how do you evaluate somebody’s like progression towards being more AI native? The, the, the, the cousin to that question is it’s like, will we need as many people, um, to operate our businesses? Like, are there layoffs coming? Are, how are we, how are we thinking about like, um, headcount growth, junior versus senior, junior versus senior? Yes, exactly. Like level mix. Um, and. And I still have more questions than I have answers there. I think, I think what has been really interesting is that I view a gentic development as being something that amplifies all the, all the good, just as much as it amplifies all the bad and the amplifies, uh, uh, sloppiness, poor architectural thinking, um, uh, misunderstanding of, of the requirements. Like there are, for all of the, the acceleration of good outcomes. It also accelerates. Bad outcomes. And I think what has been interesting is that there has been, when you sum that all together, there’s less of a obvious, um, like capacity increase. It’s, it’s more, it’s more nuanced than that. And so I’m not looking at headcount planning, uh, as we think about it next year as, as being something like, oh, well, because AI is giving us so much more leverage. We, we don’t need as many people. Um, we’ve actually, the thing I’m really proud of in, in my tenure as CTO is that we, we haven’t grown. Engineering at all. What we’ve done is we’ve, we’ve grown the business significantly, but we’ve been able to build, uh, like greater efficiencies and, and how we execute, like how we, how we, uh, we think about building, how we roadmap, um, what we choose to do and what not to do that. We’re able to, uh, to serve significantly more customers with more lines of business, um, without needing to grow engineering headcount. I think that that’s kind of the way that we’re gonna just continue on this road is like, I like having 300 engineers, like I would love, love to just, you know, a year from now have 300 engineers, but we’re still. You know, 30, 50, a hundred percent more efficient, but, uh, that, that, that is the thing that comes up with, uh, with other engineering leaders. And the other part of that conversation is like, how much is AI getting blamed for this sort of ordinary performance oriented, uh, realness, you know, like if, if Microsoft is letting go of like 4,000 people as a business, what they have 150,000 employees, I believe. Uh, is that really like AI causing that? Or is it them just using it as a way to, uh, uh, to avoid some harder, like perf management decisions? I’m not entirely sure, but I’m, I’m listening more than I’m speaking on the, on this topic because I, every time I feel like I have a pretty firm point of view, some new, uh, anecdote or experience comes in that kind of challenges or invalidates it. Yeah. Well, you know, I, I take these signals as it’s my job to go find people who think they have answers and surface them. And you may or may not disagree, but at least you have something to use as a straw man in, in your work. Exactly. Exactly. And I, and I think as, as an industry, it was just early innings on, on, on this transformation. So I’m looking forward to seeing, uh, uh, you know, listening to this, this podcast episode a year from now and, and, and seeing, you know, what we got right, what we got wrong and what’s different, uh, because so much changes, uh, quarter over quarter. Yeah. I do think AICOE is a very well established pattern. I think, uh, internal platform is very well established pattern. And this, uh, fluency thing is something that people are figuring out that I think you guys are ahead on. Um, I’m happy to hear that. That’ll be my feedback. Yeah.swyx/Alessio [01:06:38]: Any final call to action for things that you want to buy? Like what should people build for you? Like problems you’re trying to solve that you would love people to reach out for to, to help with?James [01:06:48]: The call that I’d make is for folks who are interested in, in multi-agent networks to, to get in touch with us, because I, I do feel like this is something where, where we’re, we’re innovating in, in service of, of our customers and where I, I feel like the frameworks, the tooling, um, and the. The research is, is, is there, there’s actually quite a lot of like interesting papers and things that we, we lean on. Uh, but I would love to, uh, would love to see more of that, like encoded in the, um, in the what’s available writ large in the industry, because I feel like my intuition has been that trying to craft LLMs into deterministic workflows and DAGs is, is kind of underselling, like the power that they have to actually plan and execute more and a more sophisticated, like. Yeah. Fluid way. And, and I, and I just want to see like the industry lean in more, um, on, uh, on these agent to agent, uh, uh, interactions. Okay. So, uh, I’ll, I’ll dive in a little bit here. Yeah. I have a minor opinion. You keep using the word networks. Yep. Is that a reference to a specific paper or it’s your term for it? It’s just, it’s our term. And I think that that is, that’s actually the term that master uses as well. Um, or it, it, we, um, yeah, initially we used to call them agent run times, uh, internally. Yeah. And then we just, yeah, switched to networks. Uh, and then I think the other thing I wanted to get a clarification on is, is it mostly a full agent talking with a full agent or is there a kind of like a orchestrated boss agent talking to a sub agent? And I think that does matter for a subset of people who are building all these things, because when you say multi-agents and guys, people don’t agree what that means. Yeah. So it’s, it’s a tree more than it is a graph. So it is like, yeah, we have it when you say network, it’s feels more of a graph. Yeah. But. It seems more directional as a tree. Like there, there is a hierarchy. There’s a hierarchy. Yeah. But there, but there are some violations of that. Like one of the, one of the interesting use cases, uh, and this is where like the power of, of having an, an assistant for every employee plus having agents that run and, and embody, uh, members of the finance team is really powerful because, uh, there’s this interesting use case that, that we brought to market, which is that, um, one of the finance teams is a finance team. Uh, a lot of the finance team agents that we, we, uh, launched is an audit agent where like an audit agent kind of embodies the work that a lot of larger finance teams will do to look for patterns of waste fraud or abuse or like systematic, uh, avoidance of policy that isn’t as obvious with a single expense. Like you can evaluate a single expense in the metadata around it to see if it, if it’s, um, within policy or not. But, um, what if you start seeing an employee often make a large number of like $74 transactions when receipts are required? Part of 75 or what if you, what if you see, um, certain things like, oh, okay, there’s actually a fair number of like DoorDash expenses during business hours from this individual, like on, on days that an office lunch is provided, or maybe you see like ride share patterns that are, are, um, where you have to look at a broader context. Um, so we built this audit agent that can like ingest your SOP and, and look also ingest your. This is a Bikes’ customer’s SOP. Exactly. Yep. And, uh, and what it does then is it’s, it’s basically always looking for. Potential violations and what it does is it, it is extremely zealous. Like it, it wants to have a minimum number of false negatives, so it will raise a large number of potential violations. And then a separate agent, a review agent will then apply wisdom, the wisdom of like, is this important enough to follow up on? Is the dollar amount in question high enough? Does this user seem to have like a high compliance behavior more generally? It makes a judgment call about whether it’s worthy enough to. Take that violation and make it into a case. Then once it’s made into a case, generally what happens is that you need to get more information from the individual. So if humans were doing this, there’d, there’d be some outsourced team that’s like looking for all the potential violations. Then you have some full-time employee on the finance team who’s, who’s looking at all the violations that, oh, these are the ones that are important. We need to follow up on it. Now, what they do is they hand it off to somebody who will go and Slack that employee and be like, Hey, what’s going on here? And so what we have is like the audit agent looks for violations. The review agent. Decides whether it’s worthy enough to turn into a case. And then from there, uh, when the case is filed, the, that, that will trigger an event to the Brexit assistant for that employee. And like any additional information about like the business justification, um, can be collected or maybe the assistant already knows because it, in its conversation history of the employee knew something about why this, this expense, uh, looked out of, out of policy. And so you start having the, the network becomes interesting when you have the finance team agents. Yeah. Communicating with, uh, the assistant or various employees. And then behind there, you have other, other sub-agents. And so then you start seeing like more of a graph, uh, emerge, but when you look at just what serves the employee, it looks more like a tree. Amazing. Well, I didn’t know you were going to go into that level of detail. Yeah. Yeah. Don’t worry about that. No, no, no, no. I’m, I’m actually really glad I asked like that is very impressive. And, uh, I hope you, uh, do more content about that. Yeah, absolutely. We’re really excited about it. I think, uh, it’s, it’s been, it’s been good to finally figure out, uh, a use. Use for, for agents and have the technology be as, uh, like as robust as it is to start realizing this vision. Cause it’s something that we, we kind of dreamt of a couple years ago in the tech, like to your earlier point, the tech just wasn’t there when we were trying to make the, make the, a similar concept to work with the GPT 3.5. It was like, now we were hallucinating tool calls and, uh, back in that day.swyx/Alessio [01:12:24]: Um, awesome, man. Thanks so much for joining us. This was fun.James [01:12:27]: I really enjoyed it. Uh, happy holidays guys. Thank you for having me. Thank you.</p>"
        },
        {
          "id": "4d13d86caa01",
          "title": "My picture was used in child abuse images. AI is putting others through my nightmare | Mara Wilson",
          "content": "I was a child actor, exploited by strangers on the internet. Now millions of children face the same dangerWhen I was a little girl, there was nothing scarier than a stranger.In the late 1980s and early 1990s, kids were told, by our parents, by TV specials, by teachers, that there were strangers out there who wanted to hurt us. “Stranger Danger” was everywhere. It was a well-meaning lesson, but the risk was overblown: most child abuse and exploitation is perpetrated by people the children know. It’s much rarer for children to be abused or exploited by strangers. Continue reading...",
          "url": "https://www.theguardian.com/commentisfree/2026/jan/17/child-abuse-images-ai-exploitation",
          "author": "Mara Wilson",
          "published": "2026-01-17T12:00:27",
          "source": "AI (artificial intelligence) | The Guardian",
          "source_type": "rss",
          "tags": [
            "AI (artificial intelligence)",
            "Grok AI",
            "Technology",
            "X",
            "Internet"
          ],
          "summary": "Former child actor Mara Wilson writes about how her childhood images were exploited online, drawing parallels to AI-generated child abuse imagery threatening millions of children today. The piece highlights growing concerns about generative AI's role in creating exploitative content.",
          "importance_score": 52.0,
          "reasoning": "Raises critical AI safety and ethics concerns about CSAM generation, but presented as opinion/commentary rather than breaking news. Important issue but lacks new policy or technical developments.",
          "themes": [
            "AI Safety",
            "AI Ethics",
            "Content Moderation",
            "AI Harms"
          ],
          "continuation": null,
          "summary_html": "<p>Former child actor Mara Wilson writes about how her childhood images were exploited online, drawing parallels to AI-generated child abuse imagery threatening millions of children today. The piece highlights growing concerns about generative AI's role in creating exploitative content.</p>",
          "content_html": "<p>I was a child actor, exploited by strangers on the internet. Now millions of children face the same dangerWhen I was a little girl, there was nothing scarier than a stranger.In the late 1980s and early 1990s, kids were told, by our parents, by TV specials, by teachers, that there were strangers out there who wanted to hurt us. “Stranger Danger” was everywhere. It was a well-meaning lesson, but the risk was overblown: most child abuse and exploitation is perpetrated by people the children know. It’s much rarer for children to be abused or exploited by strangers. Continue reading...</p>"
        },
        {
          "id": "8bd8d7c0d866",
          "title": "Thinking Machines Cofounder’s Office Relationship Preceded His Termination",
          "content": "Leaders at Mira Murati’s startup believe Barret Zoph engaged in an incident of “serious misconduct.” The details are now coming to light.",
          "url": "https://www.wired.com/story/thinking-machines-lab-cofounder-office-relationship-firing-openai/",
          "author": "Maxwell Zeff",
          "published": "2026-01-17T00:40:41",
          "source": "Feed: Artificial Intelligence Latest",
          "source_type": "rss",
          "tags": [
            "Business",
            "Business / Artificial Intelligence",
            "OpenAI",
            "artificial intelligence",
            "Silicon Valley",
            "Startups",
            "Personnel Matters"
          ],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-16&category=news#item-19dfc48a8156), Barret Zoph, cofounder of Mira Murati's new AI startup Thinking Machines Lab, was terminated following what leadership calls 'serious misconduct' related to an office relationship. The story connects Murati's post-OpenAI venture to emerging personnel controversy.",
          "importance_score": 42.0,
          "reasoning": "Personnel drama at a notable AI startup founded by former OpenAI CTO. While involving prominent AI figures, this is primarily gossip without technical or industry-wide significance.",
          "themes": [
            "AI Startups",
            "Personnel Matters",
            "OpenAI Alumni"
          ],
          "continuation": {
            "original_item_id": "19dfc48a8156",
            "original_date": "2026-01-16",
            "original_category": "news",
            "original_title": "Two Thinking Machines Lab Cofounders Are Leaving to Rejoin OpenAI",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-16&amp;category=news#item-19dfc48a8156\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Barret Zoph, cofounder of Mira Murati's new AI startup Thinking Machines Lab, was terminated following what leadership calls 'serious misconduct' related to an office relationship. The story connects Murati's post-OpenAI venture to emerging personnel controversy.</p>",
          "content_html": "<p>Leaders at Mira Murati’s startup believe Barret Zoph engaged in an incident of “serious misconduct.” The details are now coming to light.</p>"
        },
        {
          "id": "b65de397f3e2",
          "title": "How to Build a Self-Evaluating Agentic AI System with LlamaIndex and OpenAI Using Retrieval, Tool Use, and Automated Quality Checks",
          "content": "In this tutorial, we build an advanced agentic AI workflow using LlamaIndex and OpenAI models. We focus on designing a reliable retrieval-augmented generation (RAG) agent that can reason over evidence, use tools deliberately, and evaluate its own outputs for quality. By structuring the system around retrieval, answer synthesis, and self-evaluation, we demonstrate how agentic patterns go beyond simple chatbots and move toward more trustworthy, controllable AI systems suitable for research and analytical use cases.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U llama-index llama-index-llms-openai llama-index-embeddings-openai nest_asyncio\n\n\nimport os\nimport asyncio\nimport nest_asyncio\nnest_asyncio.apply()\n\n\nfrom getpass import getpass\n\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n   os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")\n\n\n\nWe set up the environment and install all required dependencies for running an agentic AI workflow. We securely load the OpenAI API key at runtime, ensuring that credentials are never hardcoded. We also prepare the notebook to handle asynchronous execution smoothly.\n\n\n\nCopy CodeCopiedUse a different Browserfrom llama_index.core import Document, VectorStoreIndex, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n\nSettings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n\ntexts = [\n   \"Reliable RAG systems separate retrieval, synthesis, and verification. Common failures include hallucination and shallow retrieval.\",\n   \"RAG evaluation focuses on faithfulness, answer relevancy, and retrieval quality.\",\n   \"Tool-using agents require constrained tools, validation, and self-review loops.\",\n   \"A robust workflow follows retrieve, answer, evaluate, and revise steps.\"\n]\n\n\ndocs = [Document(text=t) for t in texts]\nindex = VectorStoreIndex.from_documents(docs)\nquery_engine = index.as_query_engine(similarity_top_k=4)\n\n\n\nWe configure the OpenAI language model and embedding model and build a compact knowledge base for our agent. We transform raw text into indexed documents so that the agent can retrieve relevant evidence during reasoning.\n\n\n\nCopy CodeCopiedUse a different Browserfrom llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n\n\nfaith_eval = FaithfulnessEvaluator(llm=Settings.llm)\nrel_eval = RelevancyEvaluator(llm=Settings.llm)\n\n\ndef retrieve_evidence(q: str) -> str:\n   r = query_engine.query(q)\n   out = []\n   for i, n in enumerate(r.source_nodes or []):\n       out.append(f\"[{i+1}] {n.node.get_content()[:300]}\")\n   return \"\\n\".join(out)\n\n\ndef score_answer(q: str, a: str) -> str:\n   r = query_engine.query(q)\n   ctx = [n.node.get_content() for n in r.source_nodes or []]\n   f = faith_eval.evaluate(query=q, response=a, contexts=ctx)\n   r = rel_eval.evaluate(query=q, response=a, contexts=ctx)\n   return f\"Faithfulness: {f.score}\\nRelevancy: {r.score}\"\n\n\n\nWe define the core tools used by the agent: evidence retrieval and answer evaluation. We implement automatic scoring for faithfulness and relevancy so the agent can judge the quality of its own responses.\n\n\n\nCopy CodeCopiedUse a different Browserfrom llama_index.core.agent.workflow import ReActAgent\nfrom llama_index.core.workflow import Context\n\n\nagent = ReActAgent(\n   tools=[retrieve_evidence, score_answer],\n   llm=Settings.llm,\n   system_prompt=\"\"\"\nAlways retrieve evidence first.\nProduce a structured answer.\nEvaluate the answer and revise once if scores are low.\n\"\"\",\n   verbose=True\n)\n\n\nctx = Context(agent)\n\n\n\nWe create the ReAct-based agent and define its system behavior, guiding how it retrieves evidence, generates answers, and revises results. We also initialize the execution context that maintains the agent’s state across interactions. It step brings together tools and reasoning into a single agentic workflow.\n\n\n\nCopy CodeCopiedUse a different Browserasync def run_brief(topic: str):\n   q = f\"Design a reliable RAG + tool-using agent workflow and how to evaluate it. Topic: {topic}\"\n   handler = agent.run(q, ctx=ctx)\n   async for ev in handler.stream_events():\n       print(getattr(ev, \"delta\", \"\"), end=\"\")\n   res = await handler\n   return str(res)\n\n\ntopic = \"RAG agent reliability and evaluation\"\nloop = asyncio.get_event_loop()\nresult = loop.run_until_complete(run_brief(topic))\n\n\nprint(\"\\n\\nFINAL OUTPUT\\n\")\nprint(result)\n\n\n\nWe execute the full agent loop by passing a topic into the system and streaming the agent’s reasoning and output. We allow the agent to complete its retrieval, generation, and evaluation cycle asynchronously.\n\n\n\nIn conclusion, we showcased how an agent can retrieve supporting evidence, generate a structured response, and assess its own faithfulness and relevancy before finalizing an answer. We kept the design modular and transparent, making it easy to extend the workflow with additional tools, evaluators, or domain-specific knowledge sources. This approach illustrates how we can use agentic AI with LlamaIndex and OpenAI models to build more capable systems that are also more reliable and self-aware in their reasoning and responses.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How to Build a Self-Evaluating Agentic AI System with LlamaIndex and OpenAI Using Retrieval, Tool Use, and Automated Quality Checks appeared first on MarkTechPost.",
          "url": "https://www.marktechpost.com/2026/01/17/how-to-build-a-self-evaluating-agentic-ai-system-with-llamaindex-and-openai-using-retrieval-tool-use-and-automated-quality-checks/",
          "author": "Asif Razzaq",
          "published": "2026-01-17T21:56:49",
          "source": "MarkTechPost",
          "source_type": "rss",
          "tags": [
            "Agentic AI",
            "AI Agents",
            "Editors Pick",
            "Staff",
            "Tutorials"
          ],
          "summary": "Technical tutorial demonstrating how to build a self-evaluating agentic AI system using LlamaIndex and OpenAI. Covers retrieval-augmented generation, tool use, and automated quality checks for more reliable AI outputs.",
          "importance_score": 35.0,
          "reasoning": "Educational tutorial content rather than news. Covers existing techniques in agentic AI without announcing new capabilities or research breakthroughs.",
          "themes": [
            "AI Tutorials",
            "Agentic AI",
            "RAG Systems"
          ],
          "continuation": null,
          "summary_html": "<p>Technical tutorial demonstrating how to build a self-evaluating agentic AI system using LlamaIndex and OpenAI. Covers retrieval-augmented generation, tool use, and automated quality checks for more reliable AI outputs.</p>",
          "content_html": "<p>In this tutorial, we build an advanced agentic AI workflow using LlamaIndex and OpenAI models. We focus on designing a reliable retrieval-augmented generation (RAG) agent that can reason over evidence, use tools deliberately, and evaluate its own outputs for quality. By structuring the system around retrieval, answer synthesis, and self-evaluation, we demonstrate how agentic patterns go beyond simple chatbots and move toward more trustworthy, controllable AI systems suitable for research and analytical use cases.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U llama-index llama-index-llms-openai llama-index-embeddings-openai nest_asyncio</p>\n<p>import os</p>\n<p>import asyncio</p>\n<p>import nest_asyncio</p>\n<p>nest_asyncio.apply()</p>\n<p>from getpass import getpass</p>\n<p>if not os.environ.get(\"OPENAI_API_KEY\"):</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")</p>\n<p>We set up the environment and install all required dependencies for running an agentic AI workflow. We securely load the OpenAI API key at runtime, ensuring that credentials are never hardcoded. We also prepare the notebook to handle asynchronous execution smoothly.</p>\n<p>Copy CodeCopiedUse a different Browserfrom llama_index.core import Document, VectorStoreIndex, Settings</p>\n<p>from llama_index.llms.openai import OpenAI</p>\n<p>from llama_index.embeddings.openai import OpenAIEmbedding</p>\n<p>Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)</p>\n<p>Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")</p>\n<p>texts = [</p>\n<p>\"Reliable RAG systems separate retrieval, synthesis, and verification. Common failures include hallucination and shallow retrieval.\",</p>\n<p>\"RAG evaluation focuses on faithfulness, answer relevancy, and retrieval quality.\",</p>\n<p>\"Tool-using agents require constrained tools, validation, and self-review loops.\",</p>\n<p>\"A robust workflow follows retrieve, answer, evaluate, and revise steps.\"</p>\n<p>]</p>\n<p>docs = [Document(text=t) for t in texts]</p>\n<p>index = VectorStoreIndex.from_documents(docs)</p>\n<p>query_engine = index.as_query_engine(similarity_top_k=4)</p>\n<p>We configure the OpenAI language model and embedding model and build a compact knowledge base for our agent. We transform raw text into indexed documents so that the agent can retrieve relevant evidence during reasoning.</p>\n<p>Copy CodeCopiedUse a different Browserfrom llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator</p>\n<p>faith_eval = FaithfulnessEvaluator(llm=Settings.llm)</p>\n<p>rel_eval = RelevancyEvaluator(llm=Settings.llm)</p>\n<p>def retrieve_evidence(q: str) -&gt; str:</p>\n<p>r = query_engine.query(q)</p>\n<p>out = []</p>\n<p>for i, n in enumerate(r.source_nodes or []):</p>\n<p>out.append(f\"[{i+1}] {n.node.get_content()[:300]}\")</p>\n<p>return \"\\n\".join(out)</p>\n<p>def score_answer(q: str, a: str) -&gt; str:</p>\n<p>r = query_engine.query(q)</p>\n<p>ctx = [n.node.get_content() for n in r.source_nodes or []]</p>\n<p>f = faith_eval.evaluate(query=q, response=a, contexts=ctx)</p>\n<p>r = rel_eval.evaluate(query=q, response=a, contexts=ctx)</p>\n<p>return f\"Faithfulness: {f.score}\\nRelevancy: {r.score}\"</p>\n<p>We define the core tools used by the agent: evidence retrieval and answer evaluation. We implement automatic scoring for faithfulness and relevancy so the agent can judge the quality of its own responses.</p>\n<p>Copy CodeCopiedUse a different Browserfrom llama_index.core.agent.workflow import ReActAgent</p>\n<p>from llama_index.core.workflow import Context</p>\n<p>agent = ReActAgent(</p>\n<p>tools=[retrieve_evidence, score_answer],</p>\n<p>llm=Settings.llm,</p>\n<p>system_prompt=\"\"\"</p>\n<p>Always retrieve evidence first.</p>\n<p>Produce a structured answer.</p>\n<p>Evaluate the answer and revise once if scores are low.</p>\n<p>\"\"\",</p>\n<p>verbose=True</p>\n<p>)</p>\n<p>ctx = Context(agent)</p>\n<p>We create the ReAct-based agent and define its system behavior, guiding how it retrieves evidence, generates answers, and revises results. We also initialize the execution context that maintains the agent’s state across interactions. It step brings together tools and reasoning into a single agentic workflow.</p>\n<p>Copy CodeCopiedUse a different Browserasync def run_brief(topic: str):</p>\n<p>q = f\"Design a reliable RAG + tool-using agent workflow and how to evaluate it. Topic: {topic}\"</p>\n<p>handler = agent.run(q, ctx=ctx)</p>\n<p>async for ev in handler.stream_events():</p>\n<p>print(getattr(ev, \"delta\", \"\"), end=\"\")</p>\n<p>res = await handler</p>\n<p>return str(res)</p>\n<p>topic = \"RAG agent reliability and evaluation\"</p>\n<p>loop = asyncio.get_event_loop()</p>\n<p>result = loop.run_until_complete(run_brief(topic))</p>\n<p>print(\"\\n\\nFINAL OUTPUT\\n\")</p>\n<p>print(result)</p>\n<p>We execute the full agent loop by passing a topic into the system and streaming the agent’s reasoning and output. We allow the agent to complete its retrieval, generation, and evaluation cycle asynchronously.</p>\n<p>In conclusion, we showcased how an agent can retrieve supporting evidence, generate a structured response, and assess its own faithfulness and relevancy before finalizing an answer. We kept the design modular and transparent, making it easy to extend the workflow with additional tools, evaluators, or domain-specific knowledge sources. This approach illustrates how we can use agentic AI with LlamaIndex and OpenAI models to build more capable systems that are also more reliable and self-aware in their reasoning and responses.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How to Build a Self-Evaluating Agentic AI System with LlamaIndex and OpenAI Using Retrieval, Tool Use, and Automated Quality Checks appeared first on MarkTechPost.</p>"
        }
      ]
    },
    "research": {
      "count": 9,
      "category_summary": "An unusually light day for AI research, with only two substantive original contributions. The standout is a novel **AI-assisted policy analysis** using **Claude Sonnet 4.5** with web search to [systematically catalog](/?date=2026-01-18&category=research#item-d473a553750e) every US congressperson's public AGI positions—producing actionable governance data.\n\n- A philosophical piece on AI safety [argues that **flourishing-focused interventions**](/?date=2026-01-18&category=research#item-32842710cab1) may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing\n- **AISC** [project update](/?date=2026-01-18&category=research#item-516cf5c335c1) on 'Understanding Trust' references an **IQA paper** output from Spring 2025 cohort work\n- **MATS Summer 2026** [applications closing](/?date=2026-01-18&category=research#item-b1120185b7d9) January 18th—relevant for safety talent pipeline but not research itself\n\nRemaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.",
      "category_summary_html": "<p>An unusually light day for AI research, with only two substantive original contributions. The standout is a novel <strong>AI-assisted policy analysis</strong> using <strong>Claude Sonnet 4.5</strong> with web search to <a href=\"/?date=2026-01-18&amp;category=research#item-d473a553750e\" class=\"internal-link\" rel=\"noopener noreferrer\">systematically catalog</a> every US congressperson's public AGI positions—producing actionable governance data.</p>\n<ul>\n<li>A philosophical piece on AI safety <a href=\"/?date=2026-01-18&amp;category=research#item-32842710cab1\" class=\"internal-link\" rel=\"noopener noreferrer\">argues that <strong>flourishing-focused interventions</strong></a> may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing</li>\n<li><strong>AISC</strong> <a href=\"/?date=2026-01-18&amp;category=research#item-516cf5c335c1\" class=\"internal-link\" rel=\"noopener noreferrer\">project update</a> on 'Understanding Trust' references an <strong>IQA paper</strong> output from Spring 2025 cohort work</li>\n<li><strong>MATS Summer 2026</strong> <a href=\"/?date=2026-01-18&amp;category=research#item-b1120185b7d9\" class=\"internal-link\" rel=\"noopener noreferrer\">applications closing</a> January 18th—relevant for safety talent pipeline but not research itself</li>\n</ul>\n<p>Remaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.</p>",
      "themes": [
        {
          "name": "AI Policy & Governance",
          "description": "Analysis of political and regulatory landscape for AI development",
          "item_count": 1,
          "example_items": [],
          "importance": 58
        },
        {
          "name": "AI Safety & Alignment",
          "description": "Content related to AI safety research, existential risk from AI, and alignment community activities",
          "item_count": 4,
          "example_items": [],
          "importance": 45
        },
        {
          "name": "Non-AI Topics",
          "description": "Economics, neuroscience, and general life advice unrelated to AI research",
          "item_count": 4,
          "example_items": [],
          "importance": 12
        }
      ],
      "top_items": [
        {
          "id": "d473a553750e",
          "title": "What Washington Says About AGI",
          "content": "I spent a few hundred dollars on Anthropic API credits and let Claude individually research every current US congressperson's position on AI. This is a summary of my findings.Disclaimer: Summarizing people's beliefs is hard and inherently subjective and noisy. Likewise, US politicians change their opinions on things constantly so it's hard to know what's up-to-date. Also, I vibe-coded a lot of this.MethodologyI used Claude Sonnet 4.5 with web search to research every congressperson's public statements on AI, then used GPT-4o to score each politician on how \"AGI-pilled\" they are, how concerned they are about existential risk, and how focused they are on US-China AI competition. I plotted these scores against GovTrack ideology data to search for any partisan splits.I. AGI awareness is not partisan and not widespreadFew members of Congress have public statements taking AGI seriously. For those that do, the difference is not in political ideology. If we simply plot the AGI-pilled score vs the ideology score, we observe no obvious partisan split.There are 151 congresspeople who Claude could not find substantial quotes about AI from. These members are not included on this plot or any of the plots which follow.&nbsp;II. Existential risk is partisan at the tailsWhen you change the scoring prompt to ask how much a congressperson's statements reflect a concern about existential risk, the plot looks different. Note that the scoring prompt here emphasizes \"A politician who is most XRisk-pilled is someone who thinks AI is a risk to humanity -- not just the US.\" This separates x-risk concerns from fears related to US-China relations.This graph looks mostly like noise but it does show that the majority of the most x-risk pilled politicians are Democrats.[1]&nbsp;This is troubling. Politics is a mind-killer and if AI Safety becomes partisan, productive debate will be even more difficult than it currently is.III. Both parties are fixated on ChinaSome congresspeople have made up thei...",
          "url": "https://www.lesswrong.com/posts/WLdcvAcoFZv9enR37/what-washington-says-about-agi",
          "author": "zroe1",
          "published": "2026-01-17T00:43:46.426000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Original research using Claude Sonnet 4.5 with web search to systematically analyze every US congressperson's public statements on AI. Findings show AGI awareness is not partisan but is rare, with x-risk concerns and US-China competition focus also mapped against ideology.",
          "importance_score": 58,
          "reasoning": "Novel methodology applying AI tools to policy research. Produces actionable data on AI governance landscape. Relevant to understanding political dynamics around AI regulation. Limitations in methodology (vibe-coded, model hallucination risks) noted but still valuable.",
          "themes": [
            "AI Policy",
            "AI Governance",
            "US Politics",
            "AI-Assisted Research"
          ],
          "continuation": null,
          "summary_html": "<p>Original research using Claude Sonnet 4.5 with web search to systematically analyze every US congressperson's public statements on AI. Findings show AGI awareness is not partisan but is rare, with x-risk concerns and US-China competition focus also mapped against ideology.</p>",
          "content_html": "<p>I spent a few hundred dollars on Anthropic API credits and let Claude individually research every current US congressperson's position on AI. This is a summary of my findings.Disclaimer: Summarizing people's beliefs is hard and inherently subjective and noisy. Likewise, US politicians change their opinions on things constantly so it's hard to know what's up-to-date. Also, I vibe-coded a lot of this.MethodologyI used Claude Sonnet 4.5 with web search to research every congressperson's public statements on AI, then used GPT-4o to score each politician on how \"AGI-pilled\" they are, how concerned they are about existential risk, and how focused they are on US-China AI competition. I plotted these scores against GovTrack ideology data to search for any partisan splits.I. AGI awareness is not partisan and not widespreadFew members of Congress have public statements taking AGI seriously. For those that do, the difference is not in political ideology. If we simply plot the AGI-pilled score vs the ideology score, we observe no obvious partisan split.There are 151 congresspeople who Claude could not find substantial quotes about AI from. These members are not included on this plot or any of the plots which follow.&nbsp;II. Existential risk is partisan at the tailsWhen you change the scoring prompt to ask how much a congressperson's statements reflect a concern about existential risk, the plot looks different. Note that the scoring prompt here emphasizes \"A politician who is most XRisk-pilled is someone who thinks AI is a risk to humanity -- not just the US.\" This separates x-risk concerns from fears related to US-China relations.This graph looks mostly like noise but it does show that the majority of the most x-risk pilled politicians are Democrats.[1]&nbsp;This is troubling. Politics is a mind-killer and if AI Safety becomes partisan, productive debate will be even more difficult than it currently is.III. Both parties are fixated on ChinaSome congresspeople have made up thei...</p>"
        },
        {
          "id": "32842710cab1",
          "title": "Focusing on Flourishing Even When Survival is Unlikely (I)",
          "content": "1. The CaseYou've probably heard something like this before:If we survive this century, the expected value of the future is massive.If we don't survive, the expected value is near zero.Therefore, the value of an intervention is approximately proportional to how much it increases the chance of survival.You won't go badly wrong following the conclusion, but (3) doesn't actually follow from (1) and (2). That's because interventions might vary in how they affect the expected value of the future conditional on survival.[1]Will MacAskill makes roughly this argument in Better Futures (August 2025). See the diagram below: survival-focused interventions target the red rectangle, flourishing-focused interventions target the blue. But the blue rectangle might be much larger than the red rectangle -- if x-risk is 20% then even the best survival intervention can increase EV by at most 1.2x, whereas a flourishing intervention could increase EV by 5x or 5000x.But is x-risk only 20%? MacAskill thinks so,[2]&nbsp;but his argument applies even if extinction is very likely — say 99% — so long as there are interventions that increase flourishing by +100x. That's the scenario I want to discuss here in this post.My conclusion is:When survival is unlikely, focusing on flourishing becomes qualitatively different in ways that cut against focusing on flourishing.There are various strategies for focusing on flourishing when survival is unlikely, but none are attractive.&nbsp;Overall, when survival is unlikely, we shouldn't focus on flourishing. Even if you buy that the best possible futures are hundreds of times better than the middling futures.2. ChallengesRecall that flourishing is the expected value conditional on survival. That is, flourishing-focused interventions target the survival posterior, consisting of the green and blue rectangles. Consequentially, if survival is likely then the survival posterior consists of ordinary futures, but if survival is unlikely then the survival posterio...",
          "url": "https://www.lesswrong.com/posts/cjGALjyJEvenyP9pD/focusing-on-flourishing-even-when-survival-is-unlikely-i",
          "author": "Cleo Nardo",
          "published": "2026-01-17T13:47:59.025000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Philosophical argument that even when existential risk is very high (e.g., 99%), interventions focused on improving the quality of post-survival futures ('flourishing') may have higher expected value than survival-focused interventions. References Will MacAskill's August 2025 work 'Better Futures.'",
          "importance_score": 42,
          "reasoning": "Novel framing for AI safety prioritization with mathematical structure. Challenges common assumptions about x-risk intervention value calculations. Philosophical rather than technical, but has implications for AI safety resource allocation.",
          "themes": [
            "AI Safety",
            "Existential Risk",
            "Effective Altruism",
            "Philosophy"
          ],
          "continuation": null,
          "summary_html": "<p>Philosophical argument that even when existential risk is very high (e.g., 99%), interventions focused on improving the quality of post-survival futures ('flourishing') may have higher expected value than survival-focused interventions. References Will MacAskill's August 2025 work 'Better Futures.'</p>",
          "content_html": "<p>1. The CaseYou've probably heard something like this before:If we survive this century, the expected value of the future is massive.If we don't survive, the expected value is near zero.Therefore, the value of an intervention is approximately proportional to how much it increases the chance of survival.You won't go badly wrong following the conclusion, but (3) doesn't actually follow from (1) and (2). That's because interventions might vary in how they affect the expected value of the future conditional on survival.[1]Will MacAskill makes roughly this argument in Better Futures (August 2025). See the diagram below: survival-focused interventions target the red rectangle, flourishing-focused interventions target the blue. But the blue rectangle might be much larger than the red rectangle -- if x-risk is 20% then even the best survival intervention can increase EV by at most 1.2x, whereas a flourishing intervention could increase EV by 5x or 5000x.But is x-risk only 20%? MacAskill thinks so,[2]&nbsp;but his argument applies even if extinction is very likely — say 99% — so long as there are interventions that increase flourishing by +100x. That's the scenario I want to discuss here in this post.My conclusion is:When survival is unlikely, focusing on flourishing becomes qualitatively different in ways that cut against focusing on flourishing.There are various strategies for focusing on flourishing when survival is unlikely, but none are attractive.&nbsp;Overall, when survival is unlikely, we shouldn't focus on flourishing. Even if you buy that the best possible futures are hundreds of times better than the middling futures.2. ChallengesRecall that flourishing is the expected value conditional on survival. That is, flourishing-focused interventions target the survival posterior, consisting of the green and blue rectangles. Consequentially, if survival is likely then the survival posterior consists of ordinary futures, but if survival is unlikely then the survival posterio...</p>"
        },
        {
          "id": "516cf5c335c1",
          "title": "Understanding Trust: Project Update",
          "content": "This is a brief note on what I did with my funding in 2025, and my plans for 2026, written primarily because Manifund nudged me for an update on my project.I ran my AISC project (which I announced here) with four mentees in Spring 2025: Norman Hsia, Hanna Gabor, Paul Rapoport, and Roman Malov. A few other people attended the weekly meetings as well, and those regular meetings have continued (they are joinable -- pm me if interested). Norman and Paul ended up as coauthors of my ILIAD 2024 paper Understanding Trust, which had been drafted in 2024, so served as both an input and an output of the AISC project.I recorded most of the meetings involved in the project, as one of the hopeful outputs was publicly posted videos explaining the research agenda. I've proven to be bad at this side of things: I don't like listening to myself talk, so I found it difficult to edit or even to review edits done by others. I'm finally uploading the videos with minimal AI-orchestrated edits. Playlist here. At the time of publication, there's only two, but more coming very soon. If you are OK with the almost-unedited presentation style, it should be a good resource to get a very in-depth view on my thinking about AI safety and decision theory; a thorough snapshot of my thinking as of spring 2025.In 2025, I obtained funding for 2025 as well as 2026. (My total financial runway is longer than this, but 2025 and 2026 have been funded by grants/donations which compensated me for my continued research at specific price points.) I'm opening up my Manifund project for funding for 2027, for those who feel so inclined.In addition to publishing the ILIAD 2024 paper, I also published an ILIAD 2025 paper: Communication &amp; Trust. I consider it to be an incremental improvement: the ILIAD 2024 treated self-modifying actions as a distinct class with known effects which work with certainty. The ILIAD 2025 paper treated all actions as having some subjective chance of disrupting the agent's computation.&n...",
          "url": "https://www.lesswrong.com/posts/yig4LeEfpkFfiWpk2/understanding-trust-project-update",
          "author": "abramdemski",
          "published": "2026-01-17T16:19:09.213000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Project update from abramdemski on his AISC (AI Safety Camp) project about 'Understanding Trust,' which ran in Spring 2025 with four mentees. The project produced a co-authored ILIAD 2024 paper and recorded videos explaining the research agenda are being uploaded.",
          "importance_score": 28,
          "reasoning": "Administrative update rather than substantive research content. References actual research output (ILIAD paper) but doesn't present technical details. Useful for those tracking alignment research community activities but low standalone research value.",
          "themes": [
            "AI Safety",
            "Alignment Research",
            "Research Community"
          ],
          "continuation": null,
          "summary_html": "<p>Project update from abramdemski on his AISC (AI Safety Camp) project about 'Understanding Trust,' which ran in Spring 2025 with four mentees. The project produced a co-authored ILIAD 2024 paper and recorded videos explaining the research agenda are being uploaded.</p>",
          "content_html": "<p>This is a brief note on what I did with my funding in 2025, and my plans for 2026, written primarily because Manifund nudged me for an update on my project.I ran my AISC project (which I announced here) with four mentees in Spring 2025: Norman Hsia, Hanna Gabor, Paul Rapoport, and Roman Malov. A few other people attended the weekly meetings as well, and those regular meetings have continued (they are joinable -- pm me if interested). Norman and Paul ended up as coauthors of my ILIAD 2024 paper Understanding Trust, which had been drafted in 2024, so served as both an input and an output of the AISC project.I recorded most of the meetings involved in the project, as one of the hopeful outputs was publicly posted videos explaining the research agenda. I've proven to be bad at this side of things: I don't like listening to myself talk, so I found it difficult to edit or even to review edits done by others. I'm finally uploading the videos with minimal AI-orchestrated edits. Playlist here. At the time of publication, there's only two, but more coming very soon. If you are OK with the almost-unedited presentation style, it should be a good resource to get a very in-depth view on my thinking about AI safety and decision theory; a thorough snapshot of my thinking as of spring 2025.In 2025, I obtained funding for 2025 as well as 2026. (My total financial runway is longer than this, but 2025 and 2026 have been funded by grants/donations which compensated me for my continued research at specific price points.) I'm opening up my Manifund project for funding for 2027, for those who feel so inclined.In addition to publishing the ILIAD 2024 paper, I also published an ILIAD 2025 paper: Communication &amp; Trust. I consider it to be an incremental improvement: the ILIAD 2024 treated self-modifying actions as a distinct class with known effects which work with certainty. The ILIAD 2025 paper treated all actions as having some subjective chance of disrupting the agent's computation.&amp;n...</p>"
        },
        {
          "id": "b1120185b7d9",
          "title": "Applying to MATS: What the Program Is Like, and Who It’s For",
          "content": "Application deadline: Three days remaining! MATS Summer 2026 applications close this Sunday, January 18, 2026 AOE. We've shortened the application this year. Most people finish in 1–2 hours, and we'll get back to applicants about first stage results by the end of January. Visit our website for details: matsprogram.org/apply.TL;DR:&nbsp;This post is a follow-up to our shorter announcement that MATS Summer 2026 applications are open. It's intended for people who are considering applying and want a clearer sense of what the program is actually like, how mentorship and research support work in practice, and whether MATS is likely to be a good fit.What MATS is trying to doMATS aims to find and train talented individuals for what we see as one of the world's most urgent and talent-constrained problems: reducing risks from unaligned AI. We believe ambitious people from a wide range of backgrounds can meaningfully contribute to this work. Our program provides the mentorship, funding, training, and community to make that happen.Since late 2021, MATS has supported over 500 researchers working with more than 100 mentors from organizations like Anthropic, Google DeepMind, OpenAI, UK AISI, GovAI, METR, Apollo, RAND, AI Futures Project, Redwood Research, and more. Fellows have collectively co-authored 160+ research papers, with 7,800+ citations and an organizational h-index of 40.Fellows have contributed to research agendas like:Sparse auto-encoders for AI interpretabilityActivation/representation engineeringEmergent misalignmentEvaluating situational awarenessInoculation promptingDevelopmental interpretabilityComputational mechanicsGradient routingApproximately 80% of alumni now work directly in AI safety/security, and around 10% have gone on to co-found AI safety organizations or research teams. These 30+ initiatives include Apollo Research,&nbsp;Atla AI,&nbsp;Timaeus, Simplex,&nbsp;Leap Labs,&nbsp;Theorem Labs,&nbsp;Workshop Labs, and Watertight AIWhat fellows receiveThe initi...",
          "url": "https://www.lesswrong.com/posts/GJWgXZ3jjYfkfzKut/applying-to-mats-what-the-program-is-like-and-who-it-s-for",
          "author": "Raj Thimmiah",
          "published": "2026-01-16T19:25:16.002000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Detailed information about the MATS (ML Alignment Theory Scholars) Summer 2026 program with applications closing January 18, 2026. Describes program structure, mentorship model, and success metrics for prospective AI safety researchers.",
          "importance_score": 30,
          "reasoning": "Important community resource for AI safety talent pipeline. Not research itself but relevant to field-building. MATS has supported 500+ researchers and connects to major labs (Anthropic, DeepMind, OpenAI).",
          "themes": [
            "AI Safety",
            "Research Training",
            "Field Building"
          ],
          "continuation": null,
          "summary_html": "<p>Detailed information about the MATS (ML Alignment Theory Scholars) Summer 2026 program with applications closing January 18, 2026. Describes program structure, mentorship model, and success metrics for prospective AI safety researchers.</p>",
          "content_html": "<p>Application deadline: Three days remaining! MATS Summer 2026 applications close this Sunday, January 18, 2026 AOE. We've shortened the application this year. Most people finish in 1–2 hours, and we'll get back to applicants about first stage results by the end of January. Visit our website for details: matsprogram.org/apply.TL;DR:&nbsp;This post is a follow-up to our shorter announcement that MATS Summer 2026 applications are open. It's intended for people who are considering applying and want a clearer sense of what the program is actually like, how mentorship and research support work in practice, and whether MATS is likely to be a good fit.What MATS is trying to doMATS aims to find and train talented individuals for what we see as one of the world's most urgent and talent-constrained problems: reducing risks from unaligned AI. We believe ambitious people from a wide range of backgrounds can meaningfully contribute to this work. Our program provides the mentorship, funding, training, and community to make that happen.Since late 2021, MATS has supported over 500 researchers working with more than 100 mentors from organizations like Anthropic, Google DeepMind, OpenAI, UK AISI, GovAI, METR, Apollo, RAND, AI Futures Project, Redwood Research, and more. Fellows have collectively co-authored 160+ research papers, with 7,800+ citations and an organizational h-index of 40.Fellows have contributed to research agendas like:Sparse auto-encoders for AI interpretabilityActivation/representation engineeringEmergent misalignmentEvaluating situational awarenessInoculation promptingDevelopmental interpretabilityComputational mechanicsGradient routingApproximately 80% of alumni now work directly in AI safety/security, and around 10% have gone on to co-found AI safety organizations or research teams. These 30+ initiatives include Apollo Research,&nbsp;Atla AI,&nbsp;Timaeus, Simplex,&nbsp;Leap Labs,&nbsp;Theorem Labs,&nbsp;Workshop Labs, and Watertight AIWhat fellows receiveThe initi...</p>"
        },
        {
          "id": "d61a988bfca9",
          "title": "Turning Down the Overthinking: How Cathodal Brain Stimulation Could Transform Stuttering Therapy",
          "content": "The cruelest irony of stuttering is that trying harder to speak fluently makes it worse. Not trying harder in the sense of practice or effort, but trying harder in the sense of conscious attention to speech mechanics. When someone who stutters focuses intently on controlling their words, analyzing their breathing, and monitoring their mouth movements, their speech doesn't improve. It deteriorates.This is the reinvestment hypothesis in action: explicit, conscious control actively interferes with skills that should be automatic. A pianist who thinks too carefully about finger placement plays worse. An athlete who consciously monitors their form chokes under pressure. And a person who stutters, desperately focusing on each syllable, finds their speech becoming more fragmented, not less.For the 70 million people worldwide who stutter, this creates a devastating trap. They know their speech is broken. They focus intensely on fixing it. And that very focus makes the problem worse.What if we could temporarily turn off that interference? What if we could create a neural state where the overthinking stops, where the brain's executive control systems step aside and let procedural motor learning do its work? And what if we could do this precisely during speech practice, when the brain is trying to encode new, fluent motor patterns?TDCS Shows Promise, But We're Targeting the Wrong MechanismBrain stimulation for stuttering isn't a new idea. Over the past seven years, researchers have tested transcranial direct current stimulation (tDCS) in adults who stutter, with mixed but encouraging results.The landmark study came from Oxford in 2018. Chesters and colleagues ran a rigorous double-blind trial with 30 adults who stutter. The intervention was straightforward: 1 milliamp of anodal (excitatory) tDCS applied to the left inferior frontal cortex for 20 minutes, five days in a row, while participants practiced fluency-inducing speech techniques like choral reading and metronome-timed ...",
          "url": "https://www.lesswrong.com/posts/iXsbazekj4tTxP5zp/turning-down-the-overthinking-how-cathodal-brain-stimulation",
          "author": "Rudaiba",
          "published": "2026-01-17T09:54:46.826000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Neuroscience exploration of how cathodal brain stimulation might help stuttering by reducing conscious interference with automatic speech processes. Based on the 'reinvestment hypothesis' that conscious attention can impair motor skills that should be automatic.",
          "importance_score": 18,
          "reasoning": "Interesting neuroscience with potential parallels to AI learning (explicit vs procedural knowledge), but not AI research. Could have distant relevance to understanding human cognition for AI alignment purposes.",
          "themes": [
            "Neuroscience",
            "Medical Research",
            "Motor Learning"
          ],
          "continuation": null,
          "summary_html": "<p>Neuroscience exploration of how cathodal brain stimulation might help stuttering by reducing conscious interference with automatic speech processes. Based on the 'reinvestment hypothesis' that conscious attention can impair motor skills that should be automatic.</p>",
          "content_html": "<p>The cruelest irony of stuttering is that trying harder to speak fluently makes it worse. Not trying harder in the sense of practice or effort, but trying harder in the sense of conscious attention to speech mechanics. When someone who stutters focuses intently on controlling their words, analyzing their breathing, and monitoring their mouth movements, their speech doesn't improve. It deteriorates.This is the reinvestment hypothesis in action: explicit, conscious control actively interferes with skills that should be automatic. A pianist who thinks too carefully about finger placement plays worse. An athlete who consciously monitors their form chokes under pressure. And a person who stutters, desperately focusing on each syllable, finds their speech becoming more fragmented, not less.For the 70 million people worldwide who stutter, this creates a devastating trap. They know their speech is broken. They focus intensely on fixing it. And that very focus makes the problem worse.What if we could temporarily turn off that interference? What if we could create a neural state where the overthinking stops, where the brain's executive control systems step aside and let procedural motor learning do its work? And what if we could do this precisely during speech practice, when the brain is trying to encode new, fluent motor patterns?TDCS Shows Promise, But We're Targeting the Wrong MechanismBrain stimulation for stuttering isn't a new idea. Over the past seven years, researchers have tested transcranial direct current stimulation (tDCS) in adults who stutter, with mixed but encouraging results.The landmark study came from Oxford in 2018. Chesters and colleagues ran a rigorous double-blind trial with 30 adults who stutter. The intervention was straightforward: 1 milliamp of anodal (excitatory) tDCS applied to the left inferior frontal cortex for 20 minutes, five days in a row, while participants practiced fluency-inducing speech techniques like choral reading and metronome-timed ...</p>"
        },
        {
          "id": "ae0e0e81f2b1",
          "title": "Lightcone is hiring a generalist, a designer, and a campus operations co-lead",
          "content": "Lightcone is hiring! We build beautiful things for truth-seeking and world-saving.&nbsp;We are hiring for three different positions: a senior designer, a campus manager, and a core team generalist. This is the first time in almost two years where we are actively hiring and trying to grow our team!&nbsp;&nbsp;Senior DesignerWhen we are at our best, I think we produce world-class design. AI 2027 was I think a great design achievement, so is much of LessWrong.com itself. I also think on a product and business level, making things beautiful and intuitive and well-crafted is crucial. I like some of Patrick Collison's thinking on this:If Stripe is a monstrously successful business, but what we make isn’t beautiful, and Stripe doesn’t embody a culture of incredibly exacting craftsmanship, I’ll be much less happy. I think the returns to both of those things in the world are really high. I think even beyond the pecuniary or financial returns, the world’s just uglier than it needs to be… One can do things well or poorly, and beauty is not a rivalrous good.My intuition is that more of Stripe’s success than one would think is downstream of the fact that people like beautiful things—and for kind of rational reasons because what does a beautiful thing tell you? Well it tells you the person who made it really cared… And so if you care about the infrastructure being holistically good, indexing on the superficial characteristics that you can actually observe is not an irrational thing to do.I want us to continue making beautiful and well-designed things. Indeed, we currently have enormous demand for making more things like AI 2027 and DecidingToWin.org, with multiple new inquiries for projects like this per month, and I think many of those opportunities could be great. I also think LessWrong itself is substantially bottlenecked on design.Now, design is a very broad category. The specific role I want to hire for is someone helping us make beautiful websites. This very likely implies ...",
          "url": "https://www.lesswrong.com/posts/Wowc8jfvyrsp4a6uk/lightcone-is-hiring-a-generalist-a-designer-and-a-campus",
          "author": "habryka",
          "published": "2026-01-16T20:47:39.061000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Job posting from Lightcone (LessWrong operators) for three positions: senior designer, campus manager, and core team generalist. Discusses organizational values around design quality and truth-seeking.",
          "importance_score": 10,
          "reasoning": "Hiring announcement, not research content. Relevant only to those interested in joining the LessWrong/rationalist community infrastructure.",
          "themes": [
            "Research Community",
            "Hiring"
          ],
          "continuation": null,
          "summary_html": "<p>Job posting from Lightcone (LessWrong operators) for three positions: senior designer, campus manager, and core team generalist. Discusses organizational values around design quality and truth-seeking.</p>",
          "content_html": "<p>Lightcone is hiring! We build beautiful things for truth-seeking and world-saving.&nbsp;We are hiring for three different positions: a senior designer, a campus manager, and a core team generalist. This is the first time in almost two years where we are actively hiring and trying to grow our team!&nbsp;&nbsp;Senior DesignerWhen we are at our best, I think we produce world-class design. AI 2027 was I think a great design achievement, so is much of LessWrong.com itself. I also think on a product and business level, making things beautiful and intuitive and well-crafted is crucial. I like some of Patrick Collison's thinking on this:If Stripe is a monstrously successful business, but what we make isn’t beautiful, and Stripe doesn’t embody a culture of incredibly exacting craftsmanship, I’ll be much less happy. I think the returns to both of those things in the world are really high. I think even beyond the pecuniary or financial returns, the world’s just uglier than it needs to be… One can do things well or poorly, and beauty is not a rivalrous good.My intuition is that more of Stripe’s success than one would think is downstream of the fact that people like beautiful things—and for kind of rational reasons because what does a beautiful thing tell you? Well it tells you the person who made it really cared… And so if you care about the infrastructure being holistically good, indexing on the superficial characteristics that you can actually observe is not an irrational thing to do.I want us to continue making beautiful and well-designed things. Indeed, we currently have enormous demand for making more things like AI 2027 and DecidingToWin.org, with multiple new inquiries for projects like this per month, and I think many of those opportunities could be great. I also think LessWrong itself is substantially bottlenecked on design.Now, design is a very broad category. The specific role I want to hire for is someone helping us make beautiful websites. This very likely implies ...</p>"
        },
        {
          "id": "c9ba47395b18",
          "title": "The truth behind the 2026 J.P. Morgan Healthcare Conference",
          "content": "In 1654, a Jesuit polymath named Athanasius Kircher published Mundus Subterraneus, a comprehensive geography of the Earth’s interior. It had maps and illustrations and rivers of fire and vast subterranean oceans and air channels connecting every volcano on the planet. He wrote that “the whole Earth is not solid but everywhere gaping, and hollowed with empty rooms and spaces, and hidden burrows.”. Alongside comments like this, Athanasius identified the legendary lost island of Atlantis, pondered where one could find the remains of giants, and detailed the kinds of animals that lived in this lower world, including dragons. The book was based entirely on secondhand accounts, like travelers tales, miners reports, classical texts, so it was as comprehensive as it could’ve possibly been.But Athanasius had never been underground and neither had anyone else, not really, not in a way that mattered.Today, I am in San Francisco, the site of the 2026 J.P. Morgan Healthcare Conference, and it feels a lot like Mundus Subterraneus.There is ostensibly plenty of evidence to believe that the conference exists, that it actually occurs between January 12, 2026 to January 16, 2026 at the Westin St. Francis Hotel, 335 Powell Street, San Francisco, and that it has done so for the last forty-four years, just like everyone has told you. There is a website for it, there are articles about it, there are dozens of AI-generated posts on Linkedin about how excited people were about it. But I have never met anyone who has actually been inside the conference.I have never been approached by one, or seated next to one, or introduced to one. They do not appear in my life. They do not appear in anyone’s life that I know. I have put my boots on the ground to rectify this, and asked around, first casually and then less casually, “Do you know anyone who has attended the JPM conference?”, and then they nod, and then I refine the question to be, “No, no, like, someone who has actually been in the physical ...",
          "url": "https://www.lesswrong.com/posts/eopA4MqhrE4dkLjHX/the-truth-behind-the-2026-j-p-morgan-healthcare-conference",
          "author": "Abhishaike Mahajan",
          "published": "2026-01-17T12:28:11.623000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Satirical essay drawing parallels between Athanasius Kircher's speculative 1654 geography 'Mundus Subterraneus' and the author's experience at the 2026 J.P. Morgan Healthcare Conference. Commentary on the nature of secondhand knowledge and conferences.",
          "importance_score": 12,
          "reasoning": "Literary/satirical piece with no AI research content. Interesting writing but not relevant to AI research, safety, or policy discussions.",
          "themes": [
            "Healthcare",
            "Science Commentary"
          ],
          "continuation": null,
          "summary_html": "<p>Satirical essay drawing parallels between Athanasius Kircher's speculative 1654 geography 'Mundus Subterraneus' and the author's experience at the 2026 J.P. Morgan Healthcare Conference. Commentary on the nature of secondhand knowledge and conferences.</p>",
          "content_html": "<p>In 1654, a Jesuit polymath named Athanasius Kircher published Mundus Subterraneus, a comprehensive geography of the Earth’s interior. It had maps and illustrations and rivers of fire and vast subterranean oceans and air channels connecting every volcano on the planet. He wrote that “the whole Earth is not solid but everywhere gaping, and hollowed with empty rooms and spaces, and hidden burrows.”. Alongside comments like this, Athanasius identified the legendary lost island of Atlantis, pondered where one could find the remains of giants, and detailed the kinds of animals that lived in this lower world, including dragons. The book was based entirely on secondhand accounts, like travelers tales, miners reports, classical texts, so it was as comprehensive as it could’ve possibly been.But Athanasius had never been underground and neither had anyone else, not really, not in a way that mattered.Today, I am in San Francisco, the site of the 2026 J.P. Morgan Healthcare Conference, and it feels a lot like Mundus Subterraneus.There is ostensibly plenty of evidence to believe that the conference exists, that it actually occurs between January 12, 2026 to January 16, 2026 at the Westin St. Francis Hotel, 335 Powell Street, San Francisco, and that it has done so for the last forty-four years, just like everyone has told you. There is a website for it, there are articles about it, there are dozens of AI-generated posts on Linkedin about how excited people were about it. But I have never met anyone who has actually been inside the conference.I have never been approached by one, or seated next to one, or introduced to one. They do not appear in my life. They do not appear in anyone’s life that I know. I have put my boots on the ground to rectify this, and asked around, first casually and then less casually, “Do you know anyone who has attended the JPM conference?”, and then they nod, and then I refine the question to be, “No, no, like, someone who has actually been in the physical ...</p>"
        },
        {
          "id": "91fb79db7ee3",
          "title": "Japan is a bank",
          "content": "Among developed countries, Japan has long had the highest debt/GDP ratio, currently ~232%. That seems pretty bad, and conversely has made some people say that the US debt is fine because it's still much lower than Japan's. But here are some points that might clarify the situation: First, that ratio has declined recently, from 258% in 2020. Second, the Japanese government holds a lot of stocks and foreign bonds. Its net debt/GDP is \"only\" 140%, and has declined since 2020. The US government doesn't do that. (The government of Singapore also holds a lot of assets, and Temasek is well-known as a large investment fund, but Japan is a bigger country, and despite smaller holdings per capita, its investments are much larger than Singapore's.) Meanwhile, America's federal debt/GDP ratio is ~124%. Add in state debt and it's ~127%. So the net debt/GDP of the US government isn't that different from Japan. It's still higher, but arguably the \"quality\" of that US GDP is lower, for a couple reasons: The US has a worse trade balance than Japan. It borrows more money, and has a net inflow of investment. That investment and borrowed money then circulates around and raises GDP by some multiplier, mostly by making both prices and incomes higher in the US. Japan has higher PPP/nominal GDP than the US, by a factor of ~1.6x. Arguably this is a better measure of ability to pay back debt with real stuff than nominal GDP. On the other hand, the US does have more natural resources, and the federal goverment owns a lot of land. My point is just that, while I've often seen it said that the US government debt situation is clearly better than Japan's, that's not clearly the case. By the way, another economic metric I think is interesting to compare is median and average personal wealth.",
          "url": "https://www.lesswrong.com/posts/vbXWJSKKynepq7sqY/japan-is-a-bank",
          "author": "bhauth",
          "published": "2026-01-17T11:33:04.553000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Economic analysis comparing Japan's debt/GDP ratio to the US, arguing that when accounting for government-held assets, Japan's net debt position isn't as alarming as headline figures suggest, and the US situation is more comparable than typically assumed.",
          "importance_score": 8,
          "reasoning": "Purely economic/financial analysis with no AI relevance. Well-reasoned but completely outside AI research scope.",
          "themes": [
            "Economics",
            "Finance"
          ],
          "continuation": null,
          "summary_html": "<p>Economic analysis comparing Japan's debt/GDP ratio to the US, arguing that when accounting for government-held assets, Japan's net debt position isn't as alarming as headline figures suggest, and the US situation is more comparable than typically assumed.</p>",
          "content_html": "<p>Among developed countries, Japan has long had the highest debt/GDP ratio, currently ~232%. That seems pretty bad, and conversely has made some people say that the US debt is fine because it's still much lower than Japan's. But here are some points that might clarify the situation: First, that ratio has declined recently, from 258% in 2020. Second, the Japanese government holds a lot of stocks and foreign bonds. Its net debt/GDP is \"only\" 140%, and has declined since 2020. The US government doesn't do that. (The government of Singapore also holds a lot of assets, and Temasek is well-known as a large investment fund, but Japan is a bigger country, and despite smaller holdings per capita, its investments are much larger than Singapore's.) Meanwhile, America's federal debt/GDP ratio is ~124%. Add in state debt and it's ~127%. So the net debt/GDP of the US government isn't that different from Japan. It's still higher, but arguably the \"quality\" of that US GDP is lower, for a couple reasons: The US has a worse trade balance than Japan. It borrows more money, and has a net inflow of investment. That investment and borrowed money then circulates around and raises GDP by some multiplier, mostly by making both prices and incomes higher in the US. Japan has higher PPP/nominal GDP than the US, by a factor of ~1.6x. Arguably this is a better measure of ability to pay back debt with real stuff than nominal GDP. On the other hand, the US does have more natural resources, and the federal goverment owns a lot of land. My point is just that, while I've often seen it said that the US government debt situation is clearly better than Japan's, that's not clearly the case. By the way, another economic metric I think is interesting to compare is median and average personal wealth.</p>"
        },
        {
          "id": "7cbfeb9c04a2",
          "title": "Forfeiting Ill-Gotten Gains",
          "content": "It's a holiday. The cousins are over, and the kids are having a great time. Unfortunately, that includes rampaging through the kitchen. We're trying to cook, so there's a \"no cutting through the kitchen\" rule. Imagine enforcement looks like: Kid: [dashes into kitchen, pursued by cousin] Adult: Out of the kitchen! Kid: Sorry! [Continues their path, leaving through the other door; escapes pursuit from more rule-abiding cousin] This doesn't work! The kid got what they wanted out of this interaction, and isn't going to change their behavior. Instead, I need to make it be not worth their while: Kid: [dashes into kitchen, pursued by cousin] Adult: No cutting through the kitchen! [Physically rebuffs intruder]! Kid: Sorry! [Forced to leave through the door they entered by; caught by cousin.] Other examples: Sneak candy, spit it out and forfeit dessert. Use sibling's tablet time, lose your own. Interrupt, be ignored. The general principle is that if you want to limit behavior the combination of the gains from rule-breaking and penalty from punishment need to put the kid in a worse position than if they'd never broken the rule. This isn't just a parenting thing: it's common to say that \"crime should not pay\", and many legal systems prohibit unjust enrichment. One place I'd like to see this implemented is airplane evacuation. If the safety announcements included \"In the event of an emergency evacuation, any carry-on luggage you bring will be confiscated and destroyed. You will also be fined.\" we would have more JAL 516 (379 occupants, zero deaths) and less Aeroflot 1492 or Emirates 521. Comment via: facebook, mastodon, bluesky",
          "url": "https://www.lesswrong.com/posts/pyuhYvkqX9Lzr6QWX/forfeiting-ill-gotten-gains",
          "author": "jefftk",
          "published": "2026-01-16T19:20:38.363000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Parenting principle about enforcement: when punishing rule-breaking, the combination of penalty and forfeiture of gains must leave the violator worse off than if they hadn't broken the rule. Extended briefly to crime deterrence principles.",
          "importance_score": 8,
          "reasoning": "General life advice with loose game-theoretic framing. No AI relevance. Possibly distant connection to mechanism design principles that could apply to AI systems.",
          "themes": [
            "Parenting",
            "Incentive Design"
          ],
          "continuation": null,
          "summary_html": "<p>Parenting principle about enforcement: when punishing rule-breaking, the combination of penalty and forfeiture of gains must leave the violator worse off than if they hadn't broken the rule. Extended briefly to crime deterrence principles.</p>",
          "content_html": "<p>It's a holiday. The cousins are over, and the kids are having a great time. Unfortunately, that includes rampaging through the kitchen. We're trying to cook, so there's a \"no cutting through the kitchen\" rule. Imagine enforcement looks like: Kid: [dashes into kitchen, pursued by cousin] Adult: Out of the kitchen! Kid: Sorry! [Continues their path, leaving through the other door; escapes pursuit from more rule-abiding cousin] This doesn't work! The kid got what they wanted out of this interaction, and isn't going to change their behavior. Instead, I need to make it be not worth their while: Kid: [dashes into kitchen, pursued by cousin] Adult: No cutting through the kitchen! [Physically rebuffs intruder]! Kid: Sorry! [Forced to leave through the door they entered by; caught by cousin.] Other examples: Sneak candy, spit it out and forfeit dessert. Use sibling's tablet time, lose your own. Interrupt, be ignored. The general principle is that if you want to limit behavior the combination of the gains from rule-breaking and penalty from punishment need to put the kid in a worse position than if they'd never broken the rule. This isn't just a parenting thing: it's common to say that \"crime should not pay\", and many legal systems prohibit unjust enrichment. One place I'd like to see this implemented is airplane evacuation. If the safety announcements included \"In the event of an emergency evacuation, any carry-on luggage you bring will be confiscated and destroyed. You will also be fined.\" we would have more JAL 516 (379 occupants, zero deaths) and less Aeroflot 1492 or Emirates 521. Comment via: facebook, mastodon, bluesky</p>"
        }
      ]
    },
    "social": {
      "count": 463,
      "category_summary": "The **OpenAI-Musk dispute** dominated AI discourse as **Greg Brockman** broke years of silence with documentary evidence, [accusing Elon of 'beyond dishonest' cherry-picking](/?date=2026-01-18&category=social#item-5dd78602c5d6) from his personal journal. Brockman revealed Elon demanded majority equity and total control, wanting to accumulate $80B through OpenAI—claims backed by deposition links.\n\n- **Google's Logan Kilpatrick** [announced free access](/?date=2026-01-18&category=social#item-b79a1476357c) to **Gemini 3 Flash and Pro** in AI Studio, while also hiring interns across product and vibe coding roles\n- **Anthropic** [shipped a significant **Claude Code** update](/?date=2026-01-18&category=social#item-dfe4acb2e84b): plans now auto-clear context for fresh execution windows\n- **HuggingFace CEO Clem Delangue** [promoted local model deployment](/?date=2026-01-18&category=social#item-81ff54f5b4b4) for privacy (329K views), signaling growing enterprise concern about cloud data\n- **MIT CSAIL** [shared that **ChatGPT**](/?date=2026-01-18&category=social#item-c1141a2d3b55) now ranks as the 4th most visited website globally, surpassing Instagram\n\nDeveloper tooling discussions featured **Jerry Liu** [demoing AI form-filling agents](/?date=2026-01-18&category=social#item-bafa79dcade9) with Claude Agent SDK, while **Burkov** sparked debate [arguing 'coding is solved'](/?date=2026-01-18&category=social#item-8abc81070d79) due to its deterministic nature—contrasting with messy human work where AGI would struggle.",
      "category_summary_html": "<p>The <strong>OpenAI-Musk dispute</strong> dominated AI discourse as <strong>Greg Brockman</strong> broke years of silence with documentary evidence, <a href=\"/?date=2026-01-18&amp;category=social#item-5dd78602c5d6\" class=\"internal-link\" rel=\"noopener noreferrer\">accusing Elon of 'beyond dishonest' cherry-picking</a> from his personal journal. Brockman revealed Elon demanded majority equity and total control, wanting to accumulate $80B through OpenAI—claims backed by deposition links.</p>\n<ul>\n<li><strong>Google's Logan Kilpatrick</strong> <a href=\"/?date=2026-01-18&amp;category=social#item-b79a1476357c\" class=\"internal-link\" rel=\"noopener noreferrer\">announced free access</a> to <strong>Gemini 3 Flash and Pro</strong> in AI Studio, while also hiring interns across product and vibe coding roles</li>\n<li><strong>Anthropic</strong> <a href=\"/?date=2026-01-18&amp;category=social#item-dfe4acb2e84b\" class=\"internal-link\" rel=\"noopener noreferrer\">shipped a significant <strong>Claude Code</strong> update</a>: plans now auto-clear context for fresh execution windows</li>\n<li><strong>HuggingFace CEO Clem Delangue</strong> <a href=\"/?date=2026-01-18&amp;category=social#item-81ff54f5b4b4\" class=\"internal-link\" rel=\"noopener noreferrer\">promoted local model deployment</a> for privacy (329K views), signaling growing enterprise concern about cloud data</li>\n<li><strong>MIT CSAIL</strong> <a href=\"/?date=2026-01-18&amp;category=social#item-c1141a2d3b55\" class=\"internal-link\" rel=\"noopener noreferrer\">shared that <strong>ChatGPT</strong></a> now ranks as the 4th most visited website globally, surpassing Instagram</li>\n</ul>\n<p>Developer tooling discussions featured <strong>Jerry Liu</strong> <a href=\"/?date=2026-01-18&amp;category=social#item-bafa79dcade9\" class=\"internal-link\" rel=\"noopener noreferrer\">demoing AI form-filling agents</a> with Claude Agent SDK, while <strong>Burkov</strong> sparked debate <a href=\"/?date=2026-01-18&amp;category=social#item-8abc81070d79\" class=\"internal-link\" rel=\"noopener noreferrer\">arguing 'coding is solved'</a> due to its deterministic nature—contrasting with messy human work where AGI would struggle.</p>",
      "themes": [
        {
          "name": "OpenAI-Musk History/Dispute",
          "description": "Greg Brockman publicly responding to Elon Musk with documentary evidence about early OpenAI negotiations, Elon's demands for control and equity",
          "item_count": 6,
          "example_items": [],
          "importance": 92
        },
        {
          "name": "Google AI Studio & Gemini",
          "description": "Multiple posts from Logan Kilpatrick about AI Studio features, Gemini 3 free access, hiring, and upcoming updates",
          "item_count": 14,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "AI Democratizing Creativity",
          "description": "OpenAI's Logan sparks discussion about AI enabling engineers to be artists and vice versa - role boundaries blurring",
          "item_count": 3,
          "example_items": [],
          "importance": 80
        },
        {
          "name": "Claude Code Updates",
          "description": "Multiple updates and discussions about Claude Code features, particularly the new auto-clear on plan acceptance",
          "item_count": 14,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Claude Code & Developer Tools",
          "description": "Discussions about Claude Code features, controls (thinking toggle), multi-agent collaboration, and Cowork safety features. Includes both positive usage reports and technical tips.",
          "item_count": 7,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Model Capabilities & Evaluation",
          "description": "Practitioners sharing assessments of current models, particularly Claude Opus 4.5's research mode with extended thinking",
          "item_count": 2,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "AI Agents & Automation",
          "description": "Jerry Liu's form-filling agent demo using Claude Agent SDK + LlamaParse, showing practical agentic AI applications.",
          "item_count": 2,
          "example_items": [],
          "importance": 72
        },
        {
          "name": "AI Coding Maturity",
          "description": "Debates and observations on whether 'coding is solved', AI coding reliability improvements, and implications for developers",
          "item_count": 7,
          "example_items": [],
          "importance": 70
        },
        {
          "name": "Coding Assistant Limitations",
          "description": "Nathan Lambert's frustrations with OpenAI Codex CLI, particularly around git operations. Signals ongoing reliability issues with coding assistants.",
          "item_count": 3,
          "example_items": [],
          "importance": 70
        },
        {
          "name": "AI Engineering Learning",
          "description": "Emphasis on learning through building rather than reading - practitioners will write best practices",
          "item_count": 2,
          "example_items": [],
          "importance": 70
        }
      ],
      "top_items": [
        {
          "id": "5dd78602c5d6",
          "title": "I have great respect for Elon, but the way he cherry-picked from my personal journal is beyond disho...",
          "content": "I have great respect for Elon, but the way he cherry-picked from my personal journal is beyond dishonest.\n\nElon and we had agreed a for-profit was the next step for OpenAI's mission.\n\nThe context shows these snippets were actually about whether to accept Elon's draconian terms. https://t.co/7qYfc3sol1",
          "url": "https://twitter.com/gdb/status/2012328080678031844",
          "author": "@gdb",
          "published": "2026-01-17T00:56:16",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-17&category=social#item-b283e00162e7), Brockman accuses Elon of 'beyond dishonest' cherry-picking from personal journal; clarifies both sides agreed for-profit was next step, dispute was over Elon's 'draconian terms'",
          "importance_score": 92,
          "reasoning": "Primary thread, highest engagement (894K views), major breaking news on OpenAI-Musk dispute with documentary evidence",
          "themes": [
            "OpenAI history",
            "Elon Musk",
            "corporate governance",
            "for-profit transition"
          ],
          "continuation": {
            "original_item_id": "b283e00162e7",
            "original_date": "2026-01-17",
            "original_category": "social",
            "original_title": "I remembered a lot of this, but here is a part I had forgotten...",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-17&amp;category=social#item-b283e00162e7\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Brockman accuses Elon of 'beyond dishonest' cherry-picking from personal journal; clarifies both sides agreed for-profit was next step, dispute was over Elon's 'draconian terms'</p>",
          "content_html": "<p>I have great respect for Elon, but the way he cherry-picked from my personal journal is beyond dishonest.</p>\n<p>Elon and we had agreed a for-profit was the next step for OpenAI's mission.</p>\n<p>The context shows these snippets were actually about whether to accept Elon's draconian terms. https://t.co/7qYfc3sol1</p>"
        },
        {
          "id": "262f628f9b0b",
          "title": "Out of respect for Elon and to avoid discrediting him, the whole time we were working together, and ...",
          "content": "Out of respect for Elon and to avoid discrediting him, the whole time we were working together, and even after he quit, we tried to avoid correcting his false narratives in public.\n\nLooking forward to finally having an opportunity to talk about the real history of OpenAI.",
          "url": "https://twitter.com/gdb/status/2012328086700806481",
          "author": "@gdb",
          "published": "2026-01-17T00:56:17",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-17&category=social#item-b283e00162e7), Greg Brockman announcing he will reveal 'real history of OpenAI' after years of avoiding public correction of Elon's false narratives",
          "importance_score": 88,
          "reasoning": "Major announcement from OpenAI co-founder about revealing internal history, high engagement, significant news",
          "themes": [
            "OpenAI history",
            "Elon Musk",
            "corporate governance"
          ],
          "continuation": {
            "original_item_id": "b283e00162e7",
            "original_date": "2026-01-17",
            "original_category": "social",
            "original_title": "I remembered a lot of this, but here is a part I had forgotten...",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-17&amp;category=social#item-b283e00162e7\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Greg Brockman announcing he will reveal 'real history of OpenAI' after years of avoiding public correction of Elon's false narratives</p>",
          "content_html": "<p>Out of respect for Elon and to avoid discrediting him, the whole time we were working together, and even after he quit, we tried to avoid correcting his false narratives in public.</p>\n<p>Looking forward to finally having an opportunity to talk about the real history of OpenAI.</p>"
        },
        {
          "id": "b79a1476357c",
          "title": "PSA: You can vibe code with Gemini 3 Flash and Gemini 3 Pro for free in Google AI Studio\n\nhttps://t....",
          "content": "PSA: You can vibe code with Gemini 3 Flash and Gemini 3 Pro for free in Google AI Studio\n\nhttps://t.co/1wSsXqHUV0",
          "url": "https://twitter.com/OfficialLoganK/status/2012322509400531005",
          "author": "@OfficialLoganK",
          "published": "2026-01-17T00:34:08",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Logan Kilpatrick announces free vibe coding with Gemini 3 Flash and Gemini 3 Pro in Google AI Studio, promoting the platform's accessibility",
          "importance_score": 88,
          "reasoning": "Major product announcement from Google AI Studio lead with massive engagement (214K views, 2.7K likes). Confirms Gemini 3 models available for free coding experiences.",
          "themes": [
            "Google AI Studio",
            "Gemini Models",
            "Vibe Coding",
            "Product Announcements"
          ],
          "continuation": null,
          "summary_html": "<p>Logan Kilpatrick announces free vibe coding with Gemini 3 Flash and Gemini 3 Pro in Google AI Studio, promoting the platform's accessibility</p>",
          "content_html": "<p>PSA: You can vibe code with Gemini 3 Flash and Gemini 3 Pro for free in Google AI Studio</p>\n<p>https://t.co/1wSsXqHUV0</p>"
        },
        {
          "id": "dfe4acb2e84b",
          "title": "Now in Claude Code: when you accept a plan, Claude automatically clears your context, so your plan g...",
          "content": "Now in Claude Code: when you accept a plan, Claude automatically clears your context, so your plan gets a fresh context window.\n\nWe found this helps keep Claude on track longer, and significantly improves plan adherence.\n\nIf you prefer not to clear your context when accepting a plan, that option is still available too.",
          "url": "https://twitter.com/bcherny/status/2012663636465254662",
          "author": "@bcherny",
          "published": "2026-01-17T23:09:39",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Major Claude Code update: accepting a plan now auto-clears context for fresh window, improves plan adherence; old behavior still available",
          "importance_score": 78,
          "reasoning": "Significant product update from Anthropic, very high engagement (193K views, 6.4K likes), impacts Claude Code workflows",
          "themes": [
            "Claude Code",
            "product update",
            "developer tools",
            "context management"
          ],
          "continuation": null,
          "summary_html": "<p>Major Claude Code update: accepting a plan now auto-clears context for fresh window, improves plan adherence; old behavior still available</p>",
          "content_html": "<p>Now in Claude Code: when you accept a plan, Claude automatically clears your context, so your plan gets a fresh context window.</p>\n<p>We found this helps keep Claude on track longer, and significantly improves plan adherence.</p>\n<p>If you prefer not to clear your context when accepting a plan, that option is still available too.</p>"
        },
        {
          "id": "81ff54f5b4b4",
          "title": "Cowork but with local models not to send all your data to a remote cloud! https://t.co/2OrBMMO3NJ",
          "content": "Cowork but with local models not to send all your data to a remote cloud! https://t.co/2OrBMMO3NJ",
          "url": "https://twitter.com/ClementDelangue/status/2012322682579071166",
          "author": "@ClementDelangue",
          "published": "2026-01-17T00:34:49",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "HuggingFace CEO promoting coworking with local models to avoid sending data to remote cloud",
          "importance_score": 68,
          "reasoning": "Very high engagement (329K views), relevant privacy-focused AI deployment trend from major platform leader",
          "themes": [
            "local models",
            "privacy",
            "Hugging Face",
            "data sovereignty"
          ],
          "continuation": null,
          "summary_html": "<p>HuggingFace CEO promoting coworking with local models to avoid sending data to remote cloud</p>",
          "content_html": "<p>Cowork but with local models not to send all your data to a remote cloud! https://t.co/2OrBMMO3NJ</p>"
        },
        {
          "id": "c1141a2d3b55",
          "title": "The 5 most visited websites in the world for Oct. 2025, v/Semrush:\n1. Google\n2. YouTube\n3. Facebook\n...",
          "content": "The 5 most visited websites in the world for Oct. 2025, v/Semrush:\n1. Google\n2. YouTube\n3. Facebook\n4. ChatGPT\n5. Instagram",
          "url": "https://twitter.com/MIT_CSAIL/status/2012570670073340414",
          "author": "@MIT_CSAIL",
          "published": "2026-01-17T17:00:14",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "MIT CSAIL shares that ChatGPT ranked as 4th most visited website globally for October 2025, behind Google, YouTube, and Facebook, ahead of Instagram.",
          "importance_score": 68,
          "reasoning": "From authoritative source (MIT CSAIL), significant data point about AI adoption at scale. ChatGPT surpassing Instagram in web traffic is notable.",
          "themes": [
            "AI adoption",
            "ChatGPT",
            "web traffic",
            "industry metrics"
          ],
          "continuation": null,
          "summary_html": "<p>MIT CSAIL shares that ChatGPT ranked as 4th most visited website globally for October 2025, behind Google, YouTube, and Facebook, ahead of Instagram.</p>",
          "content_html": "<p>The 5 most visited websites in the world for Oct. 2025, v/Semrush:</p>\n<p>1. Google</p>\n<p>2. YouTube</p>\n<p>3. Facebook</p>\n<p>4. ChatGPT</p>\n<p>5. Instagram</p>"
        },
        {
          "id": "3185ef14e809",
          "title": "The negotiations touched on personal motivations, e.g. Elon told us he wanted OpenAI equity in order...",
          "content": "The negotiations touched on personal motivations, e.g. Elon told us he wanted OpenAI equity in order to accumulate $80B.\n\nI spent a lot of thought on what could be my own motivations, with my personal journal reading like a long \"chain of thought\". My deposition has some additional context which may be helpful: https://t.co/Kg8Jv3Jluh.\n\nAs a product of this reflection, Ilya and I realized we couldn't give Elon unilateral control, which we decided was against the mission. That was unacceptable to him. Prior to this, Elon was committed enough to the for-profit idea that he actually created the \"Open Artificial Intelligence Technologies\" for-profit:",
          "url": "https://twitter.com/gdb/status/2012328084985500005",
          "author": "@gdb",
          "published": "2026-01-17T00:56:17",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-17&category=social#item-b283e00162e7), Brockman reveals Elon wanted OpenAI equity to accumulate $80B; shares personal journal reflections and links to deposition for context",
          "importance_score": 82,
          "reasoning": "Significant revelation about Elon's financial motivations, provides documentary evidence",
          "themes": [
            "OpenAI history",
            "Elon Musk",
            "corporate governance"
          ],
          "continuation": {
            "original_item_id": "b283e00162e7",
            "original_date": "2026-01-17",
            "original_category": "social",
            "original_title": "I remembered a lot of this, but here is a part I had forgotten...",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-17&amp;category=social#item-b283e00162e7\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Brockman reveals Elon wanted OpenAI equity to accumulate $80B; shares personal journal reflections and links to deposition for context</p>",
          "content_html": "<p>The negotiations touched on personal motivations, e.g. Elon told us he wanted OpenAI equity in order to accumulate $80B.</p>\n<p>I spent a lot of thought on what could be my own motivations, with my personal journal reading like a long \"chain of thought\". My deposition has some additional context which may be helpful: https://t.co/Kg8Jv3Jluh.</p>\n<p>As a product of this reflection, Ilya and I realized we couldn't give Elon unilateral control, which we decided was against the mission. That was unacceptable to him. Prior to this, Elon was committed enough to the for-profit idea that he actually created the \"Open Artificial Intelligence Technologies\" for-profit:</p>"
        },
        {
          "id": "83abe0090c9f",
          "title": "Speedrunning my way through my Claude Max rate limits.\n\nPretty crazy watching 10+ agents collaborate...",
          "content": "Speedrunning my way through my Claude Max rate limits.\n\nPretty crazy watching 10+ agents collaborate on one task.\n\nMy laptop fans are blaring (turns out Claude Code is a memory hog). https://t.co/Spk1nxvhg5",
          "url": "https://twitter.com/mattshumer_/status/2012581787575206178",
          "author": "@mattshumer_",
          "published": "2026-01-17T17:44:24",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Matt Shumer shares experience running 10+ Claude agents collaborating on a single task, hitting Claude Max rate limits. Notes that Claude Code is a significant memory hog causing laptop fans to blare.",
          "importance_score": 78,
          "reasoning": "High engagement (525 likes, 128k views), practical insight into multi-agent workflows with Claude Max, reveals resource constraints and rate limit experiences from active power user.",
          "themes": [
            "multi-agent collaboration",
            "Claude Code",
            "rate limits",
            "resource usage"
          ],
          "continuation": null,
          "summary_html": "<p>Matt Shumer shares experience running 10+ Claude agents collaborating on a single task, hitting Claude Max rate limits. Notes that Claude Code is a significant memory hog causing laptop fans to blare.</p>",
          "content_html": "<p>Speedrunning my way through my Claude Max rate limits.</p>\n<p>Pretty crazy watching 10+ agents collaborate on one task.</p>\n<p>My laptop fans are blaring (turns out Claude Code is a memory hog). https://t.co/Spk1nxvhg5</p>"
        },
        {
          "id": "bafa79dcade9",
          "title": "I made an AI agent that can fill out complicated forms from unstructured context 📝\n\nFor instance: au...",
          "content": "I made an AI agent that can fill out complicated forms from unstructured context 📝\n\nFor instance: automatically fill out your expense report 💳 by drag and dropping 5-10 receipt pictures/scans 🧾\n\nUses Claude Agent SDK + LlamaParse to parse unstructured docs + custom tools for form understanding.\n\nIt now semantically understands each field, handles multi-turn conversations, and lets you drag up to 10 files.\n\nDeployed on Vercel/Render.\n\nApp: https://t.co/HYipU2fpFE\nRepo: https://t.co/GIfSzxGH4j\nLlamaCloud: https://t.co/QwzurFBE67",
          "url": "https://twitter.com/jerryjliu0/status/2012657108500857271",
          "author": "@jerryjliu0",
          "published": "2026-01-17T22:43:42",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Jerry Liu (LlamaIndex founder) built an AI agent for automated expense report form filling from receipt images using Claude Agent SDK + LlamaParse. Handles multi-turn conversations and up to 10 files.",
          "importance_score": 75,
          "reasoning": "From credible AI tooling founder, practical demo with code repo, shows Claude Agent SDK + LlamaParse integration for real-world use case. Good engagement (327 likes, 43 retweets).",
          "themes": [
            "AI agents",
            "Claude Agent SDK",
            "LlamaParse",
            "document processing",
            "form automation"
          ],
          "continuation": null,
          "summary_html": "<p>Jerry Liu (LlamaIndex founder) built an AI agent for automated expense report form filling from receipt images using Claude Agent SDK + LlamaParse. Handles multi-turn conversations and up to 10 files.</p>",
          "content_html": "<p>I made an AI agent that can fill out complicated forms from unstructured context 📝</p>\n<p>For instance: automatically fill out your expense report 💳 by drag and dropping 5-10 receipt pictures/scans 🧾</p>\n<p>Uses Claude Agent SDK + LlamaParse to parse unstructured docs + custom tools for form understanding.</p>\n<p>It now semantically understands each field, handles multi-turn conversations, and lets you drag up to 10 files.</p>\n<p>Deployed on Vercel/Render.</p>\n<p>App: https://t.co/HYipU2fpFE</p>\n<p>Repo: https://t.co/GIfSzxGH4j</p>\n<p>LlamaCloud: https://t.co/QwzurFBE67</p>"
        },
        {
          "id": "8abc81070d79",
          "title": "Coding was solved because coding is a deterministic task (code either works or doesn't work) and bec...",
          "content": "Coding was solved because coding is a deterministic task (code either works or doesn't work) and because of how simple it is to define the reward for reinforcement learning: 1 when code produces the expected output (a simple string match is enough) and less than 1 otherwise.\n\nIf it were as simple to assign rewards for everything we do in life, the AGI would already be built.\n\nBut ~90% of what we need our brain for is guessing under a messy uncertainty, solutions can be many, and the rewards are based on chemistry different for different people.\n\nLuckily for coders, they've had about 3 decades of lucrative job market where people capable of thinking like computers were rare.",
          "url": "https://twitter.com/burkov/status/2012662154806083989",
          "author": "@burkov",
          "published": "2026-01-17T23:03:45",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Burkov argues 'coding is solved' due to determinism and simple reward signals; most human work involves messy uncertainty where AGI would struggle",
          "importance_score": 72,
          "reasoning": "Provocative and substantive claim with technical reasoning, very high engagement (67K views), sparks important debate",
          "themes": [
            "AI coding",
            "AGI",
            "automation",
            "human work"
          ],
          "continuation": null,
          "summary_html": "<p>Burkov argues 'coding is solved' due to determinism and simple reward signals; most human work involves messy uncertainty where AGI would struggle</p>",
          "content_html": "<p>Coding was solved because coding is a deterministic task (code either works or doesn't work) and because of how simple it is to define the reward for reinforcement learning: 1 when code produces the expected output (a simple string match is enough) and less than 1 otherwise.</p>\n<p>If it were as simple to assign rewards for everything we do in life, the AGI would already be built.</p>\n<p>But ~90% of what we need our brain for is guessing under a messy uncertainty, solutions can be many, and the rewards are based on chemistry different for different people.</p>\n<p>Luckily for coders, they've had about 3 decades of lucrative job market where people capable of thinking like computers were rare.</p>"
        }
      ]
    },
    "reddit": {
      "count": 569,
      "category_summary": "**Frontier AI capabilities** dominated today: **GPT-5.2** [solved another open Erdős problem](/?date=2026-01-18&category=reddit#item-258a8ef625ab) (Terence Tao noted novel proof techniques), while a new **matrix multiplication algorithm** was [fully AI-developed](/?date=2026-01-18&category=reddit#item-2d27b828fd4b)—both signaling major leaps in AI-driven mathematical/algorithmic research.\n\n- **Pentagon's Grok integration** [into classified networks](/?date=2026-01-18&category=reddit#item-4ba5fc462bf7) sparked intense debate (2.7k upvotes) on xAI's defense role and geopolitical implications\n- **OpenAI sued** [over claims ChatGPT contributed](/?date=2026-01-18&category=reddit#item-4f68ae5b104f) to a user's suicide—community debating AI liability and safety guardrails\n- **DeepSeek Engram** paper [introduced static memory architecture](/?date=2026-01-18&category=reddit#item-b94a54a5d8a7) separating remembering from reasoning\n\n**Infrastructure & ecosystem**: **Colossus 2** [is operational](/?date=2026-01-18&category=reddit#item-6b589ebb5a90) as the first gigawatt data center. **r/LocalLLaMA** shared detailed [**128GB VRAM quad R9700** builds](/?date=2026-01-18&category=reddit#item-880a79156a7d). **Qwen** [announced slowing Qwen 4](/?date=2026-01-18&category=reddit#item-56ac24f0407a) development to prioritize quality. **Claude Code** tooling flourished with **GSD** [hitting 15k installs](/?date=2026-01-18&category=reddit#item-7f0c1033c172) featuring multi-agent orchestration. **LTX-2** workflows for consumer GPUs [drew massive engagement](/?date=2026-01-18&category=reddit#item-b107c307008f) in **r/StableDiffusion**.",
      "category_summary_html": "<p><strong>Frontier AI capabilities</strong> dominated today: <strong>GPT-5.2</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-258a8ef625ab\" class=\"internal-link\" rel=\"noopener noreferrer\">solved another open Erdős problem</a> (Terence Tao noted novel proof techniques), while a new <strong>matrix multiplication algorithm</strong> was <a href=\"/?date=2026-01-18&amp;category=reddit#item-2d27b828fd4b\" class=\"internal-link\" rel=\"noopener noreferrer\">fully AI-developed</a>—both signaling major leaps in AI-driven mathematical/algorithmic research.</p>\n<ul>\n<li><strong>Pentagon's Grok integration</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-4ba5fc462bf7\" class=\"internal-link\" rel=\"noopener noreferrer\">into classified networks</a> sparked intense debate (2.7k upvotes) on xAI's defense role and geopolitical implications</li>\n<li><strong>OpenAI sued</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-4f68ae5b104f\" class=\"internal-link\" rel=\"noopener noreferrer\">over claims ChatGPT contributed</a> to a user's suicide—community debating AI liability and safety guardrails</li>\n<li><strong>DeepSeek Engram</strong> paper <a href=\"/?date=2026-01-18&amp;category=reddit#item-b94a54a5d8a7\" class=\"internal-link\" rel=\"noopener noreferrer\">introduced static memory architecture</a> separating remembering from reasoning</li>\n</ul>\n<p><strong>Infrastructure &amp; ecosystem</strong>: <strong>Colossus 2</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-6b589ebb5a90\" class=\"internal-link\" rel=\"noopener noreferrer\">is operational</a> as the first gigawatt data center. <strong>r/LocalLLaMA</strong> shared detailed <a href=\"/?date=2026-01-18&amp;category=reddit#item-880a79156a7d\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>128GB VRAM quad R9700</strong> builds</a>. <strong>Qwen</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-56ac24f0407a\" class=\"internal-link\" rel=\"noopener noreferrer\">announced slowing Qwen 4</a> development to prioritize quality. <strong>Claude Code</strong> tooling flourished with <strong>GSD</strong> <a href=\"/?date=2026-01-18&amp;category=reddit#item-7f0c1033c172\" class=\"internal-link\" rel=\"noopener noreferrer\">hitting 15k installs</a> featuring multi-agent orchestration. <strong>LTX-2</strong> workflows for consumer GPUs <a href=\"/?date=2026-01-18&amp;category=reddit#item-b107c307008f\" class=\"internal-link\" rel=\"noopener noreferrer\">drew massive engagement</a> in <strong>r/StableDiffusion</strong>.</p>",
      "themes": [
        {
          "name": "AI Capabilities Breakthroughs",
          "description": "Major demonstrations of AI solving previously unsolved problems in mathematics and algorithm design, showing frontier model capabilities",
          "item_count": 3,
          "example_items": [],
          "importance": 95
        },
        {
          "name": "Claude Code Tooling & Plugins",
          "description": "Ecosystem of tools, plugins, and workflows extending Claude Code capabilities: GSD, Dreamer, ccstart, orchestration systems, boilerplates",
          "item_count": 18,
          "example_items": [],
          "importance": 90
        },
        {
          "name": "Open Model Ecosystem",
          "description": "Qwen 4 slowdown announcement, uncensored model search, DeepSeek Engram research, model recommendations for local inference",
          "item_count": 12,
          "example_items": [],
          "importance": 88
        },
        {
          "name": "Hardware & Infrastructure",
          "description": "GPU builds, Strix Halo optimization, Blackwell economics, VRAM expansion, and inference engine comparisons",
          "item_count": 14,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Infrastructure & Compute Scaling",
          "description": "Gigawatt data centers, Codex scaling announcements, and discussions about inference infrastructure gaps",
          "item_count": 4,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Autonomous & Multi-Agent Systems",
          "description": "Projects implementing autonomous coding, scheduled tasks, and multi-agent collaboration frameworks",
          "item_count": 8,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "FLUX 2 / Klein Model Ecosystem",
          "description": "Multiple posts about FLUX 2 Klein model capabilities, comparisons with z-image, and LoRA training support across tools",
          "item_count": 5,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "AI Policy & Legal Developments",
          "description": "Major news about government AI adoption, lawsuits, and regulatory responses to AI capabilities",
          "item_count": 5,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "LTX-2 Video Generation",
          "description": "Extensive discussion of LTX-2 video model workflows, optimizations, lip-sync capabilities, and comparisons with WAN 2.2",
          "item_count": 18,
          "example_items": [],
          "importance": 82
        },
        {
          "name": "Industry News & Business",
          "description": "OpenAI ads controversy, China AGI-NEXT conference insights, cloud vs local economics",
          "item_count": 6,
          "example_items": [],
          "importance": 80
        }
      ],
      "top_items": [
        {
          "id": "258a8ef625ab",
          "title": "Another Erdos problem solved by GPT-5.2",
          "content": "Tao’s comments:\n\n\\&gt; https://www.erdosproblems.com/forum/thread/281#post-3302",
          "url": "https://reddit.com/r/singularity/comments/1qfy5wh/another_erdos_problem_solved_by_gpt52/",
          "author": "u/artemisgarden",
          "published": "2026-01-17T22:55:39",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "GPT-5.2 solved another open Erdős problem, with Terence Tao commenting that the proof uses a novel variant of the Furstenberg correspondence principle. Represents continued AI progress on challenging mathematical problems.",
          "importance_score": 95,
          "reasoning": "Major capability milestone - frontier AI solving previously unsolved mathematical problems. Tao's endorsement adds significant credibility. Cross-posted to r/accelerate with similar engagement.",
          "themes": [
            "AI capabilities",
            "mathematical reasoning",
            "research breakthroughs"
          ],
          "continuation": null,
          "summary_html": "<p>GPT-5.2 solved another open Erdős problem, with Terence Tao commenting that the proof uses a novel variant of the Furstenberg correspondence principle. Represents continued AI progress on challenging mathematical problems.</p>",
          "content_html": "<p>Tao’s comments:</p>\n<p>\\&gt; https://www.erdosproblems.com/forum/thread/281#post-3302</p>"
        },
        {
          "id": "2d27b828fd4b",
          "title": "New algorithm for matrix multiplication fully developed by AI",
          "content": "Link: https://x.com/i/status/2012155529338949916",
          "url": "https://reddit.com/r/singularity/comments/1qfefqn/new_algorithm_for_matrix_multiplication_fully/",
          "author": "u/sickgeorge19",
          "published": "2026-01-17T09:21:50",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "LLM News"
          ],
          "summary": "New matrix multiplication algorithm was fully developed by AI, representing a significant advancement in algorithmic discovery by AI systems.",
          "importance_score": 92,
          "reasoning": "High engagement (468 upvotes) on fundamental computer science breakthrough. AI-generated algorithms that improve core computational operations have massive downstream implications.",
          "themes": [
            "AI capabilities",
            "algorithmic discovery",
            "research breakthroughs"
          ],
          "continuation": null,
          "summary_html": "<p>New matrix multiplication algorithm was fully developed by AI, representing a significant advancement in algorithmic discovery by AI systems.</p>",
          "content_html": "<p>Link: https://x.com/i/status/2012155529338949916</p>"
        },
        {
          "id": "4ba5fc462bf7",
          "title": "Pentagon to integrate Grok AI into classified military networks despite global backlash against Grok",
          "content": "",
          "url": "https://reddit.com/r/Futurology/comments/1qfmc9p/pentagon_to_integrate_grok_ai_into_classified/",
          "author": "u/MetaKnowing",
          "published": "2026-01-17T14:25:41",
          "source": "r/Futurology",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Pentagon announces integration of Grok AI into classified military networks, sparking debate about xAI's involvement in defense and geopolitical implications",
          "importance_score": 88,
          "reasoning": "Extremely high engagement (2787 upvotes, 209 comments), major policy development regarding AI in military/defense applications, significant geopolitical implications",
          "themes": [
            "AI policy",
            "military applications",
            "geopolitics"
          ],
          "continuation": null,
          "summary_html": "<p>Pentagon announces integration of Grok AI into classified military networks, sparking debate about xAI's involvement in defense and geopolitical implications</p>",
          "content_html": ""
        },
        {
          "id": "b94a54a5d8a7",
          "title": "DeepSeek Engram : A static memory unit for LLMs",
          "content": "DeeepSeek AI released a new paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models\" introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram **adds native memory lookup**.\n\nThink of it as separating **remembering from reasoning**. Traditional MoE focuses on conditional computation, Engram introduces **conditional memory**. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.\n\n**Key highlights:**\n\n* Knowledge is **looked up in O(1)** instead of recomputed.\n* Uses **explicit parametric memory** vs implicit weights only.\n* Improves reasoning, math, and code performance.\n* Enables massive memory scaling **without GPU limits**.\n* Frees attention for **global reasoning** rather than static knowledge.\n\nPaper : [https://github.com/deepseek-ai/Engram/blob/main/Engram\\_paper.pdf](https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf)\n\nVideo explanation : [https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub](https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub)\n\n",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/",
          "author": "u/Technical-Love-8479",
          "published": "2026-01-17T01:18:14",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "News"
          ],
          "summary": "Following yesterday's [News](/?date=2026-01-16&category=news#item-db6dfd71bdf6) coverage, DeepSeek releases paper on 'Engram' - a static memory unit for LLMs that separates remembering from reasoning via native memory lookup instead of recomputation.",
          "importance_score": 90,
          "reasoning": "Very high engagement (305 upvotes, 47 comments). Important architectural innovation from major lab addressing core LLM efficiency.",
          "themes": [
            "deepseek",
            "architecture",
            "research",
            "memory"
          ],
          "continuation": {
            "original_item_id": "db6dfd71bdf6",
            "original_date": "2026-01-16",
            "original_category": "news",
            "original_title": "DeepSeek AI Researchers Introduce Engram: A Conditional Memory Axis For Sparse LLMs",
            "continuation_type": "community_reaction",
            "should_demote": false,
            "reference_text": "Following yesterday's **News** coverage"
          },
          "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-01-16&amp;category=news#item-db6dfd71bdf6\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage, DeepSeek releases paper on 'Engram' - a static memory unit for LLMs that separates remembering from reasoning via native memory lookup instead of recomputation.</p>",
          "content_html": "<p>DeeepSeek AI released a new paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language&nbsp;Models\" introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram <strong>adds native memory lookup</strong>.</p>\n<p>Think of it as separating <strong>remembering from reasoning</strong>. Traditional MoE focuses on conditional computation, Engram introduces <strong>conditional memory</strong>. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.</p>\n<p><strong>Key highlights:</strong></p>\n<p>* Knowledge is <strong>looked up in O(1)</strong> instead of recomputed.</p>\n<p>* Uses <strong>explicit parametric memory</strong> vs implicit weights only.</p>\n<p>* Improves reasoning, math, and code performance.</p>\n<p>* Enables massive memory scaling <strong>without GPU limits</strong>.</p>\n<p>* Frees attention for <strong>global reasoning</strong> rather than static knowledge.</p>\n<p>Paper : <a href=\"https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/deepseek-ai/Engram/blob/main/Engram\\_paper.pdf</a></p>\n<p>Video explanation : <a href=\"https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub\" target=\"_blank\" rel=\"noopener noreferrer\">https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub</a></p>"
        },
        {
          "id": "4f68ae5b104f",
          "title": "OpenAI and Sam Altman sued over claims ChatGPT drove a 40-year-old man to suicide",
          "content": "",
          "url": "https://reddit.com/r/Futurology/comments/1qfg755/openai_and_sam_altman_sued_over_claims_chatgpt/",
          "author": "u/sksarkpoes3",
          "published": "2026-01-17T10:33:13",
          "source": "r/Futurology",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Following yesterday's [News](/?date=2026-01-16&category=news#item-605f23905122) coverage, OpenAI and Sam Altman face lawsuit claiming ChatGPT contributed to a 40-year-old man's suicide, raising questions about AI chatbot safety and liability",
          "importance_score": 85,
          "reasoning": "Very high engagement (1950 upvotes, 235 comments), landmark legal case with significant implications for AI liability and safety standards",
          "themes": [
            "AI ethics",
            "legal developments",
            "AI safety"
          ],
          "continuation": {
            "original_item_id": "605f23905122",
            "original_date": "2026-01-16",
            "original_category": "news",
            "original_title": "ChatGPT wrote \"Goodnight Moon\" suicide lullaby for man who later killed himself",
            "continuation_type": "community_reaction",
            "should_demote": false,
            "reference_text": "Following yesterday's **News** coverage"
          },
          "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-01-16&amp;category=news#item-605f23905122\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage, OpenAI and Sam Altman face lawsuit claiming ChatGPT contributed to a 40-year-old man's suicide, raising questions about AI chatbot safety and liability</p>",
          "content_html": ""
        },
        {
          "id": "6b589ebb5a90",
          "title": "Colossus 2 is now fully operational as the first gigawatt data center",
          "content": "",
          "url": "https://reddit.com/r/singularity/comments/1qfbzzq/colossus_2_is_now_fully_operational_as_the_first/",
          "author": "u/enigmatic_erudition",
          "published": "2026-01-17T07:28:50",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "Compute"
          ],
          "summary": "Colossus 2 is now fully operational as the first gigawatt-scale data center, marking a major infrastructure milestone for AI compute.",
          "importance_score": 88,
          "reasoning": "322 upvotes, 177 comments. First gigawatt data center represents major scaling infrastructure. Critical for understanding AI compute trajectory.",
          "themes": [
            "infrastructure",
            "compute scaling",
            "xAI"
          ],
          "continuation": null,
          "summary_html": "<p>Colossus 2 is now fully operational as the first gigawatt-scale data center, marking a major infrastructure milestone for AI compute.</p>",
          "content_html": ""
        },
        {
          "id": "7f0c1033c172",
          "title": "I've Massively Improved GSD (Get Shit Done)",
          "content": "A few weeks ago I posted about **Get Shit Done** when it was at \\~100 users. Since then we've gone on to hit 3,300 stars and crossed 15,000 installs. Time for an update.\n\n[https://github.com/glittercowboy/get-shit-done](https://github.com/glittercowboy/get-shit-done)\n\n# The Big Changes\n\n**Multi-agent orchestration that actually works.**\n\nWhen I first posted, execution was single-threaded. Now the system spawns specialized agents in parallel — 4 researchers investigating your domain simultaneously, multiple executors building different parts of your codebase at once, a dedicated verifier checking if the code actually achieves what you asked for.\n\nThe absolutely bonkers part is that your main context window stays at 30-40% even after deep research or thousands of lines of code getting written. All heavy lifting happens consistently in fresh 200k subagent contexts.\n\n**Plans get verified before they run.**\n\nI got tired of watching Claude write plans that missed requirements or had broken dependencies. Now there's a planner → checker → revise loop. Plans don't execute until they pass verification. If the checker finds issues, the planner fixes them automatically.\n\n**Automatic debugging when things break.**\n\nThe new `/gsd:verify-work` command walks you through testing what got built. \"Can you log in?\" Yes/no. If something's broken, it spawns debug agents to find the root cause, creates fix plans, verifies those plans, and hands you a ready-to-execute solution. You don't debug — you just run `/gsd:execute-phase` again.\n\n**The discuss-phase breakthrough.**\n\nThis is the best update I reckon. Before planning, you now feed your preferences into the system — how you want the UI laid out, what the error messages should say, how the CLI flags should work. That context flows into research (so it investigates the right patterns) and planning (so it builds what you actually want, not reasonable defaults).\n\n# Meta Building\n\nThe system builds itself. Every GSD improvement gets planned and executed using GSD. It's the most meta thing I've ever worked on and it just keeps getting better.\n\n# The Philosophy Hasn't Changed\n\nI still don't want to cosplay as an enterprise team. I still just want to describe what I want and have it built correctly.\n\nThe difference now is the system is *so much smarter* about how it does that. Research before planning. Verification before execution. Debugging when things break. Fresh context for every heavy operation.\n\nIt's not magic. It's just really good context engineering wrapped in a workflow that doesn't get in your way.\n\n    npx get-shit-done-cc\n\nWith love,\n\nLex\n\n**P.S.** Once you've downloaded the newest version, you can simply run `/gsd:update` to get the latest. The update command now shows you what changed and asks before installing — no more mystery upgrades.",
          "url": "https://reddit.com/r/ClaudeAI/comments/1qf6u3f/ive_massively_improved_gsd_get_shit_done/",
          "author": "u/officialtaches",
          "published": "2026-01-17T02:23:08",
          "source": "r/ClaudeAI",
          "source_type": "reddit",
          "tags": [
            "Built with Claude"
          ],
          "summary": "Major update to GSD (Get Shit Done) Claude Code plugin: now at 15,000+ installs with multi-agent orchestration, parallel specialized agents (4 researchers, coder, critic, integrator, shipper), intelligent task routing, and built-in memory. Represents significant maturation of Claude Code tooling ecosystem.",
          "importance_score": 92,
          "reasoning": "Highest engagement post (349 score, 74 comments), demonstrates substantial technical advancement in Claude Code orchestration, practical multi-agent implementation with proven adoption metrics",
          "themes": [
            "claude-code-tooling",
            "multi-agent-systems",
            "developer-productivity"
          ],
          "continuation": null,
          "summary_html": "<p>Major update to GSD (Get Shit Done) Claude Code plugin: now at 15,000+ installs with multi-agent orchestration, parallel specialized agents (4 researchers, coder, critic, integrator, shipper), intelligent task routing, and built-in memory. Represents significant maturation of Claude Code tooling ecosystem.</p>",
          "content_html": "<p>A few weeks ago I posted about <strong>Get Shit Done</strong> when it was at \\~100 users. Since then we've gone on to hit 3,300 stars and crossed 15,000 installs. Time for an update.</p>\n<p><a href=\"https://github.com/glittercowboy/get-shit-done\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/glittercowboy/get-shit-done</a></p>\n<p># The Big Changes</p>\n<p><strong>Multi-agent orchestration that actually works.</strong></p>\n<p>When I first posted, execution was single-threaded. Now the system spawns specialized agents in parallel — 4 researchers investigating your domain simultaneously, multiple executors building different parts of your codebase at once, a dedicated verifier checking if the code actually achieves what you asked for.</p>\n<p>The absolutely bonkers part is that your main context window stays at 30-40% even after deep research or thousands of lines of code getting written. All heavy lifting happens consistently in fresh 200k subagent contexts.</p>\n<p><strong>Plans get verified before they run.</strong></p>\n<p>I got tired of watching Claude write plans that missed requirements or had broken dependencies. Now there's a planner → checker → revise loop. Plans don't execute until they pass verification. If the checker finds issues, the planner fixes them automatically.</p>\n<p><strong>Automatic debugging when things break.</strong></p>\n<p>The new `/gsd:verify-work` command walks you through testing what got built. \"Can you log in?\" Yes/no. If something's broken, it spawns debug agents to find the root cause, creates fix plans, verifies those plans, and hands you a ready-to-execute solution. You don't debug — you just run `/gsd:execute-phase` again.</p>\n<p><strong>The discuss-phase breakthrough.</strong></p>\n<p>This is the best update I reckon. Before planning, you now feed your preferences into the system — how you want the UI laid out, what the error messages should say, how the CLI flags should work. That context flows into research (so it investigates the right patterns) and planning (so it builds what you actually want, not reasonable defaults).</p>\n<p># Meta Building</p>\n<p>The system builds itself. Every GSD improvement gets planned and executed using GSD. It's the most meta thing I've ever worked on and it just keeps getting better.</p>\n<p># The Philosophy Hasn't Changed</p>\n<p>I still don't want to cosplay as an enterprise team. I still just want to describe what I want and have it built correctly.</p>\n<p>The difference now is the system is *so much smarter* about how it does that. Research before planning. Verification before execution. Debugging when things break. Fresh context for every heavy operation.</p>\n<p>It's not magic. It's just really good context engineering wrapped in a workflow that doesn't get in your way.</p>\n<p>npx get-shit-done-cc</p>\n<p>With love,</p>\n<p>Lex</p>\n<p><strong>P.S.</strong> Once you've downloaded the newest version, you can simply run `/gsd:update` to get the latest. The update command now shows you what changed and asks before installing — no more mystery upgrades.</p>"
        },
        {
          "id": "880a79156a7d",
          "title": "128GB VRAM quad R9700 server",
          "content": "This is a sequel to my [previous thread](https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/) from 2024.\n\nI originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the [llama.cpp ROCm thread](https://github.com/ggml-org/llama.cpp/discussions/15021), and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.\n\nHere's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:\n\n|Component|Description|Number|Unit Price|\n|:-|:-|:-|:-|\n|CPU|AMD Ryzen 7 5700X|1|$160.00|\n|RAM|Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18|2|$105.00|\n|GPU|PowerColor AMD Radeon AI PRO R9700 32GB|4|$1,300.00|\n|Motherboard|MSI MEG X570 GODLIKE Motherboard|1|$490.00|\n|Storage|Inland Performance 1TB NVMe SSD|1|$100.00|\n|PSU|Super Flower Leadex Titanium 1600W 80+ Titanium|1|$440.00|\n|Internal Fans|Super Flower MEGACOOL 120mm fan, Triple-Pack|1|$0.00|\n|Case Fans|Noctua NF-A14 iPPC-3000 PWM|6|$30.00|\n|CPU Heatsink|AMD Wraith Prism aRGB CPU Cooler|1|$20.00|\n|Fan Hub|Noctua NA-FH1|1|$45.00|\n|Case|Phanteks Enthoo Pro 2 Server Edition|1|$190.00|\n|Total|||$7,035.00|\n\n128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.\n\nSome benchmarks:\n\n|model|size|params|backend|ngl|n\\_batch|n\\_ubatch|fa|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|llama 7B Q4\\_0|3.56 GiB|6.74 B|ROCm|99|1024|1024|1|pp8192|6524.91 ± 11.30|\n|llama 7B Q4\\_0|3.56 GiB|6.74 B|ROCm|99|1024|1024|1|tg128|90.89 ± 0.41|\n|qwen3moe 30B.A3B Q8\\_0|33.51 GiB|30.53 B|ROCm|99|1024|1024|1|pp8192|2113.82 ± 2.88|\n|qwen3moe 30B.A3B Q8\\_0|33.51 GiB|30.53 B|ROCm|99|1024|1024|1|tg128|72.51 ± 0.27|\n|qwen3vl 32B Q8\\_0|36.76 GiB|32.76 B|ROCm|99|1024|1024|1|pp8192|1725.46 ± 5.93|\n|qwen3vl 32B Q8\\_0|36.76 GiB|32.76 B|ROCm|99|1024|1024|1|tg128|14.75 ± 0.01|\n|llama 70B IQ4\\_XS - 4.25 bpw|35.29 GiB|70.55 B|ROCm|99|1024|1024|1|pp8192|1110.02 ± 3.49|\n|llama 70B IQ4\\_XS - 4.25 bpw|35.29 GiB|70.55 B|ROCm|99|1024|1024|1|tg128|14.53 ± 0.03|\n|qwen3next 80B.A3B IQ4\\_XS - 4.25 bpw|39.71 GiB|79.67 B|ROCm|99|1024|1024|1|pp8192|821.10 ± 0.27|\n|qwen3next 80B.A3B IQ4\\_XS - 4.25 bpw|39.71 GiB|79.67 B|ROCm|99|1024|1024|1|tg128|38.88 ± 0.02|\n|glm4moe ?B IQ4\\_XS - 4.25 bpw|54.33 GiB|106.85 B|ROCm|99|1024|1024|1|pp8192|1928.45 ± 3.74|\n|glm4moe ?B IQ4\\_XS - 4.25 bpw|54.33 GiB|106.85 B|ROCm|99|1024|1024|1|tg128|48.09 ± 0.16|\n|minimax-m2 230B.A10B IQ4\\_XS - 4.25 bpw|113.52 GiB|228.69 B|ROCm|99|1024|1024|1|pp8192|2082.04 ± 4.49|\n|minimax-m2 230B.A10B IQ4\\_XS - 4.25 bpw|113.52 GiB|228.69 B|ROCm|99|1024|1024|1|tg128|48.78 ± 0.06|\n|minimax-m2 230B.A10B Q8\\_0|226.43 GiB|228.69 B|ROCm|30|1024|1024|1|pp8192|42.62 ± 7.96|\n|minimax-m2 230B.A10B Q8\\_0|226.43 GiB|228.69 B|ROCm|30|1024|1024|1|tg128|6.58 ± 0.01|\n\nA few final observations:\n\n* glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.\n* There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.\n* A word on the Q8 quant of MiniMax-M2.1; `--fit on`  isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge.  It also takes 2 hours to load, but that could be because I'm loading it off external storage.\n* The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.\n* I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.\n* The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.\n* Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.\n* I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/",
          "author": "u/Ulterior-Motive_",
          "published": "2026-01-17T18:30:26",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "Discussion"
          ],
          "summary": "Detailed build log for 128GB VRAM server using quad AMD R9700 GPUs, upgrading from previous dual MI100 setup with analysis of benchmark comparisons.",
          "importance_score": 92,
          "reasoning": "Highest engagement in batch (396 upvotes, 90 comments). Excellent hardware documentation for cutting-edge local LLM inference setup.",
          "themes": [
            "hardware",
            "amd",
            "vram",
            "build_guide"
          ],
          "continuation": null,
          "summary_html": "<p>Detailed build log for 128GB VRAM server using quad AMD R9700 GPUs, upgrading from previous dual MI100 setup with analysis of benchmark comparisons.</p>",
          "content_html": "<p>This is a sequel to my <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fqwrvg/64gb_vram_dual_mi100_server/\" target=\"_blank\" rel=\"noopener noreferrer\">previous thread</a> from 2024.</p>\n<p>I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge, and I picked up a lot of hardware upgrades over the course of 2025 in preparation for this. Notably, faster, double capacity memory (last February, well before the current price jump), another motherboard, higher capacity PSU, etc. But then I saw benchmarks for the R9700, particularly in the <a href=\"https://github.com/ggml-org/llama.cpp/discussions/15021\" target=\"_blank\" rel=\"noopener noreferrer\">llama.cpp ROCm thread</a>, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.</p>\n<p>Here's the specs and BOM. Note that the CPU and SSD were taken from the previous build, and the internal fans came bundled with the PSU as part of a deal:</p>\n<p>|Component|Description|Number|Unit Price|</p>\n<p>|:-|:-|:-|:-|</p>\n<p>|CPU|AMD Ryzen 7 5700X|1|$160.00|</p>\n<p>|RAM|Corsair Vengance LPX 64GB (2 x 32GB) DDR4 3600MHz C18|2|$105.00|</p>\n<p>|GPU|PowerColor AMD Radeon AI PRO R9700 32GB|4|$1,300.00|</p>\n<p>|Motherboard|MSI MEG X570 GODLIKE Motherboard|1|$490.00|</p>\n<p>|Storage|Inland Performance 1TB NVMe SSD|1|$100.00|</p>\n<p>|PSU|Super Flower Leadex Titanium 1600W 80+ Titanium|1|$440.00|</p>\n<p>|Internal Fans|Super Flower MEGACOOL 120mm fan, Triple-Pack|1|$0.00|</p>\n<p>|Case Fans|Noctua NF-A14 iPPC-3000 PWM|6|$30.00|</p>\n<p>|CPU Heatsink|AMD Wraith Prism aRGB CPU Cooler|1|$20.00|</p>\n<p>|Fan Hub|Noctua NA-FH1|1|$45.00|</p>\n<p>|Case|Phanteks Enthoo Pro 2 Server Edition|1|$190.00|</p>\n<p>|Total|||$7,035.00|</p>\n<p>128GB VRAM, 128GB RAM for offloading, all for less than the price of a RTX 6000 Blackwell.</p>\n<p>Some benchmarks:</p>\n<p>|model|size|params|backend|ngl|n\\_batch|n\\_ubatch|fa|test|t/s|</p>\n<p>|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|</p>\n<p>|llama 7B Q4\\_0|3.56 GiB|6.74 B|ROCm|99|1024|1024|1|pp8192|6524.91 ± 11.30|</p>\n<p>|llama 7B Q4\\_0|3.56 GiB|6.74 B|ROCm|99|1024|1024|1|tg128|90.89 ± 0.41|</p>\n<p>|qwen3moe 30B.A3B Q8\\_0|33.51 GiB|30.53 B|ROCm|99|1024|1024|1|pp8192|2113.82 ± 2.88|</p>\n<p>|qwen3moe 30B.A3B Q8\\_0|33.51 GiB|30.53 B|ROCm|99|1024|1024|1|tg128|72.51 ± 0.27|</p>\n<p>|qwen3vl 32B Q8\\_0|36.76 GiB|32.76 B|ROCm|99|1024|1024|1|pp8192|1725.46 ± 5.93|</p>\n<p>|qwen3vl 32B Q8\\_0|36.76 GiB|32.76 B|ROCm|99|1024|1024|1|tg128|14.75 ± 0.01|</p>\n<p>|llama 70B IQ4\\_XS - 4.25 bpw|35.29 GiB|70.55 B|ROCm|99|1024|1024|1|pp8192|1110.02 ± 3.49|</p>\n<p>|llama 70B IQ4\\_XS - 4.25 bpw|35.29 GiB|70.55 B|ROCm|99|1024|1024|1|tg128|14.53 ± 0.03|</p>\n<p>|qwen3next 80B.A3B IQ4\\_XS - 4.25 bpw|39.71 GiB|79.67 B|ROCm|99|1024|1024|1|pp8192|821.10 ± 0.27|</p>\n<p>|qwen3next 80B.A3B IQ4\\_XS - 4.25 bpw|39.71 GiB|79.67 B|ROCm|99|1024|1024|1|tg128|38.88 ± 0.02|</p>\n<p>|glm4moe ?B IQ4\\_XS - 4.25 bpw|54.33 GiB|106.85 B|ROCm|99|1024|1024|1|pp8192|1928.45 ± 3.74|</p>\n<p>|glm4moe ?B IQ4\\_XS - 4.25 bpw|54.33 GiB|106.85 B|ROCm|99|1024|1024|1|tg128|48.09 ± 0.16|</p>\n<p>|minimax-m2 230B.A10B IQ4\\_XS - 4.25 bpw|113.52 GiB|228.69 B|ROCm|99|1024|1024|1|pp8192|2082.04 ± 4.49|</p>\n<p>|minimax-m2 230B.A10B IQ4\\_XS - 4.25 bpw|113.52 GiB|228.69 B|ROCm|99|1024|1024|1|tg128|48.78 ± 0.06|</p>\n<p>|minimax-m2 230B.A10B Q8\\_0|226.43 GiB|228.69 B|ROCm|30|1024|1024|1|pp8192|42.62 ± 7.96|</p>\n<p>|minimax-m2 230B.A10B Q8\\_0|226.43 GiB|228.69 B|ROCm|30|1024|1024|1|tg128|6.58 ± 0.01|</p>\n<p>A few final observations:</p>\n<p>* glm4 moe and minimax-m2 are actually GLM-4.6V and MiniMax-M2.1, respectively.</p>\n<p>* There's an open issue for Qwen3-Next at the moment; recent optimizations caused some pretty hefty prompt processing regressions. The numbers here are pre #18683, in case the exact issue gets resolved.</p>\n<p>* A word on the Q8 quant of MiniMax-M2.1; `--fit on`  isn't supported on llama-bench, so I can't give an apples to apples comparison to simply reducing the number of gpu layers, but it's also extremely unreliable for me in llama-server, giving me HIP error 906 on the first generation. Out of a dozen or so attempts, I've gotten it to work once, with a TG around 8.5 t/s, but take that with a grain of salt. Otherwise, maybe the quality jump is worth letting it run overnight? You be the judge.  It also takes 2 hours to load, but that could be because I'm loading it off external storage.</p>\n<p>* The internal fan mount on the case only has screws on one side; in the intended configuration, the holes for power cables are on the opposite side of where the GPU power sockets are, meaning the power cables will block airflow from the fans. How they didn't see this, I have no idea. Thankfully, it stays in place from a friction fit if you flip it 180 like I did. Really, I probably could have gone without it, it was mostly a consideration for when I was still going with MI100s, but the fans were free anyway.</p>\n<p>* I really, really wanted to go AM5 for this, but there just isn't a board out there with 4 full sized PCIe slots spaced for 2 slot GPUs. At best you can fit 3 and then cover up one of them. But if you need a bazillion m.2 slots you're golden /s. You might then ask why I didn't go for Threadripper/Epyc, and that's because I was worried about power consumption and heat. I didn't want to mess with risers and open rigs, so I found the one AM4 board that could do this, even if it comes at the cost of RAM speeds/channels and slower PCIe speeds.</p>\n<p>* The MI100s and R9700s didn't play nice for the brief period of time I had 2 of both. I didn't bother troubleshooting, just shrugged and sold them off, so it may have been a simple fix but FYI.</p>\n<p>* Going with a 1 TB SSD in my original build was a mistake, even 2 would have made a world of difference. Between LLMs, image generation, TTS, ect. I'm having trouble actually taking advantage of the extra VRAM with less quantized models due to storage constraints, which is why my benchmarks still have a lot of 4-bit quants despite being able to easily do 8-bit ones.</p>\n<p>* I don't know how to control the little LCD display on the board. I'm not sure there is a way on Linux. A shame.</p>"
        },
        {
          "id": "56ac24f0407a",
          "title": "Qwen 4 might be a long way off !? Lead Dev says they are \"slowing down\" to focus on quality.",
          "content": "",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/",
          "author": "u/Difficult-Cap-7527",
          "published": "2026-01-17T20:28:57",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "Discussion"
          ],
          "summary": "Qwen lead developer announces team is 'slowing down' Qwen 4 development to focus on quality over rapid releases.",
          "importance_score": 88,
          "reasoning": "Very high engagement (318 upvotes, 49 comments). Major strategic news from top open-weight model provider. Signals industry maturation.",
          "themes": [
            "qwen",
            "open_models",
            "industry_news"
          ],
          "continuation": null,
          "summary_html": "<p>Qwen lead developer announces team is 'slowing down' Qwen 4 development to focus on quality over rapid releases.</p>",
          "content_html": ""
        },
        {
          "id": "b107c307008f",
          "title": "LTX 2 is amazing : LTX-2 in ComfyUI on RTX 3060 12GB",
          "content": "My setup: RTX 3060 12GB VRAM + 48GB system RAM.\n\nI spent the last couple of days messing around with **LTX-2** inside ComfyUI and had an absolute blast. I created short sample scenes for a loose **spy story set in a neon-soaked, rainy Dhaka** (cyberpunk/Bangla vibes with rainy streets, umbrellas, dramatic reflections, and a mysterious female lead).\n\nWorkflow : [https://drive.google.com/file/d/1VYrKf7jq52BIi43mZpsP8QCypr9oHtCO/view](https://drive.google.com/file/d/1VYrKf7jq52BIi43mZpsP8QCypr9oHtCO/view)  \ni forgot the username who shared it under a post. This workflow worked really well! \n\nEach 8-second scene took about **12 minutes** to generate (with synced audio). I queued up **70+ scenes** total, often trying 3-4 prompt variations per scene to get the mood right. Some scenes were pure text-to-video, others image-to-video starting from Midjourney stills I generated for consistency.\n\nHere's a compilation of some of my favorite clips (rainy window reflections, coffee steam morphing into faces, walking through crowded neon markets, intense close-ups in the downpour):\n\ni cleaned up the audio. it had some squeaky sounds.\n\n**Strengths that blew me away:**\n\n1. **Speed** – Seriously fast for what it delivers, especially compared to other local video models.\n2. **Audio sync** is legitimately impressive. I tested illustration styles, anime-ish looks, realistic characters, and even puppet/weird abstract shapes – lip sync, ambient rain, subtle SFX/music all line up way better than I expected. Achieving this level of quality on just **12GB VRAM** is wild.\n3. **Handles non-realistic/abstract content extremely well** – illustrations, stylized/puppet-like figures, surreal elements (like steam forming faces or exaggerated rain effects) come out coherent and beautiful.\n\n**Weaknesses / Things to avoid:**\n\n1. Weird random zoom-in effects pop up sometimes – not sure if prompt-related or model quirk.\n2. **Actions/motion-heavy scenes** just don't work reliably yet. Keep it to subtle movements, expressions, atmosphere, rain, steam, walking slowly, etc. – anything dynamic tends to break coherence.\n\nOverall verdict: I literally couldn't believe how two full days disappeared – I was having way too much fun iterating prompts and watching the queue. LTX-2 feels like a huge step forward for local audio-video gen, especially if you lean into atmospheric/illustrative styles rather than high-action.",
          "url": "https://reddit.com/r/StableDiffusion/comments/1qflkt7/ltx_2_is_amazing_ltx2_in_comfyui_on_rtx_3060_12gb/",
          "author": "u/tanzim31",
          "published": "2026-01-17T13:56:37",
          "source": "r/StableDiffusion",
          "source_type": "reddit",
          "tags": [
            "Workflow Included"
          ],
          "summary": "Detailed showcase of LTX-2 video generation in ComfyUI on RTX 3060 12GB, creating cyberpunk spy scenes in Dhaka. 761 upvotes, 124 comments.",
          "importance_score": 88,
          "reasoning": "Highest engagement post, excellent technical showcase with workflow sharing, hardware specs, and creative results.",
          "themes": [
            "ltx-2",
            "video-generation",
            "comfyui",
            "workflow-sharing"
          ],
          "continuation": null,
          "summary_html": "<p>Detailed showcase of LTX-2 video generation in ComfyUI on RTX 3060 12GB, creating cyberpunk spy scenes in Dhaka. 761 upvotes, 124 comments.</p>",
          "content_html": "<p>My setup: RTX 3060 12GB VRAM + 48GB system RAM.</p>\n<p>I spent the last couple of days messing around with <strong>LTX-2</strong> inside ComfyUI and had an absolute blast. I created short sample scenes for a loose <strong>spy story set in a neon-soaked, rainy Dhaka</strong> (cyberpunk/Bangla vibes with rainy streets, umbrellas, dramatic reflections, and a mysterious female lead).</p>\n<p>Workflow : <a href=\"https://drive.google.com/file/d/1VYrKf7jq52BIi43mZpsP8QCypr9oHtCO/view\" target=\"_blank\" rel=\"noopener noreferrer\">https://drive.google.com/file/d/1VYrKf7jq52BIi43mZpsP8QCypr9oHtCO/view</a></p>\n<p>i forgot the username who shared it under a post. This workflow worked really well!</p>\n<p>Each 8-second scene took about <strong>12 minutes</strong> to generate (with synced audio). I queued up <strong>70+ scenes</strong> total, often trying 3-4 prompt variations per scene to get the mood right. Some scenes were pure text-to-video, others image-to-video starting from Midjourney stills I generated for consistency.</p>\n<p>Here's a compilation of some of my favorite clips (rainy window reflections, coffee steam morphing into faces, walking through crowded neon markets, intense close-ups in the downpour):</p>\n<p>i cleaned up the audio. it had some squeaky sounds.</p>\n<p><strong>Strengths that blew me away:</strong></p>\n<p>1. <strong>Speed</strong> – Seriously fast for what it delivers, especially compared to other local video models.</p>\n<p>2. <strong>Audio sync</strong> is legitimately impressive. I tested illustration styles, anime-ish looks, realistic characters, and even puppet/weird abstract shapes – lip sync, ambient rain, subtle SFX/music all line up way better than I expected. Achieving this level of quality on just <strong>12GB VRAM</strong> is wild.</p>\n<p>3. <strong>Handles non-realistic/abstract content extremely well</strong> – illustrations, stylized/puppet-like figures, surreal elements (like steam forming faces or exaggerated rain effects) come out coherent and beautiful.</p>\n<p><strong>Weaknesses / Things to avoid:</strong></p>\n<p>1. Weird random zoom-in effects pop up sometimes – not sure if prompt-related or model quirk.</p>\n<p>2. <strong>Actions/motion-heavy scenes</strong> just don't work reliably yet. Keep it to subtle movements, expressions, atmosphere, rain, steam, walking slowly, etc. – anything dynamic tends to break coherence.</p>\n<p>Overall verdict: I literally couldn't believe how two full days disappeared – I was having way too much fun iterating prompts and watching the queue. LTX-2 feels like a huge step forward for local audio-video gen, especially if you lean into atmospheric/illustrative styles rather than high-action.</p>"
        }
      ]
    }
  }
}