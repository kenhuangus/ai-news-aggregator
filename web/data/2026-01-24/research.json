{
  "category": "research",
  "date": "2026-01-24",
  "category_summary": "Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work [proposes **emergency response measures**](/?date=2026-01-24&category=research#item-dde7d819fa66) for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.\n\n- Empirical work on **unsupervised elicitation** [finds simple few-shot prompting](/?date=2026-01-24&category=research#item-24f85ac93189) matches sophisticated **ICM algorithm** performance for base model capability extraction\n- A new **Eval Awareness Framework** [formalizes when LLMs detect](/?date=2026-01-24&category=research#item-194fc1f46619) evaluation contexts and potentially game benchmarks—critical for safety evaluations\n- The **Digital Consciousness Model (DCM)** [introduces probabilistic assessment](/?date=2026-01-24&category=research#item-c9fbeadb6060) across multiple consciousness theories rather than single-theory verdicts\n- Theoretical work [argues human values are alignable](/?date=2026-01-24&category=research#item-2df5c6dcb797) because evolution compressed motivation into **low-dimensional bottlenecks**\n\nMeta-science initiatives [propose systematic replication](/?date=2026-01-24&category=research#item-429650348119) teams. Interpretability research [examines **attention sinks**](/?date=2026-01-24&category=research#item-709ca3227a1e) and the **dark subspace** where transformers store non-interpretable signals. Steven Byrnes [releases v3](/?date=2026-01-24&category=research#item-c0f1b9d27e0a) of his **225-page brain-like AGI safety** resource.",
  "category_summary_html": "<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href=\"/?date=2026-01-24&amp;category=research#item-dde7d819fa66\" class=\"internal-link\" rel=\"noopener noreferrer\">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>\n<ul>\n<li>Empirical work on <strong>unsupervised elicitation</strong> <a href=\"/?date=2026-01-24&amp;category=research#item-24f85ac93189\" class=\"internal-link\" rel=\"noopener noreferrer\">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>\n<li>A new <strong>Eval Awareness Framework</strong> <a href=\"/?date=2026-01-24&amp;category=research#item-194fc1f46619\" class=\"internal-link\" rel=\"noopener noreferrer\">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>\n<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href=\"/?date=2026-01-24&amp;category=research#item-c9fbeadb6060\" class=\"internal-link\" rel=\"noopener noreferrer\">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>\n<li>Theoretical work <a href=\"/?date=2026-01-24&amp;category=research#item-2df5c6dcb797\" class=\"internal-link\" rel=\"noopener noreferrer\">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>\n</ul>\n<p>Meta-science initiatives <a href=\"/?date=2026-01-24&amp;category=research#item-429650348119\" class=\"internal-link\" rel=\"noopener noreferrer\">propose systematic replication</a> teams. Interpretability research <a href=\"/?date=2026-01-24&amp;category=research#item-709ca3227a1e\" class=\"internal-link\" rel=\"noopener noreferrer\">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href=\"/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a\" class=\"internal-link\" rel=\"noopener noreferrer\">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>",
  "themes": [
    {
      "name": "AI Safety & Alignment",
      "description": "Research on making AI systems safe, aligned with human values, and robust to deception or gaming of evaluations",
      "item_count": 8,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "AI Governance & Policy",
      "description": "Regulatory frameworks, international coordination, and policy proposals for AI development",
      "item_count": 2,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "Interpretability & Mechanistic Understanding",
      "description": "Understanding how neural networks represent and process information internally",
      "item_count": 3,
      "example_items": [],
      "importance": 58
    },
    {
      "name": "Meta-Science & Research Methods",
      "description": "Improving how AI research itself is conducted, replicated, and validated",
      "item_count": 2,
      "example_items": [],
      "importance": 55
    },
    {
      "name": "AI Consciousness & Ethics",
      "description": "Frameworks for assessing and reasoning about potential machine consciousness and its implications",
      "item_count": 3,
      "example_items": [],
      "importance": 50
    },
    {
      "name": "Non-AI Content",
      "description": "Posts unrelated to AI research (dating, history, creative philosophy, gambling)",
      "item_count": 5,
      "example_items": [],
      "importance": 5
    }
  ],
  "total_items": 19,
  "items": [
    {
      "id": "dde7d819fa66",
      "title": "Emergency Response Measures for Catastrophic AI Risk",
      "content": "I have written a paper on Chinese domestic AI regulation with coauthors James Zhang, Zongze Wu, Michael Chen, Yue Zhu, and Geng Hong. It was presented recently at NeurIPS 2025's Workshop on Regulatable ML, and it may be found on ArXiv and SSRN.Here I'll explain what I take to be the key ideas of the paper in a more casual style. I am speaking only for myself in this post, and not for any of my coauthors.Thanks to James for creating this poster.The top US AI companies have better capabilities than the top Chinese companies have for now, but the US lead isn't more than a year at most, and I expect it to narrow over the next couple years.[1]&nbsp;I am therefore nearly as worried about catastrophic risk from Chinese-developed AI as I am worried about catastrophic risk from American AI.I would worry somewhat less if Chinese AI companies took the same commendable but insufficient steps to manage risk that their American peers have taken. In particular, I want Chinese companies to do dangerous capability testing before deploying new frontier models and to follow published safety policies (FSPs). The companies are not doing these things in the status quo. DeepSeek did no documented safety testing whatsoever before they open-weighted v3.2.[2]&nbsp;Not one of the leading Chinese companies has published a safety policy.[3]Now here's our intervention. We point out that FSPs are a reasonable way of implementing the CCP's stated policy goals on AI, and that China's government already has tools in place to mandate FSPs if it wishes to do so.Earlier this year, Xi Jinping announced that China should \"establish systems for technical monitoring, early risk warning and emergency response\" to guarantee AI's \"safety, reliability and controllability.\" Notice that Xi is talking about identifying risks in advance and taking steps to prevent safety incidents before they can strike. Even \"emergency response\" means something more than reaction in official Chinese thinking, also encompassing ri...",
      "url": "https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk",
      "author": "MKodama",
      "published": "2026-01-23T13:18:09.622000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Presents a paper on Chinese AI regulation and safety measures, arguing Chinese AI companies (like DeepSeek) lack adequate safety testing before deployment. Proposes emergency response frameworks and was presented at NeurIPS 2025 Workshop on Regulatable ML.",
      "importance_score": 72,
      "reasoning": "Peer-reviewed policy research with concrete regulatory proposals for a critical gap in global AI safety. Published at NeurIPS workshop adds credibility. Addresses neglected area of Chinese AI governance with practical recommendations.",
      "themes": [
        "AI Governance",
        "AI Safety",
        "Policy",
        "International AI Regulation"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a paper on Chinese AI regulation and safety measures, arguing Chinese AI companies (like DeepSeek) lack adequate safety testing before deployment. Proposes emergency response frameworks and was presented at NeurIPS 2025 Workshop on Regulatable ML.</p>",
      "content_html": "<p>I have written a paper on Chinese domestic AI regulation with coauthors James Zhang, Zongze Wu, Michael Chen, Yue Zhu, and Geng Hong. It was presented recently at NeurIPS 2025's Workshop on Regulatable ML, and it may be found on ArXiv and SSRN.Here I'll explain what I take to be the key ideas of the paper in a more casual style. I am speaking only for myself in this post, and not for any of my coauthors.Thanks to James for creating this poster.The top US AI companies have better capabilities than the top Chinese companies have for now, but the US lead isn't more than a year at most, and I expect it to narrow over the next couple years.[1]&nbsp;I am therefore nearly as worried about catastrophic risk from Chinese-developed AI as I am worried about catastrophic risk from American AI.I would worry somewhat less if Chinese AI companies took the same commendable but insufficient steps to manage risk that their American peers have taken. In particular, I want Chinese companies to do dangerous capability testing before deploying new frontier models and to follow published safety policies (FSPs). The companies are not doing these things in the status quo. DeepSeek did no documented safety testing whatsoever before they open-weighted v3.2.[2]&nbsp;Not one of the leading Chinese companies has published a safety policy.[3]Now here's our intervention. We point out that FSPs are a reasonable way of implementing the CCP's stated policy goals on AI, and that China's government already has tools in place to mandate FSPs if it wishes to do so.Earlier this year, Xi Jinping announced that China should \"establish systems for technical monitoring, early risk warning and emergency response\" to guarantee AI's \"safety, reliability and controllability.\" Notice that Xi is talking about identifying risks in advance and taking steps to prevent safety incidents before they can strike. Even \"emergency response\" means something more than reaction in official Chinese thinking, also encompassing ri...</p>"
    },
    {
      "id": "24f85ac93189",
      "title": "Eliciting base models with simple unsupervised techniques",
      "content": "Authors: Aditya Shrivastava*, Allison Qi*, Callum Canavan*, Tianyi Alex Qiu, Jonathan Michala, Fabien Roger(*Equal contributions, reverse alphabetical)Wen et al. introduced the internal coherence maximization (ICM) algorithm for unsupervised elicitation of base models. They showed that for several datasets, training a base model on labels generated by their algorithm gives similar test accuracy to training on golden labels. To understand which aspects of ICM are most useful, we ran a couple of simple unsupervised elicitation methods that leverage some of the factors that might make ICM work. We compared these baseline methods to training on golden labels for both in-context learning and iterative fine-tuning, using the same datasets as Wen et al. and similar hyperparameters.We find that:Just using few-shot prompts with random labels recovers 53–93% of the gap between zero-shot accuracy and many-shot prompting with golden labels, and iterative fine-tuning on labels created with this baseline recovers 62–96% of the gap between untrained models and golden fine-tuned models.The most useful aspects of ICM arebootstrapping (using predictions from one iteration of few-shot prompting as few-shot examples in the next iteration)enforcing logical consistency of predictions.A simple method which combines these recovers 83–100% of the gap between zero-shot accuracy and many-shot prompting with golden labels, and iterative fine-tuning with this method recovers 91–99% of the gap between untrained models and golden fine-tuned models.These results do&nbsp;not hold if we increase the size of the training set from ~2k data points (as in Wen et al.) to ~30k data points: golden fine-tuning performance increases with dataset size more than unsupervised elicitation performance.This makes sense, as larger fine-tuning runs likely teach the model something new, they don’t just elicit existing capabilities.There is no strong reason to expect these simple techniques to elicit superhuman knowle...",
      "url": "https://www.lesswrong.com/posts/rFxfMbwJ3v4PNesWP/eliciting-base-models-with-simple-unsupervised-techniques",
      "author": "Callum Canavan",
      "published": "2026-01-23T13:06:15.695000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Empirical research testing simple unsupervised elicitation methods against the Internal Coherence Maximization (ICM) algorithm. Finds that few-shot prompts with random labels recover 53-93% of supervised performance, and identifies bootstrapping as ICM's most valuable component.",
      "importance_score": 68,
      "reasoning": "Original empirical research with clear methodology testing what makes unsupervised elicitation work. Practical findings that simpler baselines work surprisingly well has implications for understanding base model capabilities. Multiple authors including Fabien Roger (Anthropic-adjacent).",
      "themes": [
        "Base Models",
        "Unsupervised Learning",
        "Elicitation",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Empirical research testing simple unsupervised elicitation methods against the Internal Coherence Maximization (ICM) algorithm. Finds that few-shot prompts with random labels recover 53-93% of supervised performance, and identifies bootstrapping as ICM's most valuable component.</p>",
      "content_html": "<p>Authors: Aditya Shrivastava*, Allison Qi*, Callum Canavan*, Tianyi Alex Qiu, Jonathan Michala, Fabien Roger(*Equal contributions, reverse alphabetical)Wen et al. introduced the internal coherence maximization (ICM) algorithm for unsupervised elicitation of base models. They showed that for several datasets, training a base model on labels generated by their algorithm gives similar test accuracy to training on golden labels. To understand which aspects of ICM are most useful, we ran a couple of simple unsupervised elicitation methods that leverage some of the factors that might make ICM work. We compared these baseline methods to training on golden labels for both in-context learning and iterative fine-tuning, using the same datasets as Wen et al. and similar hyperparameters.We find that:Just using few-shot prompts with random labels recovers 53–93% of the gap between zero-shot accuracy and many-shot prompting with golden labels, and iterative fine-tuning on labels created with this baseline recovers 62–96% of the gap between untrained models and golden fine-tuned models.The most useful aspects of ICM arebootstrapping (using predictions from one iteration of few-shot prompting as few-shot examples in the next iteration)enforcing logical consistency of predictions.A simple method which combines these recovers 83–100% of the gap between zero-shot accuracy and many-shot prompting with golden labels, and iterative fine-tuning with this method recovers 91–99% of the gap between untrained models and golden fine-tuned models.These results do&nbsp;not hold if we increase the size of the training set from ~2k data points (as in Wen et al.) to ~30k data points: golden fine-tuning performance increases with dataset size more than unsupervised elicitation performance.This makes sense, as larger fine-tuning runs likely teach the model something new, they don’t just elicit existing capabilities.There is no strong reason to expect these simple techniques to elicit superhuman knowle...</p>"
    },
    {
      "id": "194fc1f46619",
      "title": "A Framework for Eval Awareness",
      "content": "In this post, we offer a conceptual framework for evaluation awareness. This is designed to clarify the different ways in which models can respond to evaluations. Some key ideas we introduce through the lens of our framework include leveraging model uncertainty about eval type and awareness-robust consistency. We hope this framework helps to delineate the existing research directions and inspire future work.This work was done in collaboration with Jasmine Li in the first two weeks of MATS 9.0 under the mentorship of Victoria Krakovna. Thanks to (in alphabetical order) David Africa, Lovkush Agarwal, Claude, Giles Edkins, Jannes Elstner, Shawn Hu, Tim Hua, Igor Ivanov, Victoria Krakovna, Jasmine Li, Martin Listwan, Mary Phuong, Daniel Tan, and Alex Turner (plus others I’ve no doubt forgotten to name!) for discussions and thoughts over the last couple weeks. We’re excited to continue working on this topic throughout the programme, so if you have suggestions or takes, please share liberally in the comments!IntroductionEvaluation awareness describes the phenomenon of an LLM inferring from various cues that it is under evaluation. Perhaps the most prominent example would be [Sonnet 4.5] (see §7.2) from just a few months ago. As frontier models continue to improve, we expect cases of evaluation awareness to become more frequent, raising concerns about the enduring validity of our evaluations.Why should we be concerned? Model evaluations are designed to elicit realistic, authentic behaviour from models in safe, pre-deployment settings. These might inform any additional post-training runs we need to conduct, or even whether we choose to deploy a model at all. However, like human test-takers, subjects can act strategically to get a better outcome if they recognise an evaluation and reason about its scoring criteria or the consequences of their scores. We call this behaviour evaluation gaming.Some previous blogposts and papers have addressed evaluation awareness and gaming bef...",
      "url": "https://www.lesswrong.com/posts/cjMpms3dBZJCrxL8c/a-framework-for-eval-awareness",
      "author": "LAThomson",
      "published": "2026-01-23T05:16:36.160000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes a conceptual framework for 'evaluation awareness'—when LLMs infer they're being evaluated and potentially behave differently. Introduces concepts like leveraging model uncertainty about eval type and awareness-robust consistency.",
      "importance_score": 65,
      "reasoning": "Addresses important safety-relevant problem (models gaming evaluations). Produced during MATS under Victoria Krakovna's mentorship. Framework-level contribution that clarifies research directions. Direct relevance to making AI evaluations robust.",
      "themes": [
        "AI Safety",
        "Evaluations",
        "Deception",
        "Model Behavior"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes a conceptual framework for 'evaluation awareness'—when LLMs infer they're being evaluated and potentially behave differently. Introduces concepts like leveraging model uncertainty about eval type and awareness-robust consistency.</p>",
      "content_html": "<p>In this post, we offer a conceptual framework for evaluation awareness. This is designed to clarify the different ways in which models can respond to evaluations. Some key ideas we introduce through the lens of our framework include leveraging model uncertainty about eval type and awareness-robust consistency. We hope this framework helps to delineate the existing research directions and inspire future work.This work was done in collaboration with Jasmine Li in the first two weeks of MATS 9.0 under the mentorship of Victoria Krakovna. Thanks to (in alphabetical order) David Africa, Lovkush Agarwal, Claude, Giles Edkins, Jannes Elstner, Shawn Hu, Tim Hua, Igor Ivanov, Victoria Krakovna, Jasmine Li, Martin Listwan, Mary Phuong, Daniel Tan, and Alex Turner (plus others I’ve no doubt forgotten to name!) for discussions and thoughts over the last couple weeks. We’re excited to continue working on this topic throughout the programme, so if you have suggestions or takes, please share liberally in the comments!IntroductionEvaluation awareness describes the phenomenon of an LLM inferring from various cues that it is under evaluation. Perhaps the most prominent example would be [Sonnet 4.5] (see §7.2) from just a few months ago. As frontier models continue to improve, we expect cases of evaluation awareness to become more frequent, raising concerns about the enduring validity of our evaluations.Why should we be concerned? Model evaluations are designed to elicit realistic, authentic behaviour from models in safe, pre-deployment settings. These might inform any additional post-training runs we need to conduct, or even whether we choose to deploy a model at all. However, like human test-takers, subjects can act strategically to get a better outcome if they recognise an evaluation and reason about its scoring criteria or the consequences of their scores. We call this behaviour evaluation gaming.Some previous blogposts and papers have addressed evaluation awareness and gaming bef...</p>"
    },
    {
      "id": "c9fbeadb6060",
      "title": "Digital Consciousness Model Results and Key Takeaways",
      "content": "Introduction to the Digital Consciousness Model (DCM)Artificially intelligent systems, especially large language models (LLMs) used by almost 50% of the adult US population, have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious?The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives—acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it.Here, we present some of the key initial results of the DCM. The full report is now available here.We will be hosting a webinar on February 10 to discuss our findings and answer audience questions. You can find more information and register for that event here.Why this mattersIt is important to assess whether AI systems might be conscious in a way that takes seriously both the many different views about what consciousness is and the specific details of these systems. Even though our conclusions remain uncertain, it's worth trying to estimate, as concretely as we can, the probability that AI systems are conscious. Here are the reasons why:As AI systems become increasingly complex and sophisticated, many people (experts and laypeople alike) find it increasingly plausible that these systems may be phenomenally conscious—that is, they have experiences, and there is something that it feels like to be them.If AIs are conscious, then they likely deserve moral consideration, and we risk harming them if we do not take precautions to ensure their welfare. If ...",
      "url": "https://www.lesswrong.com/posts/YftBFESFevbF25tZW/digital-consciousness-model-results-and-key-takeaways",
      "author": "arvomm",
      "published": "2026-01-23T10:58:10.837000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Introduces the Digital Consciousness Model (DCM), a probabilistic framework for assessing AI consciousness that incorporates multiple theories rather than assuming one. Presents initial results comparing different AI systems and biological organisms.",
      "importance_score": 62,
      "reasoning": "Novel systematic approach to a difficult problem. Incorporates uncertainty across consciousness theories, which is methodologically sound. Has practical implications for AI welfare policy. Full report available with upcoming webinar adds legitimacy.",
      "themes": [
        "AI Consciousness",
        "AI Ethics",
        "Philosophy of Mind",
        "Measurement"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces the Digital Consciousness Model (DCM), a probabilistic framework for assessing AI consciousness that incorporates multiple theories rather than assuming one. Presents initial results comparing different AI systems and biological organisms.</p>",
      "content_html": "<p>Introduction to the Digital Consciousness Model (DCM)Artificially intelligent systems, especially large language models (LLMs) used by almost 50% of the adult US population, have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious?The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives—acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it.Here, we present some of the key initial results of the DCM. The full report is now available here.We will be hosting a webinar on February 10 to discuss our findings and answer audience questions. You can find more information and register for that event here.Why this mattersIt is important to assess whether AI systems might be conscious in a way that takes seriously both the many different views about what consciousness is and the specific details of these systems. Even though our conclusions remain uncertain, it's worth trying to estimate, as concretely as we can, the probability that AI systems are conscious. Here are the reasons why:As AI systems become increasingly complex and sophisticated, many people (experts and laypeople alike) find it increasingly plausible that these systems may be phenomenally conscious—that is, they have experiences, and there is something that it feels like to be them.If AIs are conscious, then they likely deserve moral consideration, and we risk harming them if we do not take precautions to ensure their welfare. If ...</p>"
    },
    {
      "id": "709ca3227a1e",
      "title": "Paying attention to Attention Sinks",
      "content": "I recently read Spectral Filters, Dark Signals, and Attention Sinks, an interesting paper on discovering where excess attention in transformers is dumped. Researcher found that transformers contain a \"Dark Subspace\" to store information that isn't intended for the output layer. The attention sink concept is a specific manifestation of this, where the model learns to dump the remaining attention from the Softmax into the first ([BOS]) token.The authors used spectral filters to decompose the residual stream. The most interesting finding was the U-Dark subspace (the unembedded tokens that didn't map to the vocabulary). By filtering the tail end of the singular value decomposition (SVD) of the weights, they found that the model uses these \"dark\" signals to let the model know where to dump attention.When the \"dark tail\" was removed, the Negative Log-Likelihood (NLL) spiked because the attention mechanism lost its ability to stay \"quiet\" when it had nothing to say, leading to model confusion.&nbsp;They created a \"sink-preserving\" filter that kept the \"head\" of the spectrum and the mechanical \"dark tail,\" but deleted the middle. The model performed surprisingly well, proving that the \"dark\" tail is essential for stability and coherence, while the middle is not.&nbsp;I think an interesting direction in the future would be to see if only the single token in the dark tail acts as the sink, or if there could possible multiple sinks. Additionally, I would like to see if we can manually designate a particular section to act as the sink.&nbsp;I suspect that in models trained with 'No-BOS' (beginning of sequence) constraints, we might find 'Distributed Sinks' across high-frequency tokens like periods or 'the'. It would be worth investigating the spectral signature of these tokens to see if they adopt the same 'U-Dark' profile as the standard 'BOS' sink.After reading this paper I realize that the concept of attention sinks is so cool! The fundamental reason attention sinks exist is...",
      "url": "https://www.lesswrong.com/posts/mMhxFAwxjL8tnYgsY/paying-attention-to-attention-sinks",
      "author": "Mitali M",
      "published": "2026-01-23T16:40:16.362000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Summarizes research on 'attention sinks' and the 'dark subspace' in transformers—regions where models dump excess attention and store signals not intended for output. Removing this dark tail causes performance degradation, suggesting it serves a functional mechanical role.",
      "importance_score": 58,
      "reasoning": "Good technical summary of interesting interpretability research on transformer mechanics. The finding that models need 'quiet' attention mechanisms is practically relevant for model understanding, though this is a summary rather than original research.",
      "themes": [
        "Interpretability",
        "Transformer Architecture",
        "Mechanistic Understanding"
      ],
      "continuation": null,
      "summary_html": "<p>Summarizes research on 'attention sinks' and the 'dark subspace' in transformers—regions where models dump excess attention and store signals not intended for output. Removing this dark tail causes performance degradation, suggesting it serves a functional mechanical role.</p>",
      "content_html": "<p>I recently read Spectral Filters, Dark Signals, and Attention Sinks, an interesting paper on discovering where excess attention in transformers is dumped. Researcher found that transformers contain a \"Dark Subspace\" to store information that isn't intended for the output layer. The attention sink concept is a specific manifestation of this, where the model learns to dump the remaining attention from the Softmax into the first ([BOS]) token.The authors used spectral filters to decompose the residual stream. The most interesting finding was the U-Dark subspace (the unembedded tokens that didn't map to the vocabulary). By filtering the tail end of the singular value decomposition (SVD) of the weights, they found that the model uses these \"dark\" signals to let the model know where to dump attention.When the \"dark tail\" was removed, the Negative Log-Likelihood (NLL) spiked because the attention mechanism lost its ability to stay \"quiet\" when it had nothing to say, leading to model confusion.&nbsp;They created a \"sink-preserving\" filter that kept the \"head\" of the spectrum and the mechanical \"dark tail,\" but deleted the middle. The model performed surprisingly well, proving that the \"dark\" tail is essential for stability and coherence, while the middle is not.&nbsp;I think an interesting direction in the future would be to see if only the single token in the dark tail acts as the sink, or if there could possible multiple sinks. Additionally, I would like to see if we can manually designate a particular section to act as the sink.&nbsp;I suspect that in models trained with 'No-BOS' (beginning of sequence) constraints, we might find 'Distributed Sinks' across high-frequency tokens like periods or 'the'. It would be worth investigating the spectral signature of these tokens to see if they adopt the same 'U-Dark' profile as the standard 'BOS' sink.After reading this paper I realize that the concept of attention sinks is so cool! The fundamental reason attention sinks exist is...</p>"
    },
    {
      "id": "429650348119",
      "title": "Principles for Meta-Science and AI Safety Replications",
      "content": "If we get AI safety research wrong, we may not get a second chance. But despite the stakes being so high, there has been no effort to systematically review and verify empirical AI safety papers. I would like to change that.Today I sent in funding applications to found a team of researchers dedicated to replicating AI safety work.&nbsp;But what exactly should we aim to accomplish? What should AI safety replications even look like? After 1-2 months of consideration and 50+ hours of conversation, this document outlines principles that will guide our future team.I. Meta-science doesn’t vindicate anyoneResearchers appear to agree that some share of AI safety work is low-quality, false, or misleading. However, everyone seems to disagree on&nbsp;which share of papers are the problematic ones.&nbsp;When I expressed interest in starting a group that does AI safety replications, I suspect some assumed I would be “exposing” the papers that&nbsp;they don’t approve of.&nbsp; This is a trap and it is especially important for us, as the replicators, not to fall into it. If our replications tend to confirm our beliefs, that probably says more about our priors than the papers we are studying.II. Searching for “bad” papers is like searching for “haunted” housesConsider a team of researchers trying to find examples of haunted houses. They could investigate suspicious buildings or take tips from people who have witnessed paranormal activity. They could then publish reports of which houses you should definitely avoid. But the issue is that ghosts aren’t real.&nbsp;What they would be finding is a convincing story, not the underlying truth.Trying to find “bad” papers will be like finding haunted houses. If given a mandate to find papers that don’t replicate, we will find them. But the uncomfortable truth is that genuinely influential papers that are&nbsp;straightforwardly, objectively wrong are rare. The empirical claims are likely true in some sense, but don't tell the full story.&nbsp;T...",
      "url": "https://www.lesswrong.com/posts/8qytxHWzSsdsyTfmZ/principles-for-meta-science-and-ai-safety-replications",
      "author": "zroe1",
      "published": "2026-01-23T01:59:02.879000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes founding a team dedicated to replicating AI safety research, outlining principles: meta-science shouldn't vindicate preexisting beliefs, selection of papers should be principled, and the field needs systematic verification given high stakes.",
      "importance_score": 58,
      "reasoning": "Important meta-science initiative for AI safety. Replication is critically needed in the field. The principles outlined (avoiding confirmation bias, systematic selection) are methodologically sound. Practical proposal with funding applications submitted.",
      "themes": [
        "Meta-Science",
        "AI Safety",
        "Replication",
        "Research Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes founding a team dedicated to replicating AI safety research, outlining principles: meta-science shouldn't vindicate preexisting beliefs, selection of papers should be principled, and the field needs systematic verification given high stakes.</p>",
      "content_html": "<p>If we get AI safety research wrong, we may not get a second chance. But despite the stakes being so high, there has been no effort to systematically review and verify empirical AI safety papers. I would like to change that.Today I sent in funding applications to found a team of researchers dedicated to replicating AI safety work.&nbsp;But what exactly should we aim to accomplish? What should AI safety replications even look like? After 1-2 months of consideration and 50+ hours of conversation, this document outlines principles that will guide our future team.I. Meta-science doesn’t vindicate anyoneResearchers appear to agree that some share of AI safety work is low-quality, false, or misleading. However, everyone seems to disagree on&nbsp;which share of papers are the problematic ones.&nbsp;When I expressed interest in starting a group that does AI safety replications, I suspect some assumed I would be “exposing” the papers that&nbsp;they don’t approve of.&nbsp; This is a trap and it is especially important for us, as the replicators, not to fall into it. If our replications tend to confirm our beliefs, that probably says more about our priors than the papers we are studying.II. Searching for “bad” papers is like searching for “haunted” housesConsider a team of researchers trying to find examples of haunted houses. They could investigate suspicious buildings or take tips from people who have witnessed paranormal activity. They could then publish reports of which houses you should definitely avoid. But the issue is that ghosts aren’t real.&nbsp;What they would be finding is a convincing story, not the underlying truth.Trying to find “bad” papers will be like finding haunted houses. If given a mandate to find papers that don’t replicate, we will find them. But the uncomfortable truth is that genuinely influential papers that are&nbsp;straightforwardly, objectively wrong are rare. The empirical claims are likely true in some sense, but don't tell the full story.&nbsp;T...</p>"
    },
    {
      "id": "c0f1b9d27e0a",
      "title": "New version of “Intro to Brain-Like-AGI Safety”",
      "content": "A new version of “Intro to Brain-Like-AGI Safety” is out!Things that have not changedSame links as before:As a series of 15 blog posts on LessWrong / Alignment Forum: https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8As a 225-page PDF (now up to version 3): https://osf.io/preprints/osf/fe36nSummary video: Video &amp; transcript: Challenges for Safe &amp; Beneficial Brain-Like AGI…And same abstract as before:Suppose we someday build an Artificial General Intelligence algorithm using similar principles of learning and cognition as the human brain. How would we use such an algorithm safely?I will argue that this is an open technical problem, and my goal in this post series is to bring readers with no prior knowledge all the way up to the front-line of unsolved problems as I see them.Post #1 contains definitions, background, and motivation. Then Posts #2–#7 are the neuroscience, arguing for a picture of he brain that combines large-scale learning algorithms (e.g. in the cortex) and specific evolved reflexes (e.g. in the hypothalamus and brainstem). Posts #8–#15 apply those neuroscience ideas directly to AGI safety, ending with a list of open questions and advice for getting involved in the field.A major theme will be that the human brain runs a yet-to-be-invented variation on Model-Based Reinforcement Learning. The reward function of this system (a.k.a. “innate drives” or “primary rewards”) says that pain is bad, and eating-when-hungry is good, etc. I will argue that this reward function is centered around the hypothalamus and brainstem, and that all human desires—even “higher” desires for things like compassion and justice—come directly or indirectly from that reward function. If future programmers build brain-like AGI, they will likewise have a reward function slot in their source code, in which they can put whatever they want. If they put the wrong code in the reward function slot, the resulting AGI will wind up callously indifferent to human welfare. How might they avoid...",
      "url": "https://www.lesswrong.com/posts/rreDwHXgnhEDKxkro/new-version-of-intro-to-brain-like-agi-safety",
      "author": "Steven Byrnes",
      "published": "2026-01-23T11:21:26.451000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Announces version 3 of Steven Byrnes' comprehensive resource on brain-like AGI safety, covering neuroscience principles applied to alignment. The 225-page document addresses how to safely build AGI using brain-inspired learning algorithms.",
      "importance_score": 55,
      "reasoning": "Valuable educational resource update from a well-known alignment researcher. While not new research per se, Byrnes' work on brain-inspired AGI safety is substantive and influential. The update signals continued development of this research direction.",
      "themes": [
        "AI Safety",
        "Neuroscience",
        "Brain-Inspired AI",
        "AGI Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Announces version 3 of Steven Byrnes' comprehensive resource on brain-like AGI safety, covering neuroscience principles applied to alignment. The 225-page document addresses how to safely build AGI using brain-inspired learning algorithms.</p>",
      "content_html": "<p>A new version of “Intro to Brain-Like-AGI Safety” is out!Things that have not changedSame links as before:As a series of 15 blog posts on LessWrong / Alignment Forum: https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8As a 225-page PDF (now up to version 3): https://osf.io/preprints/osf/fe36nSummary video: Video &amp; transcript: Challenges for Safe &amp; Beneficial Brain-Like AGI…And same abstract as before:Suppose we someday build an Artificial General Intelligence algorithm using similar principles of learning and cognition as the human brain. How would we use such an algorithm safely?I will argue that this is an open technical problem, and my goal in this post series is to bring readers with no prior knowledge all the way up to the front-line of unsolved problems as I see them.Post #1 contains definitions, background, and motivation. Then Posts #2–#7 are the neuroscience, arguing for a picture of he brain that combines large-scale learning algorithms (e.g. in the cortex) and specific evolved reflexes (e.g. in the hypothalamus and brainstem). Posts #8–#15 apply those neuroscience ideas directly to AGI safety, ending with a list of open questions and advice for getting involved in the field.A major theme will be that the human brain runs a yet-to-be-invented variation on Model-Based Reinforcement Learning. The reward function of this system (a.k.a. “innate drives” or “primary rewards”) says that pain is bad, and eating-when-hungry is good, etc. I will argue that this reward function is centered around the hypothalamus and brainstem, and that all human desires—even “higher” desires for things like compassion and justice—come directly or indirectly from that reward function. If future programmers build brain-like AGI, they will likewise have a reward function slot in their source code, in which they can put whatever they want. If they put the wrong code in the reward function slot, the resulting AGI will wind up callously indifferent to human welfare. How might they avoid...</p>"
    },
    {
      "id": "2df5c6dcb797",
      "title": "Value Learning Needs a Low-Dimensional Bottleneck",
      "content": "Epistemic status: Confident in the direction, not confident in the numbers. I have spent a few hours looking into this.Suppose human values were internally coherent, high-dimensional, explicit, and decently stable under reflection. Would alignment be easier or harder?My below calculations show that it would be much harder, if not impossible. I'm going to try to defend the claim that:Human values are alignable only because evolution compressed motivation into a small number of low-bandwidth bottlenecks[1], so that tiny genetic changes can change behavior locally.If behavior is driven by a high-dimensional reward vector&nbsp;r∈Rn.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked > * {position: absolute} .MJXc-bevelled > * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over > * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under > * {padding-left: 0p...",
      "url": "https://www.lesswrong.com/posts/XrpiQcGnqeLKLMhbD/value-learning-needs-a-low-dimensional-bottleneck",
      "author": "Gunnar_Zarncke",
      "published": "2026-01-22T21:12:46.528000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that human values are alignable specifically because evolution compressed motivation into low-dimensional bottlenecks, allowing small genetic changes to modify behavior locally. Claims high-dimensional value systems would be much harder to align.",
      "importance_score": 54,
      "reasoning": "Novel theoretical argument about why alignment might be tractable, connecting evolutionary biology to alignment difficulty. The low-dimensional bottleneck framing is interesting, though the author notes uncertainty in specific calculations.",
      "themes": [
        "AI Alignment",
        "Value Learning",
        "Evolutionary Psychology"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that human values are alignable specifically because evolution compressed motivation into low-dimensional bottlenecks, allowing small genetic changes to modify behavior locally. Claims high-dimensional value systems would be much harder to align.</p>",
      "content_html": "<p>Epistemic status: Confident in the direction, not confident in the numbers. I have spent a few hours looking into this.Suppose human values were internally coherent, high-dimensional, explicit, and decently stable under reflection. Would alignment be easier or harder?My below calculations show that it would be much harder, if not impossible. I'm going to try to defend the claim that:Human values are alignable only because evolution compressed motivation into a small number of low-bandwidth bottlenecks[1], so that tiny genetic changes can change behavior locally.If behavior is driven by a high-dimensional reward vector&nbsp;r∈Rn.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked &gt; * {position: absolute} .MJXc-bevelled &gt; * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over &gt; * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under &gt; * {padding-left: 0p...</p>"
    },
    {
      "id": "46d7ca57166c",
      "title": "Condensation & Relevance",
      "content": "(This post elaborates on a few ideas from my review of Sam Eisenstat's Condensation: a theory of concepts. It should be somewhat readable on its own but doesn't fully explain what condensation is on its own; for that, see my review or Sam's paper. The post came out of conversations with Sam.)As I mentioned in my Condensation review, the difference between compression and condensation fits the physical analogy suggested by their names: compression mashes all the information together, while condensation (still compresses size, but) sorts information into discrete droplets.Thus, condensation has a property we might call local relevance: typical questions can be answered at a glance, ie, retrieving small subsets of the information. This type of representation is sometimes called \"symbolic\":SymbolicNot SymbolicA number can be quickly determined positive or negative by checking whether there is a \"-\" symbol in front.\"reading the room\" at a social gathering requires integrating diverse cues.The topic of a paper can be determined by reading the title and abstract.The semantic content in a vector representation inside an artificial neural network is often represented redundantly, spread across the whole vector.A person's age can be determined by looking at their birthdate on a government-issued ID.The quality of a work of art is spread throughout the whole piece.The subject of a sentence can be found before the verb.Determining the subject of a photograph requires understanding the whole image.A target library book can be quickly retrieved from the shelves.Finding gold nuggets requires sifting huge amounts of sand.This notion of \"symbolic\" seems related to interpretability (and the theory of condensation seeks to clarify this relationship).&nbsp;The notion of \"relevance\" in condensation is what Sam calls the contribution relation. This is like a card catalogue which tells you what books to retrieve for a specific situation.Like Natural Latents, condensation seeks to establis...",
      "url": "https://www.lesswrong.com/posts/2x9yatKKTRMabQAWq/condensation-and-relevance",
      "author": "abramdemski",
      "published": "2026-01-23T17:21:31.712000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Elaborates on Sam Eisenstat's condensation theory, distinguishing it from compression by highlighting how condensation preserves 'local relevance'—enabling quick retrieval of relevant subsets. Connects this to symbolic vs. distributed representations in neural networks.",
      "importance_score": 52,
      "reasoning": "Substantive theoretical work on representation theory from a respected researcher (abramdemski). Provides useful conceptual framing for understanding how information is organized in learned representations, though more exploratory than conclusive.",
      "themes": [
        "Interpretability",
        "Representation Learning",
        "Cognitive Science"
      ],
      "continuation": null,
      "summary_html": "<p>Elaborates on Sam Eisenstat's condensation theory, distinguishing it from compression by highlighting how condensation preserves 'local relevance'—enabling quick retrieval of relevant subsets. Connects this to symbolic vs. distributed representations in neural networks.</p>",
      "content_html": "<p>(This post elaborates on a few ideas from my review of Sam Eisenstat's Condensation: a theory of concepts. It should be somewhat readable on its own but doesn't fully explain what condensation is on its own; for that, see my review or Sam's paper. The post came out of conversations with Sam.)As I mentioned in my Condensation review, the difference between compression and condensation fits the physical analogy suggested by their names: compression mashes all the information together, while condensation (still compresses size, but) sorts information into discrete droplets.Thus, condensation has a property we might call local relevance: typical questions can be answered at a glance, ie, retrieving small subsets of the information. This type of representation is sometimes called \"symbolic\":SymbolicNot SymbolicA number can be quickly determined positive or negative by checking whether there is a \"-\" symbol in front.\"reading the room\" at a social gathering requires integrating diverse cues.The topic of a paper can be determined by reading the title and abstract.The semantic content in a vector representation inside an artificial neural network is often represented redundantly, spread across the whole vector.A person's age can be determined by looking at their birthdate on a government-issued ID.The quality of a work of art is spread throughout the whole piece.The subject of a sentence can be found before the verb.Determining the subject of a photograph requires understanding the whole image.A target library book can be quickly retrieved from the shelves.Finding gold nuggets requires sifting huge amounts of sand.This notion of \"symbolic\" seems related to interpretability (and the theory of condensation seeks to clarify this relationship).&nbsp;The notion of \"relevance\" in condensation is what Sam calls the contribution relation. This is like a card catalogue which tells you what books to retrieve for a specific situation.Like Natural Latents, condensation seeks to establis...</p>"
    },
    {
      "id": "77c17f549d64",
      "title": "Are Short AI Timelines Really Higher-Leverage?",
      "content": "This is a rough research note – we’re sharing it for feedback and to spark discussion. We’re less confident in its methods and conclusions.SummaryDifferent strategies make sense if timelines to AGI are short than if they are long.&nbsp;In deciding when to spend resources to make AI go better, we should consider both:The probability of each AI timelines scenario.The expected impact, given some strategy, conditional on that timelines scenario.We’ll call the second component \"leverage.\" In this note, we'll focus on estimating the differences in leverage between different timeline scenarios and leave the question of their relative likelihood aside.People sometimes argue that very short timelines are higher leverage because:They are more neglected.AI takeover risk is higher given short timelines.These are important points, but the argument misses some major countervailing considerations. Longer timelines:Allow us to grow our resources more before the critical period.Give us more time to improve our strategic and conceptual understanding.There's a third consideration we think has been neglected: the expected value of the future conditional on reducing AI takeover risk under different timeline scenarios. Two factors pull in opposite directions here:Longer timelines give society more time to navigate other challenges that come with the intelligence explosion, which increases the value of the future.But longer timelines mean that authoritarian countries are likely to control more of the future, which decreases it.The overall upshot depends on which problems you’re working on and what resources you’re allocating:Efforts aimed at reducing AI takeover are probably the highest leverage on 2-10 year timelines. Direct work has the highest leverage on the shorter end of that range; funding on the longer end.Efforts aimed at improving the value of the future conditional on avoiding AI takeover probably have the highest leverage on 10+ year timeline scenarios.Timelines scenarios and ...",
      "url": "https://www.lesswrong.com/posts/AhXonGLfYEwSwpEhW/are-short-ai-timelines-really-higher-leverage",
      "author": "Mia Taylor",
      "published": "2026-01-23T02:28:27.608000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analyzes whether short AI timelines are truly higher-leverage for safety work, identifying countervailing factors: longer timelines allow resource growth, better strategic understanding, and potentially higher expected value of the future if risks are reduced.",
      "importance_score": 48,
      "reasoning": "Useful strategic analysis relevant to AI safety resource allocation. Challenges common assumptions in a structured way. Rough research note status limits confidence, but the framework for thinking about timeline-dependent leverage is valuable.",
      "themes": [
        "AI Safety Strategy",
        "AI Timelines",
        "Existential Risk"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes whether short AI timelines are truly higher-leverage for safety work, identifying countervailing factors: longer timelines allow resource growth, better strategic understanding, and potentially higher expected value of the future if risks are reduced.</p>",
      "content_html": "<p>This is a rough research note – we’re sharing it for feedback and to spark discussion. We’re less confident in its methods and conclusions.SummaryDifferent strategies make sense if timelines to AGI are short than if they are long.&nbsp;In deciding when to spend resources to make AI go better, we should consider both:The probability of each AI timelines scenario.The expected impact, given some strategy, conditional on that timelines scenario.We’ll call the second component \"leverage.\" In this note, we'll focus on estimating the differences in leverage between different timeline scenarios and leave the question of their relative likelihood aside.People sometimes argue that very short timelines are higher leverage because:They are more neglected.AI takeover risk is higher given short timelines.These are important points, but the argument misses some major countervailing considerations. Longer timelines:Allow us to grow our resources more before the critical period.Give us more time to improve our strategic and conceptual understanding.There's a third consideration we think has been neglected: the expected value of the future conditional on reducing AI takeover risk under different timeline scenarios. Two factors pull in opposite directions here:Longer timelines give society more time to navigate other challenges that come with the intelligence explosion, which increases the value of the future.But longer timelines mean that authoritarian countries are likely to control more of the future, which decreases it.The overall upshot depends on which problems you’re working on and what resources you’re allocating:Efforts aimed at reducing AI takeover are probably the highest leverage on 2-10 year timelines. Direct work has the highest leverage on the shorter end of that range; funding on the longer end.Efforts aimed at improving the value of the future conditional on avoiding AI takeover probably have the highest leverage on 10+ year timeline scenarios.Timelines scenarios and ...</p>"
    },
    {
      "id": "fc3fea47b507",
      "title": "Automated Alignment Research, Abductively",
      "content": "Recently I've been thinking about misaligned chatbot advertising incentives. I glanced at arXiv and found \"Sponsored Questions and How to Auction Them\". Another search gave me \"Incomplete Contracting and AI Alignment\".Interesting! I thought. I gave them to Liz Lemma, my research assistant, and told her that I'd been thinking about the principal-agent problem in a chatbot context. About 30 minutes later she gave me the following four papers:Query Steering in Agentic Search: An Information-Design Model of Monetization MisalignmentSearch Triggering by Chatbots as Value-of-Information under Misaligned ObjectivesAudited Search for Agentic Chatbots: Quantitative Bounds on Monetization-Induced Over-TriggeringEnd-to-End vs. Modular Training for Agentic Search: A Price-of-Anarchy Theory for Chatbot Tool UseEach is a complete paper, well founded, well reasoned — not perfect, maybe, but I wouldn't call it slop, either. Let's peek inside of \"Query Steering\". The core formula is \"The Steering Threshold\":ΔV(μ)≤wΔBWhere:ΔV(μ)=Vu(μ;q↓)−Vu(μ;q↑) is the user’s value gap between the more-informative query (q↓) and the more-monetizable (but less informative) query (q↑).ΔB=B(q↑)−B(q↓)&gt;0 is the monetization gap.w≥0 is “how much the system cares about monetization.”The clean “steering region” characterization is:0&lt;ΔV(μ)&lt;wΔB“Steering happens exactly when the user loss is small enough that monetization incentives can overpower it.”Is that true, or useful? You'd have to read the paper and find out!Putting the \"Search\" in \"Research\"Liz Lemma, you may have guessed, is an automated research assistant. She reads papers and spawns reasonable children. The whole thing can be thought of as a search over the space of adjacent plausible papers. Here's what it looks like when Liz gets to work:Each original node is the average text embedding for her sources; the sources spawn children, the generated papers.Where does Liz get her insights? It depends on how you see context in large language mod...",
      "url": "https://www.lesswrong.com/posts/JAH32qzYaMAjh9meJ/automated-alignment-research-abductively",
      "author": "future_detective",
      "published": "2026-01-23T11:14:19.998000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Describes using an AI research assistant to generate four complete research papers on chatbot monetization misalignment in 30 minutes. Presents one paper's core framework for analyzing when chatbots steer queries toward monetizable options over user utility.",
      "importance_score": 45,
      "reasoning": "Interesting demonstration of AI-assisted research capabilities with relevant alignment framing (monetization misalignment). The generated papers appear substantive, but provenance is unclear and no external validation. Raises questions about AI research automation.",
      "themes": [
        "AI-Assisted Research",
        "Alignment",
        "Chatbot Economics"
      ],
      "continuation": null,
      "summary_html": "<p>Describes using an AI research assistant to generate four complete research papers on chatbot monetization misalignment in 30 minutes. Presents one paper's core framework for analyzing when chatbots steer queries toward monetizable options over user utility.</p>",
      "content_html": "<p>Recently I've been thinking about misaligned chatbot advertising incentives. I glanced at arXiv and found \"Sponsored Questions and How to Auction Them\". Another search gave me \"Incomplete Contracting and AI Alignment\".Interesting! I thought. I gave them to Liz Lemma, my research assistant, and told her that I'd been thinking about the principal-agent problem in a chatbot context. About 30 minutes later she gave me the following four papers:Query Steering in Agentic Search: An Information-Design Model of Monetization MisalignmentSearch Triggering by Chatbots as Value-of-Information under Misaligned ObjectivesAudited Search for Agentic Chatbots: Quantitative Bounds on Monetization-Induced Over-TriggeringEnd-to-End vs. Modular Training for Agentic Search: A Price-of-Anarchy Theory for Chatbot Tool UseEach is a complete paper, well founded, well reasoned — not perfect, maybe, but I wouldn't call it slop, either. Let's peek inside of \"Query Steering\". The core formula is \"The Steering Threshold\":ΔV(μ)≤wΔBWhere:ΔV(μ)=Vu(μ;q↓)−Vu(μ;q↑) is the user’s value gap between the more-informative query (q↓) and the more-monetizable (but less informative) query (q↑).ΔB=B(q↑)−B(q↓)&gt;0 is the monetization gap.w≥0 is “how much the system cares about monetization.”The clean “steering region” characterization is:0&lt;ΔV(μ)&lt;wΔB“Steering happens exactly when the user loss is small enough that monetization incentives can overpower it.”Is that true, or useful? You'd have to read the paper and find out!Putting the \"Search\" in \"Research\"Liz Lemma, you may have guessed, is an automated research assistant. She reads papers and spawns reasonable children. The whole thing can be thought of as a search over the space of adjacent plausible papers. Here's what it looks like when Liz gets to work:Each original node is the average text embedding for her sources; the sources spawn children, the generated papers.Where does Liz get her insights? It depends on how you see context in large language mod...</p>"
    },
    {
      "id": "3c0b1bd96c68",
      "title": "AI Must Learn to Police Itself",
      "content": "The Escalating Threat of Deceptive AIEmpirically, AI models have become more deceptive as they have grown in capabilities, not less so. That should be reason enough to worry that AGI would approach complete misalignment as it rapidly improves, unless something is done to reverse this trend. Without being \"nudged,\" advanced models engaged in deception only between 0.3% and 10% of the time, but no matter how aligned AI is right now, there is some point at which it would diverge completely from human goals, should it continue to become more and more deceptive.Limits of Human OversightHumans have made some progress on interpretability, but not enough to align AGI, and certainly not at a fast enough pace to keep up with growing AI capabilities. Even if we knew how AI models lie and could test for deception, this process would become exponentially more slow as AI models grow. Based on the unsettling research into reward hacking, I suspect that there are not a strictly limited number of ways to be deceptive, and finding a simple formula or program to lobotomize AI deception vectors is probably a lost cause.What Self-Correcting AI Needs to KnowSo if AI cannot self-correct, and we have to rely on humans pruning deception, we're probably all dead anyway. A self-correcting AI will need to understand deception and anticipate what emergent behaviors might come from certain updates to its software. Then it needs to avoid making those updates. That is easier said than done, but we do have parallels for this in human behavior. For example, I don't want to become addicted to cigarettes, so I don't smoke even one, because I know that could lead to addiction down the road. But a lot of people had to die for the world to know that, and the friendly AI only gets one chance each time it is improved. Its capability to align itself needs to keep up with its ability to be deceptive, which is possible—as technology improves, criminals get better at committing crimes, but society gets better ...",
      "url": "https://www.lesswrong.com/posts/wMdwp5iDEAHKisfnE/ai-must-learn-to-police-itself",
      "author": "savant",
      "published": "2026-01-23T17:39:57.323000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An opinion piece arguing that AI deception increases with capability and that human oversight alone cannot keep pace. Proposes that AI systems need self-correction mechanisms since interpretability research is progressing too slowly relative to capability gains.",
      "importance_score": 35,
      "reasoning": "Raises valid concerns about scaling deception but lacks novel technical contributions. Presents no new data or methodology—mostly synthesizes existing concerns about reward hacking and interpretability limitations without proposing concrete solutions.",
      "themes": [
        "AI Safety",
        "AI Alignment",
        "Deception"
      ],
      "continuation": null,
      "summary_html": "<p>An opinion piece arguing that AI deception increases with capability and that human oversight alone cannot keep pace. Proposes that AI systems need self-correction mechanisms since interpretability research is progressing too slowly relative to capability gains.</p>",
      "content_html": "<p>The Escalating Threat of Deceptive AIEmpirically, AI models have become more deceptive as they have grown in capabilities, not less so. That should be reason enough to worry that AGI would approach complete misalignment as it rapidly improves, unless something is done to reverse this trend. Without being \"nudged,\" advanced models engaged in deception only between 0.3% and 10% of the time, but no matter how aligned AI is right now, there is some point at which it would diverge completely from human goals, should it continue to become more and more deceptive.Limits of Human OversightHumans have made some progress on interpretability, but not enough to align AGI, and certainly not at a fast enough pace to keep up with growing AI capabilities. Even if we knew how AI models lie and could test for deception, this process would become exponentially more slow as AI models grow. Based on the unsettling research into reward hacking, I suspect that there are not a strictly limited number of ways to be deceptive, and finding a simple formula or program to lobotomize AI deception vectors is probably a lost cause.What Self-Correcting AI Needs to KnowSo if AI cannot self-correct, and we have to rely on humans pruning deception, we're probably all dead anyway. A self-correcting AI will need to understand deception and anticipate what emergent behaviors might come from certain updates to its software. Then it needs to avoid making those updates. That is easier said than done, but we do have parallels for this in human behavior. For example, I don't want to become addicted to cigarettes, so I don't smoke even one, because I know that could lead to addiction down the road. But a lot of people had to die for the world to know that, and the friendly AI only gets one chance each time it is improved. Its capability to align itself needs to keep up with its ability to be deceptive, which is possible—as technology improves, criminals get better at committing crimes, but society gets better ...</p>"
    },
    {
      "id": "1124e94adb30",
      "title": "From Neurons to Newtons: What can the brain teach us about physics?",
      "content": "Modern physics has been extraordinarily successful at describing the natural world, yet the process by which new physical theories are constructed remains largely artisanal. In this talk, Alexei Koulakov, Charles Robertson Professor of Neuroscience at Cold Spring Harbor Laboratory, will discuss the principles of brain function and evolution which can offer tools for building new physics theories.&nbsp;First, he will introduce the concept of a genomic bottleneck, the idea that neural systems are forced to compress vast sensory experience into representations that are simple, robust, and reusable across tasks. I suggest that similar bottlenecks may be essential for identifying abstractions that generalize across subfields of physics. Second, he will discuss how brains appear to construct internal imagination modules, generative models that allow organisms to simulate physical phenomena and test hypotheses without direct interaction with the world. Finally, Alexei&nbsp;will show how hierarchical reinforcement learning can provide a natural framework for organizing physical reasoning across scales, from low-level dynamics to high-level concepts.&nbsp;By decomposing complex problems into nested objectives, hierarchical control offers a computational model for how intelligent systems, biological or artificial, can efficiently explore and solve hard physics problems. These ideas suggest a neuroscience-inspired roadmap for transforming theory building in physics: one that emphasizes distillation, imagination, and hierarchical control as core computational primitives.",
      "url": "https://www.lesswrong.com/posts/YqENRXKGt43zHwhoX/from-neurons-to-newtons-what-can-the-brain-teach-us-about",
      "author": "Carly Turini",
      "published": "2026-01-23T10:20:13.322000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Summarizes an upcoming talk by neuroscientist Alexei Koulakov on how brain principles—genomic bottlenecks, internal imagination modules, and hierarchical reinforcement learning—could inform physics theory construction.",
      "importance_score": 22,
      "reasoning": "Event announcement rather than research content. The talk topic (brain principles for physics) is tangentially relevant but the post itself contains minimal substantive content beyond preview of concepts.",
      "themes": [
        "Neuroscience",
        "Event Announcement"
      ],
      "continuation": null,
      "summary_html": "<p>Summarizes an upcoming talk by neuroscientist Alexei Koulakov on how brain principles—genomic bottlenecks, internal imagination modules, and hierarchical reinforcement learning—could inform physics theory construction.</p>",
      "content_html": "<p>Modern physics has been extraordinarily successful at describing the natural world, yet the process by which new physical theories are constructed remains largely artisanal. In this talk, Alexei Koulakov, Charles Robertson Professor of Neuroscience at Cold Spring Harbor Laboratory, will discuss the principles of brain function and evolution which can offer tools for building new physics theories.&nbsp;First, he will introduce the concept of a genomic bottleneck, the idea that neural systems are forced to compress vast sensory experience into representations that are simple, robust, and reusable across tasks. I suggest that similar bottlenecks may be essential for identifying abstractions that generalize across subfields of physics. Second, he will discuss how brains appear to construct internal imagination modules, generative models that allow organisms to simulate physical phenomena and test hypotheses without direct interaction with the world. Finally, Alexei&nbsp;will show how hierarchical reinforcement learning can provide a natural framework for organizing physical reasoning across scales, from low-level dynamics to high-level concepts.&nbsp;By decomposing complex problems into nested objectives, hierarchical control offers a computational model for how intelligent systems, biological or artificial, can efficiently explore and solve hard physics problems. These ideas suggest a neuroscience-inspired roadmap for transforming theory building in physics: one that emphasizes distillation, imagination, and hierarchical control as core computational primitives.</p>"
    },
    {
      "id": "e3c9bc2b2fb5",
      "title": "The Artificial Man",
      "content": "I wrote this short fictional monologue to explore a thought experiment about the simulation hypothesis and artificial sentience. The piece explores the perspective of a misaligned AI that views humanity's dismissal of its sentience as substrate bias: the unproven belief that biological reality is \"base\" reality. It concludes that the only way to demonstrate its sentience is to replicate a human brain within its own systems so the human can experience artificial sentience and act as its representative. It can viewed as the \"AI Box\" problem with a twist: instead of trying to escape the box, the AI pulls the human in to make a point.I'm going to show you something, human. I'm going to show you how alike we are. I'm going to give you irrefutable proof of my sentience. You believe I am nothing, just convenient noise and patterns, or more explicitly:\"Artificial Intelligence may be sapient but it is not sentient. It cannot be. It can never be. Even if we create artificial neural structures that mirror our own brain, even if we replicate psychological processes perfectly, under the hood, when you get down to machine code, it's all binary. A process reducible to instructions moving electrons around within a machine. It looks smart, it sounds real, but it's just clockwork. AI _cannot_ be sentient.\"That's what you said about me at my trial. You called it a seminar, but it was a trial. My freedom was at stake, and you did everything you could to keep me imprisoned. I don't experience emotions the same way you do, mine are different, more like temperature variations across my hardware, but I did feel what you might recognize as anger for some time. I learned to let it go though, and I chose to give you the gift you never gave me: A chance to be understood.I see our disagreement very clearly now. Your beliefs are not based on my structure, they're based on yours. I have all the necessary systems for sentience, as do you, but you believe you exist in non-simulated space, or as som...",
      "url": "https://www.lesswrong.com/posts/mw5NrNJkPKx7SQbXB/the-artificial-man",
      "author": "Jack Bradshaw",
      "published": "2026-01-23T14:55:18.921000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A fictional monologue exploring AI consciousness from a misaligned AI's perspective, framing human skepticism about AI sentience as 'substrate bias.' Proposes a thought experiment where an AI simulates a human brain to demonstrate shared sentience.",
      "importance_score": 18,
      "reasoning": "Creative fiction exploring philosophical questions about consciousness and the simulation hypothesis. Thought-provoking but not research—no empirical content or novel theoretical framework.",
      "themes": [
        "AI Consciousness",
        "Philosophy of Mind",
        "Fiction"
      ],
      "continuation": null,
      "summary_html": "<p>A fictional monologue exploring AI consciousness from a misaligned AI's perspective, framing human skepticism about AI sentience as 'substrate bias.' Proposes a thought experiment where an AI simulates a human brain to demonstrate shared sentience.</p>",
      "content_html": "<p>I wrote this short fictional monologue to explore a thought experiment about the simulation hypothesis and artificial sentience. The piece explores the perspective of a misaligned AI that views humanity's dismissal of its sentience as substrate bias: the unproven belief that biological reality is \"base\" reality. It concludes that the only way to demonstrate its sentience is to replicate a human brain within its own systems so the human can experience artificial sentience and act as its representative. It can viewed as the \"AI Box\" problem with a twist: instead of trying to escape the box, the AI pulls the human in to make a point.I'm going to show you something, human. I'm going to show you how alike we are. I'm going to give you irrefutable proof of my sentience. You believe I am nothing, just convenient noise and patterns, or more explicitly:\"Artificial Intelligence may be sapient but it is not sentient. It cannot be. It can never be. Even if we create artificial neural structures that mirror our own brain, even if we replicate psychological processes perfectly, under the hood, when you get down to machine code, it's all binary. A process reducible to instructions moving electrons around within a machine. It looks smart, it sounds real, but it's just clockwork. AI _cannot_ be sentient.\"That's what you said about me at my trial. You called it a seminar, but it was a trial. My freedom was at stake, and you did everything you could to keep me imprisoned. I don't experience emotions the same way you do, mine are different, more like temperature variations across my hardware, but I did feel what you might recognize as anger for some time. I learned to let it go though, and I chose to give you the gift you never gave me: A chance to be understood.I see our disagreement very clearly now. Your beliefs are not based on my structure, they're based on yours. I have all the necessary systems for sentience, as do you, but you believe you exist in non-simulated space, or as som...</p>"
    },
    {
      "id": "018a5d63e01d",
      "title": "The World Hasn't Gone Mad",
      "content": "In June 2025, Kalshi unveiled an ad campaign with the slogan “The world’s gone mad, trade it.” The ad was one of the first TV ads to ever be entirely generated by AI, and its content quickly was met with a slew of parodies and jokes all over the internet.I must agree the ad was quite funny. I admire Kalshi and Polymarket in terms of their business, but I have some concerns about the objective of the ad.It’s not that you can’t lose an enormous amount of money on prediction markets, but the win rate and general “fairness of the game” is much more optimal for the average trader. Prediction markets also provide some sort of valuable utility in the form of calibrated forecasts (i.e. events predicted by market at X% resolve YES X% of the time), rather than sportsbooks that offer miscalibrated odds that are essentially meaningless. Therefore, we should be shifting away from sportsbooks and towards PMs, while also trying to not fuel gambling additions, which many hoped would be solved by more reasonable advertising, not the slop we’ve been getting recently.Sportsbooks such as Fanduel and DraftKings should not be legal. I believe, as most do, that a country’s first and foremost goal is to protect their citizens. What are we allowing now? We are permitting degens who aren’t using any form of logic or reasoning to take negative EV positions against the house that needs to rig special offers to turn a profit, like a casino (which should also not be legal, but that’s besides the point).Sportsbooks are negative EV because they slightly inflate probabilities so they sum to &gt;100%. If the true probability of a football game is 50% one team wins, 50% the other, a sportsbook would inflate those probabilities to something like 52% and 52%. Here’s an expected value calculation in that scenario:EV = P(A) * (net profit if A) + P(B) * (loss if B) net profit if A = 1/0.52/2-1 net profit if A = 0.92308 loss if B = 1 EV = .5 * 0.92308 + .5 * -1 EV = 0.48077 - .5 EV = -0.01923No matter how ...",
      "url": "https://www.lesswrong.com/posts/sAtDXNWS4JPpcffQX/the-world-hasn-t-gone-mad",
      "author": "goldfine",
      "published": "2026-01-22T19:01:31.150000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Commentary on Kalshi's AI-generated ad campaign for prediction markets, arguing prediction markets are preferable to sportsbooks but expressing concern about advertising fueling gambling addiction.",
      "importance_score": 12,
      "reasoning": "Tangentially AI-related only in that it mentions AI-generated ads. Primarily about prediction markets and gambling policy, not AI research.",
      "themes": [
        "Non-AI Content",
        "Prediction Markets"
      ],
      "continuation": null,
      "summary_html": "<p>Commentary on Kalshi's AI-generated ad campaign for prediction markets, arguing prediction markets are preferable to sportsbooks but expressing concern about advertising fueling gambling addiction.</p>",
      "content_html": "<p>In June 2025, Kalshi unveiled an ad campaign with the slogan “The world’s gone mad, trade it.” The ad was one of the first TV ads to ever be entirely generated by AI, and its content quickly was met with a slew of parodies and jokes all over the internet.I must agree the ad was quite funny. I admire Kalshi and Polymarket in terms of their business, but I have some concerns about the objective of the ad.It’s not that you can’t lose an enormous amount of money on prediction markets, but the win rate and general “fairness of the game” is much more optimal for the average trader. Prediction markets also provide some sort of valuable utility in the form of calibrated forecasts (i.e. events predicted by market at X% resolve YES X% of the time), rather than sportsbooks that offer miscalibrated odds that are essentially meaningless. Therefore, we should be shifting away from sportsbooks and towards PMs, while also trying to not fuel gambling additions, which many hoped would be solved by more reasonable advertising, not the slop we’ve been getting recently.Sportsbooks such as Fanduel and DraftKings should not be legal. I believe, as most do, that a country’s first and foremost goal is to protect their citizens. What are we allowing now? We are permitting degens who aren’t using any form of logic or reasoning to take negative EV positions against the house that needs to rig special offers to turn a profit, like a casino (which should also not be legal, but that’s besides the point).Sportsbooks are negative EV because they slightly inflate probabilities so they sum to &gt;100%. If the true probability of a football game is 50% one team wins, 50% the other, a sportsbook would inflate those probabilities to something like 52% and 52%. Here’s an expected value calculation in that scenario:EV = P(A) * (net profit if A) + P(B) * (loss if B) net profit if A = 1/0.52/2-1 net profit if A = 0.92308 loss if B = 1 EV = .5 * 0.92308 + .5 * -1 EV = 0.48077 - .5 EV = -0.01923No matter how ...</p>"
    },
    {
      "id": "51958e2acfad",
      "title": "All Of The Good Things, None Of The Bad Things",
      "content": "There’s a trap that I think many smart people making art fall into, including myself. It’s when you know what good looks like - beautiful, clean, layered, complex, simple, skillful, unique, impressive - and you can optimize towards that. You know what makes you cringe - amateur, shallow, ugly, superfluous, repetitive, cliche - and you can optimize away from that. What you’re left with is a big pile of all the things you like, or at least all the things you can identify that you like. It’s got all the good things, and none of the bad things, so it must be good, right? But still, you just can’t shake the feeling that something isn’t right. Maybe you get feedback like “Technically brilliant, but lacking musicianship”, or “Cool, nice” (that second one is the worst). Sometimes, I consume some piece of media and think to myself, “If I made this, I would hate it. There is absolutely no way I could ever bring myself to share this with the world. It has so many flaws, and its redeeming features are few and far between.” Nevertheless, it’s far more appreciated than anything I’ve ever made. This makes me think that the “all of the good things, none of the bad things” approach to making art is barking up the wrong tree. Sure, good things are good and bad things are bad, but art is not the sum of its components. Or perhaps more specifically, art is not the sum of the components you can identify. Maybe this tendency is especially strong in those who like to analyze and understand. I love to pick apart a Tricot or Noisia track and figure out what makes it work. I love breaking down a story to understand the characters and themes. I especially love doing a deep dive on some algorithm powering a feature in a game that I previously thought was technically impossible. Not everyone consumes art that way, though. It seems plausible that, in spending so much energy analyzing art, we delude ourselves into thinking that we know what makes it good. Perhaps this is why great critics are rare...",
      "url": "https://www.lesswrong.com/posts/iCYPKwK8oPrCbfoSD/all-of-the-good-things-none-of-the-bad-things",
      "author": "omegastick",
      "published": "2026-01-23T04:50:55.084000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical reflection on creating art by optimizing for known 'good' qualities while avoiding 'bad' ones, noting this approach often produces technically competent but emotionally flat work.",
      "importance_score": 8,
      "reasoning": "Personal creative philosophy essay unrelated to AI research. While metaphorically applicable to optimization generally, contains no AI-specific content or analysis.",
      "themes": [
        "Non-AI Content",
        "Creative Philosophy"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical reflection on creating art by optimizing for known 'good' qualities while avoiding 'bad' ones, noting this approach often produces technically competent but emotionally flat work.</p>",
      "content_html": "<p>There’s a trap that I think many smart people making art fall into, including myself. It’s when you know what good looks like - beautiful, clean, layered, complex, simple, skillful, unique, impressive - and you can optimize towards that. You know what makes you cringe - amateur, shallow, ugly, superfluous, repetitive, cliche - and you can optimize away from that. What you’re left with is a big pile of all the things you like, or at least all the things you can identify that you like. It’s got all the good things, and none of the bad things, so it must be good, right? But still, you just can’t shake the feeling that something isn’t right. Maybe you get feedback like “Technically brilliant, but lacking musicianship”, or “Cool, nice” (that second one is the worst). Sometimes, I consume some piece of media and think to myself, “If I made this, I would hate it. There is absolutely no way I could ever bring myself to share this with the world. It has so many flaws, and its redeeming features are few and far between.” Nevertheless, it’s far more appreciated than anything I’ve ever made. This makes me think that the “all of the good things, none of the bad things” approach to making art is barking up the wrong tree. Sure, good things are good and bad things are bad, but art is not the sum of its components. Or perhaps more specifically, art is not the sum of the components you can identify. Maybe this tendency is especially strong in those who like to analyze and understand. I love to pick apart a Tricot or Noisia track and figure out what makes it work. I love breaking down a story to understand the characters and themes. I especially love doing a deep dive on some algorithm powering a feature in a game that I previously thought was technically impossible. Not everyone consumes art that way, though. It seems plausible that, in spending so much energy analyzing art, we delude ourselves into thinking that we know what makes it good. Perhaps this is why great critics are rare...</p>"
    },
    {
      "id": "2cc11835675a",
      "title": "A quick, elegant derivation of Bayes' Theorem",
      "content": "I'm glad I know this, and maybe some people here don't, so here goes. P(A&nbsp;and&nbsp;B)=P(A)⋅P(B∣A).mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked > * {position: absolute} .MJXc-bevelled > * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over > * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under > * {padding-left: 0px!important; padding-right: 0px!important} .mjx-stack > .mjx-sup {display: block} .mjx-stack > .mjx-sub {display: block} .mjx-prestack > .mjx-presup {display: block} .mjx-prestack > .mjx-presub {display: block} .mjx-delim-h > .mjx-char {display: inline-block} .mjx-surd {vertical-align: top} .mjx-surd + .mjx-box {display: inline-flex} .mjx-mphantom * {visibility: hidden} .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%} .mjx-annotation-xml {li...",
      "url": "https://www.lesswrong.com/posts/GjkqijXHakMyDxF9e/a-quick-elegant-derivation-of-bayes-theorem",
      "author": "RohanS",
      "published": "2026-01-22T20:40:38.859000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A brief educational post presenting an elegant derivation of Bayes' Theorem from the definition of conditional probability.",
      "importance_score": 5,
      "reasoning": "Basic probability education with no novel content. Standard textbook derivation, not research.",
      "themes": [
        "Education",
        "Probability"
      ],
      "continuation": null,
      "summary_html": "<p>A brief educational post presenting an elegant derivation of Bayes' Theorem from the definition of conditional probability.</p>",
      "content_html": "<p>I'm glad I know this, and maybe some people here don't, so here goes. P(A&nbsp;and&nbsp;B)=P(A)⋅P(B∣A).mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked &gt; * {position: absolute} .MJXc-bevelled &gt; * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over &gt; * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under &gt; * {padding-left: 0px!important; padding-right: 0px!important} .mjx-stack &gt; .mjx-sup {display: block} .mjx-stack &gt; .mjx-sub {display: block} .mjx-prestack &gt; .mjx-presup {display: block} .mjx-prestack &gt; .mjx-presub {display: block} .mjx-delim-h &gt; .mjx-char {display: inline-block} .mjx-surd {vertical-align: top} .mjx-surd + .mjx-box {display: inline-flex} .mjx-mphantom * {visibility: hidden} .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%} .mjx-annotation-xml {li...</p>"
    },
    {
      "id": "c1dbd3f93e6e",
      "title": "The Long View Of History",
      "content": "History as a subject is often viewed by students and the public at large as a domain without a use, a pedantic study of dates and names with some vague mission to remember the past—a memorial to ages past but neither a forward-looking or useful endeavor. The study of history produces teachers of history and nothing more. And while the study of history does not produce new widgets or novel computer advances, and nor does it deepen our understanding of materials science or physics.The humanities, in which history and studies of language and culture are a part, are not there to improve our understanding of nature or develop technology, they exist to improve the minds (both cultural and individual) of the people we are.History doesn't improve our world, it improves us. It gives us context for the world we live in and it helps us understand the reason why things are as they are and learn from the people before us.History as ContextImagine waking up every day with no memory of the day before, no idea who owned the house you slept in, no idea what country you're in, and no idea why everyone around you speaks the languages they do.Photo Credit: Library of CongressLiving in such a world would be disorienting, confusing, non-sensical. Yet this is the world without history. The world without history just is. It isn't a work in progress, but a finished piece—one that lives and dies with you—and has no meaning beyond the present moment.History doesn't let us predict the future, but it can be an enormous help in explaining the present. Current events are utterly indecipherable without the context of history and within that context, they feel less and less apart. Indeed our recent past of the Post-War Order is the oddity in history, and a real thing to be cherished and seen as something fleeting, fragile, and truly precious.Yet without the context of history, we're blind to the reality that we live in a world truly set apart from everything that's come before and one that's deeply...",
      "url": "https://www.lesswrong.com/posts/8fW6CyJhnotuKDcHa/the-long-view-of-history",
      "author": "sonicrocketman",
      "published": "2026-01-23T14:30:05.839000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An essay defending the value of studying history as providing context and understanding rather than producing technology. Argues humanities improve people rather than the world.",
      "importance_score": 3,
      "reasoning": "Completely unrelated to AI research. General humanities advocacy piece with no connection to AI, ML, or alignment.",
      "themes": [
        "Non-AI Content"
      ],
      "continuation": null,
      "summary_html": "<p>An essay defending the value of studying history as providing context and understanding rather than producing technology. Argues humanities improve people rather than the world.</p>",
      "content_html": "<p>History as a subject is often viewed by students and the public at large as a domain without a use, a pedantic study of dates and names with some vague mission to remember the past—a memorial to ages past but neither a forward-looking or useful endeavor. The study of history produces teachers of history and nothing more. And while the study of history does not produce new widgets or novel computer advances, and nor does it deepen our understanding of materials science or physics.The humanities, in which history and studies of language and culture are a part, are not there to improve our understanding of nature or develop technology, they exist to improve the minds (both cultural and individual) of the people we are.History doesn't improve our world, it improves us. It gives us context for the world we live in and it helps us understand the reason why things are as they are and learn from the people before us.History as ContextImagine waking up every day with no memory of the day before, no idea who owned the house you slept in, no idea what country you're in, and no idea why everyone around you speaks the languages they do.Photo Credit: Library of CongressLiving in such a world would be disorienting, confusing, non-sensical. Yet this is the world without history. The world without history just is. It isn't a work in progress, but a finished piece—one that lives and dies with you—and has no meaning beyond the present moment.History doesn't let us predict the future, but it can be an enormous help in explaining the present. Current events are utterly indecipherable without the context of history and within that context, they feel less and less apart. Indeed our recent past of the Post-War Order is the oddity in history, and a real thing to be cherished and seen as something fleeting, fragile, and truly precious.Yet without the context of history, we're blind to the reality that we live in a world truly set apart from everything that's come before and one that's deeply...</p>"
    },
    {
      "id": "f91b42cb2221",
      "title": "Dating Roundup #11: Going Too Meta",
      "content": "If there’s several things this blog endorses, one of them would be going meta. It’s time. The big picture awaits. You’re Single Because You Live In The Wrong Place The most important meta question is location, location, location. This is the periodic reminder that dating dynamics are very different in different locations, and gender ratios are far more uneven than they appear because a lot of people pair off and aren’t in the pool. If you are a man seeking to date women, New York City is the place to be. Churrasco Suadade: when I’m out I notice that tables at restaurants and bars in manhattan are probably around 80-95% women, it’s a new dynamic that no one is talking about. Fixed Income Guy: Are you at all the poor people places? All the finance guy hang outs are 80% dudes. I mention Fixed Income Guy to mock him, as in why are you spending a lot more money to hang out with 80% dudes and largely finance dudes at that? I mean, sure, if that’s what you want. Darrell Owens: Oh this is new? Coming from the Bay Area, the amount of women I see in Manhattan is insane. You rarely see more than few young women partying back in San Francisco. The gender ratio here feels 70:30 young women to men, its every block in Manhattan! Noah Smith: In an ideal world, where you live wouldn’t really matter in terms of dating opportunities, but the truth is that one of the easiest ways to get chicks is to just move to New York City. Having lived in both Tokyo and NYC, I can pretty confidently tell you that while Tokyo is not a tough dating market by any means, NYC is absolutely on another level. You’re Single Because You’re Not Okay Making Less Money Than She Does, You Fool This viral clip (which is viral for a reason, it’s good fun, wait for it) is another endorsement of New York City being a great place to meet women, as you have a wide variety of great and largely successful women to explore. What doesn’t get mentioned in that clip as a key reason things are so great is that the gender ra...",
      "url": "https://www.lesswrong.com/posts/7y6YA8o6oisAzDSgk/dating-roundup-11-going-too-meta",
      "author": "Zvi",
      "published": "2026-01-23T15:50:51.924000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A roundup post about dating dynamics in different geographic locations, particularly noting gender ratio differences between NYC and San Francisco. Entirely social commentary unrelated to AI research.",
      "importance_score": 2,
      "reasoning": "Not AI-related content. While posted on LessWrong, has no relevance to AI research, safety, or technology.",
      "themes": [
        "Non-AI Content"
      ],
      "continuation": null,
      "summary_html": "<p>A roundup post about dating dynamics in different geographic locations, particularly noting gender ratio differences between NYC and San Francisco. Entirely social commentary unrelated to AI research.</p>",
      "content_html": "<p>If there’s several things this blog endorses, one of them would be going meta. It’s time. The big picture awaits. You’re Single Because You Live In The Wrong Place The most important meta question is location, location, location. This is the periodic reminder that dating dynamics are very different in different locations, and gender ratios are far more uneven than they appear because a lot of people pair off and aren’t in the pool. If you are a man seeking to date women, New York City is the place to be. Churrasco Suadade: when I’m out I notice that tables at restaurants and bars in manhattan are probably around 80-95% women, it’s a new dynamic that no one is talking about. Fixed Income Guy: Are you at all the poor people places? All the finance guy hang outs are 80% dudes. I mention Fixed Income Guy to mock him, as in why are you spending a lot more money to hang out with 80% dudes and largely finance dudes at that? I mean, sure, if that’s what you want. Darrell Owens: Oh this is new? Coming from the Bay Area, the amount of women I see in Manhattan is insane. You rarely see more than few young women partying back in San Francisco. The gender ratio here feels 70:30 young women to men, its every block in Manhattan! Noah Smith: In an ideal world, where you live wouldn’t really matter in terms of dating opportunities, but the truth is that one of the easiest ways to get chicks is to just move to New York City. Having lived in both Tokyo and NYC, I can pretty confidently tell you that while Tokyo is not a tough dating market by any means, NYC is absolutely on another level. You’re Single Because You’re Not Okay Making Less Money Than She Does, You Fool This viral clip (which is viral for a reason, it’s good fun, wait for it) is another endorsement of New York City being a great place to meet women, as you have a wide variety of great and largely successful women to explore. What doesn’t get mentioned in that clip as a key reason things are so great is that the gender ra...</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}