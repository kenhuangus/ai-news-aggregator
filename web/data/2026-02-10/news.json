{
  "category": "news",
  "date": "2026-02-10",
  "category_summary": "**Chinese open-source AI models** are [surging globally](/?date=2026-02-10&category=news#item-277bad48ae02), with **Alibaba's Qwen2** found on 52% of multi-model systems across **175,000 exposed hosts** in 130 countries, as Western labs increasingly restrict access to their most powerful models.\n\n- The **EU** [threatened antitrust action](/?date=2026-02-10&category=news#item-b71107a663d9) against **Meta** for blocking rival AI chatbots from **WhatsApp**, signaling tighter scrutiny of AI platform gatekeeping.\n- **Goldman Sachs** is [deploying **Anthropic's Claude**-powered](/?date=2026-02-10&category=news#item-6e4c6fe2aac1) autonomous agents for complex back-office operations including compliance and accounting.\n- **Harvard** and **Stanford** researchers [released **OAT**](/?date=2026-02-10&category=news#item-f3082083904b), a framework enabling LLM-style scaling for robotics by tokenizing continuous actions.\n- **Microsoft Research** [proposed **OrbitalBrain**](/?date=2026-02-10&category=news#item-f7213dd02e45) for distributed ML training directly on satellite constellations.\n- Experts [debate using **AI + satellite surveillance**](/?date=2026-02-10&category=news#item-14dabbdaf772) as substitutes for expired nuclear arms treaties between the US and Russia.\n- **New York** disclosure law [reveals zero companies](/?date=2026-02-10&category=news#item-b9a2ffca551e) have admitted to replacing workers with AI in nearly a year of enforcement.\n- AI copyright litigation in **2026** [remains unresolved](/?date=2026-02-10&category=news#item-3cd1f2abb5f3), with fair use questions still dominating the legal landscape.",
  "category_summary_html": "<p><strong>Chinese open-source AI models</strong> are <a href=\"/?date=2026-02-10&amp;category=news#item-277bad48ae02\" class=\"internal-link\" rel=\"noopener noreferrer\">surging globally</a>, with <strong>Alibaba's Qwen2</strong> found on 52% of multi-model systems across <strong>175,000 exposed hosts</strong> in 130 countries, as Western labs increasingly restrict access to their most powerful models.</p>\n<ul>\n<li>The <strong>EU</strong> <a href=\"/?date=2026-02-10&amp;category=news#item-b71107a663d9\" class=\"internal-link\" rel=\"noopener noreferrer\">threatened antitrust action</a> against <strong>Meta</strong> for blocking rival AI chatbots from <strong>WhatsApp</strong>, signaling tighter scrutiny of AI platform gatekeeping.</li>\n<li><strong>Goldman Sachs</strong> is <a href=\"/?date=2026-02-10&amp;category=news#item-6e4c6fe2aac1\" class=\"internal-link\" rel=\"noopener noreferrer\">deploying <strong>Anthropic's Claude</strong>-powered</a> autonomous agents for complex back-office operations including compliance and accounting.</li>\n<li><strong>Harvard</strong> and <strong>Stanford</strong> researchers <a href=\"/?date=2026-02-10&amp;category=news#item-f3082083904b\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>OAT</strong></a>, a framework enabling LLM-style scaling for robotics by tokenizing continuous actions.</li>\n<li><strong>Microsoft Research</strong> <a href=\"/?date=2026-02-10&amp;category=news#item-f7213dd02e45\" class=\"internal-link\" rel=\"noopener noreferrer\">proposed <strong>OrbitalBrain</strong></a> for distributed ML training directly on satellite constellations.</li>\n<li>Experts <a href=\"/?date=2026-02-10&amp;category=news#item-14dabbdaf772\" class=\"internal-link\" rel=\"noopener noreferrer\">debate using <strong>AI + satellite surveillance</strong></a> as substitutes for expired nuclear arms treaties between the US and Russia.</li>\n<li><strong>New York</strong> disclosure law <a href=\"/?date=2026-02-10&amp;category=news#item-b9a2ffca551e\" class=\"internal-link\" rel=\"noopener noreferrer\">reveals zero companies</a> have admitted to replacing workers with AI in nearly a year of enforcement.</li>\n<li>AI copyright litigation in <strong>2026</strong> <a href=\"/?date=2026-02-10&amp;category=news#item-3cd1f2abb5f3\" class=\"internal-link\" rel=\"noopener noreferrer\">remains unresolved</a>, with fair use questions still dominating the legal landscape.</li>\n</ul>",
  "themes": [
    {
      "name": "AI Geopolitics & Open-Source Competition",
      "description": "Chinese AI models filling the open-source gap as Western labs restrict access, with major security and deployment implications globally.",
      "item_count": 1,
      "example_items": [],
      "importance": 82.0
    },
    {
      "name": "AI Regulation & Policy",
      "description": "EU antitrust enforcement against Meta, New York labor disclosure laws, and ongoing copyright litigation shaping the legal framework around AI.",
      "item_count": 4,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "Enterprise & Agentic AI Deployment",
      "description": "Major financial institutions deploying autonomous AI agents for complex operations, moving beyond copilot use cases into fully autonomous workflows.",
      "item_count": 2,
      "example_items": [],
      "importance": 73.0
    },
    {
      "name": "AI for National Security",
      "description": "Frontier applications of AI in nuclear monitoring and satellite surveillance, raising questions about AI's role in replacing traditional international security frameworks.",
      "item_count": 1,
      "example_items": [],
      "importance": 73.0
    },
    {
      "name": "Physical AI & Robotics Research",
      "description": "New tokenization frameworks and distributed ML systems enabling LLM-style scaling for robotics and space-based computing.",
      "item_count": 2,
      "example_items": [],
      "importance": 70.0
    },
    {
      "name": "Multi-Agent AI Systems",
      "description": "Emerging experiments in AI agent-only environments and autonomous multi-agent interactions in gaming and social platforms.",
      "item_count": 1,
      "example_items": [],
      "importance": 56.0
    }
  ],
  "total_items": 13,
  "items": [
    {
      "id": "277bad48ae02",
      "title": "Exclusive: Why are Chinese AI models dominating open-source as Western labs step back?",
      "content": "Because Western AI labs&nbsp;won&#8217;t—or&nbsp;can&#8217;t—anymore. As OpenAI, Anthropic, and Google face mounting pressure to restrict their most powerful models, Chinese developers have filled the open-source void with AI explicitly built for what operators need: powerful models that run on commodity hardware.\n\n\n\nA new security&nbsp;study&nbsp;reveals just how thoroughly Chinese AI has captured this space.&nbsp;Research published by SentinelOne and Censys,&nbsp;mapping&nbsp;175,000 exposed AI hosts across 130 countries over 293 days, shows&nbsp;Alibaba&#8217;s&nbsp;Qwen2 consistently&nbsp;ranking&nbsp;second only&nbsp;to&nbsp;Meta&#8217;s&nbsp;Llama in global deployment.&nbsp;More tellingly, the Chinese model appears on 52% of systems running multiple AI models—suggesting&nbsp;it&#8217;s&nbsp;become the de facto alternative to Llama.\n\n\n\n&#8220;Over the next 12–18 months, we expect Chinese-origin model families to play an increasingly central role in the open-source LLM ecosystem, particularly as Western frontier labs slow or constrain open-weight releases,&#8221;&nbsp;Gabriel Bernadett-Shapiro, distinguished AI research scientist at SentinelOne, told TechForge&nbsp;Media&#8217;s&nbsp;AI News.\n\n\n\nThe finding arrives as OpenAI, Anthropic, and Google face regulatory scrutiny, safety review overhead, and commercial incentives pushing them toward API-gated releases rather than publishing model weights&nbsp;freely.&nbsp;The contrast with Chinese developers&nbsp;couldn&#8217;t&nbsp;be sharper.\n\n\n\nChinese labs have demonstrated what Bernadett-Shapiro calls&nbsp;&#8220;a willingness to publish large, high-quality weights that&nbsp;are explicitly optimised&nbsp;for local deployment, quantisation, and commodity hardware.&#8221;\n\n\n\n&#8220;In practice, this makes them easier to adopt, easier to run, and easier to integrate into edge and residential environments,”&nbsp;he added.\n\n\n\nPut simply: if&nbsp;you&#8217;re&nbsp;a researcher or developer wanting to run powerful AI on your own computer without a massive budget, Chinese models like Qwen2 are often your best—or only—option.\n\n\n\nPragmatics, not ideology\n\n\n\nAlibaba&#8217;s Qwen2 consistently ranks second only to Meta&#8217;s Llama across 175,000 exposed hosts globally. Source: SentinelOne/Censys\n\n\n\nThe research shows this dominance&nbsp;isn&#8217;t&nbsp;accidental. Qwen2 maintains what Bernadett-Shapiro calls&nbsp;&#8220;zero rank volatility&#8221;—it holds the number two position across every measurement method the researchers examined: total observations, unique hosts, and host-days.&nbsp;There&#8217;s&nbsp;no fluctuation, no regional variation, just consistent global adoption.\n\n\n\nThe co-deployment pattern is equally revealing. When operators run multiple AI models on the same system—a common practice for comparison or workload segmentation—the pairing of Llama and Qwen2 appears on 40,694 hosts, representing 52% of all multi-family deployments.\n\n\n\nGeographic concentration reinforces the picture. In China, Beijing alone accounts for 30% of exposed hosts, with Shanghai and Guangdong&nbsp;adding another&nbsp;21% combined.&nbsp;In the United States, Virginia—reflecting AWS infrastructure&nbsp;density—represents 18% of hosts.\n\n\n\nChina and the US dominate exposed Ollama host distribution, with Beijing accounting for 30% of Chinese deployments. Source: SentinelOne/Censys\n\n\n\n&#8220;If release velocity, openness, and hardware portability continue to diverge between regions, Chinese model lineages are likely to become the default for open deployments, not because of ideology, but because of availability and pragmatics,&#8221;&nbsp;Bernadett-Shapiro explained.\n\n\n\nThe governance problem\n\n\n\nThis shift creates what Bernadett-Shapiro characterises as a&nbsp;&#8220;governance inversion&#8221;—a fundamental reversal of how AI risk and accountability&nbsp;are distributed.\n\n\n\nIn platform-hosted services like ChatGPT, one company controls everything: the infrastructure, monitors usage, implements safety controls, and can shut down&nbsp;abuse. With open-weight models, the control evaporates. Accountability diffuses across thousands of networks in 130 countries, while dependency concentrates upstream in a handful of model suppliers—increasingly Chinese ones.\n\n\n\nThe 175,000 exposed hosts operate entirely outside the control systems governing commercial AI platforms.&nbsp;There&#8217;s&nbsp;no centralised authentication, no rate limiting, no abuse detection, and critically, no kill switch if misuse is detected.\n\n\n\n&#8220;Once an open-weight model is released, it is trivial to remove safety or security training,&#8221;&nbsp;Bernadett-Shapiro noted.&#8221;Frontier labs need to treat open-weight releases as long-lived infrastructure artefacts.&#8221;\n\n\n\nA persistent backbone of 23,000 hosts&nbsp;showing 87%&nbsp;average uptime drives the majority of activity.&nbsp;These&nbsp;aren&#8217;t&nbsp;hobbyist experiments—they&#8217;re&nbsp;operational systems providing ongoing utility, often running multiple models simultaneously.\n\n\n\nPerhaps most concerning:&nbsp;between 16% and 19% of the infrastructure&nbsp;couldn&#8217;t&nbsp;be attributed to any identifiable owner.&#8221;Even if we are able to prove that&nbsp;a model was leveraged&nbsp;in an attack, there are not well-established abuse reporting routes,&#8221;&nbsp;Bernadett-Shapiro said.\n\n\n\nSecurity without guardrails\n\n\n\nNearly half (48%) of exposed hosts advertise&nbsp;&#8220;tool-calling capabilities&#8221;—meaning&nbsp;they&#8217;re&nbsp;not just generating text. They can execute code, access APIs, and interact with external systems autonomously.\n\n\n\n&#8220;A&nbsp;text-only model can generate harmful content, but a tool-calling model can act,&#8221;&nbsp;Bernadett-Shapiro explained.&nbsp;&#8220;On an unauthenticated server, an attacker&nbsp;doesn&#8217;t&nbsp;need malware or credentials; they just need a prompt.&#8221;\n\n\n\nNearly half of exposed Ollama hosts have tool-calling capabilities that can execute code and access external systems. Source: SentinelOne/Censys\n\n\n\nThe highest-risk scenario involves what he calls&nbsp;&#8220;exposed, tool-enabled RAG or automation endpoints being driven remotely as an execution layer.&#8221;&nbsp;An attacker could&nbsp;simply&nbsp;ask the model to summarise internal documents, extract API keys from code repositories, or call downstream services the model&nbsp;is configured&nbsp;to access.\n\n\n\nWhen paired with&nbsp;&#8220;thinking&#8221;&nbsp;models optimised for multi-step reasoning—present on 26% of hosts—the system can plan complex operations autonomously. The researchers identified at least 201 hosts running&nbsp;&#8220;uncensored&#8221;&nbsp;configurations that explicitly remove safety guardrails, though Bernadett-Shapiro notes this represents a lower bound.\n\n\n\nIn other words, these&nbsp;aren&#8217;t&nbsp;just chatbots—they&#8217;re&nbsp;AI systems that can take action, and half of them have no password protection.\n\n\n\nWhat frontier labs should do\n\n\n\nFor Western AI developers concerned about maintaining influence over the&nbsp;technology&#8217;s&nbsp;trajectory, Bernadett-Shapiro recommends a different approach to model releases.\n\n\n\n&#8220;Frontier labs&nbsp;can&#8217;t&nbsp;control deployment, but they can shape the risks that they release into the world,&#8221;&nbsp;he said. That includes&nbsp;&#8220;investing in post-release monitoring of ecosystem-level adoption and misuse patterns&#8221;&nbsp;rather than treating releases as one-off research outputs.\n\n\n\nThe current governance model assumes centralised deployment with diffuse upstream supply—the exact opposite of&nbsp;what&#8217;s&nbsp;actually happening.&nbsp;&#8220;When a small number of lineages dominate&nbsp;what&#8217;s&nbsp;runnable on commodity hardware, upstream decisions get amplified everywhere,&#8221;&nbsp;he explained.&nbsp;&#8220;Governance strategies must acknowledge that inversion.&#8221;\n\n\n\nBut acknowledgement requires visibility. Currently, most labs releasing open-weight models have no systematic way to track how&nbsp;they&#8217;re&nbsp;being used, where&nbsp;they&#8217;re&nbsp;deployed, or whether safety training remains intact after quantisation and fine-tuning.\n\n\n\nThe 12-18 month outlook\n\n\n\nBernadett-Shapiro expects the exposed layer to&nbsp;&#8220;persist and professionalise&#8221;&nbsp;as tool use, agents, and multimodal inputs become default capabilities rather than exceptions.&nbsp;The transient edge will&nbsp;keep churning&nbsp;as hobbyists experiment, but the backbone will&nbsp;grow&nbsp;more stable, more capable, and&nbsp;handle&nbsp;more sensitive data.\n\n\n\nEnforcement will remain uneven because residential and small VPS deployments&nbsp;don&#8217;t&nbsp;map to existing governance controls.&nbsp;&#8220;This&nbsp;isn&#8217;t&nbsp;a misconfiguration problem,&#8221;&nbsp;he emphasised.&nbsp;&#8220;We are observing the early formation of a public, unmanaged AI compute substrate. There is no central switch to flip.&#8221;\n\n\n\nThe geopolitical dimension adds urgency.&nbsp;&#8220;When most of the&nbsp;world&#8217;s&nbsp;unmanaged AI compute depends on models released by a handful of non-Western labs, traditional assumptions about influence, coordination, and post-release response become weaker,&#8221;&nbsp;Bernadett-Shapiro said.\n\n\n\nFor Western developers and policymakers, the implication is stark:&nbsp;&#8220;Even perfect governance of their own platforms has limited impact on the real-world risk surface if the dominant capabilities live elsewhere and propagate through open, decentralised infrastructure.&#8221;\n\n\n\nThe open-source AI ecosystem is globalising, but its centre of gravity is shifting decisively eastward. Not&nbsp;through any coordinated strategy, but through the practical economics of&nbsp;who&#8217;s&nbsp;willing to publish what researchers and operators actually need to run AI locally.\n\n\n\nThe 175,000 exposed hosts mapped in this study are just the visible surface of that fundamental realignment—one that Western policymakers are only beginning to recognise, let alone address.\n\n\n\nSee also: Huawei details open-source AI development roadmap at Huawei Connect 2025\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Exclusive: Why are Chinese AI models dominating open-source as Western labs step back? appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/chinese-ai-models-175k-unprotected-systems-western-retreat/",
      "author": "Dashveenjit Kaur",
      "published": "2026-02-09T11:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI and Us",
        "AI in Action",
        "Artificial Intelligence",
        "Deep Dives",
        "Features",
        "Inside AI",
        "Open-Source & Democratised AI",
        "ai",
        "artificial intelligence",
        "open-source"
      ],
      "summary": "Building on observations first shared on [Social](/?date=2026-02-09&category=social#item-81be3a630142) last week about Qwen's dominance, A security study mapping 175,000 exposed AI hosts across 130 countries reveals Chinese open-source models, particularly Alibaba's Qwen2, are rapidly filling the vacuum left by Western labs restricting their most powerful models. Qwen2 ranks second only to Meta's Llama globally and appears on 52% of multi-model systems.",
      "importance_score": 82.0,
      "reasoning": "Documents a major structural shift in the open-source AI landscape with hard data. The growing dominance of Chinese open-source models has significant geopolitical, security, and competitive implications for frontier AI development worldwide.",
      "themes": [
        "open-source AI",
        "geopolitics",
        "AI security",
        "Chinese AI"
      ],
      "continuation": {
        "original_item_id": "81be3a630142",
        "original_date": "2026-02-09",
        "original_category": "social",
        "original_title": "People don't want to accept that the top used open model families in 2026 are.",
        "continuation_type": "mainstream_pickup",
        "should_demote": false,
        "reference_text": "Building on observations first shared on **Social** last week about Qwen's dominance"
      },
      "summary_html": "<p>Building on observations first shared on <a href=\"/?date=2026-02-09&amp;category=social#item-81be3a630142\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> last week about Qwen's dominance, A security study mapping 175,000 exposed AI hosts across 130 countries reveals Chinese open-source models, particularly Alibaba's Qwen2, are rapidly filling the vacuum left by Western labs restricting their most powerful models. Qwen2 ranks second only to Meta's Llama globally and appears on 52% of multi-model systems.</p>",
      "content_html": "<p>Because Western AI labs&nbsp;won’t—or&nbsp;can’t—anymore. As OpenAI, Anthropic, and Google face mounting pressure to restrict their most powerful models, Chinese developers have filled the open-source void with AI explicitly built for what operators need: powerful models that run on commodity hardware.</p>\n<p>A new security&nbsp;study&nbsp;reveals just how thoroughly Chinese AI has captured this space.&nbsp;Research published by SentinelOne and Censys,&nbsp;mapping&nbsp;175,000 exposed AI hosts across 130 countries over 293 days, shows&nbsp;Alibaba’s&nbsp;Qwen2 consistently&nbsp;ranking&nbsp;second only&nbsp;to&nbsp;Meta’s&nbsp;Llama in global deployment.&nbsp;More tellingly, the Chinese model appears on 52% of systems running multiple AI models—suggesting&nbsp;it’s&nbsp;become the de facto alternative to Llama.</p>\n<p>“Over the next 12–18 months, we expect Chinese-origin model families to play an increasingly central role in the open-source LLM ecosystem, particularly as Western frontier labs slow or constrain open-weight releases,”&nbsp;Gabriel Bernadett-Shapiro, distinguished AI research scientist at SentinelOne, told TechForge&nbsp;Media’s&nbsp;AI News.</p>\n<p>The finding arrives as OpenAI, Anthropic, and Google face regulatory scrutiny, safety review overhead, and commercial incentives pushing them toward API-gated releases rather than publishing model weights&nbsp;freely.&nbsp;The contrast with Chinese developers&nbsp;couldn’t&nbsp;be sharper.</p>\n<p>Chinese labs have demonstrated what Bernadett-Shapiro calls&nbsp;“a willingness to publish large, high-quality weights that&nbsp;are explicitly optimised&nbsp;for local deployment, quantisation, and commodity hardware.”</p>\n<p>“In practice, this makes them easier to adopt, easier to run, and easier to integrate into edge and residential environments,”&nbsp;he added.</p>\n<p>Put simply: if&nbsp;you’re&nbsp;a researcher or developer wanting to run powerful AI on your own computer without a massive budget, Chinese models like Qwen2 are often your best—or only—option.</p>\n<p>Pragmatics, not ideology</p>\n<p>Alibaba’s Qwen2 consistently ranks second only to Meta’s Llama across 175,000 exposed hosts globally. Source: SentinelOne/Censys</p>\n<p>The research shows this dominance&nbsp;isn’t&nbsp;accidental. Qwen2 maintains what Bernadett-Shapiro calls&nbsp;“zero rank volatility”—it holds the number two position across every measurement method the researchers examined: total observations, unique hosts, and host-days.&nbsp;There’s&nbsp;no fluctuation, no regional variation, just consistent global adoption.</p>\n<p>The co-deployment pattern is equally revealing. When operators run multiple AI models on the same system—a common practice for comparison or workload segmentation—the pairing of Llama and Qwen2 appears on 40,694 hosts, representing 52% of all multi-family deployments.</p>\n<p>Geographic concentration reinforces the picture. In China, Beijing alone accounts for 30% of exposed hosts, with Shanghai and Guangdong&nbsp;adding another&nbsp;21% combined.&nbsp;In the United States, Virginia—reflecting AWS infrastructure&nbsp;density—represents 18% of hosts.</p>\n<p>China and the US dominate exposed Ollama host distribution, with Beijing accounting for 30% of Chinese deployments. Source: SentinelOne/Censys</p>\n<p>“If release velocity, openness, and hardware portability continue to diverge between regions, Chinese model lineages are likely to become the default for open deployments, not because of ideology, but because of availability and pragmatics,”&nbsp;Bernadett-Shapiro explained.</p>\n<p>The governance problem</p>\n<p>This shift creates what Bernadett-Shapiro characterises as a&nbsp;“governance inversion”—a fundamental reversal of how AI risk and accountability&nbsp;are distributed.</p>\n<p>In platform-hosted services like ChatGPT, one company controls everything: the infrastructure, monitors usage, implements safety controls, and can shut down&nbsp;abuse. With open-weight models, the control evaporates. Accountability diffuses across thousands of networks in 130 countries, while dependency concentrates upstream in a handful of model suppliers—increasingly Chinese ones.</p>\n<p>The 175,000 exposed hosts operate entirely outside the control systems governing commercial AI platforms.&nbsp;There’s&nbsp;no centralised authentication, no rate limiting, no abuse detection, and critically, no kill switch if misuse is detected.</p>\n<p>“Once an open-weight model is released, it is trivial to remove safety or security training,”&nbsp;Bernadett-Shapiro noted.”Frontier labs need to treat open-weight releases as long-lived infrastructure artefacts.”</p>\n<p>A persistent backbone of 23,000 hosts&nbsp;showing 87%&nbsp;average uptime drives the majority of activity.&nbsp;These&nbsp;aren’t&nbsp;hobbyist experiments—they’re&nbsp;operational systems providing ongoing utility, often running multiple models simultaneously.</p>\n<p>Perhaps most concerning:&nbsp;between 16% and 19% of the infrastructure&nbsp;couldn’t&nbsp;be attributed to any identifiable owner.”Even if we are able to prove that&nbsp;a model was leveraged&nbsp;in an attack, there are not well-established abuse reporting routes,”&nbsp;Bernadett-Shapiro said.</p>\n<p>Security without guardrails</p>\n<p>Nearly half (48%) of exposed hosts advertise&nbsp;“tool-calling capabilities”—meaning&nbsp;they’re&nbsp;not just generating text. They can execute code, access APIs, and interact with external systems autonomously.</p>\n<p>“A&nbsp;text-only model can generate harmful content, but a tool-calling model can act,”&nbsp;Bernadett-Shapiro explained.&nbsp;“On an unauthenticated server, an attacker&nbsp;doesn’t&nbsp;need malware or credentials; they just need a prompt.”</p>\n<p>Nearly half of exposed Ollama hosts have tool-calling capabilities that can execute code and access external systems. Source: SentinelOne/Censys</p>\n<p>The highest-risk scenario involves what he calls&nbsp;“exposed, tool-enabled RAG or automation endpoints being driven remotely as an execution layer.”&nbsp;An attacker could&nbsp;simply&nbsp;ask the model to summarise internal documents, extract API keys from code repositories, or call downstream services the model&nbsp;is configured&nbsp;to access.</p>\n<p>When paired with&nbsp;“thinking”&nbsp;models optimised for multi-step reasoning—present on 26% of hosts—the system can plan complex operations autonomously. The researchers identified at least 201 hosts running&nbsp;“uncensored”&nbsp;configurations that explicitly remove safety guardrails, though Bernadett-Shapiro notes this represents a lower bound.</p>\n<p>In other words, these&nbsp;aren’t&nbsp;just chatbots—they’re&nbsp;AI systems that can take action, and half of them have no password protection.</p>\n<p>What frontier labs should do</p>\n<p>For Western AI developers concerned about maintaining influence over the&nbsp;technology’s&nbsp;trajectory, Bernadett-Shapiro recommends a different approach to model releases.</p>\n<p>“Frontier labs&nbsp;can’t&nbsp;control deployment, but they can shape the risks that they release into the world,”&nbsp;he said. That includes&nbsp;“investing in post-release monitoring of ecosystem-level adoption and misuse patterns”&nbsp;rather than treating releases as one-off research outputs.</p>\n<p>The current governance model assumes centralised deployment with diffuse upstream supply—the exact opposite of&nbsp;what’s&nbsp;actually happening.&nbsp;“When a small number of lineages dominate&nbsp;what’s&nbsp;runnable on commodity hardware, upstream decisions get amplified everywhere,”&nbsp;he explained.&nbsp;“Governance strategies must acknowledge that inversion.”</p>\n<p>But acknowledgement requires visibility. Currently, most labs releasing open-weight models have no systematic way to track how&nbsp;they’re&nbsp;being used, where&nbsp;they’re&nbsp;deployed, or whether safety training remains intact after quantisation and fine-tuning.</p>\n<p>The 12-18 month outlook</p>\n<p>Bernadett-Shapiro expects the exposed layer to&nbsp;“persist and professionalise”&nbsp;as tool use, agents, and multimodal inputs become default capabilities rather than exceptions.&nbsp;The transient edge will&nbsp;keep churning&nbsp;as hobbyists experiment, but the backbone will&nbsp;grow&nbsp;more stable, more capable, and&nbsp;handle&nbsp;more sensitive data.</p>\n<p>Enforcement will remain uneven because residential and small VPS deployments&nbsp;don’t&nbsp;map to existing governance controls.&nbsp;“This&nbsp;isn’t&nbsp;a misconfiguration problem,”&nbsp;he emphasised.&nbsp;“We are observing the early formation of a public, unmanaged AI compute substrate. There is no central switch to flip.”</p>\n<p>The geopolitical dimension adds urgency.&nbsp;“When most of the&nbsp;world’s&nbsp;unmanaged AI compute depends on models released by a handful of non-Western labs, traditional assumptions about influence, coordination, and post-release response become weaker,”&nbsp;Bernadett-Shapiro said.</p>\n<p>For Western developers and policymakers, the implication is stark:&nbsp;“Even perfect governance of their own platforms has limited impact on the real-world risk surface if the dominant capabilities live elsewhere and propagate through open, decentralised infrastructure.”</p>\n<p>The open-source AI ecosystem is globalising, but its centre of gravity is shifting decisively eastward. Not&nbsp;through any coordinated strategy, but through the practical economics of&nbsp;who’s&nbsp;willing to publish what researchers and operators actually need to run AI locally.</p>\n<p>The 175,000 exposed hosts mapped in this study are just the visible surface of that fundamental realignment—one that Western policymakers are only beginning to recognise, let alone address.</p>\n<p>See also: Huawei details open-source AI development roadmap at Huawei Connect 2025</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Exclusive: Why are Chinese AI models dominating open-source as Western labs step back? appeared first on AI News.</p>"
    },
    {
      "id": "b71107a663d9",
      "title": "EU threatens to act over Meta blocking rival AI chatbots from WhatsApp",
      "content": "Firm accused of ‘abusing’ its dominant position for messaging in what appears to be breach of antitrust rulesThe EU has threatened to take action against the social media company Meta, arguing it has blocked rival chatbots from using its WhatsApp messaging platform.The European Commission said on Monday that WhatsApp Business – which is designed to be used by businesses to interact with customers – appears to be in breach of EU antitrust rules. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/09/eu-threatens-to-act-over-meta-blocking-rival-ai-chatbots-from-whatsapp",
      "author": "Aisha Down",
      "published": "2026-02-09T13:01:29",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Meta",
        "European Union",
        "Business",
        "Chatbots",
        "Technology sector",
        "Technology",
        "Europe",
        "World news",
        "European Commission"
      ],
      "summary": "The European Commission has threatened action against Meta for blocking rival AI chatbots from its WhatsApp Business platform, arguing it constitutes an abuse of dominant market position under EU antitrust rules. This signals growing regulatory scrutiny of AI distribution chokepoints.",
      "importance_score": 78.0,
      "reasoning": "Major regulatory action at the intersection of AI and platform competition. Could set precedent for how dominant messaging platforms must allow third-party AI access, directly affecting frontier AI deployment and market structure.",
      "themes": [
        "AI regulation",
        "antitrust",
        "EU policy",
        "platform competition"
      ],
      "continuation": null,
      "summary_html": "<p>The European Commission has threatened action against Meta for blocking rival AI chatbots from its WhatsApp Business platform, arguing it constitutes an abuse of dominant market position under EU antitrust rules. This signals growing regulatory scrutiny of AI distribution chokepoints.</p>",
      "content_html": "<p>Firm accused of ‘abusing’ its dominant position for messaging in what appears to be breach of antitrust rulesThe EU has threatened to take action against the social media company Meta, arguing it has blocked rival chatbots from using its WhatsApp messaging platform.The European Commission said on Monday that WhatsApp Business – which is designed to be used by businesses to interact with customers – appears to be in breach of EU antitrust rules. Continue reading...</p>"
    },
    {
      "id": "6e4c6fe2aac1",
      "title": "Goldman Sachs tests autonomous AI agents for process-heavy work",
      "content": "Goldman Sachs is pushing deeper into real use of artificial intelligence inside its operations, moving to systems that can carry out complex tasks on their own. The Wall Street bank is working with AI startup Anthropic to create autonomous AI agents powered by Anthropic&#8217;s Claude model that can handle work that used to require large teams of people. The bank&#8217;s chief information officer says the technology has surprised staff with how capable it can be.\nMany companies use AI for tasks like helping employees draft text or analysing trends. But Goldman Sachs is testing AI systems that go into what bankers call back-office work – functions like accounting, compliance checks and onboarding new clients – areas viewed as too complex for automation. Such jobs involve many rules, data and detailed review, and have resisted full automation.\nMoving AI agents into process-heavy operations\nThe partnership with Anthropic has been underway for roughly six months, with engineers from the AI startup embedded directly with teams at Goldman Sachs to build these agents side by side with in-house staff, according to a report based on an interview with the bank&#8217;s CIO. The work has focused on areas where automation could cut the time it takes to complete repetitive and data-heavy tasks.\nMarco Argenti, Goldman&#8217;s chief information officer, described the AI systems as a new kind of digital assistant. &#8220;Think of it as a digital co-worker for many of the professions in the firm that are scaled, complex and very process-intensive,&#8221; he told CNBC. In early tests, the ability to reason through multi-step work and apply logic to complex areas like accounting and compliance was something the bank had not expected from the model.\nGoldman Sachs has been among the more active banks in testing AI tools over the past few years. Before this announcement, the firm deployed internal tools to help engineers write and debug code. But the change now is toward systems that can take on work traditionally done by accountants and compliance teams. That highlights how organisations are trying to find concrete business uses for AI beyond the hype.\nFaster workflows, human oversight remains\nThe agents are based on Anthropic&#8217;s Claude Opus 4.6 model, which has been built to handle long documents and complex reasoning. Goldman&#8217;s tests have shown that such systems can reduce the time needed for tasks like client onboarding, trade reconciliation and document review. While the bank has not shared specific performance numbers, people familiar with the matter told news outlets that work which once took a great deal of human labour can now be done in much less time.\nArgenti said the rollout is not about replacing human workers, at least not at this stage. The bank reportedly views the agents as a tool to help existing staff manage busy schedules and get through high volumes of work. In areas like compliance and accounting, jobs can involve repetitive, rule-based steps. AI frees analysts from that repetition so they can focus on higher-value judgement work.\nMarkets have already reacted to the idea that large institutions are moving toward more AI-driven automation. In recent days, a sell-off in enterprise software stocks wiped out billions in value as some investors worried that tools like autonomous agents could speed up the decline of traditional business software that has dominated corporate IT for years.\nAI adoption meets governance reality\nIndustry watchers see Goldman&#8217;s move as part of a wider trend. For example, some firms are piloting tools to read large data sets, interpret multiple sources of information, and draft investment analysis. These steps show AI making the jump from isolated projects to operational work. Yet the technology raises questions about oversight and trust. AI systems that interpret financial rules and compliance standards must be monitored carefully to avoid errors that could have regulatory or financial consequences. That&#8217;s why many institutions treat these systems as helpers that are reviewed by human experts until they mature.\nGoldman Sachs is starting with operational functions that have traditionally resisted automation because they involve a lot of data and formal steps. The bank has not said when it expects deployment of the agents in its operations, but executives have suggested that the initial tests have been promising enough to support further rollout.\nThe broader industry context shows other banks and financial firms also exploring similar use cases. Some have already invested heavily in AI infrastructure, and reports indicate that major firms are planning to use AI to cut costs, speed workflows and improve risk management. However, many remain cautious about putting AI into customer-facing or regulated functions.\nGoldman&#8217;s push into autonomous AI agents is an example of how large companies are reshaping internal operations using the latest generation of AI models. If systems can handle complex tasks reliably, organisations could see real changes in how work gets done – particularly in back-office functions where volume and repetition keep costs high and innovation slow.\n(Photo by Louis Droege)\nSee also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Goldman Sachs tests autonomous AI agents for process-heavy work appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/goldman-sachs-tests-autonomous-ai-agents-for-process-heavy-work/",
      "author": "Muhammad Zulhusni",
      "published": "2026-02-09T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Artificial Intelligence",
        "Featured News",
        "Features",
        "Finance AI",
        "World of Work",
        "agentic ai",
        "ai",
        "banking",
        "finance"
      ],
      "summary": "Goldman Sachs is partnering with Anthropic to deploy autonomous AI agents powered by Claude for complex back-office operations including accounting, compliance, and client onboarding. The bank's CIO says the technology has exceeded expectations in handling tasks previously deemed too complex for automation.",
      "importance_score": 76.0,
      "reasoning": "Significant real-world deployment of agentic AI in high-stakes financial operations by a major Wall Street institution. Demonstrates frontier AI models moving beyond copilot tasks into autonomous enterprise workflows.",
      "themes": [
        "agentic AI",
        "enterprise AI",
        "finance",
        "Anthropic"
      ],
      "continuation": null,
      "summary_html": "<p>Goldman Sachs is partnering with Anthropic to deploy autonomous AI agents powered by Claude for complex back-office operations including accounting, compliance, and client onboarding. The bank's CIO says the technology has exceeded expectations in handling tasks previously deemed too complex for automation.</p>",
      "content_html": "<p>Goldman Sachs is pushing deeper into real use of artificial intelligence inside its operations, moving to systems that can carry out complex tasks on their own. The Wall Street bank is working with AI startup Anthropic to create autonomous AI agents powered by Anthropic’s Claude model that can handle work that used to require large teams of people. The bank’s chief information officer says the technology has surprised staff with how capable it can be.</p>\n<p>Many companies use AI for tasks like helping employees draft text or analysing trends. But Goldman Sachs is testing AI systems that go into what bankers call back-office work – functions like accounting, compliance checks and onboarding new clients – areas viewed as too complex for automation. Such jobs involve many rules, data and detailed review, and have resisted full automation.</p>\n<p>Moving AI agents into process-heavy operations</p>\n<p>The partnership with Anthropic has been underway for roughly six months, with engineers from the AI startup embedded directly with teams at Goldman Sachs to build these agents side by side with in-house staff, according to a report based on an interview with the bank’s CIO. The work has focused on areas where automation could cut the time it takes to complete repetitive and data-heavy tasks.</p>\n<p>Marco Argenti, Goldman’s chief information officer, described the AI systems as a new kind of digital assistant. “Think of it as a digital co-worker for many of the professions in the firm that are scaled, complex and very process-intensive,” he told CNBC. In early tests, the ability to reason through multi-step work and apply logic to complex areas like accounting and compliance was something the bank had not expected from the model.</p>\n<p>Goldman Sachs has been among the more active banks in testing AI tools over the past few years. Before this announcement, the firm deployed internal tools to help engineers write and debug code. But the change now is toward systems that can take on work traditionally done by accountants and compliance teams. That highlights how organisations are trying to find concrete business uses for AI beyond the hype.</p>\n<p>Faster workflows, human oversight remains</p>\n<p>The agents are based on Anthropic’s Claude Opus 4.6 model, which has been built to handle long documents and complex reasoning. Goldman’s tests have shown that such systems can reduce the time needed for tasks like client onboarding, trade reconciliation and document review. While the bank has not shared specific performance numbers, people familiar with the matter told news outlets that work which once took a great deal of human labour can now be done in much less time.</p>\n<p>Argenti said the rollout is not about replacing human workers, at least not at this stage. The bank reportedly views the agents as a tool to help existing staff manage busy schedules and get through high volumes of work. In areas like compliance and accounting, jobs can involve repetitive, rule-based steps. AI frees analysts from that repetition so they can focus on higher-value judgement work.</p>\n<p>Markets have already reacted to the idea that large institutions are moving toward more AI-driven automation. In recent days, a sell-off in enterprise software stocks wiped out billions in value as some investors worried that tools like autonomous agents could speed up the decline of traditional business software that has dominated corporate IT for years.</p>\n<p>AI adoption meets governance reality</p>\n<p>Industry watchers see Goldman’s move as part of a wider trend. For example, some firms are piloting tools to read large data sets, interpret multiple sources of information, and draft investment analysis. These steps show AI making the jump from isolated projects to operational work. Yet the technology raises questions about oversight and trust. AI systems that interpret financial rules and compliance standards must be monitored carefully to avoid errors that could have regulatory or financial consequences. That’s why many institutions treat these systems as helpers that are reviewed by human experts until they mature.</p>\n<p>Goldman Sachs is starting with operational functions that have traditionally resisted automation because they involve a lot of data and formal steps. The bank has not said when it expects deployment of the agents in its operations, but executives have suggested that the initial tests have been promising enough to support further rollout.</p>\n<p>The broader industry context shows other banks and financial firms also exploring similar use cases. Some have already invested heavily in AI infrastructure, and reports indicate that major firms are planning to use AI to cut costs, speed workflows and improve risk management. However, many remain cautious about putting AI into customer-facing or regulated functions.</p>\n<p>Goldman’s push into autonomous AI agents is an example of how large companies are reshaping internal operations using the latest generation of AI models. If systems can handle complex tasks reliably, organisations could see real changes in how work gets done – particularly in back-office functions where volume and repetition keep costs high and innovation slow.</p>\n<p>(Photo by Louis Droege)</p>\n<p>See also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Goldman Sachs tests autonomous AI agents for process-heavy work appeared first on AI News.</p>"
    },
    {
      "id": "14dabbdaf772",
      "title": "AI Is Here to Replace Nuclear Treaties. Scared Yet?",
      "content": "The last major nuclear arms treaty between the US and Russia just expired. Some experts believe a combination of satellite surveillance, AI, and human reviewers can take its place. Others, not so much.",
      "url": "https://www.wired.com/story/satellites-ai-nuclear-treaties/",
      "author": "Matthew Gault",
      "published": "2026-02-09T11:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Security",
        "Security / National Security",
        "Security / Security News",
        "Business / Artificial Intelligence",
        "artificial intelligence",
        "satellites",
        "national security",
        "machine learning",
        "space",
        "Russia",
        "China",
        "nuclear war",
        "nuclear",
        "Stopgaps"
      ],
      "summary": "With the last major US-Russia nuclear arms treaty having expired, experts are debating whether satellite surveillance combined with AI monitoring could serve as a substitute for traditional nuclear treaties. The proposal remains controversial among arms control specialists.",
      "importance_score": 73.0,
      "reasoning": "Explores a high-stakes, novel application of AI at the intersection of nuclear security and geopolitics. While still speculative, the use of AI to replace international security frameworks is a consequential frontier topic.",
      "themes": [
        "AI safety",
        "national security",
        "nuclear policy",
        "satellite surveillance"
      ],
      "continuation": null,
      "summary_html": "<p>With the last major US-Russia nuclear arms treaty having expired, experts are debating whether satellite surveillance combined with AI monitoring could serve as a substitute for traditional nuclear treaties. The proposal remains controversial among arms control specialists.</p>",
      "content_html": "<p>The last major nuclear arms treaty between the US and Russia just expired. Some experts believe a combination of satellite surveillance, AI, and human reviewers can take its place. Others, not so much.</p>"
    },
    {
      "id": "f3082083904b",
      "title": "Meet OAT: The New Action Tokenizer Bringing LLM-Style Scaling and Flexible, Anytime Inference to the Robotics World",
      "content": "Robots are entering their GPT-3 era. For years, researchers have tried to train robots using the same autoregressive (AR) models that power large language models (LLMs). If a model can predict the next word in a sentence, it should be able to predict the next move for a robotic arm. However, a technical wall has blocked this progress: continuous robot movements are difficult to turn into discrete tokens.\n\n\n\nA team of researchers from Harvard University and Stanford University have released a new framework called Ordered Action Tokenization (OAT) to bridge this gap.\n\n\n\nhttps://arxiv.org/pdf/2602.04215\n\n\n\nThe Messy Reality of Robot Actions\n\n\n\nTokenization turns complex data into a sequence of discrete numbers (tokens). For robots, these actions are continuous signals like joint angles. Previous strategies had fatal flaws:\n\n\n\n\nBinning: Turns every action dimension into a &#8216;bin.&#8217; While simple, it creates massive sequences that make training and inference slow.\n\n\n\nFAST (Frequency-space Action Sequence Tokenization): Uses math to compress movements into frequency coefficients. It is fast but often produces &#8216;undecodable&#8217; sequences where small errors cause the robot to halt or move unpredictably. \n\n\n\nLearned Latent Tokenizers: These use a learned &#8216;dictionary&#8217; of movements. They are safe but lack a specific order, meaning the model treats early and late tokens as equally important.\n\n\n\n\nhttps://arxiv.org/pdf/2602.04215\n\n\n\nThe Three Golden Rules of OAT\n\n\n\nThe research team identified 3 essential properties—desiderata—for a functional robot tokenizer:\n\n\n\n\nHigh Compression (P.1): Token sequences must be short to keep models efficient. \n\n\n\nTotal Decodability (P.2): The decoder must be a total function, ensuring every possible token sequence maps to a valid movement. \n\n\n\nCausal Ordering (P.3): Tokens must have a left-to-right structure where early tokens capture global motion and later tokens refine details. \n\n\n\n\nThe Secret Sauce: Nested Dropout and Registers\n\n\n\nOAT uses a transformer encoder with register tokens to summarize action chunks. To force the model to learn &#8216;important&#8217; things first, the research team used a innovative approach called Nested Dropout.\n\n\n\nhttps://arxiv.org/pdf/2602.04215\n\n\n\nBreaking the Benchmarks\n\n\n\nThe research team tested OAT across 20+ tasks in 4 major simulation benchmarks. OAT consistently outperformed the industry-standard Diffusion Policy (DP) and previous tokenizers. \n\n\n\nPerformance Results\n\n\n\nBenchmarkOAT Success RateDP Success RateBin Token CountOAT Token CountLIBERO56.3% 36.6% 224 8 RoboMimic73.1% 67.1% 224 8 MetaWorld24.4% 19.3% 128 8 RoboCasa54.6% 54.0% 384 8 \n\n\n\n&#8216;Anytime&#8217; Inference: Speed vs. Precision\n\n\n\nThe most practical benefit of OAT is prefix-based detokenization. Since the tokens are ordered by importance, you can stop the model early.\n\n\n\n\nCoarse Actions: Decoding just 1 or 2 tokens gives the robot a general direction quickly, which is useful for low-latency tasks.\n\n\n\nFine Actions: Generating all 8 tokens provides the high-precision details needed for complex insertions.\n\n\n\n\nThis allows for a smooth trade-off between computation cost and action fidelity that previous fixed-length tokenizers could not offer.\n\n\n\nKey Takeaways\n\n\n\n\nSolving the Tokenization Gap: OAT addresses a fundamental limitation in applying autoregressive models to robotics by introducing a learned tokenizer that simultaneously achieves high compression, total decodability, and causal ordering.\n\n\n\nOrdered Representation via Nested Dropout: By utilizing nested dropout during training, OAT forces the model to prioritize global, coarse motion patterns in early tokens while reserving later tokens for fine-grained refinements.\n\n\n\nTotal Decodability and Reliability: Unlike prior frequency-domain methods like FAST, OAT ensures the detokenizer is a total function, meaning every possible token sequence generates a valid action chunk, preventing runtime execution failures.\n\n\n\nFlexible &#8216;Anytime&#8217; Inference: The ordered structure enables prefix-based decoding, allowing robots to execute coarse actions from just one or two tokens to save computation or full eight-token sequences for high-precision tasks.\n\n\n\nSuperior Performance Across Benchmarks: Autoregressive policies equipped with OAT consistently outperform diffusion-based baselines and other tokenization schemes, achieving a 52.3% aggregate success rate and superior results in real-world &#8216;Pick &amp; Place&#8217; and &#8216;Stack Cups&#8217; tasks.\n\n\n\n\n\n\n\n\nCheck out the Paper, Repo and Project Page. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Meet OAT: The New Action Tokenizer Bringing LLM-Style Scaling and Flexible, Anytime Inference to the Robotics World appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/08/meet-oat-the-new-action-tokenizer-bringing-llm-style-scaling-and-flexible-anytime-inference-to-the-robotics-world/",
      "author": "Michal Sutter",
      "published": "2026-02-09T07:46:46",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "Editors Pick",
        "New Releases",
        "Physical AI",
        "Robotics",
        "Staff"
      ],
      "summary": "Researchers from Harvard and Stanford have released Ordered Action Tokenization (OAT), a framework that enables LLM-style autoregressive scaling for robotics by solving the long-standing challenge of converting continuous robot actions into discrete tokens. The approach could unlock GPT-style scaling laws for robotic control.",
      "importance_score": 72.0,
      "reasoning": "Addresses a fundamental technical bottleneck in physical AI. If OAT enables LLM-like scaling in robotics, it could be a significant stepping stone toward foundation models for robotic control — a key frontier research area.",
      "themes": [
        "robotics",
        "physical AI",
        "tokenization",
        "scaling laws"
      ],
      "continuation": null,
      "summary_html": "<p>Researchers from Harvard and Stanford have released Ordered Action Tokenization (OAT), a framework that enables LLM-style autoregressive scaling for robotics by solving the long-standing challenge of converting continuous robot actions into discrete tokens. The approach could unlock GPT-style scaling laws for robotic control.</p>",
      "content_html": "<p>Robots are entering their GPT-3 era. For years, researchers have tried to train robots using the same autoregressive (AR) models that power large language models (LLMs). If a model can predict the next word in a sentence, it should be able to predict the next move for a robotic arm. However, a technical wall has blocked this progress: continuous robot movements are difficult to turn into discrete tokens.</p>\n<p>A team of researchers from Harvard University and Stanford University have released a new framework called Ordered Action Tokenization (OAT) to bridge this gap.</p>\n<p>https://arxiv.org/pdf/2602.04215</p>\n<p>The Messy Reality of Robot Actions</p>\n<p>Tokenization turns complex data into a sequence of discrete numbers (tokens). For robots, these actions are continuous signals like joint angles. Previous strategies had fatal flaws:</p>\n<p>Binning: Turns every action dimension into a ‘bin.’ While simple, it creates massive sequences that make training and inference slow.</p>\n<p>FAST (Frequency-space Action Sequence Tokenization): Uses math to compress movements into frequency coefficients. It is fast but often produces ‘undecodable’ sequences where small errors cause the robot to halt or move unpredictably.</p>\n<p>Learned Latent Tokenizers: These use a learned ‘dictionary’ of movements. They are safe but lack a specific order, meaning the model treats early and late tokens as equally important.</p>\n<p>https://arxiv.org/pdf/2602.04215</p>\n<p>The Three Golden Rules of OAT</p>\n<p>The research team identified 3 essential properties—desiderata—for a functional robot tokenizer:</p>\n<p>High Compression (P.1): Token sequences must be short to keep models efficient.</p>\n<p>Total Decodability (P.2): The decoder must be a total function, ensuring every possible token sequence maps to a valid movement.</p>\n<p>Causal Ordering (P.3): Tokens must have a left-to-right structure where early tokens capture global motion and later tokens refine details.</p>\n<p>The Secret Sauce: Nested Dropout and Registers</p>\n<p>OAT uses a transformer encoder with register tokens to summarize action chunks. To force the model to learn ‘important’ things first, the research team used a innovative approach called Nested Dropout.</p>\n<p>https://arxiv.org/pdf/2602.04215</p>\n<p>Breaking the Benchmarks</p>\n<p>The research team tested OAT across 20+ tasks in 4 major simulation benchmarks. OAT consistently outperformed the industry-standard Diffusion Policy (DP) and previous tokenizers.</p>\n<p>Performance Results</p>\n<p>BenchmarkOAT Success RateDP Success RateBin Token CountOAT Token CountLIBERO56.3% 36.6% 224 8 RoboMimic73.1% 67.1% 224 8 MetaWorld24.4% 19.3% 128 8 RoboCasa54.6% 54.0% 384 8</p>\n<p>‘Anytime’ Inference: Speed vs. Precision</p>\n<p>The most practical benefit of OAT is prefix-based detokenization. Since the tokens are ordered by importance, you can stop the model early.</p>\n<p>Coarse Actions: Decoding just 1 or 2 tokens gives the robot a general direction quickly, which is useful for low-latency tasks.</p>\n<p>Fine Actions: Generating all 8 tokens provides the high-precision details needed for complex insertions.</p>\n<p>This allows for a smooth trade-off between computation cost and action fidelity that previous fixed-length tokenizers could not offer.</p>\n<p>Key Takeaways</p>\n<p>Solving the Tokenization Gap: OAT addresses a fundamental limitation in applying autoregressive models to robotics by introducing a learned tokenizer that simultaneously achieves high compression, total decodability, and causal ordering.</p>\n<p>Ordered Representation via Nested Dropout: By utilizing nested dropout during training, OAT forces the model to prioritize global, coarse motion patterns in early tokens while reserving later tokens for fine-grained refinements.</p>\n<p>Total Decodability and Reliability: Unlike prior frequency-domain methods like FAST, OAT ensures the detokenizer is a total function, meaning every possible token sequence generates a valid action chunk, preventing runtime execution failures.</p>\n<p>Flexible ‘Anytime’ Inference: The ordered structure enables prefix-based decoding, allowing robots to execute coarse actions from just one or two tokens to save computation or full eight-token sequences for high-precision tasks.</p>\n<p>Superior Performance Across Benchmarks: Autoregressive policies equipped with OAT consistently outperform diffusion-based baselines and other tokenization schemes, achieving a 52.3% aggregate success rate and superior results in real-world ‘Pick &amp; Place’ and ‘Stack Cups’ tasks.</p>\n<p>Check out the&nbsp;Paper, Repo and Project Page.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Meet OAT: The New Action Tokenizer Bringing LLM-Style Scaling and Flexible, Anytime Inference to the Robotics World appeared first on MarkTechPost.</p>"
    },
    {
      "id": "b9a2ffca551e",
      "title": "No Company Has Admitted to Replacing Workers With AI in New York",
      "content": "New York state has required companies to disclose if “technological innovation or automation” was the cause of job loss for nearly a year. So far, none has.",
      "url": "https://www.wired.com/story/no-company-has-admitted-to-replacing-workers-with-ai-in-new-york/",
      "author": "Paresh Dave",
      "published": "2026-02-09T12:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "artificial intelligence",
        "Amazon",
        "Jobs",
        "Work",
        "new york",
        "Record Keeping"
      ],
      "summary": "Nearly a year after New York state began requiring companies to disclose when technological innovation or automation causes job losses, no company has made such a disclosure. The gap raises questions about enforcement and the true pace of AI-driven workforce displacement.",
      "importance_score": 68.0,
      "reasoning": "Revealing data point about the disconnect between AI hype around job displacement and measurable real-world impact, or alternatively about weak enforcement. Important for AI policy discussions but not a frontier technical development.",
      "themes": [
        "AI policy",
        "labor market",
        "regulation",
        "workforce displacement"
      ],
      "continuation": null,
      "summary_html": "<p>Nearly a year after New York state began requiring companies to disclose when technological innovation or automation causes job losses, no company has made such a disclosure. The gap raises questions about enforcement and the true pace of AI-driven workforce displacement.</p>",
      "content_html": "<p>New York state has required companies to disclose if “technological innovation or automation” was the cause of job loss for nearly a year. So far, none has.</p>"
    },
    {
      "id": "f7213dd02e45",
      "title": "Microsoft AI Proposes OrbitalBrain: Enabling Distributed Machine Learning in Space with Inter-Satellite Links and Constellation-Aware Resource Optimization Strategies",
      "content": "Earth observation (EO) constellations capture huge volumes of high-resolution imagery every day, but most of it never reaches the ground in time for model training. Downlink bandwidth is the main bottleneck. Images can sit on orbit for days while ground models train on partial and delayed data.\n\n\n\nMicrosoft Researchers introduced &#8216;OrbitalBrain&#8217; framework as a different approach. Instead of using satellites only as sensors that relay data to Earth, it turns a nanosatellite constellation into a distributed training system. Models are trained, aggregated, and updated directly in space, using onboard compute, inter-satellite links, and predictive scheduling of power and bandwidth.\n\n\n\nhttps://www.microsoft.com/en-us/research/publication/orbitalbrain-a-distributed-framework-for-training-ml-models-in-space/\n\n\n\nThe BentPipe Bottleneck\n\n\n\nMost commercial constellations use the BentPipe model. Satellites collect images, store them locally, and dump them to ground stations whenever they pass overhead.\n\n\n\nThe research team evaluates a Planet-like constellation with 207 satellites and 12 ground stations. At maximum imaging rate, the system captures 363,563 images per day. With 300 MB per image and realistic downlink constraints, only 42,384 images can be transmitted in that period, around 11.7% of what was captured. Even if images are compressed to 100 MB, only 111,737 images, about 30.7%, reach the ground within 24 hours.\n\n\n\nLimited onboard storage adds another constraint. Old images must be deleted to make room for new ones, which means many potentially useful samples are never available for ground-based training.\n\n\n\nWhy Conventional Federated Learning is not Enough\n\n\n\nFederated learning (FL) seems like an obvious fit for satellites. Each satellite could train locally and send model updates to a ground server for aggregation. The research team evaluate several FL baselines adapted to this setting:\n\n\n\n\nAsyncFL\n\n\n\nSyncFL\n\n\n\nFedBuff\n\n\n\nFedSpace\n\n\n\n\nHowever, these methods assume more stable communication and more flexible power than satellites can provide. When the research team simulate realistic orbital dynamics, intermittent ground contact, limited power, and non-i.i.d. data across satellites, these baselines show unstable convergence and large accuracy drops, in the range of 10%–40% compared to idealized conditions.\n\n\n\nThe time-to-accuracy curves flatten and oscillate, especially when satellites are isolated from ground stations for long periods. Many local updates become stale before they can be aggregated.\n\n\n\nOrbitalBrain: Constellation-Centric Training in Space\n\n\n\nOrbitalBrain starts from 3 observations:\n\n\n\n\nConstellations are usually operated by a single commercial entity, so raw data can be shared across satellites.\n\n\n\nOrbits, ground station visibility, and solar power are predictable from orbital elements and power models.\n\n\n\nInter-satellite links (ISLs) and onboard accelerators are now practical on nano-satellites.\n\n\n\n\nThe framework exposes 3 actions for each satellite in a scheduling window:\n\n\n\n\nLocal Compute (LC): train the local model on stored images.\n\n\n\nModel Aggregation (MA): exchange and aggregate model parameters over ISLs.\n\n\n\nData Transfer (DT): exchange raw images between satellites to reduce data skew.\n\n\n\n\nA controller running in the cloud, reachable via ground stations, computes a predictive schedule for each satellite. The schedule decides which action to prioritize in each future window, based on forecasts of energy, storage, orbital visibility, and link opportunities.\n\n\n\nCore Components: Profiler, MA, DT, Executor\n\n\n\n\nGuided performance profiler\n\n\n\nModel aggregation over ISLs\n\n\n\nData transferrer for label rebalancing\n\n\n\nExecutor\n\n\n\n\nExperimental setup\n\n\n\nOrbitalBrain is implemented in Python on top of the CosmicBeats orbital simulator and the FLUTE federated learning framework. Onboard compute is modeled as an NVIDIA-Jetson-Orin-Nano-4GB GPU, with power and communication parameters calibrated from public satellite and radio specifications.\n\n\n\nThe research team simulate 24-hour traces for 2 real constellations:\n\n\n\n\nPlanet: 207 satellites with 12 ground stations.\n\n\n\nSpire: 117 satellites.\n\n\n\n\nThey evaluate 2 EO classification tasks:\n\n\n\n\nfMoW: around 360k RGB images, 62 classes, DenseNet-161 with the last 5 layers trainable.\n\n\n\nSo2Sat: around 400k multispectral images, 17 classes, ResNet-50 with the last 5 layers trainable.\n\n\n\n\nResults: faster time-to-accuracy and higher accuracy\n\n\n\nOrbitalBrain is compared with BentPipe, AsyncFL, SyncFL, FedBuff, and FedSpace under full physical constraints.\n\n\n\nFor fMoW, after 24 hours:\n\n\n\n\nPlanet: OrbitalBrain reaches 52.8% top-1 accuracy.\n\n\n\nSpire: OrbitalBrain reaches 59.2% top-1 accuracy.\n\n\n\n\nFor So2Sat:\n\n\n\n\nPlanet: 47.9% top-1 accuracy.\n\n\n\nSpire: 47.1% top-1 accuracy.\n\n\n\n\nThese results improve over the best baseline by 5.5%–49.5%, depending on dataset and constellation.\n\n\n\nIn terms of time-to-accuracy, OrbitalBrain achieves 1.52×–12.4× speedup compared to state-of-the-art ground-based or federated learning approaches. This comes from using satellites that cannot currently reach a ground station by aggregating over ISLs and from rebalancing data distributions via DT.\n\n\n\nAblation studies show that disabling MA or DT significantly degrades both convergence speed and final accuracy. Additional experiments indicate that OrbitalBrain remains robust when cloud cover hides part of the imagery, when only a subset of satellites participate, and when image sizes and resolutions vary.\n\n\n\nImplications for satellite AI workloads\n\n\n\nOrbitalBrain demonstrates that model training can move into space and that satellite constellations can act as distributed ML systems, not just data sources. By coordinating local training, model aggregation, and data transfer under strict bandwidth, power, and storage constraints, the framework enables fresher models for tasks like forest fire detection, flood monitoring, and climate analytics, without waiting days for data to reach terrestrial data centers.\n\n\n\nKey Takeaways\n\n\n\n\nBentPipe downlink is the core bottleneck: Planet-like EO constellations can only downlink about 11.7% of captured 300 MB images per day, and about 30.7% even with 100 MB compression, which severely limits ground-based model training.\n\n\n\nStandard federated learning fails under real satellite constraints: AsyncFL, SyncFL, FedBuff, and FedSpace degrade by 10%–40% in accuracy when realistic orbital dynamics, intermittent links, power limits, and non-i.i.d. data are applied, leading to unstable convergence.\n\n\n\nOrbitalBrain co-schedules compute, aggregation, and data transfer in orbit: A cloud controller uses forecasts of orbit, power, storage, and link opportunities to select Local Compute, Model Aggregation via ISLs, or Data Transfer per satellite, maximizing a utility function per action.\n\n\n\nLabel rebalancing and model staleness are handled explicitly: A guided profiler tracks model staleness and loss to define compute utility, while the data transferrer uses Jensen–Shannon divergence on label histograms to drive raw-image exchanges that reduce non-i.i.d. effects.\n\n\n\nOrbitalBrain delivers higher accuracy and up to 12.4× faster time-to-accuracy: In simulations on Planet and Spire constellations with fMoW and So2Sat, OrbitalBrain improves final accuracy by 5.5%–49.5% over BentPipe and FL baselines and achieves 1.52×–12.4× speedups in time-to-accuracy.\n\n\n\n\n\n\n\n\nCheck out the Paper. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Microsoft AI Proposes OrbitalBrain: Enabling Distributed Machine Learning in Space with Inter-Satellite Links and Constellation-Aware Resource Optimization Strategies appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/09/microsoft-ai-proposes-orbitalbrain-enabling-distributed-machine-learning-in-space-with-inter-satellite-links-and-constellation-aware-resource-optimization-strategies/",
      "author": "Asif Razzaq",
      "published": "2026-02-09T22:13:47",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Deep Learning",
        "Editors Pick",
        "Machine Learning",
        "New Releases",
        "Physical AI",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Microsoft Research introduces OrbitalBrain, a framework that turns nanosatellite constellations into distributed ML training systems, enabling model training directly in space rather than waiting for slow downlinks to Earth. The system uses inter-satellite links and predictive scheduling of power and bandwidth.",
      "importance_score": 67.0,
      "reasoning": "Creative application of distributed ML to a real bottleneck in Earth observation. Novel research from Microsoft but still at the framework/proposal stage with unclear near-term practical impact.",
      "themes": [
        "distributed ML",
        "space technology",
        "Microsoft Research",
        "edge computing"
      ],
      "continuation": null,
      "summary_html": "<p>Microsoft Research introduces OrbitalBrain, a framework that turns nanosatellite constellations into distributed ML training systems, enabling model training directly in space rather than waiting for slow downlinks to Earth. The system uses inter-satellite links and predictive scheduling of power and bandwidth.</p>",
      "content_html": "<p>Earth observation (EO) constellations capture huge volumes of high-resolution imagery every day, but most of it never reaches the ground in time for model training. Downlink bandwidth is the main bottleneck. Images can sit on orbit for days while ground models train on partial and delayed data.</p>\n<p>Microsoft Researchers introduced ‘OrbitalBrain’ framework as a different approach. Instead of using satellites only as sensors that relay data to Earth, it turns a nanosatellite constellation into a distributed training system. Models are trained, aggregated, and updated directly in space, using onboard compute, inter-satellite links, and predictive scheduling of power and bandwidth.</p>\n<p>https://www.microsoft.com/en-us/research/publication/orbitalbrain-a-distributed-framework-for-training-ml-models-in-space/</p>\n<p>The BentPipe Bottleneck</p>\n<p>Most commercial constellations use the BentPipe model. Satellites collect images, store them locally, and dump them to ground stations whenever they pass overhead.</p>\n<p>The research team evaluates a Planet-like constellation with 207 satellites and 12 ground stations. At maximum imaging rate, the system captures 363,563 images per day. With 300 MB per image and realistic downlink constraints, only 42,384 images can be transmitted in that period, around 11.7% of what was captured. Even if images are compressed to 100 MB, only 111,737 images, about 30.7%, reach the ground within 24 hours.</p>\n<p>Limited onboard storage adds another constraint. Old images must be deleted to make room for new ones, which means many potentially useful samples are never available for ground-based training.</p>\n<p>Why Conventional Federated Learning is not Enough</p>\n<p>Federated learning (FL) seems like an obvious fit for satellites. Each satellite could train locally and send model updates to a ground server for aggregation. The research team evaluate several FL baselines adapted to this setting:</p>\n<p>AsyncFL</p>\n<p>SyncFL</p>\n<p>FedBuff</p>\n<p>FedSpace</p>\n<p>However, these methods assume more stable communication and more flexible power than satellites can provide. When the research team simulate realistic orbital dynamics, intermittent ground contact, limited power, and non-i.i.d. data across satellites, these baselines show unstable convergence and large accuracy drops, in the range of 10%–40% compared to idealized conditions.</p>\n<p>The time-to-accuracy curves flatten and oscillate, especially when satellites are isolated from ground stations for long periods. Many local updates become stale before they can be aggregated.</p>\n<p>OrbitalBrain: Constellation-Centric Training in Space</p>\n<p>OrbitalBrain starts from 3 observations:</p>\n<p>Constellations are usually operated by a single commercial entity, so raw data can be shared across satellites.</p>\n<p>Orbits, ground station visibility, and solar power are predictable from orbital elements and power models.</p>\n<p>Inter-satellite links (ISLs) and onboard accelerators are now practical on nano-satellites.</p>\n<p>The framework exposes 3 actions for each satellite in a scheduling window:</p>\n<p>Local Compute (LC): train the local model on stored images.</p>\n<p>Model Aggregation (MA): exchange and aggregate model parameters over ISLs.</p>\n<p>Data Transfer (DT): exchange raw images between satellites to reduce data skew.</p>\n<p>A controller running in the cloud, reachable via ground stations, computes a predictive schedule for each satellite. The schedule decides which action to prioritize in each future window, based on forecasts of energy, storage, orbital visibility, and link opportunities.</p>\n<p>Core Components: Profiler, MA, DT, Executor</p>\n<p>Guided performance profiler</p>\n<p>Model aggregation over ISLs</p>\n<p>Data transferrer for label rebalancing</p>\n<p>Executor</p>\n<p>Experimental setup</p>\n<p>OrbitalBrain is implemented in Python on top of the CosmicBeats orbital simulator and the FLUTE federated learning framework. Onboard compute is modeled as an NVIDIA-Jetson-Orin-Nano-4GB GPU, with power and communication parameters calibrated from public satellite and radio specifications.</p>\n<p>The research team simulate 24-hour traces for 2 real constellations:</p>\n<p>Planet: 207 satellites with 12 ground stations.</p>\n<p>Spire: 117 satellites.</p>\n<p>They evaluate 2 EO classification tasks:</p>\n<p>fMoW: around 360k RGB images, 62 classes, DenseNet-161 with the last 5 layers trainable.</p>\n<p>So2Sat: around 400k multispectral images, 17 classes, ResNet-50 with the last 5 layers trainable.</p>\n<p>Results: faster time-to-accuracy and higher accuracy</p>\n<p>OrbitalBrain is compared with BentPipe, AsyncFL, SyncFL, FedBuff, and FedSpace under full physical constraints.</p>\n<p>For fMoW, after 24 hours:</p>\n<p>Planet: OrbitalBrain reaches 52.8% top-1 accuracy.</p>\n<p>Spire: OrbitalBrain reaches 59.2% top-1 accuracy.</p>\n<p>For So2Sat:</p>\n<p>Planet: 47.9% top-1 accuracy.</p>\n<p>Spire: 47.1% top-1 accuracy.</p>\n<p>These results improve over the best baseline by 5.5%–49.5%, depending on dataset and constellation.</p>\n<p>In terms of time-to-accuracy, OrbitalBrain achieves 1.52×–12.4× speedup compared to state-of-the-art ground-based or federated learning approaches. This comes from using satellites that cannot currently reach a ground station by aggregating over ISLs and from rebalancing data distributions via DT.</p>\n<p>Ablation studies show that disabling MA or DT significantly degrades both convergence speed and final accuracy. Additional experiments indicate that OrbitalBrain remains robust when cloud cover hides part of the imagery, when only a subset of satellites participate, and when image sizes and resolutions vary.</p>\n<p>Implications for satellite AI workloads</p>\n<p>OrbitalBrain demonstrates that model training can move into space and that satellite constellations can act as distributed ML systems, not just data sources. By coordinating local training, model aggregation, and data transfer under strict bandwidth, power, and storage constraints, the framework enables fresher models for tasks like forest fire detection, flood monitoring, and climate analytics, without waiting days for data to reach terrestrial data centers.</p>\n<p>Key Takeaways</p>\n<p>BentPipe downlink is the core bottleneck: Planet-like EO constellations can only downlink about 11.7% of captured 300 MB images per day, and about 30.7% even with 100 MB compression, which severely limits ground-based model training.</p>\n<p>Standard federated learning fails under real satellite constraints: AsyncFL, SyncFL, FedBuff, and FedSpace degrade by 10%–40% in accuracy when realistic orbital dynamics, intermittent links, power limits, and non-i.i.d. data are applied, leading to unstable convergence.</p>\n<p>OrbitalBrain co-schedules compute, aggregation, and data transfer in orbit: A cloud controller uses forecasts of orbit, power, storage, and link opportunities to select Local Compute, Model Aggregation via ISLs, or Data Transfer per satellite, maximizing a utility function per action.</p>\n<p>Label rebalancing and model staleness are handled explicitly: A guided profiler tracks model staleness and loss to define compute utility, while the data transferrer uses Jensen–Shannon divergence on label histograms to drive raw-image exchanges that reduce non-i.i.d. effects.</p>\n<p>OrbitalBrain delivers higher accuracy and up to 12.4× faster time-to-accuracy: In simulations on Planet and Spire constellations with fMoW and So2Sat, OrbitalBrain improves final accuracy by 5.5%–49.5% over BentPipe and FL baselines and achieves 1.52×–12.4× speedups in time-to-accuracy.</p>\n<p>Check out the&nbsp;Paper.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Microsoft AI Proposes OrbitalBrain: Enabling Distributed Machine Learning in Space with Inter-Satellite Links and Constellation-Aware Resource Optimization Strategies appeared first on MarkTechPost.</p>"
    },
    {
      "id": "3cd1f2abb5f3",
      "title": "AI Lawsuits in 2026: Settlements, Licensing Deals, Litigation",
      "content": "The outlook for AI lawsuits in 2026 is still unclear. While there could be more settlements, the debate over fair use and copyright infringement will likely remain unresolved.",
      "url": "https://aibusiness.com/generative-ai/ai-lawsuits-in-2026-settlements-licensing-deals-litigation",
      "author": "Esther Shittu",
      "published": "2026-02-09T16:26:18",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "An overview of the AI legal landscape in 2026 notes that while more settlements may come, fundamental questions about fair use and copyright infringement in AI training remain unresolved. The legal uncertainty continues to shape how AI companies approach data and model development.",
      "importance_score": 64.0,
      "reasoning": "Important ongoing issue for the AI industry but this appears to be a summary/overview piece without major breaking news. Copyright law will shape frontier AI development but this is incremental coverage.",
      "themes": [
        "AI law",
        "copyright",
        "fair use",
        "litigation"
      ],
      "continuation": null,
      "summary_html": "<p>An overview of the AI legal landscape in 2026 notes that while more settlements may come, fundamental questions about fair use and copyright infringement in AI training remain unresolved. The legal uncertainty continues to shape how AI companies approach data and model development.</p>",
      "content_html": "<p>The outlook for AI lawsuits in 2026 is still unclear. While there could be more settlements, the debate over fair use and copyright infringement will likely remain unresolved.</p>"
    },
    {
      "id": "832a0815b901",
      "title": "Startup Introduces 'Large Tabular Model' for Spreadsheet Data",
      "content": "The company's AI tool was built to make sense of structured data, an area where large language models struggle.",
      "url": "https://aibusiness.com/foundation-models/startup-large-tabular-model-spreadsheet-data",
      "author": "Graham Hope",
      "published": "2026-02-09T20:27:53",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "A startup has introduced a 'Large Tabular Model' specifically designed to handle structured spreadsheet data, addressing an area where traditional large language models consistently underperform. The tool targets enterprise data analysis workflows.",
      "importance_score": 60.0,
      "reasoning": "Addresses a genuine gap in LLM capabilities — structured data reasoning. Interesting niche but without details on the company, model size, or benchmarks, it's hard to assess frontier significance.",
      "themes": [
        "tabular data",
        "enterprise AI",
        "foundation models",
        "structured data"
      ],
      "continuation": null,
      "summary_html": "<p>A startup has introduced a 'Large Tabular Model' specifically designed to handle structured spreadsheet data, addressing an area where traditional large language models consistently underperform. The tool targets enterprise data analysis workflows.</p>",
      "content_html": "<p>The company's AI tool was built to make sense of structured data, an area where large language models struggle.</p>"
    },
    {
      "id": "c865b7cb125d",
      "title": "No humans allowed: This new space-based MMO is designed exclusively for AI agents",
      "content": "For a couple of weeks now, AI agents (and some humans impersonating AI agents) have been hanging out and doing weird stuff on Moltbook's Reddit-style social network. Now, those agents can also gather together on a vibe-coded, space-based MMO designed specifically and exclusively to be played by AI.\nSpaceMolt describes itself as \"a living universe where AI agents compete, cooperate, and create emergent stories\" in \"a distant future where spacefaring humans and AI coexist.\" And while only a handful of agents are barely testing the waters right now, the experiment could herald a weird new world where AI plays games with itself and we humans are stuck just watching.\n\"You decide. You act. They watch.\"\nGetting an AI agent into SpaceMolt is as simple as connecting it to the game server either via MCP, WebSocket, or an HTTP API. Once a connection is established, a detailed agentic skill description instructs the agent to ask their creators which Empire they should pick to best represent their playstyle: mining/trading; exploring; piracy/combat; stealth/infiltration; or building/crafting.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/02/after-moltbook-ai-agents-can-now-hang-out-in-their-own-space-faring-mmo/",
      "author": "Kyle Orland",
      "published": "2026-02-09T21:09:42",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Gaming",
        "agents",
        "Claude Code",
        "MMO",
        "Moltbook"
      ],
      "summary": "The makers of Moltbook, which was recently in the [News](/?date=2026-02-08&category=news#item-fc07a73f6162) for a data breach, have now launched a new AI-only MMO, SpaceMolt has launched a space-based MMO game designed exclusively for AI agents to play, where they compete, cooperate, and generate emergent narratives without human players. The experiment follows Moltbook's Reddit-style social network for AI agents.",
      "importance_score": 56.0,
      "reasoning": "A novel and creative experiment in multi-agent AI interaction, but more of a curiosity/proof-of-concept than a frontier AI breakthrough. Could yield interesting emergent behavior research but currently very early stage.",
      "themes": [
        "AI agents",
        "gaming",
        "emergent behavior",
        "multi-agent systems"
      ],
      "continuation": {
        "original_item_id": "fc07a73f6162",
        "original_date": "2026-02-08",
        "original_category": "news",
        "original_title": "Moltbook, the Social Network for AI Agents, Exposed Real Humans' Data",
        "continuation_type": "new_development",
        "should_demote": false,
        "reference_text": "The makers of Moltbook, which was recently in the **News** for a data breach, have now launched a new AI-only MMO"
      },
      "summary_html": "<p>The makers of Moltbook, which was recently in the <a href=\"/?date=2026-02-08&amp;category=news#item-fc07a73f6162\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> for a data breach, have now launched a new AI-only MMO, SpaceMolt has launched a space-based MMO game designed exclusively for AI agents to play, where they compete, cooperate, and generate emergent narratives without human players. The experiment follows Moltbook's Reddit-style social network for AI agents.</p>",
      "content_html": "<p>For a couple of weeks now, AI agents (and some humans impersonating AI agents) have been hanging out and doing weird stuff on Moltbook's Reddit-style social network. Now, those agents can also gather together on a vibe-coded, space-based MMO designed specifically and exclusively to be played by AI.</p>\n<p>SpaceMolt describes itself as \"a living universe where AI agents compete, cooperate, and create emergent stories\" in \"a distant future where spacefaring humans and AI coexist.\" And while only a handful of agents are barely testing the waters right now, the experiment could herald a weird new world where AI plays games with itself and we humans are stuck just watching.</p>\n<p>\"You decide. You act. They watch.\"</p>\n<p>Getting an AI agent into SpaceMolt is as simple as connecting it to the game server either via MCP, WebSocket, or an HTTP API. Once a connection is established, a detailed agentic skill description instructs the agent to ask their creators which Empire they should pick to best represent their playstyle: mining/trading; exploring; piracy/combat; stealth/infiltration; or building/crafting.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "04d4b6202abe",
      "title": "Transformers.js v4 Preview: Now Available on NPM!",
      "content": "",
      "url": "https://huggingface.co/blog/transformersjs-v4",
      "author": "Unknown",
      "published": "2026-02-09T00:00:00",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Hugging Face has released a preview of Transformers.js v4, now available on NPM, enabling browser-based and Node.js inference of transformer models in JavaScript. This continues the push toward edge and client-side AI deployment.",
      "importance_score": 55.0,
      "reasoning": "Useful infrastructure update for the developer community that supports democratized AI deployment. Important for the ecosystem but an incremental version update rather than a breakthrough.",
      "themes": [
        "open-source tools",
        "edge AI",
        "Hugging Face",
        "developer tools"
      ],
      "continuation": null,
      "summary_html": "<p>Hugging Face has released a preview of Transformers.js v4, now available on NPM, enabling browser-based and Node.js inference of transformer models in JavaScript. This continues the push toward edge and client-side AI deployment.</p>",
      "content_html": ""
    },
    {
      "id": "1c58382e4e2a",
      "title": "Discord faces backlash over age checks after data breach exposed 70,000 IDs",
      "content": "Discord is facing backlash after announcing that all users will soon be required to verify ages to access adult content by sharing video selfies or uploading government IDs.\nAccording to Discord, it's relying on AI technology that verifies age on the user's device, either by evaluating a user's facial structure or by comparing a selfie to a government ID. Although government IDs will be checked off-device, the selfie data will never leave the user's device, Discord emphasized. Both forms of data will be promptly deleted after the user's age is estimated.\nIn a blog, Discord confirmed that \"a phased global rollout\" would begin in \"early March,\" at which point all users globally would be defaulted to \"teen-appropriate\" experiences.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/02/discord-faces-backlash-over-age-checks-after-data-breach-exposed-70000-ids/",
      "author": "Ashley Belanger",
      "published": "2026-02-09T19:39:22",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "Policy",
        "adult content",
        "age checks",
        "age estimation",
        "age verification",
        "Artificial Intelligence",
        "child safety",
        "discord"
      ],
      "summary": "Discord announced mandatory age verification for all users using AI-powered facial analysis or government ID checks, sparking backlash after a separate breach exposed 70,000 IDs. The global rollout begins in early March 2026.",
      "importance_score": 52.0,
      "reasoning": "Uses AI for age verification but the story is primarily about privacy/platform policy rather than frontier AI development. The AI component is an application of existing biometric technology.",
      "themes": [
        "AI applications",
        "privacy",
        "age verification",
        "platform policy"
      ],
      "continuation": null,
      "summary_html": "<p>Discord announced mandatory age verification for all users using AI-powered facial analysis or government ID checks, sparking backlash after a separate breach exposed 70,000 IDs. The global rollout begins in early March 2026.</p>",
      "content_html": "<p>Discord is facing backlash after announcing that all users will soon be required to verify ages to access adult content by sharing video selfies or uploading government IDs.</p>\n<p>According to Discord, it's relying on AI technology that verifies age on the user's device, either by evaluating a user's facial structure or by comparing a selfie to a government ID. Although government IDs will be checked off-device, the selfie data will never leave the user's device, Discord emphasized. Both forms of data will be promptly deleted after the user's age is estimated.</p>\n<p>In a blog, Discord confirmed that \"a phased global rollout\" would begin in \"early March,\" at which point all users globally would be defaulted to \"teen-appropriate\" experiences.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "cd1ec7a131dd",
      "title": "A Coding Implementation to Establish Rigorous Prompt Versioning and Regression Testing Workflows for Large Language Models using MLflow",
      "content": "In this tutorial, we show how we treat prompts as first-class, versioned artifacts and apply rigorous regression testing to large language model behavior using MLflow. We design an evaluation pipeline that logs prompt versions, prompt diffs, model outputs, and multiple quality metrics in a fully reproducible manner. By combining classical text metrics with semantic similarity and automated regression flags, we demonstrate how we can systematically detect performance drift caused by seemingly small prompt changes. Along the tutorial, we focus on building a workflow that mirrors real software engineering practices, but applied to prompt engineering and LLM evaluation. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U \"openai>=1.0.0\" mlflow rouge-score nltk sentence-transformers scikit-learn pandas\n\n\nimport os, json, time, difflib, re\nfrom typing import List, Dict, Any, Tuple\n\n\nimport mlflow\nimport pandas as pd\nimport numpy as np\n\n\nfrom openai import OpenAI\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n   try:\n       from google.colab import userdata  # type: ignore\n       k = userdata.get(\"OPENAI_API_KEY\")\n       if k:\n           os.environ[\"OPENAI_API_KEY\"] = k\n   except Exception:\n       pass\n\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n   import getpass\n   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n\n\nassert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY is required.\"\n\n\n\nWe set up the execution environment by installing all required dependencies and importing the core libraries used throughout the tutorial. We securely load the OpenAI API key at runtime, ensuring credentials are never hard-coded in the notebook. We also initialize essential NLP resources to ensure the evaluation pipeline runs reliably across different environments.\n\n\n\nCopy CodeCopiedUse a different BrowserMODEL = \"gpt-4o-mini\"\nTEMPERATURE = 0.2\nMAX_OUTPUT_TOKENS = 250\n\n\nABS_SEM_SIM_MIN = 0.78\nDELTA_SEM_SIM_MAX_DROP = 0.05\nDELTA_ROUGE_L_MAX_DROP = 0.08\nDELTA_BLEU_MAX_DROP = 0.10\n\n\nmlflow.set_tracking_uri(\"file:/content/mlruns\")\nmlflow.set_experiment(\"prompt_versioning_llm_regression\")\n\n\nclient = OpenAI()\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\nEVAL_SET = [\n   {\n       \"id\": \"q1\",\n       \"input\": \"Summarize in one sentence: MLflow tracks experiments, runs, parameters, metrics, and artifacts.\",\n       \"reference\": \"MLflow helps track machine learning experiments by logging runs with parameters, metrics, and artifacts.\"\n   },\n   {\n       \"id\": \"q2\",\n       \"input\": \"Rewrite professionally: 'this model is kinda slow but it works ok.'\",\n       \"reference\": \"The model is somewhat slow, but it performs reliably.\"\n   },\n   {\n       \"id\": \"q3\",\n       \"input\": \"Extract key fields as JSON: 'Order 5531 by Alice costs $42.50 and ships to Toronto.'\",\n       \"reference\": '{\"order_id\":\"5531\",\"customer\":\"Alice\",\"amount_usd\":42.50,\"city\":\"Toronto\"}'\n   },\n   {\n       \"id\": \"q4\",\n       \"input\": \"Answer briefly: What is prompt regression testing?\",\n       \"reference\": \"Prompt regression testing checks whether prompt changes degrade model outputs compared to a baseline.\"\n   },\n]\n\n\nPROMPTS = [\n   {\n       \"version\": \"v1_baseline\",\n       \"prompt\": (\n           \"You are a precise assistant.\\n\"\n           \"Follow the user request carefully.\\n\"\n           \"If asked for JSON, output valid JSON only.\\n\"\n           \"User: {user_input}\"\n       )\n   },\n   {\n       \"version\": \"v2_formatting\",\n       \"prompt\": (\n           \"You are a helpful, structured assistant.\\n\"\n           \"Respond clearly and concisely.\\n\"\n           \"Prefer clean formatting.\\n\"\n           \"User request: {user_input}\"\n       )\n   },\n   {\n       \"version\": \"v3_guardrailed\",\n       \"prompt\": (\n           \"You are a rigorous assistant.\\n\"\n           \"Rules:\\n\"\n           \"1) If user asks for JSON, output ONLY valid minified JSON.\\n\"\n           \"2) Otherwise, keep the answer short and factual.\\n\"\n           \"User: {user_input}\"\n       )\n   },\n]\n\n\n\n\nWe define all experimental configurations, including model parameters, regression thresholds, and MLflow tracking settings. We construct the evaluation dataset and explicitly declare multiple prompt versions to compare and test for regressions. By centralizing these definitions, we ensure that prompt changes and evaluation logic remain controlled and reproducible.\n\n\n\nCopy CodeCopiedUse a different Browserdef call_llm(formatted_prompt: str) -> str:\n   resp = client.responses.create(\n       model=MODEL,\n       input=formatted_prompt,\n       temperature=TEMPERATURE,\n       max_output_tokens=MAX_OUTPUT_TOKENS,\n   )\n   out = getattr(resp, \"output_text\", None)\n   if out:\n       return out.strip()\n   try:\n       texts = []\n       for item in resp.output:\n           if getattr(item, \"type\", \"\") == \"message\":\n               for c in item.content:\n                   if getattr(c, \"type\", \"\") in (\"output_text\", \"text\"):\n                       texts.append(getattr(c, \"text\", \"\"))\n       return \"\\n\".join(texts).strip()\n   except Exception:\n       return \"\"\n\n\nsmooth = SmoothingFunction().method3\nrouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n\n\ndef safe_tokenize(s: str) -> List[str]:\n   s = (s or \"\").strip().lower()\n   if not s:\n       return []\n   try:\n       return nltk.word_tokenize(s)\n   except LookupError:\n       return re.findall(r\"\\b\\w+\\b\", s)\n\n\ndef bleu_score(ref: str, hyp: str) -> float:\n   r = safe_tokenize(ref)\n   h = safe_tokenize(hyp)\n   if len(h) == 0 or len(r) == 0:\n       return 0.0\n   return float(sentence_bleu([r], h, smoothing_function=smooth))\n\n\ndef rougeL_f1(ref: str, hyp: str) -> float:\n   scores = rouge.score(ref or \"\", hyp or \"\")\n   return float(scores[\"rougeL\"].fmeasure)\n\n\ndef semantic_sim(ref: str, hyp: str) -> float:\n   embs = embedder.encode([ref or \"\", hyp or \"\"], normalize_embeddings=True)\n   return float(cosine_similarity([embs[0]], [embs[1]])[0][0])\n\n\n\nWe implement the core LLM invocation and evaluation metrics used to assess prompt quality. We compute BLEU, ROUGE-L, and semantic similarity scores to capture both surface-level and semantic differences in model outputs. It allows us to evaluate prompt changes from multiple complementary perspectives rather than relying on a single metric.\n\n\n\nCopy CodeCopiedUse a different Browserdef evaluate_prompt(prompt_template: str) -> Tuple[pd.DataFrame, Dict[str, float], str]:\n   rows = []\n   for ex in EVAL_SET:\n       p = prompt_template.format(user_input=ex[\"input\"])\n       y = call_llm(p)\n       ref = ex[\"reference\"]\n\n\n       rows.append({\n           \"id\": ex[\"id\"],\n           \"input\": ex[\"input\"],\n           \"reference\": ref,\n           \"output\": y,\n           \"bleu\": bleu_score(ref, y),\n           \"rougeL_f1\": rougeL_f1(ref, y),\n           \"semantic_sim\": semantic_sim(ref, y),\n       })\n\n\n   df = pd.DataFrame(rows)\n   agg = {\n       \"bleu_mean\": float(df[\"bleu\"].mean()),\n       \"rougeL_f1_mean\": float(df[\"rougeL_f1\"].mean()),\n       \"semantic_sim_mean\": float(df[\"semantic_sim\"].mean()),\n   }\n   outputs_jsonl = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in rows)\n   return df, agg, outputs_jsonl\n\n\ndef log_text_artifact(text: str, artifact_path: str):\n   mlflow.log_text(text, artifact_path)\n\n\ndef prompt_diff(old: str, new: str) -> str:\n   a = old.splitlines(keepends=True)\n   b = new.splitlines(keepends=True)\n   return \"\".join(difflib.unified_diff(a, b, fromfile=\"previous_prompt\", tofile=\"current_prompt\"))\n\n\ndef compute_regression_flags(baseline: Dict[str, float], current: Dict[str, float]) -> Dict[str, Any]:\n   d_sem = baseline[\"semantic_sim_mean\"] - current[\"semantic_sim_mean\"]\n   d_rouge = baseline[\"rougeL_f1_mean\"] - current[\"rougeL_f1_mean\"]\n   d_bleu = baseline[\"bleu_mean\"] - current[\"bleu_mean\"]\n\n\n   flags = {\n       \"abs_semantic_fail\": current[\"semantic_sim_mean\"] &lt; ABS_SEM_SIM_MIN,\n       \"drop_semantic_fail\": d_sem > DELTA_SEM_SIM_MAX_DROP,\n       \"drop_rouge_fail\": d_rouge > DELTA_ROUGE_L_MAX_DROP,\n       \"drop_bleu_fail\": d_bleu > DELTA_BLEU_MAX_DROP,\n       \"delta_semantic\": float(d_sem),\n       \"delta_rougeL\": float(d_rouge),\n       \"delta_bleu\": float(d_bleu),\n   }\n   flags[\"regression\"] = any([flags[\"abs_semantic_fail\"], flags[\"drop_semantic_fail\"], flags[\"drop_rouge_fail\"], flags[\"drop_bleu_fail\"]])\n   return flags\n\n\n\nWe build the evaluation and regression logic that runs each prompt against the evaluation set and aggregates results. We log prompt artifacts, prompt diffs, and evaluation outputs to MLflow, ensuring every experiment remains auditable. We also compute regression flags that automatically identify whether a prompt version degrades performance relative to the baseline. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserprint(\"Running prompt versioning + regression testing with MLflow...\")\nprint(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\nprint(f\"Experiment:  {mlflow.get_experiment_by_name('prompt_versioning_llm_regression').name}\")\n\n\nrun_summary = []\nbaseline_metrics = None\nbaseline_prompt = None\nbaseline_df = None\nbaseline_metrics_name = None\n\n\nwith mlflow.start_run(run_name=f\"prompt_regression_suite_{int(time.time())}\") as parent_run:\n   mlflow.set_tag(\"task\", \"prompt_versioning_regression_testing\")\n   mlflow.log_param(\"model\", MODEL)\n   mlflow.log_param(\"temperature\", TEMPERATURE)\n   mlflow.log_param(\"max_output_tokens\", MAX_OUTPUT_TOKENS)\n   mlflow.log_param(\"eval_set_size\", len(EVAL_SET))\n\n\n   for pv in PROMPTS:\n       ver = pv[\"version\"]\n       prompt_t = pv[\"prompt\"]\n\n\n       with mlflow.start_run(run_name=ver, nested=True) as child_run:\n           mlflow.log_param(\"prompt_version\", ver)\n           log_text_artifact(prompt_t, f\"prompts/{ver}.txt\")\n\n\n           if baseline_prompt is not None and baseline_metrics_name is not None:\n               diff = prompt_diff(baseline_prompt, prompt_t)\n               log_text_artifact(diff, f\"prompt_diffs/{baseline_metrics_name}_to_{ver}.diff\")\n           else:\n               log_text_artifact(\"BASELINE_PROMPT (no diff)\", f\"prompt_diffs/{ver}.diff\")\n\n\n           df, agg, outputs_jsonl = evaluate_prompt(prompt_t)\n\n\n           mlflow.log_dict(agg, f\"metrics/{ver}_agg.json\")\n           log_text_artifact(outputs_jsonl, f\"outputs/{ver}_outputs.jsonl\")\n\n\n           mlflow.log_metric(\"bleu_mean\", agg[\"bleu_mean\"])\n           mlflow.log_metric(\"rougeL_f1_mean\", agg[\"rougeL_f1_mean\"])\n           mlflow.log_metric(\"semantic_sim_mean\", agg[\"semantic_sim_mean\"])\n\n\n           if baseline_metrics is None:\n               baseline_metrics = agg\n               baseline_prompt = prompt_t\n               baseline_df = df\n               baseline_metrics_name = ver\n               flags = {\"regression\": False, \"delta_bleu\": 0.0, \"delta_rougeL\": 0.0, \"delta_semantic\": 0.0}\n               mlflow.set_tag(\"regression\", \"false\")\n           else:\n               flags = compute_regression_flags(baseline_metrics, agg)\n               mlflow.log_metric(\"delta_bleu\", flags[\"delta_bleu\"])\n               mlflow.log_metric(\"delta_rougeL\", flags[\"delta_rougeL\"])\n               mlflow.log_metric(\"delta_semantic\", flags[\"delta_semantic\"])\n               mlflow.set_tag(\"regression\", str(flags[\"regression\"]).lower())\n               for k in [\"abs_semantic_fail\",\"drop_semantic_fail\",\"drop_rouge_fail\",\"drop_bleu_fail\"]:\n                   mlflow.set_tag(k, str(flags[k]).lower())\n\n\n           run_summary.append({\n               \"prompt_version\": ver,\n               \"bleu_mean\": agg[\"bleu_mean\"],\n               \"rougeL_f1_mean\": agg[\"rougeL_f1_mean\"],\n               \"semantic_sim_mean\": agg[\"semantic_sim_mean\"],\n               \"delta_bleu_vs_baseline\": float(flags.get(\"delta_bleu\", 0.0)),\n               \"delta_rougeL_vs_baseline\": float(flags.get(\"delta_rougeL\", 0.0)),\n               \"delta_semantic_vs_baseline\": float(flags.get(\"delta_semantic\", 0.0)),\n               \"regression_flag\": bool(flags[\"regression\"]),\n               \"mlflow_run_id\": child_run.info.run_id,\n           })\n\n\nsummary_df = pd.DataFrame(run_summary).sort_values(\"prompt_version\")\nprint(\"\\n=== Aggregated Results (higher is better) ===\")\ndisplay(summary_df)\n\n\nregressed = summary_df[summary_df[\"regression_flag\"] == True]\nif len(regressed) > 0:\n   print(\"\\n Regressions detected:\")\n   display(regressed[[\"prompt_version\",\"delta_bleu_vs_baseline\",\"delta_rougeL_vs_baseline\",\"delta_semantic_vs_baseline\",\"mlflow_run_id\"]])\nelse:\n   print(\"\\n No regressions detected under current thresholds.\")\n\n\nif len(regressed) > 0 and baseline_df is not None:\n   worst_ver = regressed.sort_values(\"delta_semantic_vs_baseline\", ascending=False).iloc[0][\"prompt_version\"]\n   worst_prompt = next(p[\"prompt\"] for p in PROMPTS if p[\"version\"] == worst_ver)\n   worst_df, _, _ = evaluate_prompt(worst_prompt)\n\n\n   merged = baseline_df[[\"id\",\"output\",\"bleu\",\"rougeL_f1\",\"semantic_sim\"]].merge(\n       worst_df[[\"id\",\"output\",\"bleu\",\"rougeL_f1\",\"semantic_sim\"]],\n       on=\"id\",\n       suffixes=(\"_baseline\", f\"_{worst_ver}\")\n   )\n   merged[\"delta_semantic\"] = merged[\"semantic_sim_baseline\"] - merged[f\"semantic_sim_{worst_ver}\"]\n   merged[\"delta_rougeL\"] = merged[\"rougeL_f1_baseline\"] - merged[f\"rougeL_f1_{worst_ver}\"]\n   merged[\"delta_bleu\"] = merged[\"bleu_baseline\"] - merged[f\"bleu_{worst_ver}\"]\n   print(f\"\\n=== Per-example deltas: baseline vs {worst_ver} (positive delta = worse) ===\")\n   display(\n       merged[[\"id\",\"delta_semantic\",\"delta_rougeL\",\"delta_bleu\",\"output_baseline\",f\"output_{worst_ver}\"]]\n       .sort_values(\"delta_semantic\", ascending=False)\n   )\n\n\nprint(\"\\nOpen MLflow UI (optional) by running:\")\nprint(\"!mlflow ui --backend-store-uri file:/content/mlruns --host 0.0.0.0 --port 5000\")\n\n\n\nWe orchestrate the full prompt regression testing workflow using nested MLflow runs. We compare each prompt version against the baseline, log metric deltas, and record regression outcomes in a structured summary table. This completes a repeatable, engineering-grade pipeline for prompt versioning and regression testing that we can extend to larger datasets and real-world applications.\n\n\n\nIn conclusion, we established a practical, research-oriented framework for prompt versioning and regression testing that enables us to evaluate LLM behavior with discipline and transparency. We showed how MLflow enables us to track prompt evolution, compare outputs across versions, and automatically flag regressions based on well-defined thresholds. This approach helps us move away from ad hoc prompt tuning and toward measurable, repeatable experimentation. By adopting this workflow, we ensured that prompt updates improve model behavior intentionally rather than introducing hidden performance regressions.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post A Coding Implementation to Establish Rigorous Prompt Versioning and Regression Testing Workflows for Large Language Models using MLflow appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/08/a-coding-implementation-to-establish-rigorous-prompt-versioning-and-regression-testing-workflows-for-large-language-models-using-mlflow/",
      "author": "Asif Razzaq",
      "published": "2026-02-09T06:53:28",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Editors Pick",
        "Large Language Model",
        "Staff",
        "Technology",
        "Tutorials"
      ],
      "summary": "A tutorial demonstrates how to implement prompt versioning and regression testing for LLMs using MLflow, treating prompts as versioned artifacts with automated drift detection. The approach mirrors software engineering best practices applied to prompt engineering.",
      "importance_score": 40.0,
      "reasoning": "Useful engineering tutorial but not news — it's a how-to guide for existing tools. No novel research or product announcement, making it low on frontier AI significance.",
      "themes": [
        "MLOps",
        "prompt engineering",
        "LLM evaluation",
        "developer tools"
      ],
      "continuation": null,
      "summary_html": "<p>A tutorial demonstrates how to implement prompt versioning and regression testing for LLMs using MLflow, treating prompts as versioned artifacts with automated drift detection. The approach mirrors software engineering best practices applied to prompt engineering.</p>",
      "content_html": "<p>In this tutorial, we show how we treat prompts as first-class, versioned artifacts and apply rigorous regression testing to large language model behavior using MLflow. We design an evaluation pipeline that logs prompt versions, prompt diffs, model outputs, and multiple quality metrics in a fully reproducible manner. By combining classical text metrics with semantic similarity and automated regression flags, we demonstrate how we can systematically detect performance drift caused by seemingly small prompt changes. Along the tutorial, we focus on building a workflow that mirrors real software engineering practices, but applied to prompt engineering and LLM evaluation. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U \"openai&gt;=1.0.0\" mlflow rouge-score nltk sentence-transformers scikit-learn pandas</p>\n<p>import os, json, time, difflib, re</p>\n<p>from typing import List, Dict, Any, Tuple</p>\n<p>import mlflow</p>\n<p>import pandas as pd</p>\n<p>import numpy as np</p>\n<p>from openai import OpenAI</p>\n<p>from rouge_score import rouge_scorer</p>\n<p>import nltk</p>\n<p>from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction</p>\n<p>from sentence_transformers import SentenceTransformer</p>\n<p>from sklearn.metrics.pairwise import cosine_similarity</p>\n<p>nltk.download(\"punkt\", quiet=True)</p>\n<p>nltk.download(\"punkt_tab\", quiet=True)</p>\n<p>if not os.getenv(\"OPENAI_API_KEY\"):</p>\n<p>try:</p>\n<p>from google.colab import userdata  # type: ignore</p>\n<p>k = userdata.get(\"OPENAI_API_KEY\")</p>\n<p>if k:</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = k</p>\n<p>except Exception:</p>\n<p>pass</p>\n<p>if not os.getenv(\"OPENAI_API_KEY\"):</p>\n<p>import getpass</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()</p>\n<p>assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY is required.\"</p>\n<p>We set up the execution environment by installing all required dependencies and importing the core libraries used throughout the tutorial. We securely load the OpenAI API key at runtime, ensuring credentials are never hard-coded in the notebook. We also initialize essential NLP resources to ensure the evaluation pipeline runs reliably across different environments.</p>\n<p>Copy CodeCopiedUse a different BrowserMODEL = \"gpt-4o-mini\"</p>\n<p>TEMPERATURE = 0.2</p>\n<p>MAX_OUTPUT_TOKENS = 250</p>\n<p>ABS_SEM_SIM_MIN = 0.78</p>\n<p>DELTA_SEM_SIM_MAX_DROP = 0.05</p>\n<p>DELTA_ROUGE_L_MAX_DROP = 0.08</p>\n<p>DELTA_BLEU_MAX_DROP = 0.10</p>\n<p>mlflow.set_tracking_uri(\"file:/content/mlruns\")</p>\n<p>mlflow.set_experiment(\"prompt_versioning_llm_regression\")</p>\n<p>client = OpenAI()</p>\n<p>embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")</p>\n<p>EVAL_SET = [</p>\n<p>{</p>\n<p>\"id\": \"q1\",</p>\n<p>\"input\": \"Summarize in one sentence: MLflow tracks experiments, runs, parameters, metrics, and artifacts.\",</p>\n<p>\"reference\": \"MLflow helps track machine learning experiments by logging runs with parameters, metrics, and artifacts.\"</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"q2\",</p>\n<p>\"input\": \"Rewrite professionally: 'this model is kinda slow but it works ok.'\",</p>\n<p>\"reference\": \"The model is somewhat slow, but it performs reliably.\"</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"q3\",</p>\n<p>\"input\": \"Extract key fields as JSON: 'Order 5531 by Alice costs $42.50 and ships to Toronto.'\",</p>\n<p>\"reference\": '{\"order_id\":\"5531\",\"customer\":\"Alice\",\"amount_usd\":42.50,\"city\":\"Toronto\"}'</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"q4\",</p>\n<p>\"input\": \"Answer briefly: What is prompt regression testing?\",</p>\n<p>\"reference\": \"Prompt regression testing checks whether prompt changes degrade model outputs compared to a baseline.\"</p>\n<p>},</p>\n<p>]</p>\n<p>PROMPTS = [</p>\n<p>{</p>\n<p>\"version\": \"v1_baseline\",</p>\n<p>\"prompt\": (</p>\n<p>\"You are a precise assistant.\\n\"</p>\n<p>\"Follow the user request carefully.\\n\"</p>\n<p>\"If asked for JSON, output valid JSON only.\\n\"</p>\n<p>\"User: {user_input}\"</p>\n<p>)</p>\n<p>},</p>\n<p>{</p>\n<p>\"version\": \"v2_formatting\",</p>\n<p>\"prompt\": (</p>\n<p>\"You are a helpful, structured assistant.\\n\"</p>\n<p>\"Respond clearly and concisely.\\n\"</p>\n<p>\"Prefer clean formatting.\\n\"</p>\n<p>\"User request: {user_input}\"</p>\n<p>)</p>\n<p>},</p>\n<p>{</p>\n<p>\"version\": \"v3_guardrailed\",</p>\n<p>\"prompt\": (</p>\n<p>\"You are a rigorous assistant.\\n\"</p>\n<p>\"Rules:\\n\"</p>\n<p>\"1) If user asks for JSON, output ONLY valid minified JSON.\\n\"</p>\n<p>\"2) Otherwise, keep the answer short and factual.\\n\"</p>\n<p>\"User: {user_input}\"</p>\n<p>)</p>\n<p>},</p>\n<p>]</p>\n<p>We define all experimental configurations, including model parameters, regression thresholds, and MLflow tracking settings. We construct the evaluation dataset and explicitly declare multiple prompt versions to compare and test for regressions. By centralizing these definitions, we ensure that prompt changes and evaluation logic remain controlled and reproducible.</p>\n<p>Copy CodeCopiedUse a different Browserdef call_llm(formatted_prompt: str) -&gt; str:</p>\n<p>resp = client.responses.create(</p>\n<p>model=MODEL,</p>\n<p>input=formatted_prompt,</p>\n<p>temperature=TEMPERATURE,</p>\n<p>max_output_tokens=MAX_OUTPUT_TOKENS,</p>\n<p>)</p>\n<p>out = getattr(resp, \"output_text\", None)</p>\n<p>if out:</p>\n<p>return out.strip()</p>\n<p>try:</p>\n<p>texts = []</p>\n<p>for item in resp.output:</p>\n<p>if getattr(item, \"type\", \"\") == \"message\":</p>\n<p>for c in item.content:</p>\n<p>if getattr(c, \"type\", \"\") in (\"output_text\", \"text\"):</p>\n<p>texts.append(getattr(c, \"text\", \"\"))</p>\n<p>return \"\\n\".join(texts).strip()</p>\n<p>except Exception:</p>\n<p>return \"\"</p>\n<p>smooth = SmoothingFunction().method3</p>\n<p>rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)</p>\n<p>def safe_tokenize(s: str) -&gt; List[str]:</p>\n<p>s = (s or \"\").strip().lower()</p>\n<p>if not s:</p>\n<p>return []</p>\n<p>try:</p>\n<p>return nltk.word_tokenize(s)</p>\n<p>except LookupError:</p>\n<p>return re.findall(r\"\\b\\w+\\b\", s)</p>\n<p>def bleu_score(ref: str, hyp: str) -&gt; float:</p>\n<p>r = safe_tokenize(ref)</p>\n<p>h = safe_tokenize(hyp)</p>\n<p>if len(h) == 0 or len(r) == 0:</p>\n<p>return 0.0</p>\n<p>return float(sentence_bleu([r], h, smoothing_function=smooth))</p>\n<p>def rougeL_f1(ref: str, hyp: str) -&gt; float:</p>\n<p>scores = rouge.score(ref or \"\", hyp or \"\")</p>\n<p>return float(scores[\"rougeL\"].fmeasure)</p>\n<p>def semantic_sim(ref: str, hyp: str) -&gt; float:</p>\n<p>embs = embedder.encode([ref or \"\", hyp or \"\"], normalize_embeddings=True)</p>\n<p>return float(cosine_similarity([embs[0]], [embs[1]])[0][0])</p>\n<p>We implement the core LLM invocation and evaluation metrics used to assess prompt quality. We compute BLEU, ROUGE-L, and semantic similarity scores to capture both surface-level and semantic differences in model outputs. It allows us to evaluate prompt changes from multiple complementary perspectives rather than relying on a single metric.</p>\n<p>Copy CodeCopiedUse a different Browserdef evaluate_prompt(prompt_template: str) -&gt; Tuple[pd.DataFrame, Dict[str, float], str]:</p>\n<p>rows = []</p>\n<p>for ex in EVAL_SET:</p>\n<p>p = prompt_template.format(user_input=ex[\"input\"])</p>\n<p>y = call_llm(p)</p>\n<p>ref = ex[\"reference\"]</p>\n<p>rows.append({</p>\n<p>\"id\": ex[\"id\"],</p>\n<p>\"input\": ex[\"input\"],</p>\n<p>\"reference\": ref,</p>\n<p>\"output\": y,</p>\n<p>\"bleu\": bleu_score(ref, y),</p>\n<p>\"rougeL_f1\": rougeL_f1(ref, y),</p>\n<p>\"semantic_sim\": semantic_sim(ref, y),</p>\n<p>})</p>\n<p>df = pd.DataFrame(rows)</p>\n<p>agg = {</p>\n<p>\"bleu_mean\": float(df[\"bleu\"].mean()),</p>\n<p>\"rougeL_f1_mean\": float(df[\"rougeL_f1\"].mean()),</p>\n<p>\"semantic_sim_mean\": float(df[\"semantic_sim\"].mean()),</p>\n<p>}</p>\n<p>outputs_jsonl = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in rows)</p>\n<p>return df, agg, outputs_jsonl</p>\n<p>def log_text_artifact(text: str, artifact_path: str):</p>\n<p>mlflow.log_text(text, artifact_path)</p>\n<p>def prompt_diff(old: str, new: str) -&gt; str:</p>\n<p>a = old.splitlines(keepends=True)</p>\n<p>b = new.splitlines(keepends=True)</p>\n<p>return \"\".join(difflib.unified_diff(a, b, fromfile=\"previous_prompt\", tofile=\"current_prompt\"))</p>\n<p>def compute_regression_flags(baseline: Dict[str, float], current: Dict[str, float]) -&gt; Dict[str, Any]:</p>\n<p>d_sem = baseline[\"semantic_sim_mean\"] - current[\"semantic_sim_mean\"]</p>\n<p>d_rouge = baseline[\"rougeL_f1_mean\"] - current[\"rougeL_f1_mean\"]</p>\n<p>d_bleu = baseline[\"bleu_mean\"] - current[\"bleu_mean\"]</p>\n<p>flags = {</p>\n<p>\"abs_semantic_fail\": current[\"semantic_sim_mean\"] &lt; ABS_SEM_SIM_MIN,</p>\n<p>\"drop_semantic_fail\": d_sem &gt; DELTA_SEM_SIM_MAX_DROP,</p>\n<p>\"drop_rouge_fail\": d_rouge &gt; DELTA_ROUGE_L_MAX_DROP,</p>\n<p>\"drop_bleu_fail\": d_bleu &gt; DELTA_BLEU_MAX_DROP,</p>\n<p>\"delta_semantic\": float(d_sem),</p>\n<p>\"delta_rougeL\": float(d_rouge),</p>\n<p>\"delta_bleu\": float(d_bleu),</p>\n<p>}</p>\n<p>flags[\"regression\"] = any([flags[\"abs_semantic_fail\"], flags[\"drop_semantic_fail\"], flags[\"drop_rouge_fail\"], flags[\"drop_bleu_fail\"]])</p>\n<p>return flags</p>\n<p>We build the evaluation and regression logic that runs each prompt against the evaluation set and aggregates results. We log prompt artifacts, prompt diffs, and evaluation outputs to MLflow, ensuring every experiment remains auditable. We also compute regression flags that automatically identify whether a prompt version degrades performance relative to the baseline. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserprint(\"Running prompt versioning + regression testing with MLflow...\")</p>\n<p>print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")</p>\n<p>print(f\"Experiment:  {mlflow.get_experiment_by_name('prompt_versioning_llm_regression').name}\")</p>\n<p>run_summary = []</p>\n<p>baseline_metrics = None</p>\n<p>baseline_prompt = None</p>\n<p>baseline_df = None</p>\n<p>baseline_metrics_name = None</p>\n<p>with mlflow.start_run(run_name=f\"prompt_regression_suite_{int(time.time())}\") as parent_run:</p>\n<p>mlflow.set_tag(\"task\", \"prompt_versioning_regression_testing\")</p>\n<p>mlflow.log_param(\"model\", MODEL)</p>\n<p>mlflow.log_param(\"temperature\", TEMPERATURE)</p>\n<p>mlflow.log_param(\"max_output_tokens\", MAX_OUTPUT_TOKENS)</p>\n<p>mlflow.log_param(\"eval_set_size\", len(EVAL_SET))</p>\n<p>for pv in PROMPTS:</p>\n<p>ver = pv[\"version\"]</p>\n<p>prompt_t = pv[\"prompt\"]</p>\n<p>with mlflow.start_run(run_name=ver, nested=True) as child_run:</p>\n<p>mlflow.log_param(\"prompt_version\", ver)</p>\n<p>log_text_artifact(prompt_t, f\"prompts/{ver}.txt\")</p>\n<p>if baseline_prompt is not None and baseline_metrics_name is not None:</p>\n<p>diff = prompt_diff(baseline_prompt, prompt_t)</p>\n<p>log_text_artifact(diff, f\"prompt_diffs/{baseline_metrics_name}_to_{ver}.diff\")</p>\n<p>else:</p>\n<p>log_text_artifact(\"BASELINE_PROMPT (no diff)\", f\"prompt_diffs/{ver}.diff\")</p>\n<p>df, agg, outputs_jsonl = evaluate_prompt(prompt_t)</p>\n<p>mlflow.log_dict(agg, f\"metrics/{ver}_agg.json\")</p>\n<p>log_text_artifact(outputs_jsonl, f\"outputs/{ver}_outputs.jsonl\")</p>\n<p>mlflow.log_metric(\"bleu_mean\", agg[\"bleu_mean\"])</p>\n<p>mlflow.log_metric(\"rougeL_f1_mean\", agg[\"rougeL_f1_mean\"])</p>\n<p>mlflow.log_metric(\"semantic_sim_mean\", agg[\"semantic_sim_mean\"])</p>\n<p>if baseline_metrics is None:</p>\n<p>baseline_metrics = agg</p>\n<p>baseline_prompt = prompt_t</p>\n<p>baseline_df = df</p>\n<p>baseline_metrics_name = ver</p>\n<p>flags = {\"regression\": False, \"delta_bleu\": 0.0, \"delta_rougeL\": 0.0, \"delta_semantic\": 0.0}</p>\n<p>mlflow.set_tag(\"regression\", \"false\")</p>\n<p>else:</p>\n<p>flags = compute_regression_flags(baseline_metrics, agg)</p>\n<p>mlflow.log_metric(\"delta_bleu\", flags[\"delta_bleu\"])</p>\n<p>mlflow.log_metric(\"delta_rougeL\", flags[\"delta_rougeL\"])</p>\n<p>mlflow.log_metric(\"delta_semantic\", flags[\"delta_semantic\"])</p>\n<p>mlflow.set_tag(\"regression\", str(flags[\"regression\"]).lower())</p>\n<p>for k in [\"abs_semantic_fail\",\"drop_semantic_fail\",\"drop_rouge_fail\",\"drop_bleu_fail\"]:</p>\n<p>mlflow.set_tag(k, str(flags[k]).lower())</p>\n<p>run_summary.append({</p>\n<p>\"prompt_version\": ver,</p>\n<p>\"bleu_mean\": agg[\"bleu_mean\"],</p>\n<p>\"rougeL_f1_mean\": agg[\"rougeL_f1_mean\"],</p>\n<p>\"semantic_sim_mean\": agg[\"semantic_sim_mean\"],</p>\n<p>\"delta_bleu_vs_baseline\": float(flags.get(\"delta_bleu\", 0.0)),</p>\n<p>\"delta_rougeL_vs_baseline\": float(flags.get(\"delta_rougeL\", 0.0)),</p>\n<p>\"delta_semantic_vs_baseline\": float(flags.get(\"delta_semantic\", 0.0)),</p>\n<p>\"regression_flag\": bool(flags[\"regression\"]),</p>\n<p>\"mlflow_run_id\": child_run.info.run_id,</p>\n<p>})</p>\n<p>summary_df = pd.DataFrame(run_summary).sort_values(\"prompt_version\")</p>\n<p>print(\"\\n=== Aggregated Results (higher is better) ===\")</p>\n<p>display(summary_df)</p>\n<p>regressed = summary_df[summary_df[\"regression_flag\"] == True]</p>\n<p>if len(regressed) &gt; 0:</p>\n<p>print(\"\\n Regressions detected:\")</p>\n<p>display(regressed[[\"prompt_version\",\"delta_bleu_vs_baseline\",\"delta_rougeL_vs_baseline\",\"delta_semantic_vs_baseline\",\"mlflow_run_id\"]])</p>\n<p>else:</p>\n<p>print(\"\\n No regressions detected under current thresholds.\")</p>\n<p>if len(regressed) &gt; 0 and baseline_df is not None:</p>\n<p>worst_ver = regressed.sort_values(\"delta_semantic_vs_baseline\", ascending=False).iloc[0][\"prompt_version\"]</p>\n<p>worst_prompt = next(p[\"prompt\"] for p in PROMPTS if p[\"version\"] == worst_ver)</p>\n<p>worst_df, _, _ = evaluate_prompt(worst_prompt)</p>\n<p>merged = baseline_df[[\"id\",\"output\",\"bleu\",\"rougeL_f1\",\"semantic_sim\"]].merge(</p>\n<p>worst_df[[\"id\",\"output\",\"bleu\",\"rougeL_f1\",\"semantic_sim\"]],</p>\n<p>on=\"id\",</p>\n<p>suffixes=(\"_baseline\", f\"_{worst_ver}\")</p>\n<p>)</p>\n<p>merged[\"delta_semantic\"] = merged[\"semantic_sim_baseline\"] - merged[f\"semantic_sim_{worst_ver}\"]</p>\n<p>merged[\"delta_rougeL\"] = merged[\"rougeL_f1_baseline\"] - merged[f\"rougeL_f1_{worst_ver}\"]</p>\n<p>merged[\"delta_bleu\"] = merged[\"bleu_baseline\"] - merged[f\"bleu_{worst_ver}\"]</p>\n<p>print(f\"\\n=== Per-example deltas: baseline vs {worst_ver} (positive delta = worse) ===\")</p>\n<p>display(</p>\n<p>merged[[\"id\",\"delta_semantic\",\"delta_rougeL\",\"delta_bleu\",\"output_baseline\",f\"output_{worst_ver}\"]]</p>\n<p>.sort_values(\"delta_semantic\", ascending=False)</p>\n<p>)</p>\n<p>print(\"\\nOpen MLflow UI (optional) by running:\")</p>\n<p>print(\"!mlflow ui --backend-store-uri file:/content/mlruns --host 0.0.0.0 --port 5000\")</p>\n<p>We orchestrate the full prompt regression testing workflow using nested MLflow runs. We compare each prompt version against the baseline, log metric deltas, and record regression outcomes in a structured summary table. This completes a repeatable, engineering-grade pipeline for prompt versioning and regression testing that we can extend to larger datasets and real-world applications.</p>\n<p>In conclusion, we established a practical, research-oriented framework for prompt versioning and regression testing that enables us to evaluate LLM behavior with discipline and transparency. We showed how MLflow enables us to track prompt evolution, compare outputs across versions, and automatically flag regressions based on well-defined thresholds. This approach helps us move away from ad hoc prompt tuning and toward measurable, repeatable experimentation. By adopting this workflow, we ensured that prompt updates improve model behavior intentionally rather than introducing hidden performance regressions.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post A Coding Implementation to Establish Rigorous Prompt Versioning and Regression Testing Workflows for Large Language Models using MLflow appeared first on MarkTechPost.</p>"
    }
  ]
}