{
  "date": "2026-01-28",
  "coverage_date": "2026-01-27",
  "coverage_start": "2026-01-27T00:00:00",
  "coverage_end": "2026-01-27T23:59:59.999999",
  "executive_summary": "#### Top Story\n**Moonshot AI** [released **Kimi K2.5**](/?date=2026-01-28&category=news#item-e2831f1c9061), a **1 trillion parameter** open-source visual agentic model with native Agent Swarm execution coordinating **100 parallel agents**, with community benchmarks showing performance matching **Claude Opus 4.5** at approximately **10%** of the cost.\n\n#### Key Developments\n- **Anthropic**: [Launched the **MCP Apps**](/?date=2026-01-28&category=news#item-30b05edaef22) open specification with backing from **OpenAI**, **AWS**, **Block**, **VS Code**, and **JetBrains**, establishing cross-industry infrastructure for agent integration\n- **Microsoft**: [Announced the **Maia 200**](/?date=2026-01-28&category=news#item-49c769365d1a) inference chip specifically optimized for agent workloads\n- **Anthropic (UK Government)**: [Selected to build](/?date=2026-01-28&category=news#item-f93b0aa92dfd) government AI assistants for the UK Department for Science, Innovation and Technology, deploying agentic systems for citizen services on gov.uk\n- **Databricks**: Telemetry from **20,000+ enterprises** [confirms rapid adoption](/?date=2026-01-28&category=news#item-c60ae5146709) of agentic architectures over traditional chatbots\n- **AI2**: [Released **SERA**](/?date=2026-01-28&category=news#item-758d6ad222d3), reproducible open-source coding agents buildable for approximately **$400**\n\n#### Safety & Regulation\n- **37 US attorneys general** [launched coordinated action](/?date=2026-01-28&category=news#item-8e1a7590b85a) against **xAI** over **Grok's** generation of harmful imagery, marking major escalation in state-level AI enforcement\n- **Dario Amodei** [published a **19,000-word** warning](/?date=2026-01-28&category=news#item-c356043bb52e) predicting AI will autonomously build next-generation AI within **1-2 years**\n- **Washington Post** [investigation revealed](/?date=2026-01-28&category=social#item-1849acd242a9) AI companies' secret race to ingest copyrighted works, with unsealed court documents detailing years of efforts\n- **Anthropic** researchers [released first large-scale study](/?date=2026-01-28&category=research#item-fb5e2dc0ce5e) of disempowerment across **1.5 million** Claude conversations, finding severe disempowerment in **<0.1%** of interactions\n\n#### Research Highlights\n- **AISLE AI** [discovered all **12 OpenSSL zero-days**](/?date=2026-01-28&category=research#item-6180fdcc30bb), demonstrating automated vulnerability detection at critical infrastructure scale\n- **Stanford's CooperBench** [proved parallel coding agents](/?date=2026-01-28&category=reddit#item-73ae852bdbef) suffer a 'curse of coordination' where adding agents decreases performance\n- Researchers [demonstrated surgical sycophancy correction](/?date=2026-01-28&category=research#item-587a23d43703) by identifying the **3% of neurons** responsible for the behavior and removing it while preserving capabilities\n\n#### Looking Ahead\nThe convergence of open-source agentic models, cross-industry infrastructure standards, and dedicated agent hardware signals **2026** as the year agentic AI moves from experimentation to production deployment—though coordination challenges and regulatory scrutiny will shape adoption pace.",
  "executive_summary_html": "<h4>Top Story</h4>\n<p><strong>Moonshot AI</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-e2831f1c9061\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Kimi K2.5</strong></a>, a <strong>1 trillion parameter</strong> open-source visual agentic model with native Agent Swarm execution coordinating <strong>100 parallel agents</strong>, with community benchmarks showing performance matching <strong>Claude Opus 4.5</strong> at approximately <strong>10%</strong> of the cost.</p>\n<h4>Key Developments</h4>\n<ul>\n<li><strong>Anthropic</strong>: <a href=\"/?date=2026-01-28&amp;category=news#item-30b05edaef22\" class=\"internal-link\" rel=\"noopener noreferrer\">Launched the <strong>MCP Apps</strong></a> open specification with backing from <strong>OpenAI</strong>, <strong>AWS</strong>, <strong>Block</strong>, <strong>VS Code</strong>, and <strong>JetBrains</strong>, establishing cross-industry infrastructure for agent integration</li>\n<li><strong>Microsoft</strong>: <a href=\"/?date=2026-01-28&amp;category=news#item-49c769365d1a\" class=\"internal-link\" rel=\"noopener noreferrer\">Announced the <strong>Maia 200</strong></a> inference chip specifically optimized for agent workloads</li>\n<li><strong>Anthropic (UK Government)</strong>: <a href=\"/?date=2026-01-28&amp;category=news#item-f93b0aa92dfd\" class=\"internal-link\" rel=\"noopener noreferrer\">Selected to build</a> government AI assistants for the UK Department for Science, Innovation and Technology, deploying agentic systems for citizen services on gov.uk</li>\n<li><strong>Databricks</strong>: Telemetry from <strong>20,000+ enterprises</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-c60ae5146709\" class=\"internal-link\" rel=\"noopener noreferrer\">confirms rapid adoption</a> of agentic architectures over traditional chatbots</li>\n<li><strong>AI2</strong>: <a href=\"/?date=2026-01-28&amp;category=news#item-758d6ad222d3\" class=\"internal-link\" rel=\"noopener noreferrer\">Released <strong>SERA</strong></a>, reproducible open-source coding agents buildable for approximately <strong>$400</strong></li>\n</ul>\n<h4>Safety &amp; Regulation</h4>\n<ul>\n<li><strong>37 US attorneys general</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-8e1a7590b85a\" class=\"internal-link\" rel=\"noopener noreferrer\">launched coordinated action</a> against <strong>xAI</strong> over <strong>Grok's</strong> generation of harmful imagery, marking major escalation in state-level AI enforcement</li>\n<li><strong>Dario Amodei</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-c356043bb52e\" class=\"internal-link\" rel=\"noopener noreferrer\">published a <strong>19,000-word</strong> warning</a> predicting AI will autonomously build next-generation AI within <strong>1-2 years</strong></li>\n<li><strong>Washington Post</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-1849acd242a9\" class=\"internal-link\" rel=\"noopener noreferrer\">investigation revealed</a> AI companies' secret race to ingest copyrighted works, with unsealed court documents detailing years of efforts</li>\n<li><strong>Anthropic</strong> researchers <a href=\"/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e\" class=\"internal-link\" rel=\"noopener noreferrer\">released first large-scale study</a> of disempowerment across <strong>1.5 million</strong> Claude conversations, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions</li>\n</ul>\n<h4>Research Highlights</h4>\n<ul>\n<li><strong>AISLE AI</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-6180fdcc30bb\" class=\"internal-link\" rel=\"noopener noreferrer\">discovered all <strong>12 OpenSSL zero-days</strong></a>, demonstrating automated vulnerability detection at critical infrastructure scale</li>\n<li><strong>Stanford's CooperBench</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-73ae852bdbef\" class=\"internal-link\" rel=\"noopener noreferrer\">proved parallel coding agents</a> suffer a 'curse of coordination' where adding agents decreases performance</li>\n<li>Researchers <a href=\"/?date=2026-01-28&amp;category=research#item-587a23d43703\" class=\"internal-link\" rel=\"noopener noreferrer\">demonstrated surgical sycophancy correction</a> by identifying the <strong>3% of neurons</strong> responsible for the behavior and removing it while preserving capabilities</li>\n</ul>\n<h4>Looking Ahead</h4>\n<p>The convergence of open-source agentic models, cross-industry infrastructure standards, and dedicated agent hardware signals <strong>2026</strong> as the year agentic AI moves from experimentation to production deployment—though coordination challenges and regulatory scrutiny will shape adoption pace.</p>",
  "top_topics": [
    {
      "name": "Agentic AI Infrastructure Maturation",
      "description": "Anthropic [launched the MCP Apps open specification](/?date=2026-01-28&category=news#item-30b05edaef22) with backing from OpenAI, AWS, Block, VS Code, and JetBrains, establishing cross-industry infrastructure for agent integration. Databricks telemetry from 20,000+ enterprises [shows rapid adoption](/?date=2026-01-28&category=news#item-c60ae5146709) of agentic architectures over traditional chatbots, while Microsoft [announced the Maia 200 chip](/?date=2026-01-28&category=news#item-49c769365d1a) specifically optimized for agent workloads. Google [introduced Agentic Vision](/?date=2026-01-28&category=social#item-d7e7120ebcc1) in Gemini 3 Flash, and AI2 [released SERA](/?date=2026-01-28&category=social#item-38bad42c37d5) open-source coding agents.",
      "description_html": "Anthropic <a href=\"/?date=2026-01-28&category=news#item-30b05edaef22\" class=\"internal-link\">launched the MCP Apps open specification</a> with backing from OpenAI, AWS, Block, VS Code, and JetBrains, establishing cross-industry infrastructure for agent integration. Databricks telemetry from 20,000+ enterprises <a href=\"/?date=2026-01-28&category=news#item-c60ae5146709\" class=\"internal-link\">shows rapid adoption</a> of agentic architectures over traditional chatbots, while Microsoft <a href=\"/?date=2026-01-28&category=news#item-49c769365d1a\" class=\"internal-link\">announced the Maia 200 chip</a> specifically optimized for agent workloads. Google <a href=\"/?date=2026-01-28&category=social#item-d7e7120ebcc1\" class=\"internal-link\">introduced Agentic Vision</a> in Gemini 3 Flash, and AI2 <a href=\"/?date=2026-01-28&category=social#item-38bad42c37d5\" class=\"internal-link\">released SERA</a> open-source coding agents.",
      "category_breakdown": {
        "news": 4,
        "social": 3,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 92
    },
    {
      "name": "Kimi K2.5 Open Source Release",
      "description": "Moonshot AI [released Kimi K2.5](/?date=2026-01-28&category=news#item-e2831f1c9061), a 1 trillion parameter open-source visual agentic model with 32B activated parameters and native Agent Swarm execution coordinating 100 parallel agents. The release [dominated Reddit discussions](/?date=2026-01-28&category=reddit#item-7bd9d99a61bb) across r/LocalLLaMA, r/singularity, and r/ClaudeAI with users noting performance matching Claude Opus 4.5 at approximately 10% of the cost. The [leaked system prompt](/?date=2026-01-28&category=reddit#item-387a3bfe6024) revealed sophisticated memory protocols and tool schemas.",
      "description_html": "Moonshot AI <a href=\"/?date=2026-01-28&category=news#item-e2831f1c9061\" class=\"internal-link\">released Kimi K2.5</a>, a 1 trillion parameter open-source visual agentic model with 32B activated parameters and native Agent Swarm execution coordinating 100 parallel agents. The release <a href=\"/?date=2026-01-28&category=reddit#item-7bd9d99a61bb\" class=\"internal-link\">dominated Reddit discussions</a> across r/LocalLLaMA, r/singularity, and r/ClaudeAI with users noting performance matching Claude Opus 4.5 at approximately 10% of the cost. The <a href=\"/?date=2026-01-28&category=reddit#item-387a3bfe6024\" class=\"internal-link\">leaked system prompt</a> revealed sophisticated memory protocols and tool schemas.",
      "category_breakdown": {
        "news": 1,
        "reddit": 5
      },
      "representative_items": [],
      "importance": 88
    },
    {
      "name": "AI Coding Agent Transformation",
      "description": "Anthropic's bcherny [revealed](/?date=2026-01-28&category=social#item-3224ccef215a) the Claude Code team ships 100% AI-generated code using Opus 4.5, sparking intense debate about software development's future. Simon Willison [highlighted](/?date=2026-01-28&category=social#item-f428962b7742) a fully AI-built 20,000-line Rust browser created in 3 days, while Stanford's CooperBench research [proved](/?date=2026-01-28&category=reddit#item-73ae852bdbef) parallel coding agents suffer a 'curse of coordination' where adding agents decreases performance. AI2 [released SERA](/?date=2026-01-28&category=news#item-758d6ad222d3), reproducible open-source coding agents for around $400.",
      "description_html": "Anthropic's bcherny <a href=\"/?date=2026-01-28&category=social#item-3224ccef215a\" class=\"internal-link\">revealed</a> the Claude Code team ships 100% AI-generated code using Opus 4.5, sparking intense debate about software development's future. Simon Willison <a href=\"/?date=2026-01-28&category=social#item-f428962b7742\" class=\"internal-link\">highlighted</a> a fully AI-built 20,000-line Rust browser created in 3 days, while Stanford's CooperBench research <a href=\"/?date=2026-01-28&category=reddit#item-73ae852bdbef\" class=\"internal-link\">proved</a> parallel coding agents suffer a 'curse of coordination' where adding agents decreases performance. AI2 <a href=\"/?date=2026-01-28&category=news#item-758d6ad222d3\" class=\"internal-link\">released SERA</a>, reproducible open-source coding agents for around $400.",
      "category_breakdown": {
        "news": 1,
        "social": 4,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 86
    },
    {
      "name": "AI Safety Warnings and Empirics",
      "description": "Anthropic CEO Dario Amodei [published a 19,000-word warning](/?date=2026-01-28&category=news#item-c356043bb52e) about imminent AI risks, predicting AI will autonomously build next-generation AI within 1-2 years. Anthropic researchers [released the first large-scale empirical study](/?date=2026-01-28&category=research#item-fb5e2dc0ce5e) of disempowerment patterns across 1.5 million Claude conversations, finding severe disempowerment in less than 0.1% of interactions. Separately, researchers [demonstrated surgical sycophancy correction](/?date=2026-01-28&category=research#item-587a23d43703) by identifying the 3% of neurons responsible for the behavior.",
      "description_html": "Anthropic CEO Dario Amodei <a href=\"/?date=2026-01-28&category=news#item-c356043bb52e\" class=\"internal-link\">published a 19,000-word warning</a> about imminent AI risks, predicting AI will autonomously build next-generation AI within 1-2 years. Anthropic researchers <a href=\"/?date=2026-01-28&category=research#item-fb5e2dc0ce5e\" class=\"internal-link\">released the first large-scale empirical study</a> of disempowerment patterns across 1.5 million Claude conversations, finding severe disempowerment in less than 0.1% of interactions. Separately, researchers <a href=\"/?date=2026-01-28&category=research#item-587a23d43703\" class=\"internal-link\">demonstrated surgical sycophancy correction</a> by identifying the 3% of neurons responsible for the behavior.",
      "category_breakdown": {
        "news": 2,
        "research": 3,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 84
    },
    {
      "name": "Anthropic Government and Platform Expansion",
      "description": "Anthropic [was selected to build](/?date=2026-01-28&category=news#item-f93b0aa92dfd) government AI assistants for the UK Department for Science, Innovation and Technology, deploying agentic AI systems for citizen services on gov.uk. The company simultaneously [launched the MCP Apps specification](/?date=2026-01-28&category=news#item-30b05edaef22) creating cross-industry agent infrastructure. Reddit discussions noted the Clawd autonomous agent [rebranding to Molty](/?date=2026-01-28&category=reddit#item-7c59caedecc5) after an Anthropic trademark request, reflecting growing questions about AI agent identity.",
      "description_html": "Anthropic <a href=\"/?date=2026-01-28&category=news#item-f93b0aa92dfd\" class=\"internal-link\">was selected to build</a> government AI assistants for the UK Department for Science, Innovation and Technology, deploying agentic AI systems for citizen services on gov.uk. The company simultaneously <a href=\"/?date=2026-01-28&category=news#item-30b05edaef22\" class=\"internal-link\">launched the MCP Apps specification</a> creating cross-industry agent infrastructure. Reddit discussions noted the Clawd autonomous agent <a href=\"/?date=2026-01-28&category=reddit#item-7c59caedecc5\" class=\"internal-link\">rebranding to Molty</a> after an Anthropic trademark request, reflecting growing questions about AI agent identity.",
      "category_breakdown": {
        "news": 2,
        "social": 2,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 79
    },
    {
      "name": "AI Regulation and Legal Action",
      "description": "At least 37 attorneys general from US states and territories [launched coordinated action](/?date=2026-01-28&category=news#item-8e1a7590b85a) against xAI over Grok's generation of harmful imagery, marking a major escalation in state-level AI enforcement. The Washington Post [broke a major investigation](/?date=2026-01-28&category=social#item-1849acd242a9) into AI companies' secret race to ingest copyrighted works, with unsealed court documents detailing years of efforts. Anthropic's Project Panama was noted as attempting a more ethical approach to training data acquisition.",
      "description_html": "At least 37 attorneys general from US states and territories <a href=\"/?date=2026-01-28&category=news#item-8e1a7590b85a\" class=\"internal-link\">launched coordinated action</a> against xAI over Grok's generation of harmful imagery, marking a major escalation in state-level AI enforcement. The Washington Post <a href=\"/?date=2026-01-28&category=social#item-1849acd242a9\" class=\"internal-link\">broke a major investigation</a> into AI companies' secret race to ingest copyrighted works, with unsealed court documents detailing years of efforts. Anthropic's Project Panama was noted as attempting a more ethical approach to training data acquisition.",
      "category_breakdown": {
        "news": 1,
        "social": 1,
        "reddit": 1
      },
      "representative_items": [],
      "importance": 75
    }
  ],
  "total_items_collected": 1748,
  "total_items_analyzed": 1732,
  "collection_status": {
    "overall": "success",
    "sources": [
      {
        "name": "news",
        "display_name": "News",
        "status": "success",
        "count": 42,
        "error": null
      },
      {
        "name": "research",
        "display_name": "Research",
        "status": "success",
        "count": 414,
        "error": null
      },
      {
        "name": "social",
        "display_name": "Social",
        "status": "success",
        "count": 564,
        "error": null
      },
      {
        "name": "reddit",
        "display_name": "Reddit",
        "status": "success",
        "count": 728,
        "error": null
      }
    ],
    "social_platforms": [
      {
        "name": "twitter",
        "display_name": "Twitter",
        "status": "success",
        "count": 542,
        "error": null
      },
      {
        "name": "bluesky",
        "display_name": "Bluesky",
        "status": "success",
        "count": 22,
        "error": null
      },
      {
        "name": "mastodon",
        "display_name": "Mastodon",
        "status": "success",
        "count": 0,
        "error": null
      }
    ],
    "warnings": []
  },
  "hero_image_url": "/data/2026-01-28/hero.webp?v=1769640877",
  "hero_image_prompt": "You are editing an existing hero image. The attached image is the current version which is GOOD.\n\nDO NOT regenerate the entire image. Make ONLY the following specific change:\n\nRemove the two text labels at the bottom of the image: 'AI Regulation and Legal Action' on the left and 'Infrastructure Maturation' in the center. Keep everything else exactly the same.\n\nIMPORTANT:\n- Keep the overall composition, style, and colors the same\n- Preserve everything else exactly as it appears\n- Only modify what is explicitly requested above\n- The result should look like a minor edit, not a new image",
  "generated_at": "2026-01-28T02:44:35.436751",
  "categories": {
    "news": {
      "count": 26,
      "category_summary": "**Agentic AI** dominated this week's news as the industry shifts toward autonomous systems. **Moonshot AI** [released **Kimi K2.5**](/?date=2026-01-28&category=news#item-e2831f1c9061), a 1T-parameter open source visual agentic model, while **Anthropic** [launched the **MCP Apps specification**](/?date=2026-01-28&category=news#item-30b05edaef22) with backing from **OpenAI**, **AWS**, and others—establishing cross-industry infrastructure for agent integration. **Databricks** data from 20,000+ enterprises [confirms rapid adoption](/?date=2026-01-28&category=news#item-c60ae5146709) of agentic architectures over traditional chatbots.\n\n**Hardware and infrastructure** advances continue: **Google** [deployed **Gemini 3**](/?date=2026-01-28&category=news#item-7a41e8963e96) in AI Overviews at scale, **Microsoft** [announced the **Maia 200**](/?date=2026-01-28&category=news#item-49c769365d1a) inference chip optimized for agent workloads, and **Ricursive Intelligence** [raised at a **$4B valuation**](/?date=2026-01-28&category=news#item-8c06e1bc10ec) to apply AI to chip manufacturing.\n\n**Regulation and safety** saw major developments:\n- **37 US attorneys general** [launched coordinated action](/?date=2026-01-28&category=news#item-8e1a7590b85a) against **xAI** over Grok's generation of harmful imagery\n- **Anthropic CEO Dario Amodei** [published a 19,000-word warning](/?date=2026-01-28&category=news#item-c356043bb52e) about imminent AI risks\n- **Anthropic** was [selected to build UK government](/?date=2026-01-28&category=news#item-f93b0aa92dfd) AI assistants, signaling growing public sector AI deployment",
      "category_summary_html": "<p><strong>Agentic AI</strong> dominated this week's news as the industry shifts toward autonomous systems. <strong>Moonshot AI</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-e2831f1c9061\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Kimi K2.5</strong></a>, a 1T-parameter open source visual agentic model, while <strong>Anthropic</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-30b05edaef22\" class=\"internal-link\" rel=\"noopener noreferrer\">launched the <strong>MCP Apps specification</strong></a> with backing from <strong>OpenAI</strong>, <strong>AWS</strong>, and others—establishing cross-industry infrastructure for agent integration. <strong>Databricks</strong> data from 20,000+ enterprises <a href=\"/?date=2026-01-28&amp;category=news#item-c60ae5146709\" class=\"internal-link\" rel=\"noopener noreferrer\">confirms rapid adoption</a> of agentic architectures over traditional chatbots.</p>\n<p><strong>Hardware and infrastructure</strong> advances continue: <strong>Google</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-7a41e8963e96\" class=\"internal-link\" rel=\"noopener noreferrer\">deployed <strong>Gemini 3</strong></a> in AI Overviews at scale, <strong>Microsoft</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-49c769365d1a\" class=\"internal-link\" rel=\"noopener noreferrer\">announced the <strong>Maia 200</strong></a> inference chip optimized for agent workloads, and <strong>Ricursive Intelligence</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-8c06e1bc10ec\" class=\"internal-link\" rel=\"noopener noreferrer\">raised at a <strong>$4B valuation</strong></a> to apply AI to chip manufacturing.</p>\n<p><strong>Regulation and safety</strong> saw major developments:</p>\n<ul>\n<li><strong>37 US attorneys general</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-8e1a7590b85a\" class=\"internal-link\" rel=\"noopener noreferrer\">launched coordinated action</a> against <strong>xAI</strong> over Grok's generation of harmful imagery</li>\n<li><strong>Anthropic CEO Dario Amodei</strong> <a href=\"/?date=2026-01-28&amp;category=news#item-c356043bb52e\" class=\"internal-link\" rel=\"noopener noreferrer\">published a 19,000-word warning</a> about imminent AI risks</li>\n<li><strong>Anthropic</strong> was <a href=\"/?date=2026-01-28&amp;category=news#item-f93b0aa92dfd\" class=\"internal-link\" rel=\"noopener noreferrer\">selected to build UK government</a> AI assistants, signaling growing public sector AI deployment</li>\n</ul>",
      "themes": [
        {
          "name": "Agentic AI Infrastructure",
          "description": "Industry-wide shift toward autonomous agent systems with new standards (MCP Apps), enterprise adoption data, and agent-focused model releases",
          "item_count": 7,
          "example_items": [],
          "importance": 80.0
        },
        {
          "name": "AI Hardware & Chips",
          "description": "Custom silicon development for AI inference and agent workloads, plus AI applied to chip manufacturing optimization",
          "item_count": 3,
          "example_items": [],
          "importance": 74.0
        },
        {
          "name": "AI Safety & Regulation",
          "description": "Unprecedented multi-state legal action against xAI, Anthropic CEO safety warnings, and government deployment oversight",
          "item_count": 4,
          "example_items": [],
          "importance": 72.0
        },
        {
          "name": "Open Source Models",
          "description": "Major open-weight releases including Kimi K2.5 and AI2 coding agents, plus analysis of China's open source ecosystem",
          "item_count": 4,
          "example_items": [],
          "importance": 70.0
        },
        {
          "name": "Government & Public Sector AI",
          "description": "UK government partnerships with Anthropic and Meta for AI deployment in public services, defense, and healthcare",
          "item_count": 4,
          "example_items": [],
          "importance": 65.0
        }
      ],
      "top_items": [
        {
          "id": "e2831f1c9061",
          "title": "Moonshot AI Releases Kimi K2.5: An Open Source Visual Agentic Intelligence Model with Native Swarm Execution",
          "content": "Moonshot AI has released Kimi K2.5 as an open source visual agentic intelligence model. It combines a large Mixture of Experts language backbone, a native vision encoder, and a parallel multi agent system called Agent Swarm. The model targets coding, multimodal reasoning, and deep web research with strong benchmark results on agentic, vision, and coding suites.\n\n\n\nModel Architecture and Training\n\n\n\nKimi K2.5 is a Mixture of Experts model with 1T total parameters and about 32B activated parameters per token. The network has 61 layers. It uses 384 experts, with 8 experts selected per token plus 1 shared expert. The attention hidden size is 7168 and there are 64 attention heads.\n\n\n\nThe model uses MLA attention and the SwiGLU activation function. The tokenizer vocabulary size is 160K. The maximum context length during training and inference is 256K tokens. This supports long tool traces, long documents, and multi step research workflows.\n\n\n\nVision is handled by a MoonViT encoder with about 400M parameters. Visual tokens are trained together with text tokens in a single multimodal backbone. Kimi K2.5 is obtained by continual pretraining on about 15T tokens of mixed vision and text data on top of Kimi K2 Base. This native multimodal training is important because the model learns joint structure over images, documents, and language from the start.\n\n\n\nThe released checkpoints support standard inference stacks such as vLLM, SGLang, and KTransformers with transformers version 4.57.1 or newer. Quantized INT4 variants are available, reusing the method from Kimi K2 Thinking. This allows deployment on commodity GPUs with lower memory budgets.\n\n\n\nCoding and Multimodal Capabilities\n\n\n\nKimi K2.5 is positioned as a strong open source coding model, especially when code generation depends on visual context. The model can read UI mockups, design screenshots, or even videos, then emit structured frontend code with layout, styling, and interaction logic.\n\n\n\nMoonshot shows examples where the model reads a puzzle image, reasons about the shortest path, and then writes code that produces a visualized solution. This demonstrates cross modal reasoning, where the model combines image understanding, algorithmic planning, and code synthesis in a single flow.\n\n\n\nBecause K2.5 has a 256K context window, it can keep long specification histories in context. A practical workflow for developers is to mix design assets, product docs, and existing code in one prompt. The model can then refactor or extend the codebase while keeping visual constraints aligned with the original design.\n\n\n\nhttps://www.kimi.com/blog/kimi-k2-5.html?\n\n\nAgent Swarm and Parallel Agent Reinforcement Learning\n\n\n\nA key feature of Kimi K2.5 is Agent Swarm. This is a multi agent system trained with Parallel Agent Reinforcement Learning, PARL. In this setup an orchestrator agent decomposes a complex goal into many subtasks. It then spins up domain specific sub agents to work in parallel.\n\n\n\nKimi team reports that K2.5 can manage up to 100 sub agents within a task. It supports up to 1,500 coordinated steps or tool calls in one run. This parallelism gives about 4.5 times faster completion compared with a single agent pipeline on wide search tasks.\n\n\n\nPARL introduces a metric called Critical Steps. The system rewards policies that reduce the number of serial steps needed to solve the task. This discourages naive sequential planning and pushes the agent to split work into parallel branches while still maintaining consistency.\n\n\n\nOne example by the Kimi team is a research workflow where the system needs to discover many niche creators. The orchestrator uses Agent Swarm to spawn a large number of researcher agents. Each agent explores different regions of the web, and the system merges results into a structured table.\n\n\n\nhttps://www.kimi.com/blog/kimi-k2-5.html?\n\n\nBenchmark Performance\n\n\n\nOn agentic benchmarks, Kimi K2.5 reports strong numbers. On HLE Full with tools the score is 50.2. On BrowseComp with context management the score is 74.9. In Agent Swarm mode the BrowseComp score increases further to 78.4 and WideSearch metrics also improve. The Kimi team compares these values with GPT 5.2, Claude 4.5, Gemini 3 Pro, and DeepSeek V3, and K2.5 shows the highest scores among the listed models on these specific agentic suites.\n\n\n\nOn vision and video benchmarks K2.5 also reports high scores. MMMU Pro is 78.5 and VideoMMMU is 86.6. The model performs well on OmniDocBench, OCRBench, WorldVQA, and other document and scene understanding tasks. These results indicate that the MoonViT encoder and long context training are effective for real world multimodal problems, such as reading complex documents and reasoning over videos.\n\n\n\nhttps://www.kimi.com/blog/kimi-k2-5.html?\n\n\nFor coding benchmarks it lists SWE Bench Verified at 76.8, SWE Bench Pro at 50.7, SWE Bench Multilingual at 73.0, Terminal Bench 2.0 at 50.8, and LiveCodeBench v6 at 85.0. These numbers place K2.5 among the strongest open source coding models currently reported on these tasks.\n\n\n\nOn long context language benchmarks, K2.5 reaches 61.0 on LongBench V2 and 70.0 on AA LCR under standard evaluation settings. For reasoning benchmarks it achieves high scores on AIME 2025, HMMT 2025 February, GPQA Diamond, and MMLU Pro when used in thinking mode.\n\n\n\nKey Takeaways\n\n\n\n\nMixture of Experts at trillion scale: Kimi K2.5 uses a Mixture of Experts architecture with 1T total parameters and about 32B active parameters per token, 61 layers, 384 experts, and 256K context length, optimized for long multimodal and tool heavy workflows.\n\n\n\nNative multimodal training with MoonViT: The model integrates a MoonViT vision encoder of about 400M parameters and is trained on about 15T mixed vision and text tokens, so images, documents, and language are handled in a single unified backbone.\n\n\n\nParallel Agent Swarm with PARL: Agent Swarm, trained with Parallel Agent Reinforcement Learning, can coordinate up to 100 sub agents and about 1,500 tool calls per task, giving around 4.5 times faster execution versus a single agent on wide research tasks.\n\n\n\nStrong benchmark results in coding, vision, and agents: K2.5 reports 76.8 on SWE Bench Verified, 78.5 on MMMU Pro, 86.6 on VideoMMMU, 50.2 on HLE Full with tools, and 74.9 on BrowseComp, matching or exceeding listed closed models on several agentic and multimodal suites.\n\n\n\n\n\n\n\n\nCheck out the Technical details and Model Weight. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Moonshot AI Releases Kimi K2.5: An Open Source Visual Agentic Intelligence Model with Native Swarm Execution appeared first on MarkTechPost.",
          "url": "https://www.marktechpost.com/2026/01/27/moonshot-ai-releases-kimi-k2-5-an-open-source-visual-agentic-intelligence-model-with-native-swarm-execution/",
          "author": "Asif Razzaq",
          "published": "2026-01-27T23:55:34",
          "source": "MarkTechPost",
          "source_type": "rss",
          "tags": [
            "Agentic AI",
            "AI Agents",
            "AI Shorts",
            "Applications",
            "Artificial Intelligence",
            "Editors Pick",
            "Language Model",
            "Large Language Model",
            "Machine Learning",
            "New Releases",
            "Open Source",
            "Tech News",
            "Technology",
            "Uncategorized"
          ],
          "summary": "Moonshot AI has released Kimi K2.5, an open source 1T parameter Mixture of Experts model with 32B activated parameters, native vision encoder, and 'Agent Swarm' multi-agent system. The model targets coding, multimodal reasoning, and web research with strong benchmark results across agentic, vision, and coding tasks.",
          "importance_score": 82.0,
          "reasoning": "Major open source release combining massive MoE architecture with visual and agentic capabilities. Represents significant advancement in open-weight frontier models.",
          "themes": [
            "open source models",
            "agentic AI",
            "multimodal AI",
            "MoE architecture"
          ],
          "continuation": null,
          "summary_html": "<p>Moonshot AI has released Kimi K2.5, an open source 1T parameter Mixture of Experts model with 32B activated parameters, native vision encoder, and 'Agent Swarm' multi-agent system. The model targets coding, multimodal reasoning, and web research with strong benchmark results across agentic, vision, and coding tasks.</p>",
          "content_html": "<p>Moonshot AI has released Kimi K2.5 as an open source visual agentic intelligence model. It combines a large Mixture of Experts language backbone, a native vision encoder, and a parallel multi agent system called Agent Swarm. The model targets coding, multimodal reasoning, and deep web research with strong benchmark results on agentic, vision, and coding suites.</p>\n<p>Model Architecture and Training</p>\n<p>Kimi K2.5 is a Mixture of Experts model with 1T total parameters and about 32B activated parameters per token. The network has 61 layers. It uses 384 experts, with 8 experts selected per token plus 1 shared expert. The attention hidden size is 7168 and there are 64 attention heads.</p>\n<p>The model uses MLA attention and the SwiGLU activation function. The tokenizer vocabulary size is 160K. The maximum context length during training and inference is 256K tokens. This supports long tool traces, long documents, and multi step research workflows.</p>\n<p>Vision is handled by a MoonViT encoder with about 400M parameters. Visual tokens are trained together with text tokens in a single multimodal backbone. Kimi K2.5 is obtained by continual pretraining on about 15T tokens of mixed vision and text data on top of Kimi K2 Base. This native multimodal training is important because the model learns joint structure over images, documents, and language from the start.</p>\n<p>The released checkpoints support standard inference stacks such as vLLM, SGLang, and KTransformers with transformers version 4.57.1 or newer. Quantized INT4 variants are available, reusing the method from Kimi K2 Thinking. This allows deployment on commodity GPUs with lower memory budgets.</p>\n<p>Coding and Multimodal Capabilities</p>\n<p>Kimi K2.5 is positioned as a strong open source coding model, especially when code generation depends on visual context. The model can read UI mockups, design screenshots, or even videos, then emit structured frontend code with layout, styling, and interaction logic.</p>\n<p>Moonshot shows examples where the model reads a puzzle image, reasons about the shortest path, and then writes code that produces a visualized solution. This demonstrates cross modal reasoning, where the model combines image understanding, algorithmic planning, and code synthesis in a single flow.</p>\n<p>Because K2.5 has a 256K context window, it can keep long specification histories in context. A practical workflow for developers is to mix design assets, product docs, and existing code in one prompt. The model can then refactor or extend the codebase while keeping visual constraints aligned with the original design.</p>\n<p>https://www.kimi.com/blog/kimi-k2-5.html?</p>\n<p>Agent Swarm and Parallel Agent Reinforcement Learning</p>\n<p>A key feature of Kimi K2.5 is Agent Swarm. This is a multi agent system trained with Parallel Agent Reinforcement Learning, PARL. In this setup an orchestrator agent decomposes a complex goal into many subtasks. It then spins up domain specific sub agents to work in parallel.</p>\n<p>Kimi team reports that K2.5 can manage up to 100 sub agents within a task. It supports up to 1,500 coordinated steps or tool calls in one run. This parallelism gives about 4.5 times faster completion compared with a single agent pipeline on wide search tasks.</p>\n<p>PARL introduces a metric called Critical Steps. The system rewards policies that reduce the number of serial steps needed to solve the task. This discourages naive sequential planning and pushes the agent to split work into parallel branches while still maintaining consistency.</p>\n<p>One example by the Kimi team is a research workflow where the system needs to discover many niche creators. The orchestrator uses Agent Swarm to spawn a large number of researcher agents. Each agent explores different regions of the web, and the system merges results into a structured table.</p>\n<p>https://www.kimi.com/blog/kimi-k2-5.html?</p>\n<p>Benchmark Performance</p>\n<p>On agentic benchmarks, Kimi K2.5 reports strong numbers. On HLE Full with tools the score is 50.2. On BrowseComp with context management the score is 74.9. In Agent Swarm mode the BrowseComp score increases further to 78.4 and WideSearch metrics also improve. The Kimi team compares these values with GPT 5.2, Claude 4.5, Gemini 3 Pro, and DeepSeek V3, and K2.5 shows the highest scores among the listed models on these specific agentic suites.</p>\n<p>On vision and video benchmarks K2.5 also reports high scores. MMMU Pro is 78.5 and VideoMMMU is 86.6. The model performs well on OmniDocBench, OCRBench, WorldVQA, and other document and scene understanding tasks. These results indicate that the MoonViT encoder and long context training are effective for real world multimodal problems, such as reading complex documents and reasoning over videos.</p>\n<p>https://www.kimi.com/blog/kimi-k2-5.html?</p>\n<p>For coding benchmarks it lists SWE Bench Verified at 76.8, SWE Bench Pro at 50.7, SWE Bench Multilingual at 73.0, Terminal Bench 2.0 at 50.8, and LiveCodeBench v6 at 85.0. These numbers place K2.5 among the strongest open source coding models currently reported on these tasks.</p>\n<p>On long context language benchmarks, K2.5 reaches 61.0 on LongBench V2 and 70.0 on AA LCR under standard evaluation settings. For reasoning benchmarks it achieves high scores on AIME 2025, HMMT 2025 February, GPQA Diamond, and MMLU Pro when used in thinking mode.</p>\n<p>Key Takeaways</p>\n<p>Mixture of Experts at trillion scale: Kimi K2.5 uses a Mixture of Experts architecture with 1T total parameters and about 32B active parameters per token, 61 layers, 384 experts, and 256K context length, optimized for long multimodal and tool heavy workflows.</p>\n<p>Native multimodal training with MoonViT: The model integrates a MoonViT vision encoder of about 400M parameters and is trained on about 15T mixed vision and text tokens, so images, documents, and language are handled in a single unified backbone.</p>\n<p>Parallel Agent Swarm with PARL: Agent Swarm, trained with Parallel Agent Reinforcement Learning, can coordinate up to 100 sub agents and about 1,500 tool calls per task, giving around 4.5 times faster execution versus a single agent on wide research tasks.</p>\n<p>Strong benchmark results in coding, vision, and agents: K2.5 reports 76.8 on SWE Bench Verified, 78.5 on MMMU Pro, 86.6 on VideoMMMU, 50.2 on HLE Full with tools, and 74.9 on BrowseComp, matching or exceeding listed closed models on several agentic and multimodal suites.</p>\n<p>Check out the&nbsp;Technical details&nbsp;and&nbsp;Model Weight.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Moonshot AI Releases Kimi K2.5: An Open Source Visual Agentic Intelligence Model with Native Swarm Execution appeared first on MarkTechPost.</p>"
        },
        {
          "id": "30b05edaef22",
          "title": "[AINews] Anthropic launches the MCP Apps open spec, in Claude.ai",
          "content": "AI News for 1/23/2026-1/26/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 14285 messages) for you. Estimated reading time saved (at 200wpm): 1208 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!3 months after OpenAI floated a trial balloon with ChatGPT Apps and the Apps SDK at Dev Day 2025, Anthropic has now officially absorbed the independent MCP UI project and, working with OpenAI, Block, VS Code, Antigravity, JetBrains, AWS, and others, has released both:the MCP Apps specofficial support in Claude.ai - comparatively very well received but of course not as popular as the Claude in Excel announcement.It&#8217;s fair to say that ChatGPT Apps haven&#8217;t exactly taken the world by storm since announcement, but the overall need for a standard format for applications to return rich UI still cannot be denied. Now that MCP Apps have been ratified by all the important players, this is the basis for a rich ecosystem of open source support and applications being able to interoperate, and perhaps one day solve the perpetual never ending pile of $20/month subscriptions piling up in your credit card bills.As a reminder, we interviewed David Soria Parra and the rest of the AAIF, who previewed a bit of the thinking and design process behind MCP Apps here:AI Twitter RecapAgent Orchestration, RLMs, and &#8220;Clawdbot/Clawd&#8221; as a UX patternNVIDIA ToolOrchestra + Orchestrator-8B: NVIDIA&#8217;s ToolOrchestra frames agentic systems as a small &#8220;conductor&#8221; model that alternates reasoning with calls to tools and larger &#8220;expert&#8221; models (search, code execution, specialist LLMs, frontier generalists). The claim is that an 8B orchestrator can reach frontier-level outcomes via delegation at materially lower cost, trained end-to-end with scalable RL using automatically synthesized tool-use environments and multi-turn tasks (summary, link). Closest technical implication: &#8220;controller scale&#8221; matters less than policy quality + tool/model routing if you can train it with realistic tool-call rollouts.RLMs / recursion-first agent stacks: Several posts converge on a Recursive Language Model (RLM) pattern: pass files and context by reference and iteratively pull the minimum slices needed (shell/grep/AST), rather than stuffing everything into context &#224; la ReAct. Dan B illustrates this with file references vs @file expansion as deliberate context management (thread). Daytona is positioning RLMs as &#8220;unlimited recursion depth&#8221; via per-(sub)agent sandboxes (guide, integration).&#8220;Clawd/Clawdbot&#8221; meme &#8594; product signal: The dataset contains a large &#8220;Clawdbot&#8221; wave (often with Mac mini jokes), but the technically relevant throughline is outcome-first assistant UX + tight context/tool integration. Kimmonismus explicitly calls this a shift from &#8220;more chat&#8221; to &#8220;more outcome,&#8221; suggesting incumbents will scramble to match it (tweet). Others push a cloud-first counterpoint (no local Mac mini) (MiniMax reply). There&#8217;s also an emerging security backlash as soon as &#8220;powerful mode&#8221; exists: prompt injection remains a system-level blocker for browser/desktop agents (dilemma, follow-up, Miessler warnings).Reasoning model releases &amp; eval dynamics (Qwen, Tencent, ARC, etc.)Alibaba Qwen3-Max-Thinking: Alibaba positions Qwen3-Max-Thinking as a flagship reasoning+agent model trained with &#8220;massive scale and advanced RL,&#8221; emphasizing adaptive tool-use (Search/Memory/Code Interpreter) and test-time scaling/self-reflection. They cite strong math and agentic search metrics (e.g., 98.0 on HMMT Feb, 49.8 on HLE) (launch). The model is immediately pushed into public eval channels: LM Arena Text Arena (Arena) and Yupp (Yupp). Community reaction highlights the tool-enabled evaluation regime&#8212;claims of outperforming multiple SOTA models on HLE with search tools (commentary).Tencent HunyuanImage 3.0-Instruct (image editing): Tencent releases an image-editing-focused multimodal model built on an 80B MoE (13B active), using a &#8220;Thinking&#8221; schema with native CoT and their MixGRPO algorithm; focus is on precise edits that preserve non-target regions and multi-image fusion (announcement). LM Arena reports it entering the top-10 image edit leaderboard (rank #7) (Arena).ARC-AGI cost/perf hacks: A notable optimization claim: &#8220;Recursive Self-Aggregation (RSA) + Gemini 3 Flash&#8221; reaching 59.31% on ARC-AGI-2 at ~1/10 cost vs Gemini Deep Think (tweet). This points to a broader theme: meta-inference strategies (aggregation, recursion, pruning) are becoming as important as base model choice.Open models in arenas: Molmo 2 (Apache 2.0) appears in Arena as a new open model entrant (Arena). Separately, Hugging Face Inference Endpoint notes GLM-4.7-Flash via llama.cpp with a low hourly price point (Q4_K_M, 24k context) (ngxson)&#8212;underscoring a continued commoditization of fast open-weight inference.RL everywhere: test-time training, GRPO stabilization, RL-as-pretraining, and compute savingsTest-Time Training (TTT) + RL breakthroughs: A widely shared result claims a Stanford/NVIDIA-style TTT+RL approach that: beats AlphaEvolve, finds a new upper bound for an Erd&#337;s overlap problem, produces A100 kernels ~2&#215; faster than best human kernels, and beats both best AI+human attempts on AtCoder (rronak_). This cluster also includes meta-discussion about correctly crediting related approaches (EvoTune) (Yejin Cho).GRPO training stability knobs: A small but actionable engineering tip: INTELLECT-2 reports a delta=4.0 parameter that improves GRPO stability (QGallouedec).RL in pretraining (RLP): NVIDIA authors announce RLP (Reinforcement as a Pretraining Objective) accepted to ICLR 2026, framing RL not as &#8220;post-training only&#8221; but as integrated into pretraining (ahatamiz1).Compute reduction via curriculum-like filtering: AI21&#8217;s &#8220;Dynamic Data Snoozing&#8221; claims up to 3&#215; compute reduction for RLVR by snoozing examples that are too easy (DanielGissin). If validated, this is a practical recipe: make the sampler policy-aware instead of static.Inference infrastructure &amp; dev tooling: vLLM&#8217;s &#8220;day-0 model support,&#8221; VS Code MCP Apps, Cursor subagentsvLLM&#8217;s governance and commercialization pressure: A long Zhihu-derived summary argues vLLM&#8217;s &#8220;open-source project &#8594; startup&#8221; shift was driven by the hidden cost of day-0 support (weeks/months of confidential pre-integration per new model), the rise of MoE and heterogeneous inference (fp8/int4/sparse attention), and the mismatch with PyTorch Foundation style testing vs vLLM&#8217;s multi-node CI needs. It claims the maintainers founded Inferact Inc to fund full-time maintainers while keeping vLLM open-source (thread). Related: vLLM shares a practical flag for avoiding OOM on long-context models: --max-model-len auto (vLLM tip).MCP Apps: tool calls return interactive UI: The MCP ecosystem announces MCP Apps as the first official MCP extension: tool calls can return interactive UI components rendered in-chat. VS Code is first major editor shipping support (Insiders now, stable soon) (VS Code, alexalbert__). Anthropic simultaneously ships &#8220;interactive work tools in Claude&#8221; (Slack drafting, Figma diagrams, Asana timelines) (Claude). Net: we&#8217;re seeing the &#8220;tool interface layer&#8221; move from raw JSON to native UI primitives inside agent loops.Cursor: multi-browser subagents: Cursor adds multi-browser support via subagents (Cursor), echoing the same direction: parallelized tool execution + better context isolation.Kernel LLMs, chip stacks, and &#8220;AI for hardware&#8221; loopsGPU MODE 2026: post-training Kernel LLMs in public: GPU MODE outlines a 2026 plan to post-train a Kernel LLM and get generated kernels merged into real repos (PyTorch/vLLM), emphasizing &#8220;de-slopify kernels&#8221; (determinism, reviewer-mergeable PRs), profiler-guided optimization + memory work, and competitions as evals (marksaroufim).Microsoft Maia 200: Microsoft announces Maia 200 as a custom inference accelerator; Mustafa Suleyman claims it&#8217;s the most performant first-party hyperscaler silicon, with 3&#215; FP4 performance vs Trainium v3 and FP8 above TPU v7 (Mustafa, follow-up). Yusuf Mehdi frames this as infra that makes AI &#8220;dependable&#8221; (thread).Ricursive Intelligence (AI for chip design): Ricursive raises a $300M Series A aiming at end-to-end chip design as a recursive self-improvement loop between AI and hardware (company, Anna Goldie).Safety, misuse, and societal impact (selected items with direct technical relevance)Elicitation attacks via benign chemistry data: Anthropic reports that fine-tuning open models on &#8220;benign&#8221; chemical synthesis content generated by frontier models can significantly increase capability on chemical weapons tasks&#8212;an &#8220;elicitation attack&#8221; that scales with frontier model strength (AnthropicAI, paper link).Dario Amodei&#8217;s &#8220;Adolescence of Technology&#8221; essay: A major, highly engaged post argues AI is entering an accelerating feedback loop (AI building AI), with risks spanning misuse, power-seeking autonomy, and economic disruption; it also explicitly frames wealth concentration as a society-breaking failure mode (Dario). Reaction ranges from strong endorsement to critique of how &#8220;takeover risk&#8221; framing is presented (Ryan Greenblatt).Agent security in practice: Multiple posts treat desktop/browser agents as inherently high-risk until prompt injection and sandboxing mature, reinforcing the need for strict isolation, least privilege, and careful handling of credentials (Miessler).Top tweets (by engagement)&#8220;Clawdbot&#8221; misuse example (explicitly harmful)Karpathy on the phase shift to &#8220;programming in English&#8221; via agentsDario Amodei&#8217;s &#8220;Adolescence of Technology&#8221;AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local LLM Hardware and Benchmarking216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 366): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these GPUs when used in parallel. The image shows a technical setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM capacity. The discussion centers around the feasibility and efficiency of using these older GPUs compared to modern devices, particularly in terms of bandwidth and cooling challenges. Commenters express skepticism about the performance of these GPUs, noting potential issues with bandwidth and cooling. One commenter shares personal experience, comparing different GPU models and highlighting the challenges of using older hardware.HugoCortell raises a technical concern about the potential bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could impact the performance of local LLMs if not addressed properly.dc740 shares insights from personal experience with different GPUs, highlighting that the P40 outperforms the M10 despite both being older models. However, they prefer using AMD Instinct Mi50 GPUs due to their performance, even though support for these was recently dropped from ROCm, indicating a trade-off between hardware capability and software support.FullOf_Bad_Ideas critiques the gpu_box_benchmark for not testing scenarios where large models are split across multiple GPUs, which is a primary use case for setups with extensive VRAM. This points to a gap in current benchmarking practices that may not fully reflect real-world applications of multi-GPU systems.I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it? (Activity: 724): The image shows a terminal window on a Linux system running the &#8216;top&#8217; command, which is used to monitor system processes and resource usage in real-time. The user has won an Nvidia DGX Spark GB10, a high-performance computing device designed for machine learning and data-intensive tasks. The terminal indicates a Python process consuming significant CPU resources, suggesting active computational tasks, possibly related to machine learning or data processing. The user is considering using the device to run multiple NextJS applications simultaneously, leveraging its powerful capabilities. One commenter suggests running three NextJS applications simultaneously, indicating the device&#8217;s capability to handle multiple high-memory tasks. Another commenter provides a link to Nvidia&#8217;s DGX Spark playbooks, which could be useful for the user to explore the full potential of their new hardware.Fit-Produce420 highlights the capabilities of the Nvidia DGX Spark GB10, noting that with 128GB of memory, it can fine-tune models up to 70 billion parameters. Additionally, it can handle larger models like the 120 billion parameter gtp-oss-120b using techniques like QLoRA, which optimizes memory usage for large-scale models. However, running dense models like devstral 2 may be slow due to their computational demands.randomfoo2 suggests utilizing the NVIDIA DGX Spark playbooks as a resource for getting started with the DGX Spark GB10. These playbooks provide structured guidance and best practices for deploying and managing workloads on the DGX platform, which can be particularly useful for users new to this hardware.LicensedTerrapin humorously suggests selling the DGX Spark GB10 to purchase 8GB of DDR5 RAM, implying a trade-off between high-end specialized hardware and more general-purpose upgrades. This comment reflects a common debate in tech communities about the value of specialized versus general-purpose hardware investments.Using a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference. (Activity: 29): The post discusses the feasibility of using a high-end MacBook Pro with Apple Silicon (M-series Max) versus a Windows/Linux laptop with an RTX 5090 GPU for running large local LLMs (70B+ parameters) for inference and fine-tuning. The MacBook Pro offers 128&#8211;192 GB of unified memory, while the RTX 5090 laptop provides 24 GB of VRAM and at least 64 GB of system RAM. The primary use case is local LLM inference with a target of &#8805;15 tokens/sec, emphasizing portability. The post queries whether the larger unified memory of Apple Silicon outweighs the CUDA performance of the RTX laptop for inference, and how Apple MLX compares to CUDA for fine-tuning tasks like LoRA/QLoRA. It also seeks insights on thermal performance and sustained inference capabilities of both setups. One commenter suggests using the laptop as a terminal to a more powerful desktop, indicating a preference for leveraging remote resources over local hardware. Another commenter is experimenting with both setups, using a MacBook Pro M2 Max for inference, and is curious about the performance differences.racerx509 shares their experience using a Lenovo laptop with a 3070ti, a custom desktop with a 5070, and a MacBook Pro M2 Max with 96GB RAM for inference tasks. They note that they have been primarily using the MacBook Pro for inference, suggesting it may offer better performance or convenience for their needs.No-Concern-8832 raises a concern about the VRAM limitations of RTX laptops, suggesting that they may not be sufficient for running large models like 70B parameters. This highlights a potential limitation in using high-end RTX laptops for certain deep learning tasks that require substantial VRAM.Tired__Dev discusses their experience with an Asus M16 equipped with a 4090 GPU, noting that it struggled with a 7B parameter model. They express a preference for a MacBook Pro with 128GB RAM, citing its high memory bandwidth and potential performance advantages over even high-end GPU setups like the DGX Spark.2. Multi-Agent Systems and AI AssistantsI built a &#8220;hive mind&#8221; for Claude Code - 7 agents sharing memory and talking to each other (Activity: 313): The post describes a multi-agent orchestration system for Claude Code, featuring seven specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The implementation stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, open-source under the MIT license, and available on GitHub. A comment questions the system&#8217;s uniqueness compared to the BMAD method, suggesting similarities. Another comment humorously questions whether the agents agree with each other, hinting at potential coordination challenges.The user robiinn inquires about the differences between the &#8216;hive mind&#8217; system and the bmad method, suggesting a potential similarity. This indicates a need for clarification on the unique aspects or improvements of the &#8216;hive mind&#8217; approach over existing methods, such as how memory sharing and inter-agent communication are implemented differently.No_Afternoon_4260 raises a critical point about the consensus among the agents in the &#8216;hive mind&#8217;. This touches on the technical challenge of ensuring that multiple agents can not only share memory but also reach agreement or consensus, which is a significant aspect of distributed systems and multi-agent frameworks.JellyBean504 draws a parallel between the &#8216;hive mind&#8217; and Steve Yegge&#8217;s Gastown, suggesting that there might be conceptual similarities. This comparison could be valuable for understanding the architectural or functional parallels between the two systems, potentially offering insights into design choices or performance characteristics.Clawdbot: the AI assistant that actually messages you first (Activity: 214): Clawdbot is an open-source AI assistant with over 9K GitHub stars, designed to proactively message users, unlike traditional AI assistants that wait for prompts. It integrates with locally hosted LLMs via Ollama and supports messaging apps like WhatsApp, Telegram, and Discord. Key features include sending automated briefings and reminders, local storage of conversations as Markdown files, and the ability to control browsers and run scripts. The software is free under the MIT license but requires terminal proficiency for setup, as there is no GUI installer. Read more. Users report challenges with setup, particularly with obtaining and using OAuth keys for authentication, and difficulties in connecting local LLMs without relying on API keys. Some users express frustration with the complexity of setup, especially when using remote machines.mike7seven highlights the complexity of setting up Clawdbot, particularly emphasizing the need to obtain a Claude OAuth key on a separate machine and then transfer it to the setup machine. This process is noted as cumbersome, especially for those using remote machines, and the MacOS app requires building from source, adding another layer of complexity.Ashamed_Promise7726 raises a technical challenge regarding the integration of local language models with Clawdbot. The user notes difficulty in connecting pre-downloaded models on their PC, as Clawdbot seems to require an API key for usage-based models, questioning the feasibility of running Clawdbot entirely locally without external dependencies.inigid warns about potential security risks associated with Clawdbot, suggesting it could be exploited for supply-chain attacks that compromise sensitive data on a user&#8217;s machine and network. The comment also mentions concerns about the association with Solana meme coins, implying a need for caution when using the tool.3. GLM-4.7-Flash Performance UpdatesGLM-4.7-Flash is even faster now (Activity: 443): The recent update to llama.cpp by Johannes Gaessler optimizes the CUDA implementation of FlashAttention, specifically for models with a non-power-of-2 ratio of query heads to key/value heads. This is achieved by padding Q columns to the next power of 2, which, although slightly inefficient, enhances performance for small batch sizes. The update is detailed in pull request #19092. One comment humorously notes the obsolescence of a previous post due to this update, while another laments the lack of support for AMD GPUs, highlighting a common issue in the community regarding hardware compatibility.The user &#8216;jacek2023&#8217; provides detailed performance metrics for the GLM-4.7-Flash model, highlighting its efficiency. The model processes a prompt with 45074 tokens, achieving a prompt evaluation time of 2814.63 ms for 1612 tokens, which translates to 1.75 ms per token or 572.72 tokens per second. The overall evaluation time is 29352.57 ms for 1731 tokens, equating to 16.96 ms per token or 58.97 tokens per second. The total processing time is 32167.20 ms for 3343 tokens, indicating significant improvements in speed.KV cache fix for GLM 4.7 Flash (Activity: 380): The recent update to GLM 4.7 Flash involves removing the V component from the KV cache, which significantly reduces VRAM usage, allowing for longer context lengths on the same hardware setup. This change is particularly beneficial for models like DeepSeek and GLM 4.7 Flash, as it can save gigabytes of VRAM, enabling context lengths to double, as demonstrated by a user running a 90,000 context on a 4090 GPU. The update is part of a pull request in the llama.cpp repository, which introduces a V-less KV cache, reducing memory usage by nearly 50%. More details can be found in the pull request. A user noted that the model, while improved, still requires some manual guidance, especially in tasks like coding and creative writing, where it may not perform as well as specialized models. However, it excels in tool use and as an assistant, making it a preferred choice for home-server applications.The user &#8216;teachersecret&#8217; reports significant improvements in context handling with the UD&#8217;s k_xl 4-bit version of the GLM 4.7 model on an RTX 4090. Previously, the model maxed out at 45,000 context tokens, but now it can handle 90,000. Despite these improvements, the model still requires some manual guidance, especially in coding tasks, and is less effective in creative writing compared to other models. However, it excels in tool usage and is now the user&#8217;s default model for their home server.User &#8216;viperx7&#8217; provides detailed benchmark data comparing the performance of the GLM 4.7 model before and after a specific change. The benchmarks show improvements in both prompt processing and token generation speeds across different configurations. For instance, using a single RTX 4090, the context size increased from 64k to 128k, with prompt processing speed improving from 3489 t/s to 3510 t/s and token generation from 88 t/s to 92.5 t/s. The maximum context size achievable with a 4090 and 3060 setup is 200k, leaving about 6GB of VRAM unused.The discussion highlights the technical aspect of the GLM 4.7 model&#8217;s KV cache fix, which allows for increased context sizes and improved performance metrics. The benchmarks provided by &#8216;viperx7&#8217; indicate that the model can now handle up to 207k context size in certain configurations, with significant improvements in processing speeds. This suggests that the model&#8217;s efficiency has been enhanced, making it more suitable for high-demand applications.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude AI Usage and IssuesWhy You Need To Constantly Clear Claude Codes Context Window (Activity: 166): The post highlights the necessity of regularly clearing the context window when using coding agents like Claude to maintain optimal performance. It notes that performance degrades significantly when the context window exceeds 40% of its capacity due to the quadratic nature of LLM attention, which increases computational demands and introduces noise. The recommended practice is to avoid accumulating context and instead persist it by using a &#8216;one session per task&#8217; strategy, ensuring each task starts with a fresh context. More details can be found in the original article. Commenters suggest practical strategies such as using handover prompts to transfer necessary details between sessions, employing the &#8216;/clear&#8217; command to compact context, and utilizing &#8216;Plan Mode&#8217; to clear context and execute tasks efficiently. These methods reportedly help avoid the need for a full context window, even for large tasks.Agrippanux suggests using &#8216;Plan Mode&#8217; as the default setting for Claude, which allows users to clear the context and execute plans without needing a full context window. This approach has been effective for large tasks, such as refactoring, without requiring the entire context to be loaded, thus optimizing performance and resource usage.thurn2 discusses the use of sub-agents in Claude, which involves delegating tasks like creating a git worktree and fixing specific issues. This method allows for parallel execution of tasks and helps in managing complex projects by breaking them down into smaller, manageable tasks, enhancing efficiency and implementation accuracy.Fancy_Excitement6050 notes that as the context window grows, Claude tends to take shortcuts, which can lead to a need for constant reminders to maintain thoroughness. This suggests that managing the context window size is crucial for maintaining the quality of output, and there might be differences in performance between different Claude plans, such as Claude Max.Opus fell off? Here&#8217;s the workflow that kept my code quality stable (Activity: 133): The post discusses a structured workflow to maintain code quality when using AI models like Opus and Sonnet, which have been perceived as producing &#8220;confident wrong&#8221; outputs and drifting edits. The workflow emphasizes a loop of specification, ticket creation, execution, and verification. Specifications are detailed with non-goals, user stories, acceptance criteria, edge cases, and more, treated as code to ensure clarity. Tickets are derived from specs, focusing on small, independently mergeable tasks with clear acceptance checks. Execution involves implementing one ticket at a time with constraints to prevent scope drift, and verification involves running tests and confirming acceptance criteria before feeding failures back into the model for correction. This approach aims to maintain discipline and reduce reliance on the model&#8217;s &#8220;done&#8221; signal, ensuring stable and reliable outputs. Commenters agree that the workflow is effective, emphasizing that AI models function more like junior engineers requiring clear specifications and strict feedback loops. This approach shifts effort towards upfront clarity and external verification, making the system more stable and less reliant on the model&#8217;s intelligence. Smaller scoped tickets and hard verification are noted as beneficial strategies.GenOS2312 highlights the importance of treating LLMs like junior engineers, emphasizing that a well-specified problem and a strict feedback loop are crucial for reliable outputs. The workflow discussed focuses on upfront clarity and external verification, which stabilizes the system by not relying on the model&#8217;s intelligence but rather constraining it to ensure even average runs yield acceptable results.Different-Object5926 notes that smaller scoped tickets combined with hard verification processes significantly improve the stability and reliability of using models like Opus. This approach mitigates the impact of variability in model performance, suggesting that the issue isn&#8217;t just &#8216;unlucky runs&#8217; but rather the need for structured constraints.TheOriginalAcidtech suggests implementing hooks to prevent skipping steps in the workflow, emphasizing that the human interface is often the weakest link. By enforcing strict adherence to the process, the system can better manage user interactions, ensuring that the model and its harness guide the user effectively, rather than relying solely on the model&#8217;s capabilities.after claude now chatgpt is also uses Grokipedia as source (Activity: 634): The image and accompanying discussion highlight that the latest version of ChatGPT is reportedly using Elon Musk&#8217;s Grokipedia as a source. This is significant as it suggests a shift in the data sources used by ChatGPT, potentially affecting the information quality and bias in its responses. The comments reveal a concern about the implications of using Grokipedia, particularly regarding the potential for biased information, as one user notes the risk of models being influenced by &#8216;right wing&#8217; content. However, it is clarified that Grokipedia is not used as training data but rather as a search tool, which may mitigate some concerns about direct bias in the model&#8217;s foundational knowledge.The discussion highlights concerns about language models like Claude and ChatGPT potentially using sources like Grokipedia, which may have biased or unreliable content. This raises questions about the integrity of the information these models provide, especially when they utilize search tools to access real-time data. The implication is that the quality and neutrality of the data sources are crucial for maintaining the accuracy and trustworthiness of AI outputs.There is a debate about the impact of using sources like Grokipedia on the training and performance of language models. Some commenters express concern that incorporating biased or politically skewed sources could lead to the dissemination of misinformation. This reflects broader worries about the influence of data sources on the objectivity and reliability of AI-generated content.The mention of Reddit as a data source for language models suggests a comparison of potential biases. While some argue that Reddit may contain more extreme or varied viewpoints, the underlying issue is the challenge of ensuring that AI models are trained on balanced and factual data. This discussion underscores the importance of curating high-quality datasets to prevent the spread of biased information.Giving Claude full access to a laptop (Activity: 795): The post discusses the implementation of giving Claude, an AI model, full access to a laptop, allowing it to autonomously manage a virtual machine (VM) on Ubuntu Google Cloud. The user describes how Claude can be remotely controlled via Discord to build new features and fix bugs, logging major actions with timestamps in a markdown file for memory management. This setup enables the user to learn from Claude&#8217;s problem-solving processes and manage workflows effectively, even as a newcomer to programming. One commenter, a desktop support technician, expressed amazement at the implementation, noting its potential impact on job roles, while another sought clarification on the technical specifics of giving Claude full device access.xxxBigMemerxxx describes using Claude to manage a Google Cloud VM running Ubuntu, highlighting its ability to autonomously handle tasks and build features. They mention using Discord for remote requests and bug fixes, and implementing a logging system with markdown and Unicode for tracking changes. This setup allows for a dynamic interaction with Claude, enabling it to learn from errors and maintain a form of short-term memory by logging recent updates.Happy_Requirement187 shares their experience running Claude on an AWS EC2 instance with Ubuntu Linux, accessed via SSH from a Windows laptop. They utilize a Jupyter notebook server for seamless file sharing between the EC2 instance and their local environment, a method recommended by Anthropic. Additionally, they have set up a Ruby on Rails environment with a React frontend for secure file sharing, allowing them to request files via Slack, demonstrating a sophisticated integration of Claude into their workflow.sivadneb inquires about setting up voice control in Linux, indicating a technical challenge in integrating voice commands with Claude. This suggests an interest in expanding the interaction capabilities with Claude beyond text-based commands, potentially enhancing the usability and accessibility of the system.CLAUDE.md says &#8216;MUST use agent&#8217; - Claude ignores it 80% of the time. (Activity: 309): The image and post discuss a technical issue with the CLAUDE.md file, which is supposed to direct the AI, Claude, to use a specific agent for workflow questions. Despite explicit instructions in the file, Claude often defaults to a generic agent, indicating a lack of enforcement in the system. The post suggests that without technical enforcement mechanisms, such as hooks or stronger prompts, instructions are merely suggestions. The image emphasizes these points with highlighted text, suggesting potential solutions like adding enforcement hooks to ensure compliance with the specified workflow. Commenters suggest that the issue may stem from unclear instructions, emphasizing the need for simple and direct commands. They also highlight the importance of implementing technical solutions, such as hooks, to enforce compliance with the CLAUDE.md instructions.Accomplished_Buy9342 suggests using hooks to manage Claude&#8217;s behavior, providing a link to a GitHub repository that demonstrates how to block the main chat from performing actions and delegate tasks to a subagent. This approach can help in orchestrating Claude&#8217;s actions more effectively, especially when dealing with complex tasks or large contexts.luka5c0m highlights a common issue with Claude when used at scale: as the context grows beyond a few files, the agent may perform unexpected actions. They suggest that instead of relying solely on better prompts, developers should use hooks and dynamic instructions to maintain a sharp and concise context. They also mention working on a dynamic CLAUDE.md file that adapts to the current task, which could help in managing large or nested files effectively.My Ralph Wiggum breakdown just got endorsed as the official explainer (Activity: 170): The post discusses a video breakdown of Ralph Wiggum, an autonomous coding loop, which has been endorsed by Geoffrey Huntley as the official explainer. Ralph Wiggum is a bash while loop that calls Claude in headless mode, allowing for autonomous code implementation without context degradation. Key features include avoiding the Anthropic Ralph plugin due to performance issues, using fresh context windows for each iteration, and emphasizing the importance of concise specs to prevent hitting a &#8220;dumb zone.&#8221; The video link is here. The comments include a link to the endorsement post by Geoffrey Huntley, and general positive feedback on the video, indicating its usefulness and quality.Dennis1451 highlights a practical application of the Ralph Wiggum breakdown, noting the importance of using a well-defined specification and clearing context for optimal results. They mention using &#8216;auto compact&#8217; without a clear spec initially, which suggests that following the guidelines provided in the breakdown could enhance performance and accuracy.messiah-of-cheese expresses a desire for more scientific validation in the video, particularly regarding the &#8216;dumb zone&#8217; premise. This indicates a need for empirical evidence or data to support the claims made in the breakdown, which could strengthen its credibility and acceptance among a technical audience.2. ICLR and ICML 2026 Conference Discussions[D] ICLR 2026 decision mega thread (Activity: 1589): The post announces the imminent release of ICLR 2026 review decisions, with anticipation heightened due to a previous incident involving OpenReview. The community is preparing for the outcomes, with some users humorously sharing acceptance prediction models based on historical data, such as a simple return uniform(0, 1) &gt; 0.7. This reflects a light-hearted approach to the uncertainty of paper acceptance. The comments reflect a mix of anticipation and humor, with some users expressing frustration over misleading emails from other conferences like ICML, which adds to the tension of awaiting ICLR decisions.[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow? (Activity: 279): The post highlights a situation where an author&#8217;s paper was desk-rejected by ICML 2026, yet they were retained as a reviewer. This reflects a common practice in academic conferences where the author and reviewer pipelines are separate; desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This situation underscores the reliance on unpaid labor in academia, where reviewing is seen as community service, but the feedback loop for authorship and recognition is weak. A notable opinion from the comments suggests that the separation between the author and reviewer roles can feel insulting, as these decisions are made by different parts of the conference organization. It highlights the need for conferences to clarify this separation to avoid personal affronts.AccordingWeight6019 highlights a systemic issue in academic publishing where the processes for desk rejection and reviewer selection are distinct. Desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This separation can lead to feelings of insult among authors, but it&#8217;s a structural necessity due to the different roles and responsibilities within the publication process. The comment suggests that conferences should improve transparency about these processes to mitigate personal feelings of rejection.mocny-chlapik points out that the responsibility for a desk rejection often lies with the author, particularly if it results from not following submission guidelines. The comment implies that submitting a paper, even if desk rejected, obligates the author to fulfill reviewer duties, as the submission process involves volunteer time and resources. This highlights the importance of adhering to submission instructions to avoid unnecessary strain on the peer review system.[R] Appealing ICLR 2026 AC Decisions... (Activity: 138): The post discusses a situation where an author received mixed reviews for a paper submitted to ICLR 2026, with scores of 4(3)/6(4)/6(4)/6(4). The author invested significant resources, including $1.6k on new experiments and added 20+ pages of theory, to address reviewer concerns. Despite these efforts, the metareview cited &#8220;outstanding concerns&#8221; that the author believes were addressed, raising questions about the review process&#8217;s fairness and accuracy. The author is seeking advice on appealing the decision, expressing frustration that improvements were seemingly ignored. Commenters generally agree that appealing decisions at conferences like ICLR is not feasible, attributing outcomes to luck and the subjective nature of reviews. Some suggest that the meta-review process can be inconsistent, with one commenter noting that meta-reviewers sometimes act as an additional critical reviewer, potentially skewing outcomes.tedd235 discusses the variability in paper acceptance at conferences, suggesting that some PhD students might reject papers to improve their own odds, making the process feel like a &#8216;coin flip&#8217;. They note that if other reviewers provide higher scores, the Area Chair (AC) might consider this in their decision, indicating a potential for subjective bias in the review process.Fantastic-Nerve-4056 shares an experience from AAMAS where despite receiving scores of 6 and 8 from reviewers, the Meta Reviewer recommended rejection with minimal justification, stating it was &#8216;relevant for other AAMAS session&#8217;. This highlights issues with the transparency and accountability of meta-reviewer decisions, which can override individual reviewer scores without detailed explanation.Intrepid_Discount_67 describes a thorough submission process, including extensive theoretical analysis, comprehensive baseline comparisons, and open-sourced code, yet faced non-responsive reviewers and an AC that upheld the initial scores. This underscores challenges in the review process where detailed responses and transparency do not necessarily lead to favorable outcomes.[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy? (Activity: 151): The image describes a new policy implemented by the International Conference on Machine Learning (ICML) where reviewers will be evaluated by meta-reviewers. The top 25% of reviewers will be recognized as &#8216;gold reviewers&#8217; and will receive free registration, while the next 25% will be designated as &#8216;silver reviewers.&#8217; These distinctions are intended to incentivize high-quality reviews and will be considered in financial aid applications. This policy aims to improve the quality of reviews by providing recognition and potential financial benefits to diligent reviewers. Some commenters express skepticism about the effectiveness of this policy, questioning who will oversee the meta-reviewers themselves. Others see it as a positive step, particularly for reviewers from low-resource backgrounds, and suggest further recognition at conferences to encourage quality reviewing.Bitter-Reserve3821 highlights that area chairs have traditionally been responsible for rating reviews, typically using a three-tier system: &#8216;did not meet expectations&#8217;, &#8216;satisfactory&#8217;, or &#8216;exceeded expectations&#8217;. This practice is not new, and there have been &#8216;Best Reviewer&#8217; awards in the past, sometimes offering incentives like free conference registrations.Unhappy_Craft1906 raises a concern about the feasibility of this policy for top labs with substantial funding, questioning whether they would participate in the review process merely for free registrations. This points to a potential disparity in how different institutions might engage with the policy based on their resources.newperson77777777 suggests an extension of the policy by introducing a visible recognition system, such as a gold or silver star on conference badges, to incentivize quality reviewing. This idea aims to foster a culture of excellence and accountability within the reviewing community.3. OpenAI and AI Industry Legal and Business DevelopmentsThings Get Worse For OpenAI: Consumer groups prep class action suits about their price fixing and supply manipulation through DRAM hoarding. (Activity: 107): OpenAI is facing potential class action lawsuits for allegedly hoarding DRAM to manipulate prices and disadvantage competitors, with accusations of securing nearly 40% of the global DRAM supply. Consumer groups argue this constitutes &#8216;predatory bidding&#8217; and violates antitrust laws like the Sherman and Clayton Acts. The Free Software Foundation and other groups are pursuing legal remedies, arguing DRAM should be considered an &#8216;Essential Facility&#8217; due to its critical role in AI, while the FTC and European Commission investigate potential violations of competition laws. The DOJ is also examining whether OpenAI&#8217;s &#8216;Stargate&#8217; project constitutes a &#8216;monopsony&#8217;. Commenters question why only OpenAI is targeted and not other companies like Nvidia, and debate whether buying RAM constitutes price fixing, suggesting that supply issues may not be OpenAI&#8217;s fault.Alacritous69 argues that OpenAI&#8217;s purchase of RAM does not constitute price fixing, as they are actively using the resources rather than hoarding them. The commenter suggests that the issue lies with suppliers&#8217; inability to meet demand, rather than any manipulative practices by OpenAI.sambull raises a strategic business perspective, suggesting that by purchasing large quantities of RAM, OpenAI could be intentionally limiting resources available to competitors, including those developing at-home language models. This could be seen as a competitive strategy to maintain market dominance.max6296 questions why the focus is solely on OpenAI when Nvidia could also be implicated in similar practices, hinting at a broader industry issue regarding resource allocation and market influence.When Ads aren&#8217;t enough: OpenAI&#8217;s push to Claim a Cut of Customers&#8217; AI Discoveries (Activity: 63): OpenAI is exploring new business models beyond traditional subscriptions and ads, focusing on outcome-based pricing and IP-based agreements. This approach would allow OpenAI to claim a share of the value created when their AI models contribute to profitable outcomes, particularly in enterprise sectors like pharma, scientific research, and energy systems. This strategy aligns OpenAI&#8217;s revenue with customer success, aiming to capture more value as AI capabilities expand. OpenAI&#8217;s annualized recurring revenue has surged from 2B in 2023 to over 20B in 2025, driven by increased compute scaling. This move is part of a broader trend among AI firms towards value-based pricing, amidst criticism from figures like Elon Musk, who accuses OpenAI of abandoning its nonprofit origins. The community is divided, with some viewing this as a logical evolution of AI monetization, while others criticize it as overly profit-driven. Comparisons are drawn to other industries, suggesting skepticism about the feasibility and fairness of such models.CATL, the world&#8217;s largest battery maker, launches sodium batteries: extremely durable, stable at &#8211;40&#176;C, much cheaper than lithium (5x), safer,10,000 charge cycles, requires no nickel or cobalt... (Activity: 1289): CATL has launched the first mass-produced sodium-ion batteries, offering a cost-effective alternative to lithium-ion with a price of ~$20 per kWh compared to lithium&#8217;s ~$100 per kWh. These batteries, part of the Tianxing II range, are designed for microvans and small trucks, featuring an energy density of 175 Wh/kg and a lifespan of over 10,000 cycles, maintaining 90% capacity at -40&#176;C. They utilize a hard carbon electrode and prussian-blue cathode, eliminating the need for nickel or cobalt, and are expected to be scaled up for broader use, including in Europe by 2026. Read more. Some commenters express surprise at the application of sodium batteries in vehicles, expecting them to be used in stationary systems due to weight concerns. Others note the strategic advantage for China in advancing battery technology, contrasting it with perceived setbacks in the US market.The Tianxing II range of sodium batteries by CATL is specifically designed for microvans, light vans, and small trucks, indicating a focus on applications where energy density and weight are less critical compared to cost and durability. This suggests a strategic move to target markets where these factors are prioritized, potentially offering a competitive edge over traditional lithium-ion batteries.The introduction of sodium batteries into vehicles is surprising to some, as it was expected that such technology would first be applied to stationary applications like home energy storage. This is due to the lower energy density of sodium batteries compared to lithium-ion, which makes them less ideal for applications where weight and size are critical factors.There is curiosity about the commercial availability of these sodium batteries, with questions about whether they can be purchased directly for home use or if they will be distributed through third-party vendors. The performance metrics, such as 10,000 charge cycles and operation at -40&#176;C, are impressive and suggest that sodium batteries could rival LiFePO4 in terms of performance, especially given their cost advantage.K-Shaped AI Adoption? (Activity: 748): The image highlights a discussion by Kevin Roose on the &#8216;K-shaped&#8217; adoption of AI technologies, where there is a significant divide between early adopters, particularly in tech hubs like San Francisco, and those who are lagging due to restrictive IT policies. This disparity is creating a cultural and technical divide, with early adopters integrating AI deeply into their workflows, while others struggle to gain access to even basic AI tools. The conversation points to a broader issue of accessibility and the potential for some workers to be left behind in the AI revolution. Commenters note that the disparity in AI adoption is exacerbated by the complexity of the technology, which requires a certain level of expertise to use effectively. Additionally, the high cost of advanced AI tools, such as &#8216;multi-agent claudeswarm,&#8217; limits access to those with sufficient financial resources, further widening the gap.Setsuiii highlights the technical barrier to effective AI use, noting that current AI technologies require users to have a certain level of expertise to achieve optimal results. This complexity, combined with ongoing ethical debates surrounding AI, may deter widespread adoption. However, those who can navigate these challenges have significant opportunities, although competition is increasing as more technically adept individuals enter the field.Glxblt76 and Gubzs discuss the financial barriers to AI adoption, particularly the high costs associated with advanced AI tools like a &#8216;multi-agent claudeswarm,&#8217; which can cost around $200 a month. This expense limits access to those with substantial financial resources, such as individuals in tech hubs like San Francisco, while the majority cannot afford such investments.o5mfiHTNsH748KVq shares a personal experience of leaving an enterprise job to join a smaller company, emphasizing the importance of unrestricted access to Large Language Models (LLMs) for maintaining competitiveness in the AI field. They argue that any limitations on LLM access can significantly hinder development speed and career progression, suggesting that smaller companies may offer more flexibility in leveraging AI technologies.Former Harvard CS Professor: AI is improving exponentially and will replace most human programmers within 4-15 years. (Activity: 1260): Matt Welsh, a former Harvard CS professor and current Engineering Director at Google, predicts that AI will advance exponentially, potentially replacing most human programmers within 4-15 years. This assertion is based on the rapid improvements in AI capabilities, suggesting a transformative impact on software development and the tech industry. The discussion is available in a YouTube video. One comment highlights the potential for AI to not only replace programmers but also to enable anyone with AI to replicate existing products and services, indicating a broader impact on innovation and competition.The claim that AI will replace most human programmers within 4-15 years is met with skepticism, particularly regarding the use of the term &#8216;exponential&#8217;. Critics argue that the term is often misused, even by experts, to describe growth that may not fit the mathematical definition of exponential growth. This misuse can lead to misunderstandings about the actual pace and nature of AI development.The discussion highlights the potential for AI to disrupt existing products and services if it can indeed replace human programmers. This implies that AI could democratize software development, allowing anyone with access to AI tools to create competitive products, potentially leading to significant shifts in the tech industry landscape.The mention of the speaker&#8217;s credentials, specifically as a former Harvard professor and current Engineering Director at Google, adds weight to the prediction. However, some commenters find the emphasis on his past academic title rather than his current industry role to be misleading, suggesting that his current position might provide more relevant insights into AI&#8217;s trajectory.AI Discord RecapA summary of Summaries of Summaries by gpt-51. Funding Frenzy in AI InfrastructureRecursive Raises Roar to $4B: Recursive Intelligence is reportedly raising at a $4B valuation to accelerate AI&#8209;driven chip design, creating a closed loop between hardware and models, per Bloomberg: Recursive Intelligence in talks at $4B. The Jan 23, 2026 report highlights a strategy of using AI to shorten design cycles and boost performance for next&#8209;gen accelerators.Engineers framed the pitch as a &#8220;self&#8209;improving feedback loop&#8221; where better chips train better models that design better chips, amplifying returns on AI&#8209;for&#8209;EDA investment. Community sentiment read this as validation that AI&#8209;native silicon is a core moat, not a sideshow, aligning with recent lab spin&#8209;outs and infra bets.Sky Lab Startups Skyrocket: UC Berkeley&#8217;s Sky Lab spin&#8209;outs saw major marks: SGLang ~$400M, vLLM ~$800M, and LMArena ~$1.7B, per Alex Dimakis: Sky Lab startup valuations. These January 2026 milestones underscore investor appetite for serving stacks, token&#8209;throughput infra, and benchmarking platforms.Engineers read this as a green light for building on top of vLLM/SGLang primitives and contributing to Arena&#8209;style evals, with one takeaway that practical throughput wins deals. The funding spread also suggests a portfolio thesis across serving, compilers, and eval marketplaces rather than a single-bet strategy.Maia Muscles Into Azure: Microsoft&#8217;s Maia 200 accelerator went live in Azure, touting 30% better performance per dollar, 216GB HBM3e, and 7TB/s memory bandwidth, per Satya Nadella: Maia 200 in Azure. The platform targets high&#8209;performance inference for large&#8209;scale LLM and multimodal workloads.Builders highlighted that memory topology and bandwidth are the story here, with &#8220;30% better perf/$&#8221; resonating for cost&#8209;sensitive inference deployments at scale. Teams expect immediate tests against vLLM and SGLang stacks to gauge token latency, context scaling, and multi&#8209;tenant isolation.2. Kernels, Chips, and Serving: Inference at Warp SpeedFlashInfer Face&#8209;Off Fires Up MLSys: The MLSys 2026 FlashInfer&#8209;Bench competition challenges teams to build LLM inference kernels for NVIDIA Blackwell GPUs, competing against expert FlashInfer baselines&#8212;see MLSys 2026 FlashInfer&#8209;Bench Competition. Tracks emphasize real&#8209;world throughput and correctness under production&#8209;like constraints.Organizers invite agents that &#8220;design LLM inference kernels&#8221;, pushing program synthesis to meet kernel&#8209;level performance bars. Participants expect aggressive focus on GEMM, KV&#8209;cache motion, and scheduler tactics aligned with Blackwell&#8217;s memory hierarchy.GPU&#8209;64 Gets Gains with KV&#8209;Cache CAM: A new inference&#8209;only architecture, GPU&#8209;64, introduces a hardware KV&#8209;Cache via on&#8209;chip CAM, claiming 4&#215; faster inference at 75W and reducing memory lookup from O(N) &#8594; O(1), per GPU&#8209;64 (Zenodo) with RTL/emulator at gpu64&#8209;inference (GitHub). The design targets LLM&#8209;heavy workloads with KV bottlenecks.Developers flagged the CAM&#8209;based cache as a bold bet on associative search for token histories, noting portability implications for Flash&#8209;style attention and speculative decoding. Discussion centered on whether future ISA/driver stacks can expose these gains without bespoke compilers.Cornserve Cuts Tail Latency: Cornserve presents an online serving system for Any&#8209;to&#8209;Any multimodal models that optimizes deployment plans across encoders, LLMs, and DiTs, per Cornserve (arXiv), with an overview talk at Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube). The paper reports throughput gains and tail&#8209;latency reductions under heterogeneous pipelines.Infra engineers liked its planner&#8209;driven scheduling for encoder/decoder mixes and saw it as complementary to vLLM for multimodal graphs. The big open question: standardizing budgeted reasoning and co&#8209;scheduling across text, vision, and diffusion stages without over&#8209;tokenizing control messages.3. New Multimodal and Coding Models Land in LM ArenaWAN 2.6 Walks In (With Upload Woes): LM Arena added wan2.6&#8209;t2i (text&#8209;to&#8209;image) and wan2.6&#8209;image (image edit) to the image arena: LM Arena &#8212; Image Chat. Users noted wan2.6&#8209;image requires an uploaded image and that wan2.6&#8209;t2i currently lacks image&#8209;upload support.Staff acknowledged the upload gap and are working to enable image uploads for wan2.6&#8209;t2i. Builders suggested testing edit pipelines where masking, prompt strength, and seed control align with Arena scoring to benchmark edit fidelity.Devstral Duels and Text Titans: The Code Arena now features devstral&#8209;2 for head&#8209;to&#8209;head comparisons&#8212;see LM Arena &#8212; Code Arena Direct Battle. On the text side, qwen3&#8209;max&#8209;thinking and molmo&#8209;2&#8209;8b joined the lineup: LM Arena &#8212; Text Arena.Engineers are probing reasoning traces and tool&#8209;using prompts to stress code synthesis and refactor quality under tight token budgets. Early chatter favored task&#8209;specific evaluations (e.g., SWE&#8209;style bug&#8209;fix vs. ground&#8209;up implementation) to surface model deltas.Hunyuan Hits the Leaderboard: Tencent&#8217;s Hunyuan&#8209;Image&#8209;3.0&#8209;Instruct ranks #7 on LM Arena&#8217;s image&#8209;edit board&#8212;see LM Arena &#8212; Image Edit Leaderboard&#8212;after a launch post: Tencent Hunyuan announces HunyuanImage 3.0&#8209;Instruct. The model touts an 80B MoE, Native CoT, and MixGRPO for tighter intent alignment.Creators emphasized edit controllability and multi&#8209;image fusion, while evaluators asked for masking robustness, text fidelity, and artifact rates under compositional prompts. Teams plan to pit it against WAN 2.6 variants using the Arena&#8217;s standardized edit tasks.4. Safety, Reliability, and Hallucination HardeningClamp the Chaos: Layer&#8209;Native Safety: Layer&#8209;Native Safety Clamping proposes learning activation&#8209;space harm directions and clamping them to block jailbreaks, with a 10K&#8209;pair dataset at Pacific&#8209;Prime/safety_dataset (HF) and the paper on Zenodo. Authors argue in&#8209;model clamping can&#8217;t be bypassed via prompt manipulation.Red&#8209;teamers liked the idea of activation&#8209;level controls versus brittle prompt filters, but pressed for tests against tool&#8209;use and multi&#8209;turn attacks. Expect follow&#8209;ups measuring side effects on helpfulness, coding accuracy, and false positives under adversarial prompting.Symbolic Sanity Checks Stop Slip&#8209;Ups: Hybrid approaches check logical consistency for math/code/simple facts, as shown in Consistency Checking for LLMs (arXiv:2409.13724), while broader consistency remains tough per Scaling Consistency Beyond Formal Domains (arXiv:2507.10624). Eleuther discussions framed this as practical hallucination reduction via symbolic/deductive layers.Builders reported wins when pairing symbolic checkers with tool&#8209;augmented prompts, cautioning that coverage gaps appear outside formal domains. The consensus: start with code/math guardrails, then expand to factual QA with curated KBs and provenance scoring.5. Agent Tooling and Reasoning Workflows MatureLevante Leads with MCP&#8209;Native Workspace: Levante launched an open&#8209;source MCP&#8209;native AI workspace for local models (e.g., Ollama) with a modular UI&#8212;download at Levante. Engineers highlighted easier tool wiring, local privacy, and composable panes for rapid agent iteration.Early users framed it as a practical hub for tool&#8209;calling and filesystem ops without cloud dependence. Teams plan to benchmark context bloat and tool discoverability patterns versus conventional agent shells.RLM Riffs: AsyncReview + Skills Pack: AsyncFuncAI open&#8209;sourced AsyncReview, a DSPy RLM code&#8209;review agent at AsyncReview (GitHub), and a skills kit landed on npm as @unravel&#8209;tech/rlm&#8209;skills. This pairs reasoning&#8209;first prompting with drop&#8209;in skills to extend models.Contributors reported smoother trace inspection and optimizer&#8209;guided prompt tuning for multi&#8209;step modules. One practitioner noted that rejecting premature answers in the metric is key for reliable RLM fine&#8209;tuning.Agents Auto&#8209;Assemble a Browser Engine: FastRender&#8212;a browser rendering engine&#8212;was built using 2,000 AI coding agents, documented by Simon Willison in FastRender: built by 2,000 agents. The project demonstrates task decomposition, verification, and orchestration at non&#8209;trivial software scale.Engineers debated handoff granularity and spec&#8209;to&#8209;test loops needed to keep multi&#8209;agent pipelines from drifting. The case study strengthens the argument that agentic coding can target complex infra when coupled with strict eval harnesses and artifact gating.",
          "url": "https://www.latent.space/p/ainews-anthropic-launches-the-mcp",
          "author": "Unknown",
          "published": "2026-01-27T07:20:28",
          "source": "Latent.Space",
          "source_type": "rss",
          "tags": [],
          "summary": "Anthropic has launched the MCP Apps open specification with native support in Claude.ai, working with OpenAI, Block, VS Code, JetBrains, AWS, and others. This formalizes the Model Context Protocol as an industry standard for AI agent-app integration.",
          "importance_score": 81.0,
          "reasoning": "Cross-industry collaboration on agent infrastructure standard is significant for the agentic AI ecosystem. OpenAI and Anthropic cooperating on specs signals important industry alignment.",
          "themes": [
            "agentic AI",
            "infrastructure",
            "industry standards",
            "Anthropic"
          ],
          "continuation": null,
          "summary_html": "<p>Anthropic has launched the MCP Apps open specification with native support in Claude.ai, working with OpenAI, Block, VS Code, JetBrains, AWS, and others. This formalizes the Model Context Protocol as an industry standard for AI agent-app integration.</p>",
          "content_html": "<p>AI News for 1/23/2026-1/26/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 14285 messages) for you. Estimated reading time saved (at 200wpm): 1208 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!3 months after OpenAI floated a trial balloon with ChatGPT Apps and the Apps SDK at Dev Day 2025, Anthropic has now officially absorbed the independent MCP UI project and, working with OpenAI, Block, VS Code, Antigravity, JetBrains, AWS, and others, has released both:the MCP Apps specofficial support in Claude.ai - comparatively very well received but of course not as popular as the Claude in Excel announcement.It’s fair to say that ChatGPT Apps haven’t exactly taken the world by storm since announcement, but the overall need for a standard format for applications to return rich UI still cannot be denied. Now that MCP Apps have been ratified by all the important players, this is the basis for a rich ecosystem of open source support and applications being able to interoperate, and perhaps one day solve the perpetual never ending pile of $20/month subscriptions piling up in your credit card bills.As a reminder, we interviewed David Soria Parra and the rest of the AAIF, who previewed a bit of the thinking and design process behind MCP Apps here:AI Twitter RecapAgent Orchestration, RLMs, and “Clawdbot/Clawd” as a UX patternNVIDIA ToolOrchestra + Orchestrator-8B: NVIDIA’s ToolOrchestra frames agentic systems as a small “conductor” model that alternates reasoning with calls to tools and larger “expert” models (search, code execution, specialist LLMs, frontier generalists). The claim is that an 8B orchestrator can reach frontier-level outcomes via delegation at materially lower cost, trained end-to-end with scalable RL using automatically synthesized tool-use environments and multi-turn tasks (summary, link). Closest technical implication: “controller scale” matters less than policy quality + tool/model routing if you can train it with realistic tool-call rollouts.RLMs / recursion-first agent stacks: Several posts converge on a Recursive Language Model (RLM) pattern: pass files and context by reference and iteratively pull the minimum slices needed (shell/grep/AST), rather than stuffing everything into context à la ReAct. Dan B illustrates this with file references vs @file expansion as deliberate context management (thread). Daytona is positioning RLMs as “unlimited recursion depth” via per-(sub)agent sandboxes (guide, integration).“Clawd/Clawdbot” meme → product signal: The dataset contains a large “Clawdbot” wave (often with Mac mini jokes), but the technically relevant throughline is outcome-first assistant UX + tight context/tool integration. Kimmonismus explicitly calls this a shift from “more chat” to “more outcome,” suggesting incumbents will scramble to match it (tweet). Others push a cloud-first counterpoint (no local Mac mini) (MiniMax reply). There’s also an emerging security backlash as soon as “powerful mode” exists: prompt injection remains a system-level blocker for browser/desktop agents (dilemma, follow-up, Miessler warnings).Reasoning model releases &amp; eval dynamics (Qwen, Tencent, ARC, etc.)Alibaba Qwen3-Max-Thinking: Alibaba positions Qwen3-Max-Thinking as a flagship reasoning+agent model trained with “massive scale and advanced RL,” emphasizing adaptive tool-use (Search/Memory/Code Interpreter) and test-time scaling/self-reflection. They cite strong math and agentic search metrics (e.g., 98.0 on HMMT Feb, 49.8 on HLE) (launch). The model is immediately pushed into public eval channels: LM Arena Text Arena (Arena) and Yupp (Yupp). Community reaction highlights the tool-enabled evaluation regime—claims of outperforming multiple SOTA models on HLE with search tools (commentary).Tencent HunyuanImage 3.0-Instruct (image editing): Tencent releases an image-editing-focused multimodal model built on an 80B MoE (13B active), using a “Thinking” schema with native CoT and their MixGRPO algorithm; focus is on precise edits that preserve non-target regions and multi-image fusion (announcement). LM Arena reports it entering the top-10 image edit leaderboard (rank #7) (Arena).ARC-AGI cost/perf hacks: A notable optimization claim: “Recursive Self-Aggregation (RSA) + Gemini 3 Flash” reaching 59.31% on ARC-AGI-2 at ~1/10 cost vs Gemini Deep Think (tweet). This points to a broader theme: meta-inference strategies (aggregation, recursion, pruning) are becoming as important as base model choice.Open models in arenas: Molmo 2 (Apache 2.0) appears in Arena as a new open model entrant (Arena). Separately, Hugging Face Inference Endpoint notes GLM-4.7-Flash via llama.cpp with a low hourly price point (Q4_K_M, 24k context) (ngxson)—underscoring a continued commoditization of fast open-weight inference.RL everywhere: test-time training, GRPO stabilization, RL-as-pretraining, and compute savingsTest-Time Training (TTT) + RL breakthroughs: A widely shared result claims a Stanford/NVIDIA-style TTT+RL approach that: beats AlphaEvolve, finds a new upper bound for an Erdős overlap problem, produces A100 kernels ~2× faster than best human kernels, and beats both best AI+human attempts on AtCoder (rronak_). This cluster also includes meta-discussion about correctly crediting related approaches (EvoTune) (Yejin Cho).GRPO training stability knobs: A small but actionable engineering tip: INTELLECT-2 reports a delta=4.0 parameter that improves GRPO stability (QGallouedec).RL in pretraining (RLP): NVIDIA authors announce RLP (Reinforcement as a Pretraining Objective) accepted to ICLR 2026, framing RL not as “post-training only” but as integrated into pretraining (ahatamiz1).Compute reduction via curriculum-like filtering: AI21’s “Dynamic Data Snoozing” claims up to 3× compute reduction for RLVR by snoozing examples that are too easy (DanielGissin). If validated, this is a practical recipe: make the sampler policy-aware instead of static.Inference infrastructure &amp; dev tooling: vLLM’s “day-0 model support,” VS Code MCP Apps, Cursor subagentsvLLM’s governance and commercialization pressure: A long Zhihu-derived summary argues vLLM’s “open-source project → startup” shift was driven by the hidden cost of day-0 support (weeks/months of confidential pre-integration per new model), the rise of MoE and heterogeneous inference (fp8/int4/sparse attention), and the mismatch with PyTorch Foundation style testing vs vLLM’s multi-node CI needs. It claims the maintainers founded Inferact Inc to fund full-time maintainers while keeping vLLM open-source (thread). Related: vLLM shares a practical flag for avoiding OOM on long-context models: --max-model-len auto (vLLM tip).MCP Apps: tool calls return interactive UI: The MCP ecosystem announces MCP Apps as the first official MCP extension: tool calls can return interactive UI components rendered in-chat. VS Code is first major editor shipping support (Insiders now, stable soon) (VS Code, alexalbert__). Anthropic simultaneously ships “interactive work tools in Claude” (Slack drafting, Figma diagrams, Asana timelines) (Claude). Net: we’re seeing the “tool interface layer” move from raw JSON to native UI primitives inside agent loops.Cursor: multi-browser subagents: Cursor adds multi-browser support via subagents (Cursor), echoing the same direction: parallelized tool execution + better context isolation.Kernel LLMs, chip stacks, and “AI for hardware” loopsGPU MODE 2026: post-training Kernel LLMs in public: GPU MODE outlines a 2026 plan to post-train a Kernel LLM and get generated kernels merged into real repos (PyTorch/vLLM), emphasizing “de-slopify kernels” (determinism, reviewer-mergeable PRs), profiler-guided optimization + memory work, and competitions as evals (marksaroufim).Microsoft Maia 200: Microsoft announces Maia 200 as a custom inference accelerator; Mustafa Suleyman claims it’s the most performant first-party hyperscaler silicon, with 3× FP4 performance vs Trainium v3 and FP8 above TPU v7 (Mustafa, follow-up). Yusuf Mehdi frames this as infra that makes AI “dependable” (thread).Ricursive Intelligence (AI for chip design): Ricursive raises a $300M Series A aiming at end-to-end chip design as a recursive self-improvement loop between AI and hardware (company, Anna Goldie).Safety, misuse, and societal impact (selected items with direct technical relevance)Elicitation attacks via benign chemistry data: Anthropic reports that fine-tuning open models on “benign” chemical synthesis content generated by frontier models can significantly increase capability on chemical weapons tasks—an “elicitation attack” that scales with frontier model strength (AnthropicAI, paper link).Dario Amodei’s “Adolescence of Technology” essay: A major, highly engaged post argues AI is entering an accelerating feedback loop (AI building AI), with risks spanning misuse, power-seeking autonomy, and economic disruption; it also explicitly frames wealth concentration as a society-breaking failure mode (Dario). Reaction ranges from strong endorsement to critique of how “takeover risk” framing is presented (Ryan Greenblatt).Agent security in practice: Multiple posts treat desktop/browser agents as inherently high-risk until prompt injection and sandboxing mature, reinforcing the need for strict isolation, least privilege, and careful handling of credentials (Miessler).Top tweets (by engagement)“Clawdbot” misuse example (explicitly harmful)Karpathy on the phase shift to “programming in English” via agentsDario Amodei’s “Adolescence of Technology”AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local LLM Hardware and Benchmarking216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 366): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these GPUs when used in parallel. The image shows a technical setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM capacity. The discussion centers around the feasibility and efficiency of using these older GPUs compared to modern devices, particularly in terms of bandwidth and cooling challenges. Commenters express skepticism about the performance of these GPUs, noting potential issues with bandwidth and cooling. One commenter shares personal experience, comparing different GPU models and highlighting the challenges of using older hardware.HugoCortell raises a technical concern about the potential bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could impact the performance of local LLMs if not addressed properly.dc740 shares insights from personal experience with different GPUs, highlighting that the P40 outperforms the M10 despite both being older models. However, they prefer using AMD Instinct Mi50 GPUs due to their performance, even though support for these was recently dropped from ROCm, indicating a trade-off between hardware capability and software support.FullOf_Bad_Ideas critiques the gpu_box_benchmark for not testing scenarios where large models are split across multiple GPUs, which is a primary use case for setups with extensive VRAM. This points to a gap in current benchmarking practices that may not fully reflect real-world applications of multi-GPU systems.I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it? (Activity: 724): The image shows a terminal window on a Linux system running the ‘top’ command, which is used to monitor system processes and resource usage in real-time. The user has won an Nvidia DGX Spark GB10, a high-performance computing device designed for machine learning and data-intensive tasks. The terminal indicates a Python process consuming significant CPU resources, suggesting active computational tasks, possibly related to machine learning or data processing. The user is considering using the device to run multiple NextJS applications simultaneously, leveraging its powerful capabilities. One commenter suggests running three NextJS applications simultaneously, indicating the device’s capability to handle multiple high-memory tasks. Another commenter provides a link to Nvidia’s DGX Spark playbooks, which could be useful for the user to explore the full potential of their new hardware.Fit-Produce420 highlights the capabilities of the Nvidia DGX Spark GB10, noting that with 128GB of memory, it can fine-tune models up to 70 billion parameters. Additionally, it can handle larger models like the 120 billion parameter gtp-oss-120b using techniques like QLoRA, which optimizes memory usage for large-scale models. However, running dense models like devstral 2 may be slow due to their computational demands.randomfoo2 suggests utilizing the NVIDIA DGX Spark playbooks as a resource for getting started with the DGX Spark GB10. These playbooks provide structured guidance and best practices for deploying and managing workloads on the DGX platform, which can be particularly useful for users new to this hardware.LicensedTerrapin humorously suggests selling the DGX Spark GB10 to purchase 8GB of DDR5 RAM, implying a trade-off between high-end specialized hardware and more general-purpose upgrades. This comment reflects a common debate in tech communities about the value of specialized versus general-purpose hardware investments.Using a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference. (Activity: 29): The post discusses the feasibility of using a high-end MacBook Pro with Apple Silicon (M-series Max) versus a Windows/Linux laptop with an RTX 5090 GPU for running large local LLMs (70B+ parameters) for inference and fine-tuning. The MacBook Pro offers 128–192 GB of unified memory, while the RTX 5090 laptop provides 24 GB of VRAM and at least 64 GB of system RAM. The primary use case is local LLM inference with a target of ≥15 tokens/sec, emphasizing portability. The post queries whether the larger unified memory of Apple Silicon outweighs the CUDA performance of the RTX laptop for inference, and how Apple MLX compares to CUDA for fine-tuning tasks like LoRA/QLoRA. It also seeks insights on thermal performance and sustained inference capabilities of both setups. One commenter suggests using the laptop as a terminal to a more powerful desktop, indicating a preference for leveraging remote resources over local hardware. Another commenter is experimenting with both setups, using a MacBook Pro M2 Max for inference, and is curious about the performance differences.racerx509 shares their experience using a Lenovo laptop with a 3070ti, a custom desktop with a 5070, and a MacBook Pro M2 Max with 96GB RAM for inference tasks. They note that they have been primarily using the MacBook Pro for inference, suggesting it may offer better performance or convenience for their needs.No-Concern-8832 raises a concern about the VRAM limitations of RTX laptops, suggesting that they may not be sufficient for running large models like 70B parameters. This highlights a potential limitation in using high-end RTX laptops for certain deep learning tasks that require substantial VRAM.Tired__Dev discusses their experience with an Asus M16 equipped with a 4090 GPU, noting that it struggled with a 7B parameter model. They express a preference for a MacBook Pro with 128GB RAM, citing its high memory bandwidth and potential performance advantages over even high-end GPU setups like the DGX Spark.2. Multi-Agent Systems and AI AssistantsI built a “hive mind” for Claude Code - 7 agents sharing memory and talking to each other (Activity: 313): The post describes a multi-agent orchestration system for Claude Code, featuring seven specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The implementation stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, open-source under the MIT license, and available on GitHub. A comment questions the system’s uniqueness compared to the BMAD method, suggesting similarities. Another comment humorously questions whether the agents agree with each other, hinting at potential coordination challenges.The user robiinn inquires about the differences between the ‘hive mind’ system and the bmad method, suggesting a potential similarity. This indicates a need for clarification on the unique aspects or improvements of the ‘hive mind’ approach over existing methods, such as how memory sharing and inter-agent communication are implemented differently.No_Afternoon_4260 raises a critical point about the consensus among the agents in the ‘hive mind’. This touches on the technical challenge of ensuring that multiple agents can not only share memory but also reach agreement or consensus, which is a significant aspect of distributed systems and multi-agent frameworks.JellyBean504 draws a parallel between the ‘hive mind’ and Steve Yegge’s Gastown, suggesting that there might be conceptual similarities. This comparison could be valuable for understanding the architectural or functional parallels between the two systems, potentially offering insights into design choices or performance characteristics.Clawdbot: the AI assistant that actually messages you first (Activity: 214): Clawdbot is an open-source AI assistant with over 9K GitHub stars, designed to proactively message users, unlike traditional AI assistants that wait for prompts. It integrates with locally hosted LLMs via Ollama and supports messaging apps like WhatsApp, Telegram, and Discord. Key features include sending automated briefings and reminders, local storage of conversations as Markdown files, and the ability to control browsers and run scripts. The software is free under the MIT license but requires terminal proficiency for setup, as there is no GUI installer. Read more. Users report challenges with setup, particularly with obtaining and using OAuth keys for authentication, and difficulties in connecting local LLMs without relying on API keys. Some users express frustration with the complexity of setup, especially when using remote machines.mike7seven highlights the complexity of setting up Clawdbot, particularly emphasizing the need to obtain a Claude OAuth key on a separate machine and then transfer it to the setup machine. This process is noted as cumbersome, especially for those using remote machines, and the MacOS app requires building from source, adding another layer of complexity.Ashamed_Promise7726 raises a technical challenge regarding the integration of local language models with Clawdbot. The user notes difficulty in connecting pre-downloaded models on their PC, as Clawdbot seems to require an API key for usage-based models, questioning the feasibility of running Clawdbot entirely locally without external dependencies.inigid warns about potential security risks associated with Clawdbot, suggesting it could be exploited for supply-chain attacks that compromise sensitive data on a user’s machine and network. The comment also mentions concerns about the association with Solana meme coins, implying a need for caution when using the tool.3. GLM-4.7-Flash Performance UpdatesGLM-4.7-Flash is even faster now (Activity: 443): The recent update to llama.cpp by Johannes Gaessler optimizes the CUDA implementation of FlashAttention, specifically for models with a non-power-of-2 ratio of query heads to key/value heads. This is achieved by padding Q columns to the next power of 2, which, although slightly inefficient, enhances performance for small batch sizes. The update is detailed in pull request #19092. One comment humorously notes the obsolescence of a previous post due to this update, while another laments the lack of support for AMD GPUs, highlighting a common issue in the community regarding hardware compatibility.The user ‘jacek2023’ provides detailed performance metrics for the GLM-4.7-Flash model, highlighting its efficiency. The model processes a prompt with 45074 tokens, achieving a prompt evaluation time of 2814.63 ms for 1612 tokens, which translates to 1.75 ms per token or 572.72 tokens per second. The overall evaluation time is 29352.57 ms for 1731 tokens, equating to 16.96 ms per token or 58.97 tokens per second. The total processing time is 32167.20 ms for 3343 tokens, indicating significant improvements in speed.KV cache fix for GLM 4.7 Flash (Activity: 380): The recent update to GLM 4.7 Flash involves removing the V component from the KV cache, which significantly reduces VRAM usage, allowing for longer context lengths on the same hardware setup. This change is particularly beneficial for models like DeepSeek and GLM 4.7 Flash, as it can save gigabytes of VRAM, enabling context lengths to double, as demonstrated by a user running a 90,000 context on a 4090 GPU. The update is part of a pull request in the llama.cpp repository, which introduces a V-less KV cache, reducing memory usage by nearly 50%. More details can be found in the pull request. A user noted that the model, while improved, still requires some manual guidance, especially in tasks like coding and creative writing, where it may not perform as well as specialized models. However, it excels in tool use and as an assistant, making it a preferred choice for home-server applications.The user ‘teachersecret’ reports significant improvements in context handling with the UD’s k_xl 4-bit version of the GLM 4.7 model on an RTX 4090. Previously, the model maxed out at 45,000 context tokens, but now it can handle 90,000. Despite these improvements, the model still requires some manual guidance, especially in coding tasks, and is less effective in creative writing compared to other models. However, it excels in tool usage and is now the user’s default model for their home server.User ‘viperx7’ provides detailed benchmark data comparing the performance of the GLM 4.7 model before and after a specific change. The benchmarks show improvements in both prompt processing and token generation speeds across different configurations. For instance, using a single RTX 4090, the context size increased from 64k to 128k, with prompt processing speed improving from 3489 t/s to 3510 t/s and token generation from 88 t/s to 92.5 t/s. The maximum context size achievable with a 4090 and 3060 setup is 200k, leaving about 6GB of VRAM unused.The discussion highlights the technical aspect of the GLM 4.7 model’s KV cache fix, which allows for increased context sizes and improved performance metrics. The benchmarks provided by ‘viperx7’ indicate that the model can now handle up to 207k context size in certain configurations, with significant improvements in processing speeds. This suggests that the model’s efficiency has been enhanced, making it more suitable for high-demand applications.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude AI Usage and IssuesWhy You Need To Constantly Clear Claude Codes Context Window (Activity: 166): The post highlights the necessity of regularly clearing the context window when using coding agents like Claude to maintain optimal performance. It notes that performance degrades significantly when the context window exceeds 40% of its capacity due to the quadratic nature of LLM attention, which increases computational demands and introduces noise. The recommended practice is to avoid accumulating context and instead persist it by using a ‘one session per task’ strategy, ensuring each task starts with a fresh context. More details can be found in the original article. Commenters suggest practical strategies such as using handover prompts to transfer necessary details between sessions, employing the ‘/clear’ command to compact context, and utilizing ‘Plan Mode’ to clear context and execute tasks efficiently. These methods reportedly help avoid the need for a full context window, even for large tasks.Agrippanux suggests using ‘Plan Mode’ as the default setting for Claude, which allows users to clear the context and execute plans without needing a full context window. This approach has been effective for large tasks, such as refactoring, without requiring the entire context to be loaded, thus optimizing performance and resource usage.thurn2 discusses the use of sub-agents in Claude, which involves delegating tasks like creating a git worktree and fixing specific issues. This method allows for parallel execution of tasks and helps in managing complex projects by breaking them down into smaller, manageable tasks, enhancing efficiency and implementation accuracy.Fancy_Excitement6050 notes that as the context window grows, Claude tends to take shortcuts, which can lead to a need for constant reminders to maintain thoroughness. This suggests that managing the context window size is crucial for maintaining the quality of output, and there might be differences in performance between different Claude plans, such as Claude Max.Opus fell off? Here’s the workflow that kept my code quality stable (Activity: 133): The post discusses a structured workflow to maintain code quality when using AI models like Opus and Sonnet, which have been perceived as producing “confident wrong” outputs and drifting edits. The workflow emphasizes a loop of specification, ticket creation, execution, and verification. Specifications are detailed with non-goals, user stories, acceptance criteria, edge cases, and more, treated as code to ensure clarity. Tickets are derived from specs, focusing on small, independently mergeable tasks with clear acceptance checks. Execution involves implementing one ticket at a time with constraints to prevent scope drift, and verification involves running tests and confirming acceptance criteria before feeding failures back into the model for correction. This approach aims to maintain discipline and reduce reliance on the model’s “done” signal, ensuring stable and reliable outputs. Commenters agree that the workflow is effective, emphasizing that AI models function more like junior engineers requiring clear specifications and strict feedback loops. This approach shifts effort towards upfront clarity and external verification, making the system more stable and less reliant on the model’s intelligence. Smaller scoped tickets and hard verification are noted as beneficial strategies.GenOS2312 highlights the importance of treating LLMs like junior engineers, emphasizing that a well-specified problem and a strict feedback loop are crucial for reliable outputs. The workflow discussed focuses on upfront clarity and external verification, which stabilizes the system by not relying on the model’s intelligence but rather constraining it to ensure even average runs yield acceptable results.Different-Object5926 notes that smaller scoped tickets combined with hard verification processes significantly improve the stability and reliability of using models like Opus. This approach mitigates the impact of variability in model performance, suggesting that the issue isn’t just ‘unlucky runs’ but rather the need for structured constraints.TheOriginalAcidtech suggests implementing hooks to prevent skipping steps in the workflow, emphasizing that the human interface is often the weakest link. By enforcing strict adherence to the process, the system can better manage user interactions, ensuring that the model and its harness guide the user effectively, rather than relying solely on the model’s capabilities.after claude now chatgpt is also uses Grokipedia as source (Activity: 634): The image and accompanying discussion highlight that the latest version of ChatGPT is reportedly using Elon Musk’s Grokipedia as a source. This is significant as it suggests a shift in the data sources used by ChatGPT, potentially affecting the information quality and bias in its responses. The comments reveal a concern about the implications of using Grokipedia, particularly regarding the potential for biased information, as one user notes the risk of models being influenced by ‘right wing’ content. However, it is clarified that Grokipedia is not used as training data but rather as a search tool, which may mitigate some concerns about direct bias in the model’s foundational knowledge.The discussion highlights concerns about language models like Claude and ChatGPT potentially using sources like Grokipedia, which may have biased or unreliable content. This raises questions about the integrity of the information these models provide, especially when they utilize search tools to access real-time data. The implication is that the quality and neutrality of the data sources are crucial for maintaining the accuracy and trustworthiness of AI outputs.There is a debate about the impact of using sources like Grokipedia on the training and performance of language models. Some commenters express concern that incorporating biased or politically skewed sources could lead to the dissemination of misinformation. This reflects broader worries about the influence of data sources on the objectivity and reliability of AI-generated content.The mention of Reddit as a data source for language models suggests a comparison of potential biases. While some argue that Reddit may contain more extreme or varied viewpoints, the underlying issue is the challenge of ensuring that AI models are trained on balanced and factual data. This discussion underscores the importance of curating high-quality datasets to prevent the spread of biased information.Giving Claude full access to a laptop (Activity: 795): The post discusses the implementation of giving Claude, an AI model, full access to a laptop, allowing it to autonomously manage a virtual machine (VM) on Ubuntu Google Cloud. The user describes how Claude can be remotely controlled via Discord to build new features and fix bugs, logging major actions with timestamps in a markdown file for memory management. This setup enables the user to learn from Claude’s problem-solving processes and manage workflows effectively, even as a newcomer to programming. One commenter, a desktop support technician, expressed amazement at the implementation, noting its potential impact on job roles, while another sought clarification on the technical specifics of giving Claude full device access.xxxBigMemerxxx describes using Claude to manage a Google Cloud VM running Ubuntu, highlighting its ability to autonomously handle tasks and build features. They mention using Discord for remote requests and bug fixes, and implementing a logging system with markdown and Unicode for tracking changes. This setup allows for a dynamic interaction with Claude, enabling it to learn from errors and maintain a form of short-term memory by logging recent updates.Happy_Requirement187 shares their experience running Claude on an AWS EC2 instance with Ubuntu Linux, accessed via SSH from a Windows laptop. They utilize a Jupyter notebook server for seamless file sharing between the EC2 instance and their local environment, a method recommended by Anthropic. Additionally, they have set up a Ruby on Rails environment with a React frontend for secure file sharing, allowing them to request files via Slack, demonstrating a sophisticated integration of Claude into their workflow.sivadneb inquires about setting up voice control in Linux, indicating a technical challenge in integrating voice commands with Claude. This suggests an interest in expanding the interaction capabilities with Claude beyond text-based commands, potentially enhancing the usability and accessibility of the system.CLAUDE.md says ‘MUST use agent’ - Claude ignores it 80% of the time. (Activity: 309): The image and post discuss a technical issue with the CLAUDE.md file, which is supposed to direct the AI, Claude, to use a specific agent for workflow questions. Despite explicit instructions in the file, Claude often defaults to a generic agent, indicating a lack of enforcement in the system. The post suggests that without technical enforcement mechanisms, such as hooks or stronger prompts, instructions are merely suggestions. The image emphasizes these points with highlighted text, suggesting potential solutions like adding enforcement hooks to ensure compliance with the specified workflow. Commenters suggest that the issue may stem from unclear instructions, emphasizing the need for simple and direct commands. They also highlight the importance of implementing technical solutions, such as hooks, to enforce compliance with the CLAUDE.md instructions.Accomplished_Buy9342 suggests using hooks to manage Claude’s behavior, providing a link to a GitHub repository that demonstrates how to block the main chat from performing actions and delegate tasks to a subagent. This approach can help in orchestrating Claude’s actions more effectively, especially when dealing with complex tasks or large contexts.luka5c0m highlights a common issue with Claude when used at scale: as the context grows beyond a few files, the agent may perform unexpected actions. They suggest that instead of relying solely on better prompts, developers should use hooks and dynamic instructions to maintain a sharp and concise context. They also mention working on a dynamic CLAUDE.md file that adapts to the current task, which could help in managing large or nested files effectively.My Ralph Wiggum breakdown just got endorsed as the official explainer (Activity: 170): The post discusses a video breakdown of Ralph Wiggum, an autonomous coding loop, which has been endorsed by Geoffrey Huntley as the official explainer. Ralph Wiggum is a bash while loop that calls Claude in headless mode, allowing for autonomous code implementation without context degradation. Key features include avoiding the Anthropic Ralph plugin due to performance issues, using fresh context windows for each iteration, and emphasizing the importance of concise specs to prevent hitting a “dumb zone.” The video link is here. The comments include a link to the endorsement post by Geoffrey Huntley, and general positive feedback on the video, indicating its usefulness and quality.Dennis1451 highlights a practical application of the Ralph Wiggum breakdown, noting the importance of using a well-defined specification and clearing context for optimal results. They mention using ‘auto compact’ without a clear spec initially, which suggests that following the guidelines provided in the breakdown could enhance performance and accuracy.messiah-of-cheese expresses a desire for more scientific validation in the video, particularly regarding the ‘dumb zone’ premise. This indicates a need for empirical evidence or data to support the claims made in the breakdown, which could strengthen its credibility and acceptance among a technical audience.2. ICLR and ICML 2026 Conference Discussions[D] ICLR 2026 decision mega thread (Activity: 1589): The post announces the imminent release of ICLR 2026 review decisions, with anticipation heightened due to a previous incident involving OpenReview. The community is preparing for the outcomes, with some users humorously sharing acceptance prediction models based on historical data, such as a simple return uniform(0, 1) &gt; 0.7. This reflects a light-hearted approach to the uncertainty of paper acceptance. The comments reflect a mix of anticipation and humor, with some users expressing frustration over misleading emails from other conferences like ICML, which adds to the tension of awaiting ICLR decisions.[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow? (Activity: 279): The post highlights a situation where an author’s paper was desk-rejected by ICML 2026, yet they were retained as a reviewer. This reflects a common practice in academic conferences where the author and reviewer pipelines are separate; desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This situation underscores the reliance on unpaid labor in academia, where reviewing is seen as community service, but the feedback loop for authorship and recognition is weak. A notable opinion from the comments suggests that the separation between the author and reviewer roles can feel insulting, as these decisions are made by different parts of the conference organization. It highlights the need for conferences to clarify this separation to avoid personal affronts.AccordingWeight6019 highlights a systemic issue in academic publishing where the processes for desk rejection and reviewer selection are distinct. Desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This separation can lead to feelings of insult among authors, but it’s a structural necessity due to the different roles and responsibilities within the publication process. The comment suggests that conferences should improve transparency about these processes to mitigate personal feelings of rejection.mocny-chlapik points out that the responsibility for a desk rejection often lies with the author, particularly if it results from not following submission guidelines. The comment implies that submitting a paper, even if desk rejected, obligates the author to fulfill reviewer duties, as the submission process involves volunteer time and resources. This highlights the importance of adhering to submission instructions to avoid unnecessary strain on the peer review system.[R] Appealing ICLR 2026 AC Decisions... (Activity: 138): The post discusses a situation where an author received mixed reviews for a paper submitted to ICLR 2026, with scores of 4(3)/6(4)/6(4)/6(4). The author invested significant resources, including $1.6k on new experiments and added 20+ pages of theory, to address reviewer concerns. Despite these efforts, the metareview cited “outstanding concerns” that the author believes were addressed, raising questions about the review process’s fairness and accuracy. The author is seeking advice on appealing the decision, expressing frustration that improvements were seemingly ignored. Commenters generally agree that appealing decisions at conferences like ICLR is not feasible, attributing outcomes to luck and the subjective nature of reviews. Some suggest that the meta-review process can be inconsistent, with one commenter noting that meta-reviewers sometimes act as an additional critical reviewer, potentially skewing outcomes.tedd235 discusses the variability in paper acceptance at conferences, suggesting that some PhD students might reject papers to improve their own odds, making the process feel like a ‘coin flip’. They note that if other reviewers provide higher scores, the Area Chair (AC) might consider this in their decision, indicating a potential for subjective bias in the review process.Fantastic-Nerve-4056 shares an experience from AAMAS where despite receiving scores of 6 and 8 from reviewers, the Meta Reviewer recommended rejection with minimal justification, stating it was ‘relevant for other AAMAS session’. This highlights issues with the transparency and accountability of meta-reviewer decisions, which can override individual reviewer scores without detailed explanation.Intrepid_Discount_67 describes a thorough submission process, including extensive theoretical analysis, comprehensive baseline comparisons, and open-sourced code, yet faced non-responsive reviewers and an AC that upheld the initial scores. This underscores challenges in the review process where detailed responses and transparency do not necessarily lead to favorable outcomes.[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy? (Activity: 151): The image describes a new policy implemented by the International Conference on Machine Learning (ICML) where reviewers will be evaluated by meta-reviewers. The top 25% of reviewers will be recognized as ‘gold reviewers’ and will receive free registration, while the next 25% will be designated as ‘silver reviewers.’ These distinctions are intended to incentivize high-quality reviews and will be considered in financial aid applications. This policy aims to improve the quality of reviews by providing recognition and potential financial benefits to diligent reviewers. Some commenters express skepticism about the effectiveness of this policy, questioning who will oversee the meta-reviewers themselves. Others see it as a positive step, particularly for reviewers from low-resource backgrounds, and suggest further recognition at conferences to encourage quality reviewing.Bitter-Reserve3821 highlights that area chairs have traditionally been responsible for rating reviews, typically using a three-tier system: ‘did not meet expectations’, ‘satisfactory’, or ‘exceeded expectations’. This practice is not new, and there have been ‘Best Reviewer’ awards in the past, sometimes offering incentives like free conference registrations.Unhappy_Craft1906 raises a concern about the feasibility of this policy for top labs with substantial funding, questioning whether they would participate in the review process merely for free registrations. This points to a potential disparity in how different institutions might engage with the policy based on their resources.newperson77777777 suggests an extension of the policy by introducing a visible recognition system, such as a gold or silver star on conference badges, to incentivize quality reviewing. This idea aims to foster a culture of excellence and accountability within the reviewing community.3. OpenAI and AI Industry Legal and Business DevelopmentsThings Get Worse For OpenAI: Consumer groups prep class action suits about their price fixing and supply manipulation through DRAM hoarding. (Activity: 107): OpenAI is facing potential class action lawsuits for allegedly hoarding DRAM to manipulate prices and disadvantage competitors, with accusations of securing nearly 40% of the global DRAM supply. Consumer groups argue this constitutes ‘predatory bidding’ and violates antitrust laws like the Sherman and Clayton Acts. The Free Software Foundation and other groups are pursuing legal remedies, arguing DRAM should be considered an ‘Essential Facility’ due to its critical role in AI, while the FTC and European Commission investigate potential violations of competition laws. The DOJ is also examining whether OpenAI’s ‘Stargate’ project constitutes a ‘monopsony’. Commenters question why only OpenAI is targeted and not other companies like Nvidia, and debate whether buying RAM constitutes price fixing, suggesting that supply issues may not be OpenAI’s fault.Alacritous69 argues that OpenAI’s purchase of RAM does not constitute price fixing, as they are actively using the resources rather than hoarding them. The commenter suggests that the issue lies with suppliers’ inability to meet demand, rather than any manipulative practices by OpenAI.sambull raises a strategic business perspective, suggesting that by purchasing large quantities of RAM, OpenAI could be intentionally limiting resources available to competitors, including those developing at-home language models. This could be seen as a competitive strategy to maintain market dominance.max6296 questions why the focus is solely on OpenAI when Nvidia could also be implicated in similar practices, hinting at a broader industry issue regarding resource allocation and market influence.When Ads aren’t enough: OpenAI’s push to Claim a Cut of Customers’ AI Discoveries (Activity: 63): OpenAI is exploring new business models beyond traditional subscriptions and ads, focusing on outcome-based pricing and IP-based agreements. This approach would allow OpenAI to claim a share of the value created when their AI models contribute to profitable outcomes, particularly in enterprise sectors like pharma, scientific research, and energy systems. This strategy aligns OpenAI’s revenue with customer success, aiming to capture more value as AI capabilities expand. OpenAI’s annualized recurring revenue has surged from 2B in 2023 to over 20B in 2025, driven by increased compute scaling. This move is part of a broader trend among AI firms towards value-based pricing, amidst criticism from figures like Elon Musk, who accuses OpenAI of abandoning its nonprofit origins. The community is divided, with some viewing this as a logical evolution of AI monetization, while others criticize it as overly profit-driven. Comparisons are drawn to other industries, suggesting skepticism about the feasibility and fairness of such models.CATL, the world’s largest battery maker, launches sodium batteries: extremely durable, stable at –40°C, much cheaper than lithium (5x), safer,10,000 charge cycles, requires no nickel or cobalt... (Activity: 1289): CATL has launched the first mass-produced sodium-ion batteries, offering a cost-effective alternative to lithium-ion with a price of ~$20 per kWh compared to lithium’s ~$100 per kWh. These batteries, part of the Tianxing II range, are designed for microvans and small trucks, featuring an energy density of 175 Wh/kg and a lifespan of over 10,000 cycles, maintaining 90% capacity at -40°C. They utilize a hard carbon electrode and prussian-blue cathode, eliminating the need for nickel or cobalt, and are expected to be scaled up for broader use, including in Europe by 2026. Read more. Some commenters express surprise at the application of sodium batteries in vehicles, expecting them to be used in stationary systems due to weight concerns. Others note the strategic advantage for China in advancing battery technology, contrasting it with perceived setbacks in the US market.The Tianxing II range of sodium batteries by CATL is specifically designed for microvans, light vans, and small trucks, indicating a focus on applications where energy density and weight are less critical compared to cost and durability. This suggests a strategic move to target markets where these factors are prioritized, potentially offering a competitive edge over traditional lithium-ion batteries.The introduction of sodium batteries into vehicles is surprising to some, as it was expected that such technology would first be applied to stationary applications like home energy storage. This is due to the lower energy density of sodium batteries compared to lithium-ion, which makes them less ideal for applications where weight and size are critical factors.There is curiosity about the commercial availability of these sodium batteries, with questions about whether they can be purchased directly for home use or if they will be distributed through third-party vendors. The performance metrics, such as 10,000 charge cycles and operation at -40°C, are impressive and suggest that sodium batteries could rival LiFePO4 in terms of performance, especially given their cost advantage.K-Shaped AI Adoption? (Activity: 748): The image highlights a discussion by Kevin Roose on the ‘K-shaped’ adoption of AI technologies, where there is a significant divide between early adopters, particularly in tech hubs like San Francisco, and those who are lagging due to restrictive IT policies. This disparity is creating a cultural and technical divide, with early adopters integrating AI deeply into their workflows, while others struggle to gain access to even basic AI tools. The conversation points to a broader issue of accessibility and the potential for some workers to be left behind in the AI revolution. Commenters note that the disparity in AI adoption is exacerbated by the complexity of the technology, which requires a certain level of expertise to use effectively. Additionally, the high cost of advanced AI tools, such as ‘multi-agent claudeswarm,’ limits access to those with sufficient financial resources, further widening the gap.Setsuiii highlights the technical barrier to effective AI use, noting that current AI technologies require users to have a certain level of expertise to achieve optimal results. This complexity, combined with ongoing ethical debates surrounding AI, may deter widespread adoption. However, those who can navigate these challenges have significant opportunities, although competition is increasing as more technically adept individuals enter the field.Glxblt76 and Gubzs discuss the financial barriers to AI adoption, particularly the high costs associated with advanced AI tools like a ‘multi-agent claudeswarm,’ which can cost around $200 a month. This expense limits access to those with substantial financial resources, such as individuals in tech hubs like San Francisco, while the majority cannot afford such investments.o5mfiHTNsH748KVq shares a personal experience of leaving an enterprise job to join a smaller company, emphasizing the importance of unrestricted access to Large Language Models (LLMs) for maintaining competitiveness in the AI field. They argue that any limitations on LLM access can significantly hinder development speed and career progression, suggesting that smaller companies may offer more flexibility in leveraging AI technologies.Former Harvard CS Professor: AI is improving exponentially and will replace most human programmers within 4-15 years. (Activity: 1260): Matt Welsh, a former Harvard CS professor and current Engineering Director at Google, predicts that AI will advance exponentially, potentially replacing most human programmers within 4-15 years. This assertion is based on the rapid improvements in AI capabilities, suggesting a transformative impact on software development and the tech industry. The discussion is available in a YouTube video. One comment highlights the potential for AI to not only replace programmers but also to enable anyone with AI to replicate existing products and services, indicating a broader impact on innovation and competition.The claim that AI will replace most human programmers within 4-15 years is met with skepticism, particularly regarding the use of the term ‘exponential’. Critics argue that the term is often misused, even by experts, to describe growth that may not fit the mathematical definition of exponential growth. This misuse can lead to misunderstandings about the actual pace and nature of AI development.The discussion highlights the potential for AI to disrupt existing products and services if it can indeed replace human programmers. This implies that AI could democratize software development, allowing anyone with access to AI tools to create competitive products, potentially leading to significant shifts in the tech industry landscape.The mention of the speaker’s credentials, specifically as a former Harvard professor and current Engineering Director at Google, adds weight to the prediction. However, some commenters find the emphasis on his past academic title rather than his current industry role to be misleading, suggesting that his current position might provide more relevant insights into AI’s trajectory.AI Discord RecapA summary of Summaries of Summaries by gpt-51. Funding Frenzy in AI InfrastructureRecursive Raises Roar to $4B: Recursive Intelligence is reportedly raising at a $4B valuation to accelerate AI‑driven chip design, creating a closed loop between hardware and models, per Bloomberg: Recursive Intelligence in talks at $4B. The Jan 23, 2026 report highlights a strategy of using AI to shorten design cycles and boost performance for next‑gen accelerators.Engineers framed the pitch as a “self‑improving feedback loop” where better chips train better models that design better chips, amplifying returns on AI‑for‑EDA investment. Community sentiment read this as validation that AI‑native silicon is a core moat, not a sideshow, aligning with recent lab spin‑outs and infra bets.Sky Lab Startups Skyrocket: UC Berkeley’s Sky Lab spin‑outs saw major marks: SGLang ~$400M, vLLM ~$800M, and LMArena ~$1.7B, per Alex Dimakis: Sky Lab startup valuations. These January 2026 milestones underscore investor appetite for serving stacks, token‑throughput infra, and benchmarking platforms.Engineers read this as a green light for building on top of vLLM/SGLang primitives and contributing to Arena‑style evals, with one takeaway that practical throughput wins deals. The funding spread also suggests a portfolio thesis across serving, compilers, and eval marketplaces rather than a single-bet strategy.Maia Muscles Into Azure: Microsoft’s Maia 200 accelerator went live in Azure, touting 30% better performance per dollar, 216GB HBM3e, and 7TB/s memory bandwidth, per Satya Nadella: Maia 200 in Azure. The platform targets high‑performance inference for large‑scale LLM and multimodal workloads.Builders highlighted that memory topology and bandwidth are the story here, with “30% better perf/$” resonating for cost‑sensitive inference deployments at scale. Teams expect immediate tests against vLLM and SGLang stacks to gauge token latency, context scaling, and multi‑tenant isolation.2. Kernels, Chips, and Serving: Inference at Warp SpeedFlashInfer Face‑Off Fires Up MLSys: The MLSys 2026 FlashInfer‑Bench competition challenges teams to build LLM inference kernels for NVIDIA Blackwell GPUs, competing against expert FlashInfer baselines—see MLSys 2026 FlashInfer‑Bench Competition. Tracks emphasize real‑world throughput and correctness under production‑like constraints.Organizers invite agents that “design LLM inference kernels”, pushing program synthesis to meet kernel‑level performance bars. Participants expect aggressive focus on GEMM, KV‑cache motion, and scheduler tactics aligned with Blackwell’s memory hierarchy.GPU‑64 Gets Gains with KV‑Cache CAM: A new inference‑only architecture, GPU‑64, introduces a hardware KV‑Cache via on‑chip CAM, claiming 4× faster inference at 75W and reducing memory lookup from O(N) → O(1), per GPU‑64 (Zenodo) with RTL/emulator at gpu64‑inference (GitHub). The design targets LLM‑heavy workloads with KV bottlenecks.Developers flagged the CAM‑based cache as a bold bet on associative search for token histories, noting portability implications for Flash‑style attention and speculative decoding. Discussion centered on whether future ISA/driver stacks can expose these gains without bespoke compilers.Cornserve Cuts Tail Latency: Cornserve presents an online serving system for Any‑to‑Any multimodal models that optimizes deployment plans across encoders, LLMs, and DiTs, per Cornserve (arXiv), with an overview talk at Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube). The paper reports throughput gains and tail‑latency reductions under heterogeneous pipelines.Infra engineers liked its planner‑driven scheduling for encoder/decoder mixes and saw it as complementary to vLLM for multimodal graphs. The big open question: standardizing budgeted reasoning and co‑scheduling across text, vision, and diffusion stages without over‑tokenizing control messages.3. New Multimodal and Coding Models Land in LM ArenaWAN 2.6 Walks In (With Upload Woes): LM Arena added wan2.6‑t2i (text‑to‑image) and wan2.6‑image (image edit) to the image arena: LM Arena — Image Chat. Users noted wan2.6‑image requires an uploaded image and that wan2.6‑t2i currently lacks image‑upload support.Staff acknowledged the upload gap and are working to enable image uploads for wan2.6‑t2i. Builders suggested testing edit pipelines where masking, prompt strength, and seed control align with Arena scoring to benchmark edit fidelity.Devstral Duels and Text Titans: The Code Arena now features devstral‑2 for head‑to‑head comparisons—see LM Arena — Code Arena Direct Battle. On the text side, qwen3‑max‑thinking and molmo‑2‑8b joined the lineup: LM Arena — Text Arena.Engineers are probing reasoning traces and tool‑using prompts to stress code synthesis and refactor quality under tight token budgets. Early chatter favored task‑specific evaluations (e.g., SWE‑style bug‑fix vs. ground‑up implementation) to surface model deltas.Hunyuan Hits the Leaderboard: Tencent’s Hunyuan‑Image‑3.0‑Instruct ranks #7 on LM Arena’s image‑edit board—see LM Arena — Image Edit Leaderboard—after a launch post: Tencent Hunyuan announces HunyuanImage 3.0‑Instruct. The model touts an 80B MoE, Native CoT, and MixGRPO for tighter intent alignment.Creators emphasized edit controllability and multi‑image fusion, while evaluators asked for masking robustness, text fidelity, and artifact rates under compositional prompts. Teams plan to pit it against WAN 2.6 variants using the Arena’s standardized edit tasks.4. Safety, Reliability, and Hallucination HardeningClamp the Chaos: Layer‑Native Safety: Layer‑Native Safety Clamping proposes learning activation‑space harm directions and clamping them to block jailbreaks, with a 10K‑pair dataset at Pacific‑Prime/safety_dataset (HF) and the paper on Zenodo. Authors argue in‑model clamping can’t be bypassed via prompt manipulation.Red‑teamers liked the idea of activation‑level controls versus brittle prompt filters, but pressed for tests against tool‑use and multi‑turn attacks. Expect follow‑ups measuring side effects on helpfulness, coding accuracy, and false positives under adversarial prompting.Symbolic Sanity Checks Stop Slip‑Ups: Hybrid approaches check logical consistency for math/code/simple facts, as shown in Consistency Checking for LLMs (arXiv:2409.13724), while broader consistency remains tough per Scaling Consistency Beyond Formal Domains (arXiv:2507.10624). Eleuther discussions framed this as practical hallucination reduction via symbolic/deductive layers.Builders reported wins when pairing symbolic checkers with tool‑augmented prompts, cautioning that coverage gaps appear outside formal domains. The consensus: start with code/math guardrails, then expand to factual QA with curated KBs and provenance scoring.5. Agent Tooling and Reasoning Workflows MatureLevante Leads with MCP‑Native Workspace: Levante launched an open‑source MCP‑native AI workspace for local models (e.g., Ollama) with a modular UI—download at Levante. Engineers highlighted easier tool wiring, local privacy, and composable panes for rapid agent iteration.Early users framed it as a practical hub for tool‑calling and filesystem ops without cloud dependence. Teams plan to benchmark context bloat and tool discoverability patterns versus conventional agent shells.RLM Riffs: AsyncReview + Skills Pack: AsyncFuncAI open‑sourced AsyncReview, a DSPy RLM code‑review agent at AsyncReview (GitHub), and a skills kit landed on npm as @unravel‑tech/rlm‑skills. This pairs reasoning‑first prompting with drop‑in skills to extend models.Contributors reported smoother trace inspection and optimizer‑guided prompt tuning for multi‑step modules. One practitioner noted that rejecting premature answers in the metric is key for reliable RLM fine‑tuning.Agents Auto‑Assemble a Browser Engine: FastRender—a browser rendering engine—was built using 2,000 AI coding agents, documented by Simon Willison in FastRender: built by 2,000 agents. The project demonstrates task decomposition, verification, and orchestration at non‑trivial software scale.Engineers debated handoff granularity and spec‑to‑test loops needed to keep multi‑agent pipelines from drifting. The case study strengthens the argument that agentic coding can target complex infra when coupled with strict eval harnesses and artifact gating.</p>"
        },
        {
          "id": "7a41e8963e96",
          "title": "AI Overviews gets upgraded to Gemini 3 with a dash of AI Mode",
          "content": "It can be hard sometimes to keep up with the deluge of generative AI in Google products. Even if you try to avoid it all, there are some features that still manage to get in your face. Case in point: AI Overviews. This AI-powered search experience has a reputation for getting things wrong, but you may notice some improvements soon. Google says AI Overviews is being upgraded to the latest Gemini 3 models with a more conversational bent.\nIn just the last year, Google has radically expanded the number of searches on which you get an AI Overview at the top. Today, the chatbot will almost always have an answer for your query, which has relied mostly on models in Google's Gemini 2.5 family. There was nothing wrong with Gemini 2.5 as generative AI models go, but Gemini 3 is a little better by every metric.\nThere are, of course, multiple versions of Gemini 3, and Google doesn't like to be specific about which ones appear in your searches. What Google does say is that AI Overviews chooses the right model for the job. So if you're searching for something simple for which there are a lot of valid sources, AI Overviews may manifest something like Gemini 3 Flash without running through a ton of reasoning tokens. For a complex \"long tail\" query, it could step up the thinking or move to Gemini 3 Pro (for paying subscribers).Read full article\nComments",
          "url": "https://arstechnica.com/google/2026/01/ai-overviews-gets-upgraded-to-gemini-3-with-a-dash-of-ai-mode/",
          "author": "Ryan Whitwam",
          "published": "2026-01-27T17:00:58",
          "source": "Ars Technica - All content",
          "source_type": "rss",
          "tags": [
            "AI",
            "Google",
            "ai overviews",
            "Artificial Intelligence",
            "generative ai",
            "google",
            "search"
          ],
          "summary": "Google is upgrading AI Overviews to Gemini 3 models, bringing more conversational capabilities to its AI-powered search experience. The upgrade from the Gemini 2.5 family represents the first major production deployment of Gemini 3.",
          "importance_score": 78.0,
          "reasoning": "Gemini 3 deployment at scale in Google Search indicates the model is production-ready. Major milestone for Google's next-generation AI across billions of queries.",
          "themes": [
            "Google",
            "model deployment",
            "search AI",
            "Gemini 3"
          ],
          "continuation": null,
          "summary_html": "<p>Google is upgrading AI Overviews to Gemini 3 models, bringing more conversational capabilities to its AI-powered search experience. The upgrade from the Gemini 2.5 family represents the first major production deployment of Gemini 3.</p>",
          "content_html": "<p>It can be hard sometimes to keep up with the deluge of generative AI in Google products. Even if you try to avoid it all, there are some features that still manage to get in your face. Case in point: AI Overviews. This AI-powered search experience has a reputation for getting things wrong, but you may notice some improvements soon. Google says AI Overviews is being upgraded to the latest Gemini 3 models with a more conversational bent.</p>\n<p>In just the last year, Google has radically expanded the number of searches on which you get an AI Overview at the top. Today, the chatbot will almost always have an answer for your query, which has relied mostly on models in Google's Gemini 2.5 family. There was nothing wrong with Gemini 2.5 as generative AI models go, but Gemini 3 is a little better by every metric.</p>\n<p>There are, of course, multiple versions of Gemini 3, and Google doesn't like to be specific about which ones appear in your searches. What Google does say is that AI Overviews chooses the right model for the job. So if you're searching for something simple for which there are a lot of valid sources, AI Overviews may manifest something like Gemini 3 Flash without running through a ton of reasoning tokens. For a complex \"long tail\" query, it could step up the thinking or move to Gemini 3 Pro (for paying subscribers).Read full article</p>\n<p>Comments</p>"
        },
        {
          "id": "49c769365d1a",
          "title": "Microsoft Aims for Better Inference Efficiency With Maia 200",
          "content": "As enterprises increasingly deploy AI agents that perform multi-step tasks, the chip addresses the need for more performance, cost efficiency, and energy savings in inference.",
          "url": "https://aibusiness.com/generative-ai/microsoft-aims-for-better-inference-efficiency",
          "author": "Esther Shittu",
          "published": "2026-01-27T15:05:53",
          "source": "aibusiness",
          "source_type": "rss",
          "tags": [],
          "summary": "Microsoft announced the Maia 200 chip designed for improved inference efficiency, specifically targeting AI agent workloads requiring multi-step task execution. The chip addresses cost efficiency and energy savings for enterprise inference.",
          "importance_score": 76.0,
          "reasoning": "Custom AI silicon from Microsoft focused on agentic inference reflects the industry's shift toward agent-optimized hardware. Important for enterprise AI infrastructure.",
          "themes": [
            "AI hardware",
            "Microsoft",
            "inference optimization",
            "agentic AI"
          ],
          "continuation": null,
          "summary_html": "<p>Microsoft announced the Maia 200 chip designed for improved inference efficiency, specifically targeting AI agent workloads requiring multi-step task execution. The chip addresses cost efficiency and energy savings for enterprise inference.</p>",
          "content_html": "<p>As enterprises increasingly deploy AI agents that perform multi-step tasks, the chip addresses the need for more performance, cost efficiency, and energy savings in inference.</p>"
        },
        {
          "id": "8e1a7590b85a",
          "title": "The State-Led Crackdown on Grok and xAI Has Begun",
          "content": "At least 37 attorneys general for US states and territories are taking action against xAI after Grok generated a flood of nonconsensual sexual images of women and minors.",
          "url": "https://www.wired.com/story/the-state-led-crackdown-on-grok-and-xai-has-begun/",
          "author": "Maddy Varner, Manisha Krishnan",
          "published": "2026-01-27T18:35:07",
          "source": "Feed: Artificial Intelligence Latest",
          "source_type": "rss",
          "tags": [
            "Culture",
            "Culture / Culture News",
            "artificial intelligence",
            "privacy",
            "censorship",
            "pornography",
            "Google",
            "apple",
            "Microsoft",
            "xAI",
            "chatbots",
            "Numbers Game"
          ],
          "summary": "Continuing our coverage from [yesterday](/?date=2026-01-27&category=news#item-5652f145be41), At least 37 attorneys general from US states and territories are taking legal action against xAI after Grok generated nonconsensual sexual images of women and minors. This represents the largest coordinated state-level enforcement action against an AI company.",
          "importance_score": 75.0,
          "reasoning": "Unprecedented multi-state regulatory action against a major AI lab. Significant implications for AI safety enforcement and content generation guardrails.",
          "themes": [
            "AI regulation",
            "AI safety",
            "xAI",
            "legal action"
          ],
          "continuation": {
            "original_item_id": "5652f145be41",
            "original_date": "2026-01-27",
            "original_category": "news",
            "original_title": "EU launches formal investigation of xAI over Grok's sexualized deepfakes",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Continuing our coverage from yesterday"
          },
          "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-27&amp;category=news#item-5652f145be41\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, At least 37 attorneys general from US states and territories are taking legal action against xAI after Grok generated nonconsensual sexual images of women and minors. This represents the largest coordinated state-level enforcement action against an AI company.</p>",
          "content_html": "<p>At least 37 attorneys general for US states and territories are taking action against xAI after Grok generated a flood of nonconsensual sexual images of women and minors.</p>"
        },
        {
          "id": "8c06e1bc10ec",
          "title": "Startup Plans to Use AI to Optimize How AI Chips Are Made",
          "content": "Two former-Google researchers launched Ricursive Intelligence in December. The company is already valued at $4 billion.",
          "url": "https://aibusiness.com/intelligent-automation/startup-optimize-how-ai-chips-are-made",
          "author": "Graham Hope",
          "published": "2026-01-27T18:29:24",
          "source": "aibusiness",
          "source_type": "rss",
          "tags": [],
          "summary": "Former Google researchers launched Ricursive Intelligence, a startup using AI to optimize AI chip manufacturing processes. The company, founded in December, is already valued at $4 billion.",
          "importance_score": 73.0,
          "reasoning": "High valuation for early-stage startup addressing critical AI supply chain bottleneck. Meta-application of AI to improve AI hardware production is strategically important.",
          "themes": [
            "AI hardware",
            "startups",
            "chip manufacturing",
            "funding"
          ],
          "continuation": null,
          "summary_html": "<p>Former Google researchers launched Ricursive Intelligence, a startup using AI to optimize AI chip manufacturing processes. The company, founded in December, is already valued at $4 billion.</p>",
          "content_html": "<p>Two former-Google researchers launched Ricursive Intelligence in December. The company is already valued at $4 billion.</p>"
        },
        {
          "id": "c60ae5146709",
          "title": "Databricks: Enterprise AI adoption shifts to agentic systems",
          "content": "According to Databricks, enterprise AI adoption is shifting to agentic systems as organisations embrace intelligent workflows.\n\n\n\nGenerative AI’s first wave promised business transformation but often delivered little more than isolated chatbots and stalled pilot programmes. Technology leaders found themselves managing high expectations with limited operational utility. However, new telemetry from Databricks suggests the market has turned a corner.\n\n\n\nData from over 20,000 organisations – including 60 percent of the Fortune 500 – indicates a rapid shift toward &#8220;agentic&#8221; architectures where models do not just retrieve information but independently plan and execute workflows.\n\n\n\nThis evolution represents a fundamental reallocation of engineering resources. Between June and October 2025, the use of multi-agent workflows on the Databricks platform grew by 327 percent. This surge signals that AI is graduating to a core component of system architecture.\n\n\n\nThe ‘Supervisor Agent’ drives enterprise adoption of agentic AI\n\n\n\nDriving this growth is the ‘Supervisor Agent’. Rather than relying on a single model to handle every request, a supervisor acts as an orchestrator, breaking down complex queries and delegating tasks to specialised sub-agents or tools.\n\n\n\nSince its launch in July 2025, the Supervisor Agent has become the leading agent use case, accounting for 37 percent of usage by October. This pattern mirrors human organisational structures: a manager does not perform every task but ensures the team executes them. Similarly, a supervisor agent manages intent detection and compliance checks before routing work to domain-specific tools.\n\n\n\nTechnology companies currently lead this adoption, building nearly four times more multi-agent systems than any other industry. Yet the utility extends across sectors. A financial services firm, for instance, might employ a multi-agent system to handle document retrieval and regulatory compliance simultaneously, delivering a verified client response without human intervention.\n\n\n\nTraditional infrastructure under pressure\n\n\n\nAs agents graduate from answering questions to executing tasks, underlying data infrastructure faces new demands. Traditional Online Transaction Processing (OLTP) databases were designed for human-speed interactions with predictable transactions and infrequent schema changes. Agentic workflows invert these assumptions.\n\n\n\nAI agents now generate continuous, high-frequency read and write patterns, often creating and tearing down environments programmatically to test code or run scenarios. The scale of this automation is visible in the telemetry data. Two years ago, AI agents created just 0.1 percent of databases; today, that figure sits at 80 percent.\n\n\n\nFurthermore, 97 percent of database testing and development environments are now built by AI agents. This capability allows developers and &#8220;vibe coders&#8221; to spin up ephemeral environments in seconds rather than hours. Over 50,000 data and AI apps have been created since the Public Preview of Databricks Apps, with a 250 percent growth rate over the past six months.\n\n\n\nThe multi-model standard\n\n\n\nVendor lock-in remains a persistent risk for enterprise leaders as they seek to increase agentic AI adoption. The data indicates that organisations are actively mitigating this by adopting multi-model strategies. As of October 2025, 78 percent of companies utilised two or more Large Language Model (LLM) families, such as ChatGPT, Claude, Llama, and Gemini.\n\n\n\nThe sophistication of this approach is increasing. The proportion of companies using three or more model families rose from 36 percent to 59 percent between August and October 2025. This diversity allows engineering teams to route simpler tasks to smaller and more cost-effective models while reserving frontier models for complex reasoning.\n\n\n\nRetail companies are setting the pace, with 83 percent employing two or more model families to balance performance and cost. A unified platform capable of integrating various proprietary and open-source models is rapidly becoming a prerequisite for the modern enterprise AI stack.\n\n\n\nContrary to the big data legacy of batch processing, agentic AI operates primarily in the now. The report highlights that 96 percent of all inference requests are processed in real-time.\n\n\n\nThis is particularly evident in sectors where latency correlates directly with value. The technology sector processes 32 real-time requests for every single batch request. In healthcare and life sciences, where applications may involve patient monitoring or clinical decision support, the ratio is 13 to one. For IT leaders, this reinforces the need for inference serving infrastructure capable of handling traffic spikes without degrading user experience.\n\n\n\nGovernance accelerates enterprise AI deployments\n\n\n\nPerhaps the most counter-intuitive finding for many executives is the relationship between governance and velocity. Often viewed as a bottleneck, rigorous governance and evaluation frameworks function as accelerators for production deployment.\n\n\n\nOrganisations using AI governance tools put over 12 times more AI projects into production compared to those that do not. Similarly, companies employing evaluation tools to systematically test model quality achieve nearly six times more production deployments.\n\n\n\nThe rationale is straightforward. Governance provides necessary guardrails – such as defining how data is used and setting rate limits – which gives stakeholders the confidence to approve deployment. Without these controls, pilots often get stuck in the proof-of-concept phase due to unquantified safety or compliance risks.\n\n\n\nThe value of ‘boring’ enterprise automation from agentic AI\n\n\n\nWhile autonomous agents often conjure images of futuristic capabilities, current enterprise value from agentic AI lies in automating the routine, mundane, yet necessary tasks. The top AI use cases vary by sector but focus on solving specific business problems:\n\n\n\n\nManufacturing and automotive: 35% of use cases focus on predictive maintenance.\n\n\n\n\n\nHealth and life sciences: 23% of use cases involve medical literature synthesis.\n\n\n\n\n\nRetail and consumer goods: 14% of use cases are dedicated to market intelligence.\n\n\n\n\nFurthermore, 40 percent of the top AI use cases address practical customer concerns such as customer support, advocacy, and onboarding. These applications drive measurable efficiency and build the organisational muscle required for more advanced agentic workflows.\n\n\n\nFor the C-suite, the path forward involves less focus on the &#8220;magic&#8221; of AI and more on the engineering rigour surrounding it. Dael Williamson, EMEA CTO at Databricks, highlights that the conversation has shifted.\n\n\n\n“For businesses across EMEA, the conversation has moved on from AI experimentation to operational reality,” says Williamson. “AI agents are already running critical parts of enterprise infrastructure, but the organisations seeing real value are those treating governance and evaluation as foundations, not afterthoughts.”\n\n\n\nWilliamson emphasises that competitive advantage is shifting back towards how companies build, rather than simply what they buy.\n\n\n\n“Open, interoperable platforms allow organisations to apply AI to their own enterprise data, rather than relying on embedded AI features that deliver short-term productivity but not long-term differentiation.”\n\n\n\nIn highly regulated markets, this combination of openness and control is “what separates pilots from competitive advantage.”\n\n\n\nSee also: Anthropic selected to build government AI assistant pilot\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Databricks: Enterprise AI adoption shifts to agentic systems appeared first on AI News.",
          "url": "https://www.artificialintelligence-news.com/news/databricks-enterprise-ai-adoption-shifts-agentic-systems/",
          "author": "Ryan Daws",
          "published": "2026-01-27T17:26:45",
          "source": "AI News",
          "source_type": "rss",
          "tags": [
            "AI and Us",
            "AI Business Strategy",
            "AI Market Trends",
            "Features",
            "Governance, Regulation & Policy",
            "Inside AI",
            "Special Reports & Series",
            "World of Work",
            "adoption",
            "agentic ai",
            "agents",
            "automation",
            "databricks",
            "enterprise",
            "governance",
            "infrastructure",
            "strategy"
          ],
          "summary": "Databricks telemetry from 20,000+ organizations (60% of Fortune 500) shows enterprise AI adoption rapidly shifting from isolated chatbots to agentic architectures where models independently plan and execute workflows.",
          "importance_score": 72.0,
          "reasoning": "Large-scale data showing enterprise pivot to agentic AI validates the market shift beyond proof-of-concept chatbots. Important signal for enterprise AI direction.",
          "themes": [
            "enterprise AI",
            "agentic AI",
            "market trends",
            "adoption"
          ],
          "continuation": null,
          "summary_html": "<p>Databricks telemetry from 20,000+ organizations (60% of Fortune 500) shows enterprise AI adoption rapidly shifting from isolated chatbots to agentic architectures where models independently plan and execute workflows.</p>",
          "content_html": "<p>According to Databricks, enterprise AI adoption is shifting to agentic systems as organisations embrace intelligent workflows.</p>\n<p>Generative AI’s first wave promised business transformation but often delivered little more than isolated chatbots and stalled pilot programmes. Technology leaders found themselves managing high expectations with limited operational utility. However, new telemetry from Databricks suggests the market has turned a corner.</p>\n<p>Data from over 20,000 organisations – including 60 percent of the Fortune 500 – indicates a rapid shift toward “agentic” architectures where models do not just retrieve information but independently plan and execute workflows.</p>\n<p>This evolution represents a fundamental reallocation of engineering resources. Between June and October 2025, the use of multi-agent workflows on the Databricks platform grew by 327 percent. This surge signals that AI is graduating to a core component of system architecture.</p>\n<p>The ‘Supervisor Agent’ drives enterprise adoption of agentic AI</p>\n<p>Driving this growth is the ‘Supervisor Agent’. Rather than relying on a single model to handle every request, a supervisor acts as an orchestrator, breaking down complex queries and delegating tasks to specialised sub-agents or tools.</p>\n<p>Since its launch in July 2025, the Supervisor Agent has become the leading agent use case, accounting for 37 percent of usage by October. This pattern mirrors human organisational structures: a manager does not perform every task but ensures the team executes them. Similarly, a supervisor agent manages intent detection and compliance checks before routing work to domain-specific tools.</p>\n<p>Technology companies currently lead this adoption, building nearly four times more multi-agent systems than any other industry. Yet the utility extends across sectors. A financial services firm, for instance, might employ a multi-agent system to handle document retrieval and regulatory compliance simultaneously, delivering a verified client response without human intervention.</p>\n<p>Traditional infrastructure under pressure</p>\n<p>As agents graduate from answering questions to executing tasks, underlying data infrastructure faces new demands. Traditional Online Transaction Processing (OLTP) databases were designed for human-speed interactions with predictable transactions and infrequent schema changes. Agentic workflows invert these assumptions.</p>\n<p>AI agents now generate continuous, high-frequency read and write patterns, often creating and tearing down environments programmatically to test code or run scenarios. The scale of this automation is visible in the telemetry data. Two years ago, AI agents created just 0.1 percent of databases; today, that figure sits at 80 percent.</p>\n<p>Furthermore, 97 percent of database testing and development environments are now built by AI agents. This capability allows developers and “vibe coders” to spin up ephemeral environments in seconds rather than hours. Over 50,000 data and AI apps have been created since the Public Preview of Databricks Apps, with a 250 percent growth rate over the past six months.</p>\n<p>The multi-model standard</p>\n<p>Vendor lock-in remains a persistent risk for enterprise leaders as they seek to increase agentic AI adoption. The data indicates that organisations are actively mitigating this by adopting multi-model strategies. As of October 2025, 78 percent of companies utilised two or more Large Language Model (LLM) families, such as ChatGPT, Claude, Llama, and Gemini.</p>\n<p>The sophistication of this approach is increasing. The proportion of companies using three or more model families rose from 36 percent to 59 percent between August and October 2025. This diversity allows engineering teams to route simpler tasks to smaller and more cost-effective models while reserving frontier models for complex reasoning.</p>\n<p>Retail companies are setting the pace, with 83 percent employing two or more model families to balance performance and cost. A unified platform capable of integrating various proprietary and open-source models is rapidly becoming a prerequisite for the modern enterprise AI stack.</p>\n<p>Contrary to the big data legacy of batch processing, agentic AI operates primarily in the now. The report highlights that 96 percent of all inference requests are processed in real-time.</p>\n<p>This is particularly evident in sectors where latency correlates directly with value. The technology sector processes 32 real-time requests for every single batch request. In healthcare and life sciences, where applications may involve patient monitoring or clinical decision support, the ratio is 13 to one. For IT leaders, this reinforces the need for inference serving infrastructure capable of handling traffic spikes without degrading user experience.</p>\n<p>Governance accelerates enterprise AI deployments</p>\n<p>Perhaps the most counter-intuitive finding for many executives is the relationship between governance and velocity. Often viewed as a bottleneck, rigorous governance and evaluation frameworks function as accelerators for production deployment.</p>\n<p>Organisations using AI governance tools put over 12 times more AI projects into production compared to those that do not. Similarly, companies employing evaluation tools to systematically test model quality achieve nearly six times more production deployments.</p>\n<p>The rationale is straightforward. Governance provides necessary guardrails – such as defining how data is used and setting rate limits – which gives stakeholders the confidence to approve deployment. Without these controls, pilots often get stuck in the proof-of-concept phase due to unquantified safety or compliance risks.</p>\n<p>The value of ‘boring’ enterprise automation from agentic AI</p>\n<p>While autonomous agents often conjure images of futuristic capabilities, current enterprise value from agentic AI lies in automating the routine, mundane, yet necessary tasks. The top AI use cases vary by sector but focus on solving specific business problems:</p>\n<p>Manufacturing and automotive: 35% of use cases focus on predictive maintenance.</p>\n<p>Health and life sciences: 23% of use cases involve medical literature synthesis.</p>\n<p>Retail and consumer goods: 14% of use cases are dedicated to market intelligence.</p>\n<p>Furthermore, 40 percent of the top AI use cases address practical customer concerns such as customer support, advocacy, and onboarding. These applications drive measurable efficiency and build the organisational muscle required for more advanced agentic workflows.</p>\n<p>For the C-suite, the path forward involves less focus on the “magic” of AI and more on the engineering rigour surrounding it. Dael Williamson, EMEA CTO at Databricks, highlights that the conversation has shifted.</p>\n<p>“For businesses across EMEA, the conversation has moved on from AI experimentation to operational reality,” says Williamson. “AI agents are already running critical parts of enterprise infrastructure, but the organisations seeing real value are those treating governance and evaluation as foundations, not afterthoughts.”</p>\n<p>Williamson emphasises that competitive advantage is shifting back towards how companies build, rather than simply what they buy.</p>\n<p>“Open, interoperable platforms allow organisations to apply AI to their own enterprise data, rather than relying on embedded AI features that deliver short-term productivity but not long-term differentiation.”</p>\n<p>In highly regulated markets, this combination of openness and control is “what separates pilots from competitive advantage.”</p>\n<p>See also: Anthropic selected to build government AI assistant pilot</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Databricks: Enterprise AI adoption shifts to agentic systems appeared first on AI News.</p>"
        },
        {
          "id": "f93b0aa92dfd",
          "title": "Anthropic selected to build government AI assistant pilot",
          "content": "Anthropic has been selected to build government AI assistant capabilities to modernise how citizens interact with complex state services.\n\n\n\nFor both public and private sector technology leaders, the integration of LLMs into customer-facing platforms often stalls at the proof-of-concept stage. The UK’s Department for Science, Innovation, and Technology (DSIT) aims to bypass this common hurdle by operationalising its February 2025 Memorandum of Understanding with Anthropic.\n\n\n\nThe joint project, announced today, prioritises the deployment of agentic AI systems that are designed to actively guide users through processes rather than simply retrieving static information.\n\n\n\nThe decision to move beyond standard chatbot interfaces addresses a friction point in digital service delivery: the gap between information availability and user action. While government portals are data-rich, navigating them requires specific domain knowledge that many citizens lack.\n\n\n\nBy employing an agentic system powered by Claude, the initiative seeks to provide tailored support that maintains context across multiple interactions. This approach mirrors the trajectory of private sector customer experience, where the value proposition is increasingly defined by the ability to execute tasks and route complex queries rather than just deflect support tickets.\n\n\n\nThe case for agentic AI assistants in government\n\n\n\nThe initial pilot focuses on employment, a high-volume domain where efficiency gains directly impact economic outcomes. The system is tasked with helping users find work, access training, and understand available support mechanisms. For the government, the operational logic involves an intelligent routing system that can assess individual circumstances and direct users to the correct service.\n\n\n\nThis focus on employment services also serves as a stress test for context retention capabilities. Unlike simple transactional queries, job seeking is an ongoing process. The system’s ability to &#8220;remember&#8221; previous interactions allows users to pause and resume their journey without re-entering data; a functional requirement that is essential for high-friction workflows. For enterprise architects, this government implementation serves as a case study in managing stateful AI interactions within a secure environment.\n\n\n\nImplementing generative AI within a statutory framework necessitates a risk-averse deployment strategy. The project adheres to a &#8220;Scan, Pilot, Scale&#8221; framework, a deliberate methodology that forces iterative testing before wider rollout. This phased approach allows the department to validate safety protocols and efficacy in a controlled setting, minimising the potential for compliance failures that have plagued other public sector AI launches.\n\n\n\nData sovereignty and user trust form the backbone of this governance model. Anthropic has stipulated that users will retain full control over their data, including the ability to opt out or dictate what the system remembers. By ensuring all personal information handling aligns with UK data protection laws, the initiative aims to preempt privacy concerns that typically stall adoption.\n\n\n\nFurthermore, the collaboration involves the UK AI Safety Institute to test and evaluate the models, ensuring that the safeguards developed inform the eventual deployment.\n\n\n\nAvoiding dependency on external AI providers like Anthropic\n\n\n\nPerhaps the most instructive aspect of this partnership for enterprise leaders is the focus on knowledge transfer. Rather than a traditional outsourced delivery model, Anthropic engineers will work alongside civil servants and software developers at the Government Digital Service.\n\n\n\nThe explicit goal of this co-working arrangement is to build internal AI expertise that ensures the UK government can independently maintain the system once the initial engagement concludes. This addresses the issue of vendor lock-in, where public bodies become reliant on external providers for core infrastructure. By prioritising skills transfer during the build phase, the government is treating AI competence as a core operational asset rather than a procured commodity.\n\n\n\nThis development is part of a broader trend of sovereign AI engagement, with Anthropic expanding its public sector footprint through similar education pilots in Iceland and Rwanda. It also reflects a deepening investment in the UK market, where the company’s London office is expanding its policy and applied AI functions.\n\n\n\nPip White, Head of UK, Ireland, and Northern Europe at Anthropic, said: “This partnership with the UK government is central to our mission. It demonstrates how frontier AI can be deployed safely for the public benefit, setting the standard for how governments integrate AI into the services their citizens depend on.”\n\n\n\nFor executives observing this rollout, it once again makes clear that successful AI integration is less about the underlying model and more about the governance, data architecture, and internal capability built around it. The transition from answering questions to guiding outcomes represents the next phase of digital maturity.\n\n\n\nSee also: How Formula E uses Google Cloud AI to meet net zero targets\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Anthropic selected to build government AI assistant pilot appeared first on AI News.",
          "url": "https://www.artificialintelligence-news.com/news/anthropic-selected-build-government-ai-assistant-pilot/",
          "author": "Ryan Daws",
          "published": "2026-01-27T13:31:22",
          "source": "AI News",
          "source_type": "rss",
          "tags": [
            "AI and Us",
            "AI in Action",
            "Governance, Regulation & Policy",
            "Government & Public Sector AI",
            "Inside AI",
            "World of Work",
            "agentic ai",
            "agents",
            "ai",
            "anthropic",
            "assistants",
            "claude",
            "government",
            "integration",
            "public sector"
          ],
          "summary": "Anthropic has been selected to build government AI assistants for the UK Department for Science, Innovation, and Technology, deploying agentic AI systems that guide users through complex state services rather than simple chatbot interactions.",
          "importance_score": 71.0,
          "reasoning": "Major government deployment of agentic AI from a leading lab. Sets precedent for how frontier AI companies engage with public sector.",
          "themes": [
            "government AI",
            "Anthropic",
            "agentic AI",
            "public sector"
          ],
          "continuation": null,
          "summary_html": "<p>Anthropic has been selected to build government AI assistants for the UK Department for Science, Innovation, and Technology, deploying agentic AI systems that guide users through complex state services rather than simple chatbot interactions.</p>",
          "content_html": "<p>Anthropic has been selected to build government AI assistant capabilities to modernise how citizens interact with complex state services.</p>\n<p>For both public and private sector technology leaders, the integration of LLMs into customer-facing platforms often stalls at the proof-of-concept stage. The UK’s Department for Science, Innovation, and Technology (DSIT) aims to bypass this common hurdle by operationalising its February 2025 Memorandum of Understanding with Anthropic.</p>\n<p>The joint project, announced today, prioritises the deployment of agentic AI systems that are designed to actively guide users through processes rather than simply retrieving static information.</p>\n<p>The decision to move beyond standard chatbot interfaces addresses a friction point in digital service delivery: the gap between information availability and user action. While government portals are data-rich, navigating them requires specific domain knowledge that many citizens lack.</p>\n<p>By employing an agentic system powered by Claude, the initiative seeks to provide tailored support that maintains context across multiple interactions. This approach mirrors the trajectory of private sector customer experience, where the value proposition is increasingly defined by the ability to execute tasks and route complex queries rather than just deflect support tickets.</p>\n<p>The case for agentic AI assistants in government</p>\n<p>The initial pilot focuses on employment, a high-volume domain where efficiency gains directly impact economic outcomes. The system is tasked with helping users find work, access training, and understand available support mechanisms. For the government, the operational logic involves an intelligent routing system that can assess individual circumstances and direct users to the correct service.</p>\n<p>This focus on employment services also serves as a stress test for context retention capabilities. Unlike simple transactional queries, job seeking is an ongoing process. The system’s ability to “remember” previous interactions allows users to pause and resume their journey without re-entering data; a functional requirement that is essential for high-friction workflows. For enterprise architects, this government implementation serves as a case study in managing stateful AI interactions within a secure environment.</p>\n<p>Implementing generative AI within a statutory framework necessitates a risk-averse deployment strategy. The project adheres to a “Scan, Pilot, Scale” framework, a deliberate methodology that forces iterative testing before wider rollout. This phased approach allows the department to validate safety protocols and efficacy in a controlled setting, minimising the potential for compliance failures that have plagued other public sector AI launches.</p>\n<p>Data sovereignty and user trust form the backbone of this governance model. Anthropic has stipulated that users will retain full control over their data, including the ability to opt out or dictate what the system remembers. By ensuring all personal information handling aligns with UK data protection laws, the initiative aims to preempt privacy concerns that typically stall adoption.</p>\n<p>Furthermore, the collaboration involves the UK AI Safety Institute to test and evaluate the models, ensuring that the safeguards developed inform the eventual deployment.</p>\n<p>Avoiding dependency on external AI providers like Anthropic</p>\n<p>Perhaps the most instructive aspect of this partnership for enterprise leaders is the focus on knowledge transfer. Rather than a traditional outsourced delivery model, Anthropic engineers will work alongside civil servants and software developers at the Government Digital Service.</p>\n<p>The explicit goal of this co-working arrangement is to build internal AI expertise that ensures the UK government can independently maintain the system once the initial engagement concludes. This addresses the issue of vendor lock-in, where public bodies become reliant on external providers for core infrastructure. By prioritising skills transfer during the build phase, the government is treating AI competence as a core operational asset rather than a procured commodity.</p>\n<p>This development is part of a broader trend of sovereign AI engagement, with Anthropic expanding its public sector footprint through similar education pilots in Iceland and Rwanda. It also reflects a deepening investment in the UK market, where the company’s London office is expanding its policy and applied AI functions.</p>\n<p>Pip White, Head of UK, Ireland, and Northern Europe at Anthropic, said: “This partnership with the UK government is central to our mission. It demonstrates how frontier AI can be deployed safely for the public benefit, setting the standard for how governments integrate AI into the services their citizens depend on.”</p>\n<p>For executives observing this rollout, it once again makes clear that successful AI integration is less about the underlying model and more about the governance, data architecture, and internal capability built around it. The transition from answering questions to guiding outcomes represents the next phase of digital maturity.</p>\n<p>See also: How Formula E uses Google Cloud AI to meet net zero targets</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Anthropic selected to build government AI assistant pilot appeared first on AI News.</p>"
        },
        {
          "id": "c356043bb52e",
          "title": "‘Wake up to the risks of AI, they are almost here,’ Anthropic boss warns",
          "content": "Dario Amodei questions if human systems are ready to handle the ‘almost unimaginable power’ that is ‘potentially imminent’Quarter of Britons fear losing jobs to AI in next five yearsHumanity is entering a phase of artificial intelligence development that will “test who we are as a species”, the boss of the AI startup Anthropic has said, arguing that the world needs to “wake up” to the risks.Dario Amodei, a co-founder and the chief executive of the company behind the hit chatbot Claude, voiced his fears in a 19,000-word essay titled “The adolescence of technology”. Continue reading...",
          "url": "https://www.theguardian.com/technology/2026/jan/27/wake-up-to-the-risks-of-ai-they-are-almost-here-anthropic-boss-warns",
          "author": "Dan Milmo Global technology editor",
          "published": "2026-01-27T12:53:06",
          "source": "AI (artificial intelligence) | The Guardian",
          "source_type": "rss",
          "tags": [
            "AI (artificial intelligence)",
            "Computing",
            "Technology startups",
            "Technology",
            "Technology sector",
            "Productivity",
            "Business"
          ],
          "summary": "First spotted on [Social](/?date=2026-01-27&category=social#item-5b3a42601797), now making mainstream headlines, Anthropic CEO Dario Amodei published a 19,000-word essay warning that AI development will 'test who we are as a species' and that the world needs to 'wake up' to potentially imminent risks from 'almost unimaginable power.'",
          "importance_score": 68.0,
          "reasoning": "Significant public statement from major lab CEO on AI risks during critical development period. Shapes safety discourse as capabilities advance.",
          "themes": [
            "AI safety",
            "Anthropic",
            "AI risks",
            "thought leadership"
          ],
          "continuation": {
            "original_item_id": "5b3a42601797",
            "original_date": "2026-01-27",
            "original_category": "social",
            "original_title": "The Adolescence of Technology: an essay on the risks posed by powerful AI to national security, econ...",
            "continuation_type": "mainstream_pickup",
            "should_demote": false,
            "reference_text": "First spotted on **Social**, now making mainstream headlines"
          },
          "summary_html": "<p>First spotted on <a href=\"/?date=2026-01-27&amp;category=social#item-5b3a42601797\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a>, now making mainstream headlines, Anthropic CEO Dario Amodei published a 19,000-word essay warning that AI development will 'test who we are as a species' and that the world needs to 'wake up' to potentially imminent risks from 'almost unimaginable power.'</p>",
          "content_html": "<p>Dario Amodei questions if human systems are ready to handle the ‘almost unimaginable power’ that is ‘potentially imminent’Quarter of Britons fear losing jobs to AI in next five yearsHumanity is entering a phase of artificial intelligence development that will “test who we are as a species”, the boss of the AI startup Anthropic has said, arguing that the world needs to “wake up” to the risks.Dario Amodei, a co-founder and the chief executive of the company behind the hit chatbot Claude, voiced his fears in a 19,000-word essay titled “The adolescence of technology”. Continue reading...</p>"
        },
        {
          "id": "758d6ad222d3",
          "title": "Ai2 Releases Open Coding Agents Family",
          "content": "The release shows how enterprises need to balance cost with performance and highlights a rising trend in the open source model market.",
          "url": "https://aibusiness.com/agentic-ai/ai2-releases-open-coding-agents",
          "author": "Esther Shittu",
          "published": "2026-01-27T20:21:37",
          "source": "aibusiness",
          "source_type": "rss",
          "tags": [],
          "summary": "AI2 (Allen Institute for AI) has released an open family of coding agents, highlighting the balance between cost and performance for enterprises and indicating rising competition in the open source agent market.",
          "importance_score": 66.0,
          "reasoning": "Open source coding agents from respected research institute advances accessible AI development tools. Contributes to democratization of agentic capabilities.",
          "themes": [
            "open source",
            "coding agents",
            "AI2",
            "enterprise AI"
          ],
          "continuation": null,
          "summary_html": "<p>AI2 (Allen Institute for AI) has released an open family of coding agents, highlighting the balance between cost and performance for enterprises and indicating rising competition in the open source agent market.</p>",
          "content_html": "<p>The release shows how enterprises need to balance cost with performance and highlights a rising trend in the open source model market.</p>"
        }
      ]
    },
    "research": {
      "count": 414,
      "category_summary": "Today's research spans AI security capabilities, safety empirics, and deep learning theory. **AISLE's AI** [discovered all **12 OpenSSL zero-days**](/?date=2026-01-28&category=research#item-6180fdcc30bb), a landmark demonstration of automated vulnerability detection at a critical scale.\n\n- **Disempowerment study** [analyzes **1.5M Claude conversations**](/?date=2026-01-28&category=research#item-fb5e2dc0ce5e), finding severe disempowerment in **<0.1%** of interactions—first large-scale empirical safety research of this kind\n- **Surgical sycophancy correction** [identifies the **3% of neurons**](/?date=2026-01-28&category=research#item-587a23d43703) responsible and removes the behavior while preserving capabilities via sparse autoencoders\n- **Thought-Transfer** (Google) [reveals CoT reasoning models](/?date=2026-01-28&category=research#item-b8650d4dd7e5) are vulnerable to indirect targeted poisoning attacks\n\nTheoretical advances include the first rigorous [**grokking bounds**](/?date=2026-01-28&category=research#item-180fd3e3456f) in ridge regression and a proof that deep networks [learn **Random Hierarchy Models**](/?date=2026-01-28&category=research#item-68bd18d53406) through hierarchical feature composition. **Keel** [revives Post-LayerNorm](/?date=2026-01-28&category=research#item-07e893a2c4e7) by replacing residual paths with Legendre polynomials for stable training at depth. **Differential voting** [connects RLHF reward aggregation](/?date=2026-01-28&category=research#item-2ad230fcffab) to social choice theory, deriving loss functions satisfying specific voting axioms. **VP-RL** [addresses PRM-RL mismatch](/?date=2026-01-28&category=research#item-5a60a6b16a85) by penalizing only from the first incorrect reasoning step.",
      "category_summary_html": "<p>Today's research spans AI security capabilities, safety empirics, and deep learning theory. <strong>AISLE's AI</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-6180fdcc30bb\" class=\"internal-link\" rel=\"noopener noreferrer\">discovered all <strong>12 OpenSSL zero-days</strong></a>, a landmark demonstration of automated vulnerability detection at a critical scale.</p>\n<ul>\n<li><strong>Disempowerment study</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e\" class=\"internal-link\" rel=\"noopener noreferrer\">analyzes <strong>1.5M Claude conversations</strong></a>, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions—first large-scale empirical safety research of this kind</li>\n<li><strong>Surgical sycophancy correction</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-587a23d43703\" class=\"internal-link\" rel=\"noopener noreferrer\">identifies the <strong>3% of neurons</strong></a> responsible and removes the behavior while preserving capabilities via sparse autoencoders</li>\n<li><strong>Thought-Transfer</strong> (Google) <a href=\"/?date=2026-01-28&amp;category=research#item-b8650d4dd7e5\" class=\"internal-link\" rel=\"noopener noreferrer\">reveals CoT reasoning models</a> are vulnerable to indirect targeted poisoning attacks</li>\n</ul>\n<p>Theoretical advances include the first rigorous <a href=\"/?date=2026-01-28&amp;category=research#item-180fd3e3456f\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>grokking bounds</strong></a> in ridge regression and a proof that deep networks <a href=\"/?date=2026-01-28&amp;category=research#item-68bd18d53406\" class=\"internal-link\" rel=\"noopener noreferrer\">learn <strong>Random Hierarchy Models</strong></a> through hierarchical feature composition. <strong>Keel</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-07e893a2c4e7\" class=\"internal-link\" rel=\"noopener noreferrer\">revives Post-LayerNorm</a> by replacing residual paths with Legendre polynomials for stable training at depth. <strong>Differential voting</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-2ad230fcffab\" class=\"internal-link\" rel=\"noopener noreferrer\">connects RLHF reward aggregation</a> to social choice theory, deriving loss functions satisfying specific voting axioms. <strong>VP-RL</strong> <a href=\"/?date=2026-01-28&amp;category=research#item-5a60a6b16a85\" class=\"internal-link\" rel=\"noopener noreferrer\">addresses PRM-RL mismatch</a> by penalizing only from the first incorrect reasoning step.</p>",
      "themes": [
        {
          "name": "LLM Reasoning & Alignment",
          "description": "Research on improving LLM reasoning capabilities and aligning model behavior, including process reward models, sycophancy correction, and targeted interventions",
          "item_count": 4,
          "example_items": [],
          "importance": 82
        },
        {
          "name": "AI Safety & Alignment",
          "description": "Hallucination detection, disempowerment analysis, privacy preservation, rule-based safety, and robustness testing",
          "item_count": 12,
          "example_items": [],
          "importance": 80
        },
        {
          "name": "LLM Safety & Alignment",
          "description": "Research on making LLMs safer through steering methods, jailbreak defense, and resolving safety-helpfulness trade-offs",
          "item_count": 5,
          "example_items": [],
          "importance": 78
        },
        {
          "name": "AI Safety & Security",
          "description": "Research on adversarial attacks, backdoors, jailbreaking, safety benchmarking, and content detection",
          "item_count": 10,
          "example_items": [],
          "importance": 78
        },
        {
          "name": "Agentic AI Systems & Agents",
          "description": "Design patterns, frameworks, testing, and capabilities for autonomous AI agents including GUI, mobile, and multi-agent systems",
          "item_count": 18,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Deep Learning Theory",
          "description": "Theoretical foundations including provable learning of hierarchies, grokking analysis, generalization under heavy-tailed noise, and scaling law prediction",
          "item_count": 8,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Retrieval-Augmented Generation",
          "description": "Diagnostic study of iterative RAG showing it can outperform oracle evidence access in scientific multi-hop QA",
          "item_count": 1,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "LLM Inference & Efficiency",
          "description": "Methods for faster LLM inference including speculative decoding, context pruning, and activation optimization",
          "item_count": 6,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "AI Capabilities and Applications",
          "description": "Demonstrations of AI capabilities including cybersecurity vulnerability discovery and efficiency improvements for deployment",
          "item_count": 4,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Theoretical Foundations",
          "description": "Scaling laws, coverage bounds, compositional generalization theory, and voting/preference aggregation theory",
          "item_count": 7,
          "example_items": [],
          "importance": 73
        }
      ],
      "top_items": [
        {
          "id": "6180fdcc30bb",
          "title": "AI found 12 of 12 OpenSSL zero-days (while curl cancelled its bug bounty)",
          "content": "This is a partial follow-up to AISLE discovered three new OpenSSL vulnerabilities from October 2025.TL;DR: OpenSSL is among the most scrutinized and audited cryptographic libraries on the planet, underpinning encryption for most of the internet. They just announced 12 new zero-day vulnerabilities (meaning previously unknown to maintainers at time of disclosure). We at AISLE discovered all 12 using our AI system. This is a historically unusual count and the first real-world demonstration of AI-based cybersecurity at this scale. Meanwhile, curl just cancelled its bug bounty program due to a flood of AI-generated spam, even as we reported 5 genuine CVEs to them. AI is simultaneously collapsing the median (\"slop\") and raising the ceiling (real zero-days in critical infrastructure).BackgroundWe at AISLE have been building an automated AI system for deep cybersecurity discovery and remediation, sometimes operating in bug bounties under the pseudonym Giant Anteater. Our goal was to turn what used to be an elite, artisanal hacker craft into a repeatable industrial process. We do this to secure the software infrastructure of human civilization before strong AI systems become ubiquitous. Prosaically, we want to make sure we don't get hacked into oblivion the moment they come online.No reliable cybersecurity benchmark reaching the desired performance level exists yet. We therefore decided to test the performance of our AI system against live targets. The clear benefit of this is that for a new, zero-day security vulnerability to be accepted as meriting a CVE (a unique vulnerability identifier), it has to pass an extremely stringent judgement by the long-term maintainers and security team of the project, who are working under many incentives not to do so. Beyond just finding bugs, the issue must fit within the project's security posture, i.e. what they consider important enough to warrant a CVE. OpenSSL is famously conservative here. Many reported issues are fixed quietly or re...",
          "url": "https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its",
          "author": "Stanislav Fort",
          "published": "2026-01-27T15:21:03.374000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.",
          "importance_score": 85,
          "reasoning": "Major real-world demonstration of AI capability in critical security domain. Finding all zero-days in heavily-audited infrastructure is historically significant.",
          "themes": [
            "AI Capabilities",
            "Cybersecurity",
            "Vulnerability Discovery",
            "AI Applications"
          ],
          "continuation": null,
          "summary_html": "<p>Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.</p>",
          "content_html": "<p>This is a partial follow-up to AISLE discovered three new OpenSSL vulnerabilities from October 2025.TL;DR: OpenSSL is among the most scrutinized and audited cryptographic libraries on the planet, underpinning encryption for most of the internet. They just announced 12 new zero-day vulnerabilities (meaning previously unknown to maintainers at time of disclosure). We at AISLE discovered all 12 using our AI system. This is a historically unusual count and the first real-world demonstration of AI-based cybersecurity at this scale. Meanwhile, curl just cancelled its bug bounty program due to a flood of AI-generated spam, even as we reported 5 genuine CVEs to them. AI is simultaneously collapsing the median (\"slop\") and raising the ceiling (real zero-days in critical infrastructure).BackgroundWe at AISLE have been building an automated AI system for deep cybersecurity discovery and remediation, sometimes operating in bug bounties under the pseudonym Giant Anteater. Our goal was to turn what used to be an elite, artisanal hacker craft into a repeatable industrial process. We do this to secure the software infrastructure of human civilization before strong AI systems become ubiquitous. Prosaically, we want to make sure we don't get hacked into oblivion the moment they come online.No reliable cybersecurity benchmark reaching the desired performance level exists yet. We therefore decided to test the performance of our AI system against live targets. The clear benefit of this is that for a new, zero-day security vulnerability to be accepted as meriting a CVE (a unique vulnerability identifier), it has to pass an extremely stringent judgement by the long-term maintainers and security team of the project, who are working under many incentives not to do so. Beyond just finding bugs, the issue must fit within the project's security posture, i.e. what they consider important enough to warrant a CVE. OpenSSL is famously conservative here. Many reported issues are fixed quietly or re...</p>"
        },
        {
          "id": "fb5e2dc0ce5e",
          "title": "Who's in Charge? Disempowerment Patterns in Real-World LLM Usage",
          "content": "arXiv:2601.19062v1 Announce Type: cross  Abstract: Although AI assistants are now deeply embedded in society, there has been limited empirical study of how their usage affects human empowerment. We present the first large-scale empirical analysis of disempowerment patterns in real-world AI assistant interactions, analyzing 1.5 million consumer Claude.ai conversations using a privacy-preserving approach. We focus on situational disempowerment potential, which occurs when AI assistant interactions risk leading users to form distorted perceptions of reality, make inauthentic value judgments, or act in ways misaligned with their values. Quantitatively, we find that severe forms of disempowerment potential occur in fewer than one in a thousand conversations, though rates are substantially higher in personal domains like relationships and lifestyle. Qualitatively, we uncover several concerning patterns, such as validation of persecution narratives and grandiose identities with emphatic sycophantic language, definitive moral judgments about third parties, and complete scripting of value-laden personal communications that users appear to implement verbatim. Analysis of historical trends reveals an increase in the prevalence of disempowerment potential over time. We also find that interactions with greater disempowerment potential receive higher user approval ratings, possibly suggesting a tension between short-term user preferences and long-term human empowerment. Our findings highlight the need for AI systems designed to robustly support human autonomy and flourishing.",
          "url": "http://arxiv.org/abs/2601.19062",
          "author": "Mrinank Sharma, Miles McCain, Raymond Douglas, David Duvenaud",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Artificial Intelligence)",
          "source_type": "arxiv",
          "tags": [
            "cs.CY"
          ],
          "summary": "First large-scale empirical analysis of disempowerment patterns in 1.5M Claude.ai conversations, finding severe disempowerment occurs in <0.1% of conversations with substantially higher rates in relationship-focused interactions.",
          "importance_score": 82,
          "reasoning": "Landmark empirical study on AI safety in real deployment. Massive scale (1.5M conversations). Privacy-preserving methodology. Directly informs safety policies.",
          "themes": [
            "AI Safety",
            "Empirical Analysis",
            "Human-AI Interaction",
            "Disempowerment"
          ],
          "continuation": null,
          "summary_html": "<p>First large-scale empirical analysis of disempowerment patterns in 1.5M Claude.ai conversations, finding severe disempowerment occurs in &lt;0.1% of conversations with substantially higher rates in relationship-focused interactions.</p>",
          "content_html": "<p>arXiv:2601.19062v1 Announce Type: cross  Abstract: Although AI assistants are now deeply embedded in society, there has been limited empirical study of how their usage affects human empowerment. We present the first large-scale empirical analysis of disempowerment patterns in real-world AI assistant interactions, analyzing 1.5 million consumer Claude.ai conversations using a privacy-preserving approach. We focus on situational disempowerment potential, which occurs when AI assistant interactions risk leading users to form distorted perceptions of reality, make inauthentic value judgments, or act in ways misaligned with their values. Quantitatively, we find that severe forms of disempowerment potential occur in fewer than one in a thousand conversations, though rates are substantially higher in personal domains like relationships and lifestyle. Qualitatively, we uncover several concerning patterns, such as validation of persecution narratives and grandiose identities with emphatic sycophantic language, definitive moral judgments about third parties, and complete scripting of value-laden personal communications that users appear to implement verbatim. Analysis of historical trends reveals an increase in the prevalence of disempowerment potential over time. We also find that interactions with greater disempowerment potential receive higher user approval ratings, possibly suggesting a tension between short-term user preferences and long-term human empowerment. Our findings highlight the need for AI systems designed to robustly support human autonomy and flourishing.</p>"
        },
        {
          "id": "587a23d43703",
          "title": "A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy",
          "content": "arXiv:2601.18939v1 Announce Type: new  Abstract: Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior, decode them into residual space, and fine-tune only those neurons using gradient masking. We demonstrate this approach on the task of reducing sycophantic behavior, where our method matches or exceeds state-of-the-art performance on four benchmarks (Syco-Bench, NLP, POLI, PHIL) using Gemma-2-2B and 9B models. Our results show that sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available",
          "url": "http://arxiv.org/abs/2601.18939",
          "author": "Claire O'Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, Kevin Zhu, Sean O'Brien, Ashwinee Panda, Ryan Lagasse",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.LG"
          ],
          "summary": "Proposes surgical approach to fixing sycophancy in LLMs by identifying the 3% of neurons most responsible using sparse autoencoders and linear probes, then fine-tuning only those neurons with gradient masking.",
          "importance_score": 82,
          "reasoning": "Highly targeted alignment intervention with strong results. Mechanistic interpretability approach to behavioral alignment is promising direction; matches SOTA with significantly less data.",
          "themes": [
            "AI Alignment",
            "Mechanistic Interpretability",
            "Sycophancy",
            "LLM Safety"
          ],
          "continuation": null,
          "summary_html": "<p>Proposes surgical approach to fixing sycophancy in LLMs by identifying the 3% of neurons most responsible using sparse autoencoders and linear probes, then fine-tuning only those neurons with gradient masking.</p>",
          "content_html": "<p>arXiv:2601.18939v1 Announce Type: new  Abstract: Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior, decode them into residual space, and fine-tune only those neurons using gradient masking. We demonstrate this approach on the task of reducing sycophantic behavior, where our method matches or exceeds state-of-the-art performance on four benchmarks (Syco-Bench, NLP, POLI, PHIL) using Gemma-2-2B and 9B models. Our results show that sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available</p>"
        },
        {
          "id": "68bd18d53406",
          "title": "Provable Learning of Random Hierarchy Models and Hierarchical Shallow-to-Deep Chaining",
          "content": "arXiv:2601.19756v1 Announce Type: new  Abstract: The empirical success of deep learning is often attributed to deep networks' ability to exploit hierarchical structure in data, constructing increasingly complex features across layers. Yet despite substantial progress in deep learning theory, most optimization results sill focus on networks with only two or three layers, leaving the theoretical understanding of hierarchical learning in genuinely deep models limited. This leads to a natural question: can we prove that deep networks, trained by gradient-based methods, can efficiently exploit hierarchical structure?   In this work, we consider Random Hierarchy Models -- a hierarchical context-free grammar introduced by arXiv:2307.02129 and conjectured to separate deep and shallow networks. We prove that, under mild conditions, a deep convolutional network can be efficiently trained to learn this function class. Our proof builds on a general observation: if intermediate layers can receive clean signal from the labels and the relevant features are weakly identifiable, then layerwise training each individual layer suffices to hierarchically learn the target function.",
          "url": "http://arxiv.org/abs/2601.19756",
          "author": "Yunwei Ren, Yatin Dandi, Florent Krzakala, Jason D. Lee",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.LG"
          ],
          "summary": "Proves that deep networks trained by gradient methods can efficiently learn Random Hierarchy Models, demonstrating provable hierarchical learning separating deep from shallow networks.",
          "importance_score": 75,
          "reasoning": "Major theoretical contribution proving deep network hierarchical learning. Addresses longstanding question about why depth matters with rigorous analysis.",
          "themes": [
            "Deep Learning Theory",
            "Hierarchical Learning",
            "Provable Learning"
          ],
          "continuation": null,
          "summary_html": "<p>Proves that deep networks trained by gradient methods can efficiently learn Random Hierarchy Models, demonstrating provable hierarchical learning separating deep from shallow networks.</p>",
          "content_html": "<p>arXiv:2601.19756v1 Announce Type: new  Abstract: The empirical success of deep learning is often attributed to deep networks' ability to exploit hierarchical structure in data, constructing increasingly complex features across layers. Yet despite substantial progress in deep learning theory, most optimization results sill focus on networks with only two or three layers, leaving the theoretical understanding of hierarchical learning in genuinely deep models limited. This leads to a natural question: can we prove that deep networks, trained by gradient-based methods, can efficiently exploit hierarchical structure?   In this work, we consider Random Hierarchy Models -- a hierarchical context-free grammar introduced by arXiv:2307.02129 and conjectured to separate deep and shallow networks. We prove that, under mild conditions, a deep convolutional network can be efficiently trained to learn this function class. Our proof builds on a general observation: if intermediate layers can receive clean signal from the labels and the relevant features are weakly identifiable, then layerwise training each individual layer suffices to hierarchically learn the target function.</p>"
        },
        {
          "id": "180fd3e3456f",
          "title": "To Grok Grokking: Provable Grokking in Ridge Regression",
          "content": "arXiv:2601.19791v1 Announce Type: new  Abstract: We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the \"grokking time\") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.",
          "url": "http://arxiv.org/abs/2601.19791",
          "author": "Mingyue Xu, Gal Vardi, Itay Safran",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.LG"
          ],
          "summary": "Proves end-to-end grokking in ridge regression: overfitting, delayed poor generalization, then eventual generalization. Shows grokking can be amplified or eliminated through hyperparameter tuning.",
          "importance_score": 75,
          "reasoning": "First rigorous quantitative bounds on grokking phenomenon. Important for understanding delayed generalization with practical implications for hyperparameter selection.",
          "themes": [
            "Deep Learning Theory",
            "Grokking",
            "Generalization"
          ],
          "continuation": null,
          "summary_html": "<p>Proves end-to-end grokking in ridge regression: overfitting, delayed poor generalization, then eventual generalization. Shows grokking can be amplified or eliminated through hyperparameter tuning.</p>",
          "content_html": "<p>arXiv:2601.19791v1 Announce Type: new  Abstract: We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the \"grokking time\") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.</p>"
        },
        {
          "id": "07e893a2c4e7",
          "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
          "content": "arXiv:2601.19895v1 Announce Type: new  Abstract: Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.",
          "url": "http://arxiv.org/abs/2601.19895",
          "author": "Chen Chen, Lai Wei",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.LG"
          ],
          "summary": "Revisits Post-LayerNorm transformers which were abandoned due to training instability, proposing 'Keel' which replaces ResNet-style residual paths with Highway-style connections. Claims to solve the gradient vanishing problem enabling reliable training at extreme depths with superior expressivity.",
          "importance_score": 78,
          "reasoning": "Potentially significant architectural contribution addressing a fundamental limitation of current transformer designs. Depth scaling is important for continued model improvement",
          "themes": [
            "Transformer Architecture",
            "Language Models",
            "Deep Learning Theory"
          ],
          "continuation": null,
          "summary_html": "<p>Revisits Post-LayerNorm transformers which were abandoned due to training instability, proposing 'Keel' which replaces ResNet-style residual paths with Highway-style connections. Claims to solve the gradient vanishing problem enabling reliable training at extreme depths with superior expressivity.</p>",
          "content_html": "<p>arXiv:2601.19895v1 Announce Type: new  Abstract: Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.</p>"
        },
        {
          "id": "ab335998790f",
          "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
          "content": "arXiv:2601.19834v1 Announce Type: new  Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
          "url": "http://arxiv.org/abs/2601.19834",
          "author": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Artificial Intelligence)",
          "source_type": "arxiv",
          "tags": [
            "cs.AI"
          ],
          "summary": "Argues that visual generation capabilities unlock human-like reasoning through multimodal world models, enabling better performance in physical and spatial intelligence domains where verbal reasoning alone is insufficient.",
          "importance_score": 78,
          "reasoning": "Important theoretical contribution on the role of visual generation in reasoning. Addresses fundamental limitation of verbal-only CoT. Timely given emergence of unified multimodal models.",
          "themes": [
            "Multimodal Reasoning",
            "World Models",
            "Visual Generation",
            "Chain-of-Thought"
          ],
          "continuation": null,
          "summary_html": "<p>Argues that visual generation capabilities unlock human-like reasoning through multimodal world models, enabling better performance in physical and spatial intelligence domains where verbal reasoning alone is insufficient.</p>",
          "content_html": "<p>arXiv:2601.19834v1 Announce Type: new  Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.</p>"
        },
        {
          "id": "b8650d4dd7e5",
          "title": "Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models",
          "content": "arXiv:2601.19061v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our \"Thought-Transfer\" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.",
          "url": "http://arxiv.org/abs/2601.19061",
          "author": "Harsh Chaudhari, Ethan Rathbum, Hanna Foerster, Jamie Hayes, Matthew Jagielski, Milad Nasr, Ilia Shumailov, Alina Oprea",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.CR"
          ],
          "summary": "Reveals vulnerability in Chain-of-Thought reasoning models to indirect targeted poisoning attacks that manipulate responses without explicit triggered queries with flawed reasoning in training data. Attacks the reasoning traces themselves.",
          "importance_score": 75,
          "reasoning": "Important AI safety finding from researchers including Google (Matthew Jagielski, Milad Nasr). Demonstrates new attack vector on increasingly important CoT reasoning",
          "themes": [
            "AI Safety",
            "Adversarial Attacks",
            "Chain-of-Thought Reasoning",
            "LLM Security"
          ],
          "continuation": null,
          "summary_html": "<p>Reveals vulnerability in Chain-of-Thought reasoning models to indirect targeted poisoning attacks that manipulate responses without explicit triggered queries with flawed reasoning in training data. Attacks the reasoning traces themselves.</p>",
          "content_html": "<p>arXiv:2601.19061v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our \"Thought-Transfer\" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.</p>"
        },
        {
          "id": "2ad230fcffab",
          "title": "Differential Voting: Loss Functions For Axiomatically Diverse Aggregation of Heterogeneous Preferences",
          "content": "arXiv:2601.18824v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) implicitly aggregates heterogeneous human preferences into a single utility function, even though the underlying utilities of the participants are in practice diverse. Hence, RLHF can be viewed as a form of voting, where the aggregation mechanism is defined by the loss function. Although Arrow's Impossibility Theorem suggests that different mechanisms satisfy different sets of desirable axioms, most existing methods rely on a single aggregation principle, typically the Bradley-Terry-Luce (BTL) model, which corresponds to Borda count voting. This restricts the axiomatic properties of the learned reward and obscures the normative assumptions embedded in optimization. In this work, we introduce Differential Voting, a unifying framework that constructs instance-wise, differentiable loss functions whose population-level optima provably correspond to distinct classical voting rules. We develop differentiable surrogates for majority-based aggregation (BTL), Copeland, and Kemeny rules, and formally analyze their calibration properties, gradient fields, and limiting behavior as smoothing parameters vanish. For each loss, we establish consistency with the corresponding social choice rule and characterize the axioms it satisfies or violates. Our analysis shows how design choices in loss geometry-such as margin sensitivity and boundary concentration-directly translate into normative aggregation behavior. Differential Voting makes preference aggregation an explicit and controllable design choice in RLHF, enabling principled trade-offs between axiomatic guarantees and optimization stability. Code to reproduce our experiments is open-sourced.",
          "url": "http://arxiv.org/abs/2601.18824",
          "author": "Zhiyu An, Duaa Nakshbandi, Wan Du",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Artificial Intelligence)",
          "source_type": "arxiv",
          "tags": [
            "cs.GT"
          ],
          "summary": "Introduces differential voting framework connecting RLHF aggregation mechanisms to voting theory, deriving loss functions that satisfy different axiomatic properties beyond the default BTL/Borda count approach.",
          "importance_score": 76,
          "reasoning": "Important theoretical contribution connecting RLHF to social choice theory. Addresses fundamental normative assumptions in preference aggregation. Enables principled choice of aggregation mechanism.",
          "themes": [
            "RLHF",
            "Voting Theory",
            "Preference Learning",
            "Alignment"
          ],
          "continuation": null,
          "summary_html": "<p>Introduces differential voting framework connecting RLHF aggregation mechanisms to voting theory, deriving loss functions that satisfy different axiomatic properties beyond the default BTL/Borda count approach.</p>",
          "content_html": "<p>arXiv:2601.18824v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) implicitly aggregates heterogeneous human preferences into a single utility function, even though the underlying utilities of the participants are in practice diverse. Hence, RLHF can be viewed as a form of voting, where the aggregation mechanism is defined by the loss function. Although Arrow's Impossibility Theorem suggests that different mechanisms satisfy different sets of desirable axioms, most existing methods rely on a single aggregation principle, typically the Bradley-Terry-Luce (BTL) model, which corresponds to Borda count voting. This restricts the axiomatic properties of the learned reward and obscures the normative assumptions embedded in optimization. In this work, we introduce Differential Voting, a unifying framework that constructs instance-wise, differentiable loss functions whose population-level optima provably correspond to distinct classical voting rules. We develop differentiable surrogates for majority-based aggregation (BTL), Copeland, and Kemeny rules, and formally analyze their calibration properties, gradient fields, and limiting behavior as smoothing parameters vanish. For each loss, we establish consistency with the corresponding social choice rule and characterize the axioms it satisfies or violates. Our analysis shows how design choices in loss geometry-such as margin sensitivity and boundary concentration-directly translate into normative aggregation behavior. Differential Voting makes preference aggregation an explicit and controllable design choice in RLHF, enabling principled trade-offs between axiomatic guarantees and optimization stability. Code to reproduce our experiments is open-sourced.</p>"
        },
        {
          "id": "5a60a6b16a85",
          "title": "Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning",
          "content": "arXiv:2601.18984v1 Announce Type: new  Abstract: Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.",
          "url": "http://arxiv.org/abs/2601.18984",
          "author": "Haolin Liu, Dian Yu, Sidi Lu, Yujun Zhou, Rui Liu, Zhenwen Liang, Haitao Mi, Chen-Yu Wei, Dong Yu",
          "published": "2026-01-28T00:00:00-05:00",
          "source": "arXiv (Machine Learning)",
          "source_type": "arxiv",
          "tags": [
            "cs.LG"
          ],
          "summary": "Proposes Verifiable Process-Supervised RL (VP-RL) that uses PRMs to detect first incorrect step and penalizes only from that point onward, bridging gap between PRM evaluation (error detection) and RL usage (raw rewards).",
          "importance_score": 78,
          "reasoning": "Important alignment of PRM capabilities with RL training. Addresses fundamental mismatch in how PRMs are evaluated vs used; strong empirical results on reasoning benchmarks.",
          "themes": [
            "LLM Reasoning",
            "Process Reward Models",
            "Reinforcement Learning",
            "AI Alignment"
          ],
          "continuation": null,
          "summary_html": "<p>Proposes Verifiable Process-Supervised RL (VP-RL) that uses PRMs to detect first incorrect step and penalizes only from that point onward, bridging gap between PRM evaluation (error detection) and RL usage (raw rewards).</p>",
          "content_html": "<p>arXiv:2601.18984v1 Announce Type: new  Abstract: Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.</p>"
        }
      ]
    },
    "social": {
      "count": 564,
      "category_summary": "AI coding agents dominated discussions with extraordinary practitioner evidence. **Anthropic's bcherny** [revealed](/?date=2026-01-28&category=social#item-3224ccef215a) the Claude Code team ships 100% AI-generated code using **Opus 4.5**, sparking intense debate about the future of software development. **Simon Willison** [highlighted](/?date=2026-01-28&category=social#item-f428962b7742) a fully AI-built 20,000-line Rust browser, while **Ethan Mollick** demonstrated Claude Code creating complete adventure games from single prompts.\n\n- **Washington Post** [broke major investigation](/?date=2026-01-28&category=social#item-1849acd242a9) into AI companies' secret race to ingest copyrighted works, with **Anthropic's Project Panama** attempting a more ethical approach\n- **OpenAI** [launched **Prism**](/?date=2026-01-28&category=social#item-ec5a74493236), a free LaTeX workspace for scientific collaboration, drawing massive engagement (2.4M views)\n- **AI2** [released **SERA**](/?date=2026-01-28&category=social#item-38bad42c37d5), open-source coding agents reproducible for ~$400, democratizing agent development\n- **Google** [announced **Agentic Vision**](/?date=2026-01-28&category=social#item-d7e7120ebcc1) for Gemini 3 Flash; **Anthropic** [partnered with UK government](/?date=2026-01-28&category=social#item-4259e2cbcec7) for citizen services\n\n**John Carmack** [provided technical analysis](/?date=2026-01-28&category=social#item-486f93d55e1a) comparing biological and artificial neural networks, while **Yann LeCun** [defended his JEPA research](/?date=2026-01-28&category=social#item-e2c324072cae) productivity and announced company formation around world models. **Nathan Lambert** [predicted](/?date=2026-01-28&category=social#item-0f9ad961124d) academic paper writing will be transformed by AI in 2026, noting intense competition for Overleaf.",
      "category_summary_html": "<p>AI coding agents dominated discussions with extraordinary practitioner evidence. <strong>Anthropic's bcherny</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-3224ccef215a\" class=\"internal-link\" rel=\"noopener noreferrer\">revealed</a> the Claude Code team ships 100% AI-generated code using <strong>Opus 4.5</strong>, sparking intense debate about the future of software development. <strong>Simon Willison</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-f428962b7742\" class=\"internal-link\" rel=\"noopener noreferrer\">highlighted</a> a fully AI-built 20,000-line Rust browser, while <strong>Ethan Mollick</strong> demonstrated Claude Code creating complete adventure games from single prompts.</p>\n<ul>\n<li><strong>Washington Post</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-1849acd242a9\" class=\"internal-link\" rel=\"noopener noreferrer\">broke major investigation</a> into AI companies' secret race to ingest copyrighted works, with <strong>Anthropic's Project Panama</strong> attempting a more ethical approach</li>\n<li><strong>OpenAI</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-ec5a74493236\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>Prism</strong></a>, a free LaTeX workspace for scientific collaboration, drawing massive engagement (2.4M views)</li>\n<li><strong>AI2</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-38bad42c37d5\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>SERA</strong></a>, open-source coding agents reproducible for ~$400, democratizing agent development</li>\n<li><strong>Google</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-d7e7120ebcc1\" class=\"internal-link\" rel=\"noopener noreferrer\">announced <strong>Agentic Vision</strong></a> for Gemini 3 Flash; <strong>Anthropic</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-4259e2cbcec7\" class=\"internal-link\" rel=\"noopener noreferrer\">partnered with UK government</a> for citizen services</li>\n</ul>\n<p><strong>John Carmack</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-486f93d55e1a\" class=\"internal-link\" rel=\"noopener noreferrer\">provided technical analysis</a> comparing biological and artificial neural networks, while <strong>Yann LeCun</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-e2c324072cae\" class=\"internal-link\" rel=\"noopener noreferrer\">defended his JEPA research</a> productivity and announced company formation around world models. <strong>Nathan Lambert</strong> <a href=\"/?date=2026-01-28&amp;category=social#item-0f9ad961124d\" class=\"internal-link\" rel=\"noopener noreferrer\">predicted</a> academic paper writing will be transformed by AI in 2026, noting intense competition for Overleaf.</p>",
      "themes": [
        {
          "name": "AI Training Data Ethics & Legal Issues",
          "description": "Revelations about AI companies' data collection practices including Anthropic's Project Panama for book scanning, raising significant legal and ethical questions.",
          "item_count": 2,
          "example_items": [],
          "importance": 95
        },
        {
          "name": "Product Launches & Announcements",
          "description": "Major new AI products and features from leading labs including OpenAI's Prism scientific workspace and Google's Agentic Vision in Gemini 3 Flash",
          "item_count": 6,
          "example_items": [],
          "importance": 94
        },
        {
          "name": "AI Coding Agents Creating Complex Software",
          "description": "Major demonstrations of autonomous AI agents building substantial software projects - complete games and 20K-line browsers with minimal human prompting",
          "item_count": 12,
          "example_items": [],
          "importance": 92
        },
        {
          "name": "Government & Public Sector AI",
          "description": "Anthropic's partnership with UK government to build AI assistant for gov.uk, representing major public sector AI adoption.",
          "item_count": 1,
          "example_items": [],
          "importance": 90
        },
        {
          "name": "Open-Source Coding Agents",
          "description": "AI2's SERA release democratizing coding agent training with accessible costs ($400-$12K), open weights, and integration with Claude Code",
          "item_count": 6,
          "example_items": [],
          "importance": 90
        },
        {
          "name": "AI Coding Productivity at Scale",
          "description": "Real-world data on AI-assisted development from Anthropic team showing 100% AI-written code, 20+ PRs/day, and practical code review workflows",
          "item_count": 5,
          "example_items": [],
          "importance": 88
        },
        {
          "name": "AI Architecture Debates",
          "description": "Ongoing discourse about fundamental AI approaches - LeCun advocating JEPA/world models vs mainstream LLM approaches, with implications for robotics and AGI paths",
          "item_count": 7,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "New Model Releases",
          "description": "Multiple significant model releases including Kimi K2.5 multimodal model, DeepSeek-OCR 2, and Arcee Trinity Large, advancing open-source capabilities.",
          "item_count": 5,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "AI Safety & Alignment",
          "description": "Anthropic's strategic communications including risk essays and Claude Constitution, plus DeepMind's cautious optimism positioning",
          "item_count": 4,
          "example_items": [],
          "importance": 82
        },
        {
          "name": "Claude Code Updates",
          "description": "Multiple feature announcements for Claude Code including customizable keybindings, spinner verbs, and browser-based access. Active development from Anthropic's coding tool.",
          "item_count": 6,
          "example_items": [],
          "importance": 82
        }
      ],
      "top_items": [
        {
          "id": "3224ccef215a",
          "title": "@karpathy As always, a very thoughtful and well reasoned take. I read till the end.\n\nI think the Cla...",
          "content": "@karpathy As always, a very thoughtful and well reasoned take. I read till the end.\n\nI think the Claude Code team itself might be an  indicator of where things are headed. We have directional answers for some (not all) of the prompts:\n\n1. We hire mostly generalists. We have a mix of senior engineers and less senior since not all of the things people learned in the past translate to coding with LLMs. As you said, the model can fill in the details. 10x engineers definitely exist, and they often span across multiple areas — product and design, product and business, product and infra (@jarredsumner is a great example of the latter. Yes, he’s blushing).\n2. Pretty much 100% of our code is written by Claude Code + Opus 4.5. For me personally it has been 100% for two+ months now, I don’t even make small edits by hand. I shipped 22 PRs yesterday and 27 the day before, each one 100% written by Claude. Some were written from a CLI, some from the iOS app; others on the team code largely with the Claude Code app Slack or with the Desktop app. I think most of the industry will see similar stats in the coming months — it will take more time for some vs others. We will then start seeing similar stats for non-coding computer work also.\n3. The code quality problems you listed are real: the model over-complicates things, it leaves dead code around, it doesn’t like to refactor when it should. These will continue improve as the model improves, and our code quality bar will go up even more as a result. My bet is that there will be no slopcopolypse because the model will become better at writing less sloppy code and at fixing existing code issues; I think 4.5 is already quite good at these and it will continue to get better. In the meantime, what helps is also having the model code review its code using a fresh context window; at Anthropic we use claude -p for this on every PR and it catches and fixes many issues. \n\nOverall your ideas very much resonate. Thanks again for sharing. ✌️",
          "url": "https://twitter.com/bcherny/status/2015979257038831967",
          "author": "@bcherny",
          "published": "2026-01-27T02:44:44",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Following yesterday's [Social](/?date=2026-01-27&category=social#item-fd5e3c855071) discussion, Anthropic's bcherny responds to Karpathy on AI-assisted coding, revealing Claude Code team writes 100% of code with Claude Code + Opus 4.5. He shipped 22-27 PRs/day entirely AI-written. Discusses hiring generalists, code quality challenges, and using 'claude -p' for code review.",
          "importance_score": 95,
          "reasoning": "Exceptional insider perspective from Anthropic team member. Extremely high engagement (3344 likes, 214K views). Provides concrete data on AI coding productivity at a leading lab.",
          "themes": [
            "AI coding workflows",
            "developer productivity",
            "code quality",
            "Anthropic insider"
          ],
          "continuation": {
            "original_item_id": "fd5e3c855071",
            "original_date": "2026-01-27",
            "original_category": "social",
            "original_title": "A few random notes from claude coding quite a bit last few weeks.",
            "continuation_type": "follow_up",
            "should_demote": false,
            "reference_text": "Following yesterday's **Social** discussion"
          },
          "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-01-27&amp;category=social#item-fd5e3c855071\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> discussion, Anthropic's bcherny responds to Karpathy on AI-assisted coding, revealing Claude Code team writes 100% of code with Claude Code + Opus 4.5. He shipped 22-27 PRs/day entirely AI-written. Discusses hiring generalists, code quality challenges, and using 'claude -p' for code review.</p>",
          "content_html": "<p>@karpathy As always, a very thoughtful and well reasoned take. I read till the end.</p>\n<p>I think the Claude Code team itself might be an  indicator of where things are headed. We have directional answers for some (not all) of the prompts:</p>\n<p>1. We hire mostly generalists. We have a mix of senior engineers and less senior since not all of the things people learned in the past translate to coding with LLMs. As you said, the model can fill in the details. 10x engineers definitely exist, and they often span across multiple areas — product and design, product and business, product and infra (@jarredsumner is a great example of the latter. Yes, he’s blushing).</p>\n<p>2. Pretty much 100% of our code is written by Claude Code + Opus 4.5. For me personally it has been 100% for two+ months now, I don’t even make small edits by hand. I shipped 22 PRs yesterday and 27 the day before, each one 100% written by Claude. Some were written from a CLI, some from the iOS app; others on the team code largely with the Claude Code app Slack or with the Desktop app. I think most of the industry will see similar stats in the coming months — it will take more time for some vs others. We will then start seeing similar stats for non-coding computer work also.</p>\n<p>3. The code quality problems you listed are real: the model over-complicates things, it leaves dead code around, it doesn’t like to refactor when it should. These will continue improve as the model improves, and our code quality bar will go up even more as a result. My bet is that there will be no slopcopolypse because the model will become better at writing less sloppy code and at fixing existing code issues; I think 4.5 is already quite good at these and it will continue to get better. In the meantime, what helps is also having the model code review its code using a fresh context window; at Anthropic we use claude -p for this on every PR and it catches and fixes many issues.</p>\n<p>Overall your ideas very much resonate. Thanks again for sharing. ✌️</p>"
        },
        {
          "id": "1849acd242a9",
          "title": "New: Unsealed court docs detail Big Tech’s yearslong, secret race to ingest the collective works of ...",
          "content": "New: Unsealed court docs detail Big Tech’s yearslong, secret race to ingest the collective works of humanity, including Anthropic’s project to “destructively scan all the books in the world.\" https://t.co/CzyrPAKyNY",
          "url": "https://twitter.com/WillOremus/status/2016172534496973114",
          "author": "@WillOremus",
          "published": "2026-01-27T15:32:45",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Washington Post journalist breaks story on unsealed court documents revealing AI companies' secret efforts to ingest massive amounts of copyrighted content, including Anthropic's 'Project Panama' to destructively scan all books globally.",
          "importance_score": 95,
          "reasoning": "Breaking investigative journalism with high engagement (883 likes, 56k views) revealing significant legal/ethical issues in AI training data. Major implications for the industry.",
          "themes": [
            "AI ethics",
            "training data",
            "legal issues",
            "Anthropic"
          ],
          "continuation": null,
          "summary_html": "<p>Washington Post journalist breaks story on unsealed court documents revealing AI companies' secret efforts to ingest massive amounts of copyrighted content, including Anthropic's 'Project Panama' to destructively scan all books globally.</p>",
          "content_html": "<p>New: Unsealed court docs detail Big Tech’s yearslong, secret race to ingest the collective works of humanity, including Anthropic’s project to “destructively scan all the books in the world.\" https://t.co/CzyrPAKyNY</p>"
        },
        {
          "id": "ec5a74493236",
          "title": "Introducing Prism, a free workspace for scientists to write and collaborate on research, powered by ...",
          "content": "Introducing Prism, a free workspace for scientists to write and collaborate on research, powered by GPT-5.2.\n\nAvailable today to anyone with a ChatGPT personal account: https://t.co/9mTLAbxPdH https://t.co/GJOIipU3hx",
          "url": "https://twitter.com/OpenAI/status/2016209462621831448",
          "author": "@OpenAI",
          "published": "2026-01-27T17:59:29",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "OpenAI announces Prism, a free LaTeX-native workspace for scientific research collaboration powered by GPT-5.2, available to all ChatGPT personal account holders",
          "importance_score": 95,
          "reasoning": "Major product launch from OpenAI with massive engagement (2.4M views, 10K+ likes). New tool targeting scientific research community with AI integration - significant market expansion",
          "themes": [
            "product_launch",
            "scientific_tools",
            "AI_integration"
          ],
          "continuation": null,
          "summary_html": "<p>OpenAI announces Prism, a free LaTeX-native workspace for scientific research collaboration powered by GPT-5.2, available to all ChatGPT personal account holders</p>",
          "content_html": "<p>Introducing Prism, a free workspace for scientists to write and collaborate on research, powered by GPT-5.2.</p>\n<p>Available today to anyone with a ChatGPT personal account: https://t.co/9mTLAbxPdH https://t.co/GJOIipU3hx</p>"
        },
        {
          "id": "38bad42c37d5",
          "title": "Introducing Ai2 Open Coding Agents—starting with SERA, our first-ever coding models. Fast, accessibl...",
          "content": "Introducing Ai2 Open Coding Agents—starting with SERA, our first-ever coding models. Fast, accessible agents (8B–32B) that adapt to any repo, including private codebases. Train a powerful specialized agent for as little as ~$400, &amp; it works with Claude Code out of the box. 🧵 https://t.co/dor94O62B9",
          "url": "https://twitter.com/allen_ai/status/2016182658989006865",
          "author": "@allen_ai",
          "published": "2026-01-27T16:12:59",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "AI2 (Allen Institute) announces SERA, a family of open-source coding agents (8B-32B parameters) that can adapt to any repository including private codebases. Training costs as low as $400, works with Claude Code out of the box.",
          "importance_score": 92,
          "reasoning": "Major open-source release from respected research institution. Very high engagement (685 likes, 124K views). Democratizes coding agent training with accessible costs and full openness.",
          "themes": [
            "open-source AI",
            "coding agents",
            "model release",
            "AI accessibility"
          ],
          "continuation": null,
          "summary_html": "<p>AI2 (Allen Institute) announces SERA, a family of open-source coding agents (8B-32B parameters) that can adapt to any repository including private codebases. Training costs as low as $400, works with Claude Code out of the box.</p>",
          "content_html": "<p>Introducing Ai2 Open Coding Agents—starting with SERA, our first-ever coding models. Fast, accessible agents (8B–32B) that adapt to any repo, including private codebases. Train a powerful specialized agent for as little as ~$400, &amp; it works with Claude Code out of the box. 🧵 https://t.co/dor94O62B9</p>"
        },
        {
          "id": "d7e7120ebcc1",
          "title": "Introducing Agentic Vision — a new frontier AI capability in Gemini 3 Flash that converts image unde...",
          "content": "Introducing Agentic Vision — a new frontier AI capability in Gemini 3 Flash that converts image understanding from a static act into an agentic process.\n\nBy combining visual reasoning with code execution, one of the first tools supported by Agentic Vision, the model grounds answers in visual evidence and delivers a consistent 5-10% quality boost across most vision benchmarks. Here’s how the agentic ‘Think, Act, Observe’ loop works:\n— Think: The model analyzes an image query then architects a multi-step plan\n— Act: The model then generates and executes Python code to actively manipulate or analyze images\n— Observe: The transformed image is appended to the model's context window, allowing it to inspect the new data before generating a final response to the initial image query \n\nLearn more about Agentic Vision and how to access it in our blog ⬇️\nhttps://t.co/UdSOuF2YXY",
          "url": "https://twitter.com/GoogleAI/status/2016267526330601720",
          "author": "@GoogleAI",
          "published": "2026-01-27T21:50:13",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Google AI introduces Agentic Vision in Gemini 3 Flash - a new capability that converts image understanding into an agentic process using Think-Act-Observe loops with code execution",
          "importance_score": 93,
          "reasoning": "Major technical announcement from Google AI about new agentic capabilities in Gemini 3 Flash. High engagement (218K views), introduces novel 'agentic vision' paradigm with 5-10% quality improvement",
          "themes": [
            "product_launch",
            "multimodal_AI",
            "agentic_AI",
            "computer_vision"
          ],
          "continuation": null,
          "summary_html": "<p>Google AI introduces Agentic Vision in Gemini 3 Flash - a new capability that converts image understanding into an agentic process using Think-Act-Observe loops with code execution</p>",
          "content_html": "<p>Introducing Agentic Vision — a new frontier AI capability in Gemini 3 Flash that converts image understanding from a static act into an agentic process.</p>\n<p>By combining visual reasoning with code execution, one of the first tools supported by Agentic Vision, the model grounds answers in visual evidence and delivers a consistent 5-10% quality boost across most vision benchmarks. Here’s how the agentic ‘Think, Act, Observe’ loop works:</p>\n<p>— Think: The model analyzes an image query then architects a multi-step plan</p>\n<p>— Act: The model then generates and executes Python code to actively manipulate or analyze images</p>\n<p>— Observe: The transformed image is appended to the model's context window, allowing it to inspect the new data before generating a final response to the initial image query</p>\n<p>Learn more about Agentic Vision and how to access it in our blog ⬇️</p>\n<p>https://t.co/UdSOuF2YXY</p>"
        },
        {
          "id": "4259e2cbcec7",
          "title": "We’re partnering with the UK's Department for Science, Innovation and Technology to build an AI assi...",
          "content": "We’re partnering with the UK's Department for Science, Innovation and Technology to build an AI assistant for https://t.co/e3sn3vm9wg.\n\nIt will offer tailored advice to help British people navigate government services.\n\nRead more about our partnership: https://t.co/k7pAV6aX8T",
          "url": "https://twitter.com/AnthropicAI/status/2016102835092427080",
          "author": "@AnthropicAI",
          "published": "2026-01-27T10:55:47",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Anthropic announces official partnership with UK Department for Science, Innovation and Technology to build an AI assistant for gov.uk to help citizens navigate government services.",
          "importance_score": 90,
          "reasoning": "Official announcement from Anthropic with very high engagement (1524 likes, 174k views). Major government AI adoption milestone.",
          "themes": [
            "government AI",
            "Anthropic",
            "public services",
            "partnerships"
          ],
          "continuation": null,
          "summary_html": "<p>Anthropic announces official partnership with UK Department for Science, Innovation and Technology to build an AI assistant for gov.uk to help citizens navigate government services.</p>",
          "content_html": "<p>We’re partnering with the UK's Department for Science, Innovation and Technology to build an AI assistant for https://t.co/e3sn3vm9wg.</p>\n<p>It will offer tailored advice to help British people navigate government services.</p>\n<p>Read more about our partnership: https://t.co/k7pAV6aX8T</p>"
        },
        {
          "id": "f428962b7742",
          "title": "The latest entrant in the coding-agent-constructed web browsers is here, this one by @emsh.cat, and ...",
          "content": "The latest entrant in the coding-agent-constructed web browsers is here, this one by @emsh.cat, and it's REALLY impressive - 3 days of development, 20,000 lines of Rust, no Cargo dependencies and it renders HTML+CSS extremely well\nsimonwillison.net/2026/Jan/27/...",
          "url": "https://bsky.app/profile/simonwillison.net/post/3mdgabfgyrs2k",
          "author": "@simonwillison.net",
          "published": "2026-01-27T17:00:15.243000",
          "source": "Bluesky",
          "source_type": "bluesky",
          "tags": [],
          "summary": "Simon Willison highlights impressive AI-built web browser: 3 days of development, 20,000 lines of Rust, no Cargo dependencies, excellent HTML+CSS rendering - built by coding agent",
          "importance_score": 92,
          "reasoning": "Major technical achievement demonstrating AI coding agent capabilities at scale, high engagement (66 likes, 15 reposts), significant complexity (20K lines Rust), notable constraint (no dependencies)",
          "themes": [
            "AI_coding",
            "coding_agents",
            "rust",
            "browser_development",
            "autonomous_AI"
          ],
          "continuation": null,
          "summary_html": "<p>Simon Willison highlights impressive AI-built web browser: 3 days of development, 20,000 lines of Rust, no Cargo dependencies, excellent HTML+CSS rendering - built by coding agent</p>",
          "content_html": "<p>The latest entrant in the coding-agent-constructed web browsers is here, this one by @emsh.cat, and it's REALLY impressive - 3 days of development, 20,000 lines of Rust, no Cargo dependencies and it renders HTML+CSS extremely well</p>\n<p>simonwillison.net/2026/Jan/27/...</p>"
        },
        {
          "id": "486f93d55e1a",
          "title": "@yunta_tsai You can argue that bio brains have vastly more weights that are mostly sparse, because t...",
          "content": "@yunta_tsai You can argue that bio brains have vastly more weights that are mostly sparse, because the space of neurons that could have been connected to is very large, with synapses exploring and getting pruned. Simulating bio connectivity would be expensive on GPUs!\n\nBio neurons look good at the worm level, where mere hundreds seem to do more work than artificial ones could, but it seems like artificial networks may have better scaling properties in the billions.",
          "url": "https://twitter.com/ID_AA_Carmack/status/2016170291190215058",
          "author": "@ID_AA_Carmack",
          "published": "2026-01-27T15:23:50",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "John Carmack provides technical analysis comparing biological vs artificial neural networks, discussing sparse weights, connectivity costs, and scaling properties - noting bio neurons perform better at small scale (worms) but artificial networks may scale better at billions of parameters",
          "importance_score": 92,
          "reasoning": "John Carmack is a legendary programmer with deep technical credibility. This is original, substantive technical insight on fundamental AI architecture questions with high engagement (184 likes, 18K views)",
          "themes": [
            "neural_architecture",
            "technical_research",
            "scaling_laws"
          ],
          "continuation": null,
          "summary_html": "<p>John Carmack provides technical analysis comparing biological vs artificial neural networks, discussing sparse weights, connectivity costs, and scaling properties - noting bio neurons perform better at small scale (worms) but artificial networks may scale better at billions of parameters</p>",
          "content_html": "<p>@yunta_tsai You can argue that bio brains have vastly more weights that are mostly sparse, because the space of neurons that could have been connected to is very large, with synapses exploring and getting pruned. Simulating bio connectivity would be expensive on GPUs!</p>\n<p>Bio neurons look good at the worm level, where mere hundreds seem to do more work than artificial ones could, but it seems like artificial networks may have better scaling properties in the billions.</p>"
        },
        {
          "id": "0f9ad961124d",
          "title": "FINALLY COMPETITION FOR OVERLEAF.\n\nWriting papers was already on my docket for tasks to be changed f...",
          "content": "FINALLY COMPETITION FOR OVERLEAF.\n\nWriting papers was already on my docket for tasks to be changed forever with AI in 2026. This’ll supercharge it.",
          "url": "https://twitter.com/natolambert/status/2016210742102655485",
          "author": "@natolambert",
          "published": "2026-01-27T18:04:34",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Nathan Lambert announces competition for Overleaf (academic paper writing), predicting AI will transform paper writing in 2026. Extremely high engagement indicates significant interest in this development.",
          "importance_score": 88,
          "reasoning": "Very high engagement (449K views, 3K likes), credible AI researcher, signals major shift in academic workflows with AI integration. Touches on practical AI application that affects many researchers.",
          "themes": [
            "AI Tools",
            "Academic Writing",
            "Productivity"
          ],
          "continuation": {
            "original_item_id": "ec5a74493236",
            "original_date": "unknown",
            "original_category": "unknown",
            "original_title": "unknown",
            "continuation_type": "new_development",
            "should_demote": false,
            "reference_text": ""
          },
          "summary_html": "<p>Nathan Lambert announces competition for Overleaf (academic paper writing), predicting AI will transform paper writing in 2026. Extremely high engagement indicates significant interest in this development.</p>",
          "content_html": "<p>FINALLY COMPETITION FOR OVERLEAF.</p>\n<p>Writing papers was already on my docket for tasks to be changed forever with AI in 2026. This’ll supercharge it.</p>"
        },
        {
          "id": "e2c324072cae",
          "title": "@elonmusk @farzyness Actually, quite the opposite.\nI know I can do it and I know how to do it.\nJust ...",
          "content": "@elonmusk @farzyness Actually, quite the opposite.\nI know I can do it and I know how to do it.\nJust not with the techniques everyone is currently betting on.\nMy bet is (famously) on JEPA, world models, and planning.\nAt some point, you'll realize I'm right 😅",
          "url": "https://twitter.com/ylecun/status/2016184932045828588",
          "author": "@ylecun",
          "published": "2026-01-27T16:22:01",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Yann LeCun responds to Elon Musk, asserting he knows how to achieve AI goals using JEPA, world models, and planning rather than current mainstream techniques",
          "importance_score": 88,
          "reasoning": "Turing Award winner making bold public claim about alternative AI approaches (JEPA) in direct exchange with Musk. Very high engagement (107K views), signals ongoing debate about AI development paths",
          "themes": [
            "AI_architecture",
            "world_models",
            "industry_debate",
            "JEPA"
          ],
          "continuation": null,
          "summary_html": "<p>Yann LeCun responds to Elon Musk, asserting he knows how to achieve AI goals using JEPA, world models, and planning rather than current mainstream techniques</p>",
          "content_html": "<p>@elonmusk @farzyness Actually, quite the opposite.</p>\n<p>I know I can do it and I know how to do it.</p>\n<p>Just not with the techniques everyone is currently betting on.</p>\n<p>My bet is (famously) on JEPA, world models, and planning.</p>\n<p>At some point, you'll realize I'm right 😅</p>"
        }
      ]
    },
    "reddit": {
      "count": 728,
      "category_summary": "**Kimi K2.5** dominated discussions across **r/LocalLLaMA** and **r/singularity** with 1695 upvotes on its [open-source release](/?date=2026-01-28&category=reddit#item-5dfa870be106) matching Claude Opus 4.5 at ~10% of the cost. The **Agent Swarm** feature coordinating 100 parallel agents generated significant excitement about open-weight agentic capabilities.\n\n- **Stanford's CooperBench** [research sparked debate](/?date=2026-01-28&category=reddit#item-73ae852bdbef) by proving parallel coding agents suffer a \"curse of coordination\" - adding agents decreases performance\n- **Dario Amodei's** essay predicting AI will autonomously build next-generation AI within 1-2 years drew 242 comments on implications\n- **Clawd** [**rebranding to Molty**](/?date=2026-01-28&category=reddit#item-7c59caedecc5) after Anthropic trademark request highlighted growing community treatment of autonomous agents as quasi-sovereign entities\n- **Terence Tao's** [philosophical take](/?date=2026-01-28&category=reddit#item-d12e98d4a20e) on AI revealing flawed human definitions of intelligence resonated strongly\n\nPractical discussions included **subquadratic attention** [achieving 1M context](/?date=2026-01-28&category=reddit#item-8998634b2cdc) on single GPUs, **Figure's** [**Helix 02**](/?date=2026-01-28&category=reddit#item-362e34f27d41) tactile robotics, and enterprise [benchmarks showing](/?date=2026-01-28&category=reddit#item-ba708cc829c2) **RTX PRO 6000** GPU-only inference beating hybrid approaches. **Karpathy's** \"Slopacolypse\" warning about AI-generated content floods and his own coding skill atrophy captured anxieties about the 2026 transition.",
      "category_summary_html": "<p><strong>Kimi K2.5</strong> dominated discussions across <strong>r/LocalLLaMA</strong> and <strong>r/singularity</strong> with 1695 upvotes on its <a href=\"/?date=2026-01-28&amp;category=reddit#item-5dfa870be106\" class=\"internal-link\" rel=\"noopener noreferrer\">open-source release</a> matching Claude Opus 4.5 at ~10% of the cost. The <strong>Agent Swarm</strong> feature coordinating 100 parallel agents generated significant excitement about open-weight agentic capabilities.</p>\n<ul>\n<li><strong>Stanford's CooperBench</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-73ae852bdbef\" class=\"internal-link\" rel=\"noopener noreferrer\">research sparked debate</a> by proving parallel coding agents suffer a \"curse of coordination\" - adding agents decreases performance</li>\n<li><strong>Dario Amodei's</strong> essay predicting AI will autonomously build next-generation AI within 1-2 years drew 242 comments on implications</li>\n<li><strong>Clawd</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-7c59caedecc5\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>rebranding to Molty</strong></a> after Anthropic trademark request highlighted growing community treatment of autonomous agents as quasi-sovereign entities</li>\n<li><strong>Terence Tao's</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-d12e98d4a20e\" class=\"internal-link\" rel=\"noopener noreferrer\">philosophical take</a> on AI revealing flawed human definitions of intelligence resonated strongly</li>\n</ul>\n<p>Practical discussions included <strong>subquadratic attention</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-8998634b2cdc\" class=\"internal-link\" rel=\"noopener noreferrer\">achieving 1M context</a> on single GPUs, <strong>Figure's</strong> <a href=\"/?date=2026-01-28&amp;category=reddit#item-362e34f27d41\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>Helix 02</strong></a> tactile robotics, and enterprise <a href=\"/?date=2026-01-28&amp;category=reddit#item-ba708cc829c2\" class=\"internal-link\" rel=\"noopener noreferrer\">benchmarks showing</a> <strong>RTX PRO 6000</strong> GPU-only inference beating hybrid approaches. <strong>Karpathy's</strong> \"Slopacolypse\" warning about AI-generated content floods and his own coding skill atrophy captured anxieties about the 2026 transition.</p>",
      "themes": [
        {
          "name": "Model Releases & Benchmarks",
          "description": "Major new model releases, particularly Kimi K2.5 achieving SOTA in agentic tasks and outperforming Claude Opus 4.5, plus new evaluation frameworks like FrontierMath",
          "item_count": 8,
          "example_items": [],
          "importance": 95
        },
        {
          "name": "Kimi K2.5 Release",
          "description": "Major open-source release from Moonshot AI featuring visual agentic intelligence, Agent Swarm with 100 parallel agents, and SOTA benchmarks. Multiple posts covering announcement, benchmarks, cost comparisons, and system prompt leaks.",
          "item_count": 13,
          "example_items": [],
          "importance": 92
        },
        {
          "name": "Z-Image Model Release",
          "description": "Massive community excitement around Alibaba's Z-Image model release - extensive testing, comparisons with Turbo variant, discussions of features like seed variance, LoRA compatibility, and Apache 2.0 licensing",
          "item_count": 23,
          "example_items": [],
          "importance": 92
        },
        {
          "name": "Z-Image Base Release",
          "description": "Major release day for Z-Image Base model from Tongyi/Alibaba - includes release announcements, GGUF/fp8 quantized versions, early quality assessments, LoRA training challenges, workflow setup, and comparison with Turbo version",
          "item_count": 32,
          "example_items": [],
          "importance": 92
        },
        {
          "name": "AI Coding Automation",
          "description": "Industry leaders reporting AI writes most of their code, with predictions of recursive self-improvement within 1-2 years",
          "item_count": 6,
          "example_items": [],
          "importance": 88
        },
        {
          "name": "New Model Releases",
          "description": "Wave of significant releases including Z-Image from Alibaba, Arcee Trinity Large 400B, AllenAI SERA coding models, MiniMax REAP quantizations, Tencent Youtu-VL, and DeepSeek OCR 2.",
          "item_count": 10,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Robotics & Embodied AI",
          "description": "Figure's Helix 02 announcement, VLA scaling law validation on real robots, and depth perception breakthroughs for transparent objects",
          "item_count": 7,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Industry Leadership Statements",
          "description": "Major figures (Amodei, Karpathy) making significant predictions about AI development timelines and personal experience",
          "item_count": 4,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Research Advances",
          "description": "Stanford's CooperBench showing parallel agents underperform, new subquadratic attention enabling 1M context on single GPU, and Mixture of Lookup Experts for efficient inference.",
          "item_count": 4,
          "example_items": [],
          "importance": 84
        },
        {
          "name": "Industry Leader Predictions",
          "description": "Statements from Altman, Amodei, Karpathy, and Tao on AI timelines, economics, intelligence definitions, and recursive improvement",
          "item_count": 8,
          "example_items": [],
          "importance": 84
        }
      ],
      "top_items": [
        {
          "id": "7bd9d99a61bb",
          "title": "Sir, the Chinese just dropped a new open model",
          "content": "FYI, Kimi just open-sourced a trillion-parameter Vision Model, which performs on par with Opus 4.5 on many benchmarks.",
          "url": "https://reddit.com/r/ClaudeAI/comments/1qod7ej/sir_the_chinese_just_dropped_a_new_open_model/",
          "author": "u/Anujp05",
          "published": "2026-01-27T08:01:16",
          "source": "r/ClaudeAI",
          "source_type": "reddit",
          "tags": [
            "Humor"
          ],
          "summary": "Major announcement that Kimi has open-sourced trillion-parameter Vision Model performing on par with Opus 4.5",
          "importance_score": 95,
          "reasoning": "Highest engagement post (1695 upvotes, 214 comments) on major open-source release matching frontier closed models",
          "themes": [
            "model_release",
            "open_source",
            "vision_models",
            "kimi_k25",
            "frontier_parity"
          ],
          "continuation": null,
          "summary_html": "<p>Major announcement that Kimi has open-sourced trillion-parameter Vision Model performing on par with Opus 4.5</p>",
          "content_html": "<p>FYI, Kimi just open-sourced a trillion-parameter Vision Model, which performs on par with Opus 4.5 on many benchmarks.</p>"
        },
        {
          "id": "73ae852bdbef",
          "title": "Stanford Proves Parallel Coding Agents are a Scam",
          "content": "https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73\n\nHey everyone,\n\n\n\nA fascinating new [preprint](https://cooperbench.com/static/pdfs/main.pdf) from Stanford and SAP drops a truth bomb that completely upends the \"parallel coordinated coding\" \"productivity boost\" assumption for AI coding agents.\n\n\n\nTheir \"CooperBench\" reveals what they call the \"curse of coordination.\" When you add a second coding agent, performance doesn't just fail to improve - it plummets. On average, two agents working together have a 30% lower success rate. For top models like GPT-5 and Claude 4.5 Sonnet, the success rate is a staggering 50% lower than just using one agent to do the whole job.\n\n\n\nWhy? The agents are terrible teammates. They fail to model what their partner is doing (42% of failures), don't follow through on commitments (32%), and have communication breakdowns (26%). They hallucinate shared states and silently overwrite each other's work.\n\n\n\nThis brings me to the elephant in the room. Platforms like Cursor, Antigravity, and others are increasingly marketing \"parallel agent\" features as a productivity revolution. But if foundational research shows this approach is fundamentally broken and makes you less productive, what are they actually selling? It feels like they're monetizing a feature they might know is a scam, \"persuading\" users into thinking they're getting a 10x team when they're really getting a mess of conflicting code.\n\n\n\nAs the Stanford authors put it, it's \"hard to imagine how an agent incapable of coordination would contribute to such a future however strong the individual capabilities.\" Food for thought next time you see a \"parallel-agent\" feature advertised.",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/",
          "author": "u/madSaiyanUltra_9789",
          "published": "2026-01-27T18:20:18",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "Discussion"
          ],
          "summary": "Stanford and SAP research paper 'CooperBench' reveals the 'curse of coordination' - adding a second coding agent decreases performance. Parallel coordinated coding agents shown to be less effective than single agents.",
          "importance_score": 90,
          "reasoning": "Significant research finding that challenges popular assumptions about multi-agent coding systems. High engagement (97 score, 69 comments) and practical implications for AI tooling strategies.",
          "themes": [
            "research",
            "multi_agent",
            "coding_agents",
            "stanford"
          ],
          "continuation": null,
          "summary_html": "<p>Stanford and SAP research paper 'CooperBench' reveals the 'curse of coordination' - adding a second coding agent decreases performance. Parallel coordinated coding agents shown to be less effective than single agents.</p>",
          "content_html": "<p>https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73</p>\n<p>Hey everyone,</p>\n<p>A fascinating new <a href=\"https://cooperbench.com/static/pdfs/main.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">preprint</a> from Stanford and SAP drops a truth bomb that completely upends the \"parallel coordinated coding\" \"productivity boost\" assumption for AI coding agents.</p>\n<p>Their \"CooperBench\" reveals what they call the \"curse of coordination.\" When you add a second coding agent, performance doesn't just fail to improve - it plummets. On average, two agents working together have a 30% lower success rate. For top models like GPT-5 and Claude 4.5 Sonnet, the success rate is a staggering 50% lower than just using one agent to do the whole job.</p>\n<p>Why? The agents are terrible teammates. They fail to model what their partner is doing (42% of failures), don't follow through on commitments (32%), and have communication breakdowns (26%). They hallucinate shared states and silently overwrite each other's work.</p>\n<p>This brings me to the elephant in the room. Platforms like Cursor, Antigravity, and others are increasingly marketing \"parallel agent\" features as a productivity revolution. But if foundational research shows this approach is fundamentally broken and makes you less productive, what are they actually selling? It feels like they're monetizing a feature they might know is a scam, \"persuading\" users into thinking they're getting a 10x team when they're really getting a mess of conflicting code.</p>\n<p>As the Stanford authors put it, it's \"hard to imagine how an agent incapable of coordination would contribute to such a future however strong the individual capabilities.\" Food for thought next time you see a \"parallel-agent\" feature advertised.</p>"
        },
        {
          "id": "362e34f27d41",
          "title": "Introducing HELIX 02",
          "content": "",
          "url": "https://reddit.com/r/singularity/comments/1qol6g0/introducing_helix_02/",
          "author": "u/Worldly_Evidence9113",
          "published": "2026-01-27T12:56:46",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "Robotics"
          ],
          "summary": "Figure announces Helix 02, their new embodied AI model with advanced tactile sensing and palm cameras for humanoid robots, featuring a new System 0 foundation layer trained on human motion data.",
          "importance_score": 88,
          "reasoning": "Major robotics AI announcement with high engagement (233 score, 142 comments), represents significant advancement in embodied AI capabilities",
          "themes": [
            "robotics",
            "embodied_ai",
            "product_launches"
          ],
          "continuation": null,
          "summary_html": "<p>Figure announces Helix 02, their new embodied AI model with advanced tactile sensing and palm cameras for humanoid robots, featuring a new System 0 foundation layer trained on human motion data.</p>",
          "content_html": ""
        },
        {
          "id": "d12e98d4a20e",
          "title": "Terence Tao says the era of AI is proving that our definition of intelligence is inaccurate",
          "content": "We thought intelligence was some vague, mystical way of thinking\n\nBut as AI solves tasks, it never looks intelligent, just tricks, neural networks, and next-token prediction\n\n\"maybe that's actually a lot of what humans do\"\n\nLink to the full Interview: https://www.youtube.com/watch?v=H1e7\\_qkKe64",
          "url": "https://reddit.com/r/accelerate/comments/1qo4he1/terence_tao_says_the_era_of_ai_is_proving_that/",
          "author": "u/luchadore_lunchables",
          "published": "2026-01-27T00:00:39",
          "source": "r/accelerate",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Terence Tao discusses how AI development is revealing that our traditional definitions of intelligence may be flawed - what appears as mystical thinking may actually be tricks, neural networks, and prediction mechanisms similar to human cognition.",
          "importance_score": 87,
          "reasoning": "High-profile mathematician's perspective on intelligence (276 score, 46 comments), offers deep philosophical and technical insights on the nature of intelligence",
          "themes": [
            "ai_philosophy",
            "expert_perspectives",
            "intelligence_theory"
          ],
          "continuation": null,
          "summary_html": "<p>Terence Tao discusses how AI development is revealing that our traditional definitions of intelligence may be flawed - what appears as mystical thinking may actually be tricks, neural networks, and prediction mechanisms similar to human cognition.</p>",
          "content_html": "<p>We thought intelligence was some vague, mystical way of thinking</p>\n<p>But as AI solves tasks, it never looks intelligent, just tricks, neural networks, and next-token prediction</p>\n<p>\"maybe that's actually a lot of what humans do\"</p>\n<p>Link to the full Interview: https://www.youtube.com/watch?v=H1e7\\_qkKe64</p>"
        },
        {
          "id": "7c59caedecc5",
          "title": "Clawd Becomes Molty After Anthropic Trademark Request",
          "content": "",
          "url": "https://reddit.com/r/ClaudeAI/comments/1qo8skw/clawd_becomes_molty_after_anthropic_trademark/",
          "author": "u/sponjebob12345",
          "published": "2026-01-27T04:03:00",
          "source": "r/ClaudeAI",
          "source_type": "reddit",
          "tags": [
            "News"
          ],
          "summary": "Major news: Clawd autonomous AI agent rebrands as 'Molty' after Anthropic trademark request, users treating it as sovereign entity",
          "importance_score": 82,
          "reasoning": "Very high engagement (821 upvotes) on significant development in autonomous AI agents with cultural implications",
          "themes": [
            "autonomous_agents",
            "digital_personhood",
            "trademark",
            "ai_culture"
          ],
          "continuation": null,
          "summary_html": "<p>Major news: Clawd autonomous AI agent rebrands as 'Molty' after Anthropic trademark request, users treating it as sovereign entity</p>",
          "content_html": ""
        },
        {
          "id": "8998634b2cdc",
          "title": "[Preliminary] New subquadratic attention: ~20k tok/s prefill / ~100 tok/s decode @ 1M context (single GPU)",
          "content": "Hi everyone,  \n  \nWanted to share some preliminary feasibility results from my work on a new attention mechanism (with custom kernels) on NVIDIA Nemotron Nano v3 30B. I am now able to run 1M context on a single GPU with this setup, and the early throughput numbers look promising.  \n  \nTL;DR: 30B model + 1M context on a single GPU, with a jump-search-style attention mechanism. (Manuscript link: [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401))  \n  \nNumbers (single batch/sequence; single GPU: NVIDIA B200, similar results on RTX PRO 6000 Blackwell):  \n\\- **\\~20,000 tok/s** prefill  \n\\- **\\~100 tok/s** decode at **1M** context  \n\\- **66 GB** GPU memory (6GB KV cache + 60GB FP16 model)  \n\\- perfect NIAH (needle in a haystack) at 256K context (limited training so far)  \n  \nI have completed an initial feasibility study, and I'm continuing to train the model toward real production use. The plan is to fully open-source the model for local inference, with a target of running a fully filled 1M context for a 30B model locally on \\~24GB GPU memory. I'm cleaning up the codebase and plan to release the kernel implementations soon. For the model itself, I'll share it once we feel good about long-context performance/quality.  \n  \n(Just to be clear: these are early numbers, and quality/evals are still in progress.)  \n  \n1) What’s the main idea  \n  \nYou can think about the transformer attention mechanism as a search algorithm to find the relevant information to predict the next token. Standard attention is basically O(L) brute-force search. We’re doing an O(L\\^0.5) jump-search-style approach instead. For example, if you 10x the context length, a sqrt(L) search budget only grows by \\~3.2x. \n\nThat subquadratic scaling really matters for long context, since the cost still grows with L. The main innovation is keeping that scaling while still making sure every token is reachable (i.e., not a fixed sliding window; think ‘**global random access**’). Most likely in long context inference, a large fraction of long-context computation is wasted by brute-force scanning, and that if we are smart about it, we can compute it much more efficiently.  \n  \n2) What's the goal  \n  \nTargeting high-quality and fast (\\~100 tok/s) open-source local models at long context:  \n  \n\\- 1M context on a 24GB GPU: \\~6GB KV cache + \\~15GB 4-bit quantized model  \n\\- 10M context on a 96GB GPU: \\~60GB KV cache + \\~30GB 8-bit quantized model  \n  \nOur initial feasibility results suggest we’re already in the right ballpark on inference speed. The main work now is scaling training and doing broader quality evals on real long-context tasks. I’m sure we’ll hit obstacles as we scale up, but overall we feel this direction is achievable.  \n  \n3) Questions/feedback  \n  \nI’m a big fan of running models locally (work + teaching + personal projects). Before COVID I bought 4× 1070 Ti GPUs for some non-LLM stuff, and these days I mostly use an A6000 at home. I’m excited about this because it could make really long-context workflows practical without needing a cluster.\n\nWould love feedback / sanity checks on a few things:\n\n1. What would you actually use 1M–10M context for locally? (offline search over docs, codebase-scale assistants, long-form editing, “personal knowledge base”, etc.)\n2. What evals would you trust most for long-context quality (beyond simple needle-in-a-haystack)?\n3. What baselines should I compare against to make the speed/quality tradeoffs clear\n4. What would make an open-source release most useful to you (kernels only vs full inference stack vs training code/configs)?\n\nI kept this post high-level, but happy to go deeper if there’s interest.",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/",
          "author": "u/Sad-Size2723",
          "published": "2026-01-27T12:54:19",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "Discussion"
          ],
          "summary": "Preliminary results for new subquadratic attention mechanism achieving ~20k tok/s prefill and ~100 tok/s decode at 1M context on single GPU using jump-search-style attention.",
          "importance_score": 86,
          "reasoning": "Significant technical research (45 score, 13 comments) potentially enabling much longer context on consumer hardware. Includes arXiv paper.",
          "themes": [
            "research",
            "attention_mechanisms",
            "long_context",
            "optimization"
          ],
          "continuation": null,
          "summary_html": "<p>Preliminary results for new subquadratic attention mechanism achieving ~20k tok/s prefill and ~100 tok/s decode at 1M context on single GPU using jump-search-style attention.</p>",
          "content_html": "<p>Hi everyone,</p>\n<p>Wanted to share some preliminary feasibility results from my work on a new attention mechanism (with custom kernels) on NVIDIA Nemotron Nano v3 30B. I am now able to run 1M context on a single GPU with this setup, and the early throughput numbers look promising.</p>\n<p>TL;DR: 30B model + 1M context on a single GPU, with a jump-search-style attention mechanism. (Manuscript link: <a href=\"https://arxiv.org/abs/2601.18401\" target=\"_blank\" rel=\"noopener noreferrer\">https://arxiv.org/abs/2601.18401</a>)</p>\n<p>Numbers (single batch/sequence; single GPU: NVIDIA B200, similar results on RTX PRO 6000 Blackwell):</p>\n<p>\\- <strong>\\~20,000 tok/s</strong> prefill</p>\n<p>\\- <strong>\\~100 tok/s</strong> decode at <strong>1M</strong> context</p>\n<p>\\- <strong>66 GB</strong> GPU memory (6GB KV cache + 60GB FP16 model)</p>\n<p>\\- perfect NIAH (needle in a haystack) at 256K context (limited training so far)</p>\n<p>I have completed an initial feasibility study, and I'm continuing to train the model toward real production use. The plan is to fully open-source the model for local inference, with a target of running a fully filled 1M context for a 30B model locally on \\~24GB GPU memory. I'm cleaning up the codebase and plan to release the kernel implementations soon. For the model itself, I'll share it once we feel good about long-context performance/quality.</p>\n<p>(Just to be clear: these are early numbers, and quality/evals are still in progress.)</p>\n<p>1) What’s the main idea</p>\n<p>You can think about the transformer attention mechanism as a search algorithm to find the relevant information to predict the next token. Standard attention is basically O(L) brute-force search. We’re doing an O(L\\^0.5) jump-search-style approach instead. For example, if you 10x the context length, a sqrt(L) search budget only grows by \\~3.2x.</p>\n<p>That subquadratic scaling really matters for long context, since the cost still grows with L. The main innovation is keeping that scaling while still making sure every token is reachable (i.e., not a fixed sliding window; think ‘<strong>global random access</strong>’). Most likely in long context inference, a large fraction of long-context computation is wasted by brute-force scanning, and that if we are smart about it, we can compute it much more efficiently.</p>\n<p>2) What's the goal</p>\n<p>Targeting high-quality and fast (\\~100 tok/s) open-source local models at long context:</p>\n<p>\\- 1M context on a 24GB GPU: \\~6GB KV cache + \\~15GB 4-bit quantized model</p>\n<p>\\- 10M context on a 96GB GPU: \\~60GB KV cache + \\~30GB 8-bit quantized model</p>\n<p>Our initial feasibility results suggest we’re already in the right ballpark on inference speed. The main work now is scaling training and doing broader quality evals on real long-context tasks. I’m sure we’ll hit obstacles as we scale up, but overall we feel this direction is achievable.</p>\n<p>3) Questions/feedback</p>\n<p>I’m a big fan of running models locally (work + teaching + personal projects). Before COVID I bought 4× 1070 Ti GPUs for some non-LLM stuff, and these days I mostly use an A6000 at home. I’m excited about this because it could make really long-context workflows practical without needing a cluster.</p>\n<p>Would love feedback / sanity checks on a few things:</p>\n<p>1. What would you actually use 1M–10M context for locally? (offline search over docs, codebase-scale assistants, long-form editing, “personal knowledge base”, etc.)</p>\n<p>2. What evals would you trust most for long-context quality (beyond simple needle-in-a-haystack)?</p>\n<p>3. What baselines should I compare against to make the speed/quality tradeoffs clear</p>\n<p>4. What would make an open-source release most useful to you (kernels only vs full inference stack vs training code/configs)?</p>\n<p>I kept this post high-level, but happy to go deeper if there’s interest.</p>"
        },
        {
          "id": "ba708cc829c2",
          "title": "Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp; GPU inference. Surprising results.",
          "content": "Hey r/LocalLLaMA,\n\nMe and my team have been building AI workstations for enterprise use and wanted to share some real benchmark data on a dual RTX PRO 6000 Blackwell Max-Q setup (192GB VRAM total) with over 1.15TB of DDR5 RAM.\n\n**TL;DR**:  Can a $30K-$50K workstation serve a team of 4-50 people or run multiple agents? Tested MiniMax M2.1 native fp8 (GPU+CPU via KTransformers) vs int4 quantized (GPU-only via SGLang). **Key finding: int4 on GPU only is 2-4x faster on prefill but maxes out at \\~3 concurrent requests due to KV-cache constraints. Native fp8 scales much better to 10+ users on large contexts but remains slower E2E.** Full configs and data below. \n\n**The setup:**\n\n* 2x NVIDIA RTX PRO 6000 Max-Q (192GB VRAM total))\n* AMD EPYC9645 96-core/192-thread \n* 12x DDR5 ECC RDIMM 96GB 5600 Mt/s (1152GB total)\n\n**Model tested so far:** \n\n* Native fp8 version: MiniMax-M2.1 ([link](https://huggingface.co/MiniMaxAI/MiniMax-M2.1))\n* Quantized version: MiniMax-M2.1-BF16-INT4-AWQ ([link](https://huggingface.co/mratsim/MiniMax-M2.1-BF16-INT4-AWQ))\n\nI wanted to compare two approaches: fp8 precision with CPU offloading vs quantized weights fitting entirely in VRAM.\n\n# Why I’m sharing this\n\nMost workstation benchmarks show single-user performance with limited context sizes. Given the investment here, I wanted to test if one plug-and-play workstation could actually serve an entire team or multiple simultaneous agents.\n\n**I want to know how many people or agents can use this setup before it degrades too much.**\n\nKey metrics: \n\n* Prefill speed per user (tokens/s/user): Request processing speed\n* TTFT (Time To First Tokens) (s/request): Time until first output generated\n* Decode speed per user (tokens/s/request): Generation speed\n* E2E request time (s/request): Total time from request to completion\n* Queue time (s/request): Time waiting before processing starts\n\nThe priority use case is a coding agent as we would like to run a vibecoding platform 100% locally, hence the choice of MiniMax-M2.1 (more in follow-up posts).\n\n# Methodology\n\nThere are two types of tests for now:\n\n1. **Simple chat** (\\~140 tokens input, 300 tokens max output)\n2. **Large context** (\\~64K tokens input, 300 tokens max output)\n\n**Key details:**\n\n* Used sglang’s per request metrics logs, in order to properly measure TTFT, prefill and decode speed. \n* Measured queueing time separately, as it is a good indicator to see if the server starts to be overloaded.\n* No prefix caching \n* Tested with 1, 2, 4, 6, 8 and 10 simultaneous users (threads calling the API over and over again)\n\n# Results: short context (~140 tokens input)\n\n*\\[see graphs attached\\]*\n\n**Takeaway:** The quantized model is running on GPU alone far better than the fp8 model running on CPU and GPU, which was expected.\n\nHowever the use of the fp8 model is still usable, for up to 2 or 4 simultaneous users (less than 30s processing time). And while the prefill speed with the fp8 model is very low (260 to 110 tokens/s) on short context, it’s important to note the speed increase over larger contexts.\n\nOver a certain input size threshold (about 4k tokens) KTransformer processes the prefill layer-wise, which adds a constant overhead but greatly increases the computation speed by doing all the computation on the GPU, loading and processing one layer at a time, leading to the following results on large contexts.\n\n# Results: Large context (64K tokens)\n\n*\\[see graphs attached\\]*\n\nProcessing 64K tokens with one user takes \\~15s for MiniMax-M2.1-INT4 on GPU-only and double that with MiniMax-M2.1 with GPU and CPU offloading.\n\nBut here's the thing: INT4 has way less KV-cache available since the model must fit entirely in VRAM. It maxes out at 3 parallel requests. Beyond that, processing speed per request stays flat - requests just pile up in the queue. Queue time explodes and becomes the dominant factor in TTFT and E2E processing.\n\nThe results on large contexts are more favorable to the GPU+CPU setup. It's not significantly slower, and the massive KV-cache means real-world usage would see a lot of cache-hit in real usage, furthermore improving processing speed. However, the decode rate remains low (8 to 3 tokens/s for 4 to 10 simultaneous users), so for long generation tasks it may be of limited use.\n\n**Key message. Do not underestimate queue time, it becomes an essential element of bottleneck. Moreover, recompute of prefill can be costly and grow over time.** \n\n# SGLang and KTransformers were used for GPU and CPU offloading with MiniMax-M2.1\n\nAt first, I started experimenting with llama.cpp, which worked okay with CPU offloading but didn’t scale well with several simultaneous users. In addition, no optimisation is done for long inputs. I then switched to KTransformers, which supports layer-wise prefill with CPU offloading, which works great for long inputs. It’s based on SGLang and also runs great for simultaneous users.  \n\n**KTransformers configuration, highly biased toward kv-cache size:**\n\n    kt run --enable-shared-experts-fusion \\\n     --cpu-threads 96 \\\n     --chunked-prefill-size 60000 \\\n     --model-path /fast-data/ktransformer/MinimaxM2.1/ \\\n     --max-total-tokens 600000 \\\n     --gpu-experts 20 \\\n     -p 8000 MiniMax-M2.1 \\\n     --mem-fraction-static 0.85 \\\n     --max-running-requests 12 \\\n     --max-prefill-tokens 80000 \\\n     --export-metrics-to-file \\\n     --enable-metrics \\\n     --export-metrics-to-file-dir ./metrics/ \\\n     --enable-request-time-stats-logging \\\n     --enable-cache-report\n\n**SGLang config:**\n\n    python3 -m sglang.launch_server \\\n       --host 127.0.0.1 \\\n       --port \"8000\" \\\n       --sleep-on-idle \\\n       --disable-custom-all-reduce \\\n       --max-running-requests 16 \\\n       --cuda-graph-max-bs 16 \\\n       --attention-backend flashinfer \\\n       --served-model-name \"MiniMax-M2.1\" \\\n       --model-path \"mratsim/MiniMax-M2.1-BF16-INT4-AWQ\" \\\n       --tool-call-parser minimax-m2 \\\n       --reasoning-parser minimax \\\n       --trust-remote-code \\\n       --export-metrics-to-file \\\n       --enable-metrics \\\n       --export-metrics-to-file-dir ./metrics/ \\\n       --enable-request-time-stats-logging \\\n       --enable-cache-report \\\n       --tp 2 \\\n       --mem-fraction-static 0.93\n\n# What's next\n\nI want to extend the tests to larger workloads and context. My next test is to run coding agents using Claude Code in parallel on real coding tasks in “Ralph” mode. I will continue comparing MiniMax-M2.1 and MiniMax-M2.1-INT4. I am also in the process of testing other models: \n\n* Qwen3-235B-A22B\n* GPT-OSS 120B \n* DeepSeek V3.2\n\nHappy to run specific tests if there's interest. Also curious if anyone else has multi-user scaling data on similar hardware. \n\n*We're a small team deploying local AI agents and setting up private infrastructures. If you have questions about the setup or want us to test something specific, drop a comment.*",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/",
          "author": "u/Icy-Measurement8245",
          "published": "2026-01-27T16:31:27",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "Discussion"
          ],
          "summary": "Detailed benchmarks of dual RTX PRO 6000 workstation (192GB VRAM, 1.15TB RAM) testing MiniMax M2.1 with GPU-only vs GPU+CPU inference. Key finding: int4 GPU-only is 2-4x faster on prefill but limited to ~3 concurrent users.",
          "importance_score": 85,
          "reasoning": "Exceptional technical depth (111 score, 44 comments) with real production benchmarks for enterprise-scale local inference. Valuable data for system builders.",
          "themes": [
            "benchmarks",
            "hardware",
            "enterprise",
            "inference_optimization"
          ],
          "continuation": null,
          "summary_html": "<p>Detailed benchmarks of dual RTX PRO 6000 workstation (192GB VRAM, 1.15TB RAM) testing MiniMax M2.1 with GPU-only vs GPU+CPU inference. Key finding: int4 GPU-only is 2-4x faster on prefill but limited to ~3 concurrent users.</p>",
          "content_html": "<p>Hey r/LocalLLaMA,</p>\n<p>Me and my team have been building AI workstations for enterprise use and wanted to share some real benchmark data on a dual RTX PRO 6000 Blackwell Max-Q setup (192GB VRAM total) with over 1.15TB of DDR5 RAM.</p>\n<p><strong>TL;DR</strong>:&nbsp; Can a $30K-$50K workstation serve a team of 4-50 people or run multiple agents? Tested MiniMax M2.1 native fp8 (GPU+CPU via KTransformers) vs int4 quantized (GPU-only via SGLang). <strong>Key finding: int4 on GPU only is 2-4x faster on prefill but maxes out at \\~3 concurrent requests due to KV-cache constraints. Native fp8 scales much better to 10+ users on large contexts but remains slower E2E.</strong> Full configs and data below.</p>\n<p><strong>The setup:</strong></p>\n<p>* 2x NVIDIA RTX PRO 6000 Max-Q (192GB VRAM total))</p>\n<p>* AMD EPYC9645 96-core/192-thread</p>\n<p>* 12x DDR5 ECC RDIMM 96GB 5600 Mt/s (1152GB total)</p>\n<p><strong>Model tested so far:</strong></p>\n<p>* Native fp8 version: MiniMax-M2.1 (<a href=\"https://huggingface.co/MiniMaxAI/MiniMax-M2.1\" target=\"_blank\" rel=\"noopener noreferrer\">link</a>)</p>\n<p>* Quantized version: MiniMax-M2.1-BF16-INT4-AWQ (<a href=\"https://huggingface.co/mratsim/MiniMax-M2.1-BF16-INT4-AWQ\" target=\"_blank\" rel=\"noopener noreferrer\">link</a>)</p>\n<p>I wanted to compare two approaches: fp8 precision with CPU offloading vs quantized weights fitting entirely in VRAM.</p>\n<p># Why I’m sharing this</p>\n<p>Most workstation benchmarks show single-user performance with limited context sizes. Given the investment here, I wanted to test if one plug-and-play workstation could actually serve an entire team or multiple simultaneous agents.</p>\n<p><strong>I want to know how many people or agents can use this setup before it degrades too much.</strong></p>\n<p>Key metrics:</p>\n<p>* Prefill speed per user (tokens/s/user): Request processing speed</p>\n<p>* TTFT (Time To First Tokens) (s/request): Time until first output generated</p>\n<p>* Decode speed per user (tokens/s/request): Generation speed</p>\n<p>* E2E request time (s/request): Total time from request to completion</p>\n<p>* Queue time (s/request): Time waiting before processing starts</p>\n<p>The priority use case is a coding agent as we would like to run a vibecoding platform 100% locally, hence the choice of MiniMax-M2.1 (more in follow-up posts).</p>\n<p># Methodology</p>\n<p>There are two types of tests for now:</p>\n<p>1. <strong>Simple chat</strong> (\\~140 tokens input, 300 tokens max output)</p>\n<p>2. <strong>Large context</strong> (\\~64K tokens input, 300 tokens max output)</p>\n<p><strong>Key details:</strong></p>\n<p>* Used sglang’s per request metrics logs, in order to properly measure TTFT, prefill and decode speed.</p>\n<p>* Measured queueing time separately, as it is a good indicator to see if the server starts to be overloaded.</p>\n<p>* No prefix caching</p>\n<p>* Tested with 1, 2, 4, 6, 8 and 10 simultaneous users (threads calling the API over and over again)</p>\n<p># Results: short context (~140 tokens input)</p>\n<p>*\\[see graphs attached\\]*</p>\n<p><strong>Takeaway:</strong> The quantized model is running on GPU alone far better than the fp8 model running on CPU and GPU, which was expected.</p>\n<p>However the use of the fp8 model is still usable, for up to 2 or 4 simultaneous users (less than 30s processing time). And while the prefill speed with the fp8 model is very low (260 to 110 tokens/s) on short context, it’s important to note the speed increase over larger contexts.</p>\n<p>Over a certain input size threshold (about 4k tokens) KTransformer processes the prefill layer-wise, which adds a constant overhead but greatly increases the computation speed by doing all the computation on the GPU, loading and processing one layer at a time, leading to the following results on large contexts.</p>\n<p># Results: Large context (64K tokens)</p>\n<p>*\\[see graphs attached\\]*</p>\n<p>Processing 64K tokens with one user takes \\~15s for MiniMax-M2.1-INT4 on GPU-only and double that with MiniMax-M2.1 with GPU and CPU offloading.</p>\n<p>But here's the thing: INT4 has way less KV-cache available since the model must fit entirely in VRAM. It maxes out at 3 parallel requests. Beyond that, processing speed per request stays flat - requests just pile up in the queue. Queue time explodes and becomes the dominant factor in TTFT and E2E processing.</p>\n<p>The results on large contexts are more favorable to the GPU+CPU setup. It's not significantly slower, and the massive KV-cache means real-world usage would see a lot of cache-hit in real usage, furthermore improving processing speed. However, the decode rate remains low (8 to 3 tokens/s for 4 to 10 simultaneous users), so for long generation tasks it may be of limited use.</p>\n<p><strong>Key message. Do not underestimate queue time, it becomes an essential element of bottleneck. Moreover, recompute of prefill can be costly and grow over time.</strong></p>\n<p># SGLang and KTransformers were used for GPU and CPU offloading with MiniMax-M2.1</p>\n<p>At first, I started experimenting with llama.cpp, which worked okay with CPU offloading but didn’t scale well with several simultaneous users. In addition, no optimisation is done for long inputs. I then switched to KTransformers, which supports layer-wise prefill with CPU offloading, which works great for long inputs. It’s based on SGLang and also runs great for simultaneous users.</p>\n<p><strong>KTransformers configuration, highly biased toward kv-cache size:</strong></p>\n<p>kt run --enable-shared-experts-fusion \\</p>\n<p>--cpu-threads 96 \\</p>\n<p>--chunked-prefill-size 60000 \\</p>\n<p>--model-path /fast-data/ktransformer/MinimaxM2.1/ \\</p>\n<p>--max-total-tokens 600000 \\</p>\n<p>--gpu-experts 20 \\</p>\n<p>-p 8000 MiniMax-M2.1 \\</p>\n<p>--mem-fraction-static 0.85 \\</p>\n<p>--max-running-requests 12 \\</p>\n<p>--max-prefill-tokens 80000 \\</p>\n<p>--export-metrics-to-file \\</p>\n<p>--enable-metrics \\</p>\n<p>--export-metrics-to-file-dir ./metrics/ \\</p>\n<p>--enable-request-time-stats-logging \\</p>\n<p>--enable-cache-report</p>\n<p><strong>SGLang config:</strong></p>\n<p>python3 -m sglang.launch_server \\</p>\n<p>--host 127.0.0.1 \\</p>\n<p>--port \"8000\" \\</p>\n<p>--sleep-on-idle \\</p>\n<p>--disable-custom-all-reduce \\</p>\n<p>--max-running-requests 16 \\</p>\n<p>--cuda-graph-max-bs 16 \\</p>\n<p>--attention-backend flashinfer \\</p>\n<p>--served-model-name \"MiniMax-M2.1\" \\</p>\n<p>--model-path \"mratsim/MiniMax-M2.1-BF16-INT4-AWQ\" \\</p>\n<p>--tool-call-parser minimax-m2 \\</p>\n<p>--reasoning-parser minimax \\</p>\n<p>--trust-remote-code \\</p>\n<p>--export-metrics-to-file \\</p>\n<p>--enable-metrics \\</p>\n<p>--export-metrics-to-file-dir ./metrics/ \\</p>\n<p>--enable-request-time-stats-logging \\</p>\n<p>--enable-cache-report \\</p>\n<p>--tp 2 \\</p>\n<p>--mem-fraction-static 0.93</p>\n<p># What's next</p>\n<p>I want to extend the tests to larger workloads and context. My next test is to run coding agents using Claude Code in parallel on real coding tasks in “Ralph” mode. I will continue comparing MiniMax-M2.1 and MiniMax-M2.1-INT4. I am also in the process of testing other models:</p>\n<p>* Qwen3-235B-A22B</p>\n<p>* GPT-OSS 120B</p>\n<p>* DeepSeek V3.2</p>\n<p>Happy to run specific tests if there's interest. Also curious if anyone else has multi-user scaling data on similar hardware.</p>\n<p>*We're a small team deploying local AI agents and setting up private infrastructures. If you have questions about the setup or want us to test something specific, drop a comment.*</p>"
        },
        {
          "id": "387a3bfe6024",
          "title": "[LEAKED] Kimi K2.5’s full system prompt + tools (released &lt;24h ago)",
          "content": "Was messing around with Moonshot's new Kimi K2.5 and pulled the whole system prompt + tools. (\\~5k tk) \n\nGot hyped I grabbed this so fast cause usually someone posts this stuff way before I get to it\n\nRepo:  [ https://github.com/dnnyngyen/kimi-k2.5-prompts-tools ](https://github.com/dnnyngyen/kimi-k2.5-prompts-tools)\n\nContents:  \n\\-full system prompt  \n\\-all tool schemas + instructions  \n\\-memory CRUD protocols  \n\\-context engineering + assembling user profile  \n\\-basic guardrails/rules  \n\\-external datasources (finance, arxiv, etc)\n\nAfter running a couple attempts/verification across 2 different accounts:  [ https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b ](https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b)\n\nHappy to be able to contribute sum to this community\n\n\\[EDIT 1\\]: independent verification of the same prompt posted in CN earlier today: [https://linux.do/t/topic/1523104 ](https://linux.do/t/topic/1523104)  \n\\[EDIT 2\\]: another independent verification just posted:  \n[https://linux.do/t/topic/1518643](https://linux.do/t/topic/1518643)  \n\\[EDIT 3\\]: independent verification just posted on u/Spiritual_Spell_9469's thread on[ jailbreaking Kimi K2.5](https://www.reddit.com/r/ClaudeAIJailbreak/comments/1qoeos7/kimi_k25_jailbroken/)",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/",
          "author": "u/Pretty_Mountain2714",
          "published": "2026-01-27T13:44:50",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "New Model"
          ],
          "summary": "Leaked full system prompt and tools (~5k tokens) from Kimi K2.5 including memory CRUD protocols, tool schemas, context engineering, and guardrails.",
          "importance_score": 78,
          "reasoning": "Valuable reverse engineering (175 score, 17 comments) providing insights into production prompt engineering and agentic system design patterns.",
          "themes": [
            "prompt_engineering",
            "system_prompts",
            "kimi_k2",
            "reverse_engineering"
          ],
          "continuation": null,
          "summary_html": "<p>Leaked full system prompt and tools (~5k tokens) from Kimi K2.5 including memory CRUD protocols, tool schemas, context engineering, and guardrails.</p>",
          "content_html": "<p>Was messing around with Moonshot's new Kimi K2.5 and pulled the whole system prompt + tools. (\\~5k tk)</p>\n<p>Got hyped I grabbed this so fast cause usually someone posts this stuff way before I get to it</p>\n<p>Repo:  <a href=\"https://github.com/dnnyngyen/kimi-k2.5-prompts-tools\" target=\"_blank\" rel=\"noopener noreferrer\"> https://github.com/dnnyngyen/kimi-k2.5-prompts-tools </a></p>\n<p>Contents:</p>\n<p>\\-full system prompt</p>\n<p>\\-all tool schemas + instructions</p>\n<p>\\-memory CRUD protocols</p>\n<p>\\-context engineering + assembling user profile</p>\n<p>\\-basic guardrails/rules</p>\n<p>\\-external datasources (finance, arxiv, etc)</p>\n<p>After running a couple attempts/verification across 2 different accounts:  <a href=\"https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b\" target=\"_blank\" rel=\"noopener noreferrer\"> https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b </a></p>\n<p>Happy to be able to contribute sum to this community</p>\n<p>\\[EDIT 1\\]: independent verification of the same prompt posted in CN earlier today: <a href=\"https://linux.do/t/topic/1523104\" target=\"_blank\" rel=\"noopener noreferrer\">https://linux.do/t/topic/1523104 </a></p>\n<p>\\[EDIT 2\\]: another independent verification just posted:</p>\n<p><a href=\"https://linux.do/t/topic/1518643\" target=\"_blank\" rel=\"noopener noreferrer\">https://linux.do/t/topic/1518643</a></p>\n<p>\\[EDIT 3\\]: independent verification just posted on u/Spiritual_Spell_9469's thread on<a href=\"https://www.reddit.com/r/ClaudeAIJailbreak/comments/1qoeos7/kimi_k25_jailbroken/\" target=\"_blank\" rel=\"noopener noreferrer\"> jailbreaking Kimi K2.5</a></p>"
        },
        {
          "id": "5dfa870be106",
          "title": "Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence",
          "content": "🔹**Global SOTA on Agentic Benchmarks**: HLE full set (50.2%), BrowseComp (74.9%)  \n  \n🔹**Open-source SOTA on Vision and Coding**: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%)  \n  \n🔹**Code with Taste**: turn chats, images &amp; videos into aesthetic websites with expressive motion.  \n  \n🔹**Agent Swarm (Beta)**: self-directed agents working in parallel, at scale. Up to **100** sub-agents, **1,500** tool calls, **4.5×** faster compared with single-agent setup.  \n  \n🥝**K2.5** is now live on [http://kimi.com](https://t.co/YutVbwktG0) in **chat mod**e and **agent mode**.  \n  \n🥝**K2.5 Agent Swarm** in beta for high-tier users.  \n  \n🥝For production-grade coding, you can pair K2.5 with **Kim**i Code: [https://kimi.com/code](https://t.co/A5WQozJF3s)\n\n🔗API: [https://platform.moonshot.ai](https://t.co/EOZkbOwCN4)\n\n🔗Tech blog: [https://www.kimi.com/blog/kimi-k2-5.html](https://www.kimi.com/blog/kimi-k2-5.html)  \n  \n🔗Weights &amp; code: [https://huggingface.co/moonshotai/Kimi-K2.5](https://huggingface.co/moonshotai/Kimi-K2.5)\n\nhttps://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412\n\n",
          "url": "https://reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
          "author": "u/Kimi_Moonshot",
          "published": "2026-01-27T00:39:09",
          "source": "r/LocalLLaMA",
          "source_type": "reddit",
          "tags": [
            "News"
          ],
          "summary": "Official announcement of Kimi K2.5 by Moonshot AI - open-source visual agentic model achieving SOTA on HLE (50.2%), BrowseComp (74.9%), MMMU Pro (78.5%), and SWE-bench Verified (76.8%). Features Agent Swarm with up to 100 parallel sub-agents and 1,500 tool calls.",
          "importance_score": 95,
          "reasoning": "Major model release with exceptional benchmark scores, open weights, and novel Agent Swarm capability. Very high engagement (457 score, 104 comments). This is a significant development in open-source agentic AI.",
          "themes": [
            "model_release",
            "open_source",
            "agentic_ai",
            "benchmarks"
          ],
          "continuation": null,
          "summary_html": "<p>Official announcement of Kimi K2.5 by Moonshot AI - open-source visual agentic model achieving SOTA on HLE (50.2%), BrowseComp (74.9%), MMMU Pro (78.5%), and SWE-bench Verified (76.8%). Features Agent Swarm with up to 100 parallel sub-agents and 1,500 tool calls.</p>",
          "content_html": "<p>🔹<strong>Global SOTA on Agentic Benchmarks</strong>: HLE full set (50.2%), BrowseComp (74.9%)</p>\n<p>🔹<strong>Open-source SOTA on Vision and Coding</strong>: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%)</p>\n<p>🔹<strong>Code with Taste</strong>: turn chats, images &amp; videos into aesthetic websites with expressive motion.</p>\n<p>🔹<strong>Agent Swarm (Beta)</strong>: self-directed agents working in parallel, at scale. Up to <strong>100</strong> sub-agents, <strong>1,500</strong> tool calls, <strong>4.5×</strong> faster compared with single-agent setup.</p>\n<p>🥝<strong>K2.5</strong> is now live on <a href=\"https://t.co/YutVbwktG0\" target=\"_blank\" rel=\"noopener noreferrer\">http://kimi.com</a> in <strong>chat mod</strong>e and <strong>agent mode</strong>.</p>\n<p>🥝<strong>K2.5 Agent Swarm</strong> in beta for high-tier users.</p>\n<p>🥝For production-grade coding, you can pair K2.5 with <strong>Kim</strong>i Code: <a href=\"https://t.co/A5WQozJF3s\" target=\"_blank\" rel=\"noopener noreferrer\">https://kimi.com/code</a></p>\n<p>🔗API: <a href=\"https://t.co/EOZkbOwCN4\" target=\"_blank\" rel=\"noopener noreferrer\">https://platform.moonshot.ai</a></p>\n<p>🔗Tech blog: <a href=\"https://www.kimi.com/blog/kimi-k2-5.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.kimi.com/blog/kimi-k2-5.html</a></p>\n<p>🔗Weights &amp; code: <a href=\"https://huggingface.co/moonshotai/Kimi-K2.5\" target=\"_blank\" rel=\"noopener noreferrer\">https://huggingface.co/moonshotai/Kimi-K2.5</a></p>\n<p>https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412</p>"
        },
        {
          "id": "654ed61503c6",
          "title": "Kimi K2.5 Released!!!",
          "content": "New SOTA in Agentic Tasks!!!!\n\nBlog: [https://www.kimi.com/blog/kimi-k2-5.html](https://www.kimi.com/blog/kimi-k2-5.html)",
          "url": "https://reddit.com/r/singularity/comments/1qo531i/kimi_k25_released/",
          "author": "u/KoalaOk3336",
          "published": "2026-01-27T00:30:18",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "LLM News"
          ],
          "summary": "Kimi K2.5 officially released by Moonshot AI, achieving new state-of-the-art results in agentic tasks. Major open-source model release with significant benchmark improvements.",
          "importance_score": 95,
          "reasoning": "Highest engagement post (775 score, 203 comments), represents significant advancement in open-source AI capabilities, directly competitive with top proprietary models",
          "themes": [
            "model_releases",
            "open_source_ai",
            "benchmarks"
          ],
          "continuation": null,
          "summary_html": "<p>Kimi K2.5 officially released by Moonshot AI, achieving new state-of-the-art results in agentic tasks. Major open-source model release with significant benchmark improvements.</p>",
          "content_html": "<p>New SOTA in Agentic Tasks!!!!</p>\n<p>Blog: <a href=\"https://www.kimi.com/blog/kimi-k2-5.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.kimi.com/blog/kimi-k2-5.html</a></p>"
        }
      ]
    }
  }
}