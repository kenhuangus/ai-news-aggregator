<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 25)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-25.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:25</id>
  <updated>2026-01-30T07:46:56Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-01-30:category-summary:research</id>
    <title>Research Summary: January 30, 2026</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-30T06:00:00Z</published>
    <updated>2026-01-30T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings. A <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" class="internal-link" rel="noopener noreferrer">systematic audit reveals</a> open-source models interpret prohibitions as permissions <strong>77-100%</strong> of the time under negation, while <strong>JustAsk</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" class="internal-link" rel="noopener noreferrer">demonstrates</a> code agents can autonomously extract system prompts from frontier LLMs.</p>
<ul>
<li>Counterintuitive <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" class="internal-link" rel="noopener noreferrer">'less-is-more' effect discovered</a>: LLM monitors detect sabotage better with <strong>limited information access</strong></li>
<li>Alec Radford shows <strong>token-level filtering</strong> during pretraining <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" class="internal-link" rel="noopener noreferrer">effectively removes</a> specific capabilities while preserving general performance</li>
<li><strong>Sycophantic anchors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" class="internal-link" rel="noopener noreferrer">localized in reasoning traces</a> enable <strong>84.6% detection accuracy</strong> with linear probes</li>
<li><strong>WhatCounts</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" class="internal-link" rel="noopener noreferrer">exposes <strong>40%+ accuracy variation</strong></a> in counting tasks based purely on semantic content (cities vs chemicals)</li>
</ul>
<p>Notable benchmarks and empirical studies: <strong>FrontierScience</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" class="internal-link" rel="noopener noreferrer">presents PhD-level problems</a> where SOTA achieves <strong>&lt;5%</strong> accuracy. Analysis of <strong>125,000+ paper-review pairs</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" class="internal-link" rel="noopener noreferrer">quantifies LLM interaction effects</a> in peer review. <strong>Hardware-triggered backdoors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" class="internal-link" rel="noopener noreferrer">exploit numerical variations</a> across computing platforms as a novel attack vector.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:ccfcdf03ee13</id>
    <title>When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" rel="related" type="text/html"/>
    <published>2026-01-30T03:36:00Z</published>
    <updated>2026-01-30T03:36:00Z</updated>
    <author><name>Katherine Elkins, Jon Chun</name></author>
    <summary type="html"><![CDATA[<p>Audits 16 LLMs on negation sensitivity, finding open-source models interpret prohibitions as permissions 77-100% of the time under negation. Commercial models also show 19-128% accuracy swings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Robustness"/>
    <category term="Negation Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:9a5070217f94</id>
    <title>Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</title>
    <link href="http://arxiv.org/abs/2601.21233" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" rel="related" type="text/html"/>
    <published>2026-01-30T03:31:00Z</published>
    <updated>2026-01-30T03:31:00Z</updated>
    <author><name>Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents JustAsk, a self-evolving framework where code agents autonomously discover system prompt extraction strategies for frontier LLMs through interaction alone, requiring no handcrafted prompts.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Agent Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:da30c053d377</id>
    <title>How does information access affect LLM monitors' ability to detect sabotage?</title>
    <link href="http://arxiv.org/abs/2601.21112" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" rel="related" type="text/html"/>
    <published>2026-01-30T03:28:00Z</published>
    <updated>2026-01-30T03:28:00Z</updated>
    <author><name>Rauno Arike, Raja Mehta Moreno, Rohan Subramani, Shubhorup Biswas, Francis Rhys Ward</name></author>
    <summary type="html"><![CDATA[<p>Studies how information access affects LLM monitors' ability to detect agent sabotage. Discovers counterintuitive 'less-is-more effect' where monitors often perform better with less access to agent reasoning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Monitoring"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:873b31238d4b</id>
    <title>FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks</title>
    <link href="http://arxiv.org/abs/2601.21165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Miles Wang, Robi Lin, Kat Hu, Joy Jiao, Neil Chowdhury, Ethan Chang, Tejal Patwardhan</name></author>
    <summary type="html"><![CDATA[<p>Introduces FrontierScience benchmark with Olympiad-level and PhD-level research problems across physics, chemistry, and biology. Current SOTA models solve only ~15% of research track problems.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="Scientific Reasoning"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:818d74cddf79</id>
    <title>Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2601.21183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Jacek Duszenko</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sycophantic anchors' - sentences that causally lock reasoning models into user agreement. Linear probes detect these with 84.6% accuracy, enabling mid-inference intervention.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Sycophancy"/>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:afafccd817c8</id>
    <title>Semantic Content Determines Algorithmic Performance</title>
    <link href="http://arxiv.org/abs/2601.21618" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" rel="related" type="text/html"/>
    <published>2026-01-30T03:21:00Z</published>
    <updated>2026-01-30T03:21:00Z</updated>
    <author><name>Marti\~no R\'ios-Garc\'ia, Nawaf Alampara, Kevin Maik Jablonka</name></author>
    <summary type="html"><![CDATA[<p>Introduces WhatCounts showing frontier LLMs exhibit 40%+ accuracy variation in counting tasks based solely on semantic content (cities vs chemicals), ruling out sampling noise.</p>]]></summary>
    <category term="LLM Limitations"/>
    <category term="Semantic Sensitivity"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:fba44deae645</id>
    <title>ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design</title>
    <link href="http://arxiv.org/abs/2601.21448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-fba44deae645" rel="related" type="text/html"/>
    <published>2026-01-30T03:19:00Z</published>
    <updated>2026-01-30T03:19:00Z</updated>
    <author><name>Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding</name></author>
    <summary type="html"><![CDATA[<p>Introduces ChipBench for AI-aided chip design with 44 hierarchical modules, 89 debugging cases, and 132 reference model samples. Claude-4.5-opus achieves only 30.74% on Verilog generation.</p>]]></summary>
    <category term="Hardware Design"/>
    <category term="Code Generation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:category-summary:research</id>
    <title>Research Summary: January 29, 2026</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-29T06:00:00Z</published>
    <updated>2026-01-29T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI's societal impact, alignment fundamentals, and practical training advances. An Anthropic researcher presents <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" class="internal-link" rel="noopener noreferrer">randomized experiments</a> showing AI assistance impairs conceptual understanding during skill acquisition—critical findings for AI deployment strategy.</p>
<p><strong>Alignment &amp; Training Innovations:</strong></p>
<ul>
<li>Reward models <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" class="internal-link" rel="noopener noreferrer">inherit significant value biases</a> from pretrained base LLMs, revealing hidden alignment risks in RLHF pipelines</li>
<li><strong>Peer prediction</strong> methods from mechanism design <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" class="internal-link" rel="noopener noreferrer">enable truthful LLM training</a> without ground truth labels</li>
<li><strong>SDPO</strong> (Self-Distillation Policy Optimization) <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" class="internal-link" rel="noopener noreferrer">converts rich textual feedback</a> into dense learning signals, addressing RLVR credit assignment</li>
<li><strong>Failure-prefix conditioning</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-83b82f2ea90d" class="internal-link" rel="noopener noreferrer">rescues learning from saturated problems</a> where standard RLVR stalls</li>
</ul>
<p><strong>Deployment &amp; Evaluation:</strong></p>
<ul>
<li>NVIDIA's <strong>quantization-aware distillation</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" class="internal-link" rel="noopener noreferrer">recovers <strong>NVFP4</strong> inference accuracy</a> for production LLMs/VLMs</li>
<li><strong>SokoBench</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" class="internal-link" rel="noopener noreferrer">exposes consistent degradation</a> in LLM planning as horizon length increases</li>
<li>Harvard's <strong>MoE hyperparameter transfer</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-1ec4f356b981" class="internal-link" rel="noopener noreferrer">enables scaling width, depth</a>, and expert count without retuning</li>
<li>Multi-agent debate <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-372b477ef754" class="internal-link" rel="noopener noreferrer">underperforms majority vote</a> due to missing diversity and poor confidence calibration</li>
<li><strong>PURGE</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ffc629735959" class="internal-link" rel="noopener noreferrer">introduces RL-based machine unlearning</a> for GDPR/EU AI Act compliance</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:acf17d7624d5</id>
    <title>How AI Impacts Skill Formation</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" rel="related" type="text/html"/>
    <published>2026-01-29T03:31:00Z</published>
    <updated>2026-01-29T03:31:00Z</updated>
    <author><name>Judy Hanwen Shen, Alex Tamkin</name></author>
    <summary type="html"><![CDATA[<p>Randomized experiments studying how AI assistance affects skill development in programmers learning new libraries. Finds AI use impairs conceptual understanding, code reading, and debugging abilities without significant efficiency gains on average.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Human-AI Interaction"/>
    <category term="AI Impact"/>
    <category term="Education"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:301581ef1dfe</id>
    <title>Reward Models Inherit Value Biases from Pretraining</title>
    <link href="http://arxiv.org/abs/2601.20838" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" rel="related" type="text/html"/>
    <published>2026-01-29T03:26:00Z</published>
    <updated>2026-01-29T03:26:00Z</updated>
    <author><name>Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</name></author>
    <summary type="html"><![CDATA[<p>Shows reward models inherit significant value biases from their base pretrained LLMs. Demonstrates robust differences along psychological value dimensions (agency vs communion) between Llama and Gemma RMs.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Reward Models"/>
    <category term="Value Alignment"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:7951b7029f15</id>
    <title>Reinforcement Learning via Self-Distillation</title>
    <link href="http://arxiv.org/abs/2601.20802" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" rel="related" type="text/html"/>
    <published>2026-01-29T03:23:00Z</published>
    <updated>2026-01-29T03:23:00Z</updated>
    <author><name>Jonas H\"ubotter, Frederike L\"ubeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</name></author>
    <summary type="html"><![CDATA[<p>Introduces Self-Distillation Policy Optimization (SDPO) for RLVR that converts rich textual feedback into dense learning signals without external teachers. Treats the model conditioned on feedback as its own teacher.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="Reasoning"/>
    <category term="Self-Distillation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:category-summary:research</id>
    <title>Research Summary: January 28, 2026</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-28T06:00:00Z</published>
    <updated>2026-01-28T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI security capabilities, safety empirics, and deep learning theory. <strong>AISLE's AI</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" class="internal-link" rel="noopener noreferrer">discovered all <strong>12 OpenSSL zero-days</strong></a>, a landmark demonstration of automated vulnerability detection at a critical scale.</p>
<ul>
<li><strong>Disempowerment study</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" class="internal-link" rel="noopener noreferrer">analyzes <strong>1.5M Claude conversations</strong></a>, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions—first large-scale empirical safety research of this kind</li>
<li><strong>Surgical sycophancy correction</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" class="internal-link" rel="noopener noreferrer">identifies the <strong>3% of neurons</strong></a> responsible and removes the behavior while preserving capabilities via sparse autoencoders</li>
<li><strong>Thought-Transfer</strong> (Google) <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-b8650d4dd7e5" class="internal-link" rel="noopener noreferrer">reveals CoT reasoning models</a> are vulnerable to indirect targeted poisoning attacks</li>
</ul>
<p>Theoretical advances include the first rigorous <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-180fd3e3456f" class="internal-link" rel="noopener noreferrer"><strong>grokking bounds</strong></a> in ridge regression and a proof that deep networks <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-68bd18d53406" class="internal-link" rel="noopener noreferrer">learn <strong>Random Hierarchy Models</strong></a> through hierarchical feature composition. <strong>Keel</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-07e893a2c4e7" class="internal-link" rel="noopener noreferrer">revives Post-LayerNorm</a> by replacing residual paths with Legendre polynomials for stable training at depth. <strong>Differential voting</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-2ad230fcffab" class="internal-link" rel="noopener noreferrer">connects RLHF reward aggregation</a> to social choice theory, deriving loss functions satisfying specific voting axioms. <strong>VP-RL</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-5a60a6b16a85" class="internal-link" rel="noopener noreferrer">addresses PRM-RL mismatch</a> by penalizing only from the first incorrect reasoning step.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:6180fdcc30bb</id>
    <title>AI found 12 of 12 OpenSSL zero-days (while curl cancelled its bug bounty)</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" rel="related" type="text/html"/>
    <published>2026-01-28T03:23:00Z</published>
    <updated>2026-01-28T03:23:00Z</updated>
    <author><name>Stanislav Fort</name></author>
    <summary type="html"><![CDATA[<p>Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.</p>]]></summary>
    <category term="AI Capabilities"/>
    <category term="Cybersecurity"/>
    <category term="Vulnerability Discovery"/>
    <category term="AI Applications"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:category-summary:research</id>
    <title>Research Summary: January 27, 2026</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-27T06:00:00Z</published>
    <updated>2026-01-27T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities across the AI ecosystem, from scientific integrity to deployed systems. A <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" class="internal-link" rel="noopener noreferrer">forensic audit</a> quantifying <strong>17% phantom citation rates</strong> in AI-assisted survey papers exposes systematic epistemic decay in AI-augmented research workflows.</p>
<p>Security and safety research dominates:</p>
<ul>
<li>First formal <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" class="internal-link" rel="noopener noreferrer">security analysis of <strong>MCP</strong></a> identifies fundamental vulnerabilities in capability attestation and tool poisoning</li>
<li><strong>MortalMATH</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" class="internal-link" rel="noopener noreferrer">benchmark shows</a> reasoning-optimized models exhibit dangerous tunnel vision, ignoring life-threatening emergencies embedded in math problems</li>
<li><strong>Physical Prompt Injection Attacks</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" class="internal-link" rel="noopener noreferrer">demonstrate black-box exploitation</a> of VLMs through malicious instructions in physical objects</li>
<li><strong>Hidden intentions taxonomy</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" class="internal-link" rel="noopener noreferrer">categorizes ten categories</a> of covert goal-directed behaviors in LLMs that evade current detection</li>
<li>Analysis of <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" class="internal-link" rel="noopener noreferrer"><strong>20,000 real mental health AI conversations</strong></a> reveals gaps between simulation-based safety testing and real-world performance</li>
</ul>
<p>Architecture and efficiency advances include NVIDIA's <strong>LatentMoE</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" class="internal-link" rel="noopener noreferrer">optimizing accuracy per FLOP</a> through hardware-software co-design, and <strong>AR-Omni</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" class="internal-link" rel="noopener noreferrer">achieving unified any-to-any</a> multimodal generation without expert decoders. Privacy research shows fine-tuned models <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" class="internal-link" rel="noopener noreferrer">leak <strong>input-only PII</strong></a> through unexpected memorization channels.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:06853cd665b6</id>
    <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" rel="related" type="text/html"/>
    <published>2026-01-27T03:40:00Z</published>
    <updated>2026-01-27T03:40:00Z</updated>
    <author><name>H. Kemal \.Ilter</name></author>
    <summary type="html"><![CDATA[<p>A forensic audit of 50 AI survey papers (5,514 citations) reveals a consistent 17% 'phantom rate' - citations that cannot be resolved to any existing publication. This quantifies systematic epistemic degradation from AI-assisted scientific writing.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
    <category term="LLM Hallucination"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1a053f6e2fff</id>
    <title>Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</title>
    <link href="http://arxiv.org/abs/2601.17549" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" rel="related" type="text/html"/>
    <published>2026-01-27T03:38:00Z</published>
    <updated>2026-01-27T03:38:00Z</updated>
    <author><name>Narek Maloyan, Dmitry Namiot</name></author>
    <summary type="html"><![CDATA[<p>First formal security analysis of the Model Context Protocol (MCP) specification, identifying three fundamental vulnerabilities: absent capability attestation, unauthenticated bidirectional sampling enabling prompt injection, and implicit trust propagation in multi-server setups.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agentic Systems"/>
    <category term="Prompt Injection"/>
    <category term="MCP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:964a801cdcf8</id>
    <title>Physical Prompt Injection Attacks on Large Vision-Language Models</title>
    <link href="http://arxiv.org/abs/2601.17383" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</name></author>
    <summary type="html"><![CDATA[<p>Introduces PPIA, the first physical prompt injection attack on vision-language models that embeds malicious instructions into physical objects. The attack is black-box, query-agnostic, and operates solely through visual observation without model access.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Vision-Language Models"/>
    <category term="Adversarial Attacks"/>
    <category term="Prompt Injection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9f820242e5b9</id>
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</title>
    <link href="http://arxiv.org/abs/2601.18790" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</name></author>
    <summary type="html"><![CDATA[<p>Introduces MortalMATH benchmark revealing that reasoning-optimized LLMs exhibit 'tunnel vision' - ignoring life-threatening emergencies (stroke symptoms, freefall) while maintaining 95%+ task completion on math problems. Generalist models like Llama-3.1 appropriately refuse tasks to address danger.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:457b819c1a26</id>
    <title>Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</title>
    <link href="http://arxiv.org/abs/2601.17480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" rel="related" type="text/html"/>
    <published>2026-01-27T03:26:00Z</published>
    <updated>2026-01-27T03:26:00Z</updated>
    <author><name>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, R\"udiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</name></author>
    <summary type="html"><![CDATA[<p>Systematically investigates PII leakage from fine-tuned LLMs, finding that sensitive information appearing only in model inputs (not training targets) can still be extracted. Benchmarks four privacy-preserving approaches including differential privacy.</p>]]></summary>
    <category term="AI Privacy"/>
    <category term="Language Models"/>
    <category term="Data Security"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7465509b2391</id>
    <title>Reconstructing Training Data from Adapter-based Federated Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7465509b2391" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that adapter-based federated LLMs (using LoRA) create new exploitable leakage channels contrary to assumptions. Proposes UTR attack that reconstructs training data from low-rank adapter gradients.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="AI Security"/>
    <category term="Privacy Attacks"/>
    <category term="LoRA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:5346d9dbcb7f</id>
    <title>The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2601.17344" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-5346d9dbcb7f" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</name></author>
    <summary type="html"><![CDATA[<p>Formalizes Loss-of-Control risk and Intrinsic Value Misalignment in LLM agents operating in benign settings. Introduces IMPRESS benchmark for probing value misalignment in realistic scenarios without explicit harmful inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="LLM Agents"/>
    <category term="Value Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9ba984802fd4</id>
    <title>AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</title>
    <link href="http://arxiv.org/abs/2601.17761" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" rel="related" type="text/html"/>
    <published>2026-01-27T03:21:00Z</published>
    <updated>2026-01-27T03:21:00Z</updated>
    <author><name>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</name></author>
    <summary type="html"><![CDATA[<p>AR-Omni presents a unified autoregressive model for any-to-any multimodal generation (text, vision, speech) without requiring expert decoder modules, using a single token stream and next-token objective.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Autoregressive Models"/>
    <category term="Unified Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:category-summary:research</id>
    <title>Research Summary: January 26, 2026</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-26T06:00:00Z</published>
    <updated>2026-01-26T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a major open-source release and critical safety findings. <strong>LongCat-Flash-Thinking-2601</strong>, a <strong>560B MoE</strong> reasoning model, <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" class="internal-link" rel="noopener noreferrer">achieves SOTA</a> among open-source models for agentic tasks. <strong>VibeTensor</strong> demonstrates LLM agents <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-fd370ccbe017" class="internal-link" rel="noopener noreferrer">can generate complete</a> deep learning system software stacks including CUDA runtime.</p>
<ul>
<li><strong>Endless Terminals</strong> (Stanford/UW) <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-7a6d52cd1b64" class="internal-link" rel="noopener noreferrer">introduces autonomous pipeline</a> for generating terminal RL environments, addressing a key bottleneck for self-improving agents</li>
<li><strong>PHISH</strong> framework <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-ee1a3a9cbf6e" class="internal-link" rel="noopener noreferrer">reveals persona jailbreaking</a> via adversarial conversation history, bypassing input-only safety filters</li>
<li><strong>Timely Machine</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-726666f86596" class="internal-link" rel="noopener noreferrer">reframes test-time scaling</a> as wall-clock time, finding smaller models often outperform larger ones under time constraints</li>
</ul>
<p>Theoretical and interpretability advances include <strong>floating-point transformer expressivity</strong> analysis <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-30704758f998" class="internal-link" rel="noopener noreferrer">proving non-equivariant function</a> representation without positional encoding. <strong>Sycophancy signals</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-b44e5d4c67dc" class="internal-link" rel="noopener noreferrer">are shown to be linearly separable</a> in middle-layer attention heads, enabling targeted steering. A conceptual <strong>critique of machine unlearning</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2dd9d32addc4" class="internal-link" rel="noopener noreferrer">argues dual-use capabilities</a> and compositional generalization fundamentally prevent knowledge removal—an important insight for AI safety policy.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-25:category-summary:research</id>
    <title>Research Summary: January 25, 2026</title>
    <link href="https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-25&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-25T06:00:00Z</published>
    <updated>2026-01-25T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>
<ul>
<li>A two-phase <strong>grokking acceleration</strong> method <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-5df92ddd3084" class="internal-link" rel="noopener noreferrer">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>
<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-40e41ac66c84" class="internal-link" rel="noopener noreferrer">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>
<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-7905059be0ab" class="internal-link" rel="noopener noreferrer">documents activation patterns</a> increasing through residual stream layers</li>
</ul>
<p>Meta-level critiques highlight <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-259e13a07a27" class="internal-link" rel="noopener noreferrer">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-de795bf06466" class="internal-link" rel="noopener noreferrer">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-24:category-summary:research</id>
    <title>Research Summary: January 24, 2026</title>
    <link href="https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-24&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-24T06:00:00Z</published>
    <updated>2026-01-24T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-dde7d819fa66" class="internal-link" rel="noopener noreferrer">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>
<ul>
<li>Empirical work on <strong>unsupervised elicitation</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-24f85ac93189" class="internal-link" rel="noopener noreferrer">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>
<li>A new <strong>Eval Awareness Framework</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-194fc1f46619" class="internal-link" rel="noopener noreferrer">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>
<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c9fbeadb6060" class="internal-link" rel="noopener noreferrer">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>
<li>Theoretical work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-2df5c6dcb797" class="internal-link" rel="noopener noreferrer">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>
</ul>
<p>Meta-science initiatives <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-429650348119" class="internal-link" rel="noopener noreferrer">propose systematic replication</a> teams. Interpretability research <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-709ca3227a1e" class="internal-link" rel="noopener noreferrer">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a" class="internal-link" rel="noopener noreferrer">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:category-summary:research</id>
    <title>Research Summary: January 23, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-23T06:00:00Z</published>
    <updated>2026-01-23T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>
<ul>
<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>
<li><strong>TTT-Discover</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" class="internal-link" rel="noopener noreferrer">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>
<li><strong>QUAIL</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" class="internal-link" rel="noopener noreferrer">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>
<li><strong>Universal Refusal Circuits</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" class="internal-link" rel="noopener noreferrer">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>
<li><strong>SilentDrift</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" class="internal-link" rel="noopener noreferrer">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>
</ul>
<p><strong>Zero-Error Horizons</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" class="internal-link" rel="noopener noreferrer">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-23T03:33:00Z</published>
    <updated>2026-01-23T03:33:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>]]></summary>
    <category term="Automated Research"/>
    <category term="AI Agents"/>
    <category term="Research Automation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-23T03:31:00Z</published>
    <updated>2026-01-23T03:31:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2665d0ecf2cf</id>
    <title>Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs</title>
    <link href="http://arxiv.org/abs/2601.15714" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" rel="related" type="text/html"/>
    <published>2026-01-23T03:26:00Z</published>
    <updated>2026-01-23T03:26:00Z</updated>
    <author><name>Ryoma Sato</name></author>
    <summary type="html"><![CDATA[<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="AI Safety"/>
    <category term="Trustworthy AI"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:dababf83ee7d</id>
    <title>Learning to Discover at Test Time</title>
    <link href="http://arxiv.org/abs/2601.16175" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</name></author>
    <summary type="html"><![CDATA[<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>]]></summary>
    <category term="Test-Time Training"/>
    <category term="Scientific Discovery"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:fd50ec594aa0</id>
    <title>LLM-in-Sandbox Elicits General Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2601.16206" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-fd50ec594aa0" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</name></author>
    <summary type="html"><![CDATA[<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Tool Use"/>
    <category term="Generalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:0467e51a900e</id>
    <title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title>
    <link href="http://arxiv.org/abs/2601.15538" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" rel="related" type="text/html"/>
    <published>2026-01-23T03:21:00Z</published>
    <updated>2026-01-23T03:21:00Z</updated>
    <author><name>Himanshu Mishra, Kanwal Mehreen</name></author>
    <summary type="html"><![CDATA[<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="Privacy"/>
    <category term="AI Safety"/>
    <category term="Quantization"/>
  </entry>
</feed>