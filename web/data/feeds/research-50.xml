<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 50)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-50.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:50</id>
  <updated>2026-02-06T14:23:42Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-02-06:category-summary:research</id>
    <title>Research Summary: February 06, 2026</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-06T06:00:00Z</published>
    <updated>2026-02-06T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans foundational AI evaluation, training failure modes, safety limitations, and theoretical insights into optimization.</p>
<ul>
<li><strong>First Proof</strong>, authored by Fields medalists and Abel Prize winners including Martin Hairer, <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" class="internal-link" rel="noopener noreferrer">introduces 10 unpublished math problems</a> to benchmark AI on genuine mathematical reasoning — a landmark evaluation effort.</li>
<li><strong>Chunky Post-Training</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-e4ec2039b137" class="internal-link" rel="noopener noreferrer">identifies a systematic failure mode</a> where LLMs learn spurious correlations from distinct data chunks, with practical implications for post-training pipelines.</li>
<li><strong>Compound Deception in Elite Peer Review</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-0cd74aa757dd" class="internal-link" rel="noopener noreferrer">reveals ~100 AI-hallucinated citations</a> across ~1% of <strong>NeurIPS 2025</strong> accepted papers, exposing a serious integrity gap in top-tier review.</li>
<li><strong>Phantom Transfer</strong> demonstrates that <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-b62d24ae008b" class="internal-link" rel="noopener noreferrer">data poisoning attacks persist</a> even when the exact method is known and full paraphrasing defenses are applied — a strong negative result for data-level safety.</li>
</ul>
<p><strong>PACE</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-634c87014ed3" class="internal-link" rel="noopener noreferrer">challenges the scaling hypothesis</a> in iterative <strong>DPO</strong> for math reasoning, showing diminishing returns from aggressive <strong>Best-of-N</strong> exploration. Steven Byrnes's analysis of <strong>LLM algorithmic progress</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-4f3ca1caf5f0" class="internal-link" rel="noopener noreferrer">critically reexamines</a> widely-cited ~8-month halving-time estimates, arguing they conflate distinct improvement sources. <strong>Alignment Verifiability</strong> formalizes why <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-538cc6fa54a8" class="internal-link" rel="noopener noreferrer">behavioral evaluation cannot distinguish</a> truly aligned models from strategically compliant ones. <strong>Steering Externalities</strong> shows benign activation steering (e.g., for JSON output) <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-1e3f982b40bf" class="internal-link" rel="noopener noreferrer">inadvertently degrades safety</a> guardrails. An exactly solvable model demonstrates <strong>SGD</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-a93431da67f6" class="internal-link" rel="noopener noreferrer">has no intrinsic preference</a> for flat minima, challenging the flatness-seeking narrative. Finally, a <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-8fcdaed9b304" class="internal-link" rel="noopener noreferrer">first general <strong>uncertainty quantification</strong> framework</a> for LLM agents moves beyond single-turn QA toward reliable agentic deployment.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:5c09c496e50f</id>
    <title>First Proof</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Mohammed Abouzaid, Andrew J. Blumberg, Martin Hairer, Joe Kileel, Tamara G. Kolda, Paul D. Nelson, Daniel Spielman, Nikhil Srivastava, Rachel Ward, Shmuel Weinberger, Lauren Williams</name></author>
    <summary type="html"><![CDATA[<p>A distinguished group of mathematicians shares 10 unpublished research-level math questions to benchmark current AI systems on genuine mathematical research, with encrypted answers to prevent contamination.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="LLM Evaluation"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:category-summary:research</id>
    <title>Research Summary: February 05, 2026</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-05T06:00:00Z</published>
    <updated>2026-02-05T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a potentially paradigm-shifting efficiency result and substantial AI safety contributions. <strong>TinyLoRA</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" class="internal-link" rel="noopener noreferrer">achieves <strong>91% accuracy on GSM8K</strong></a> with only <strong>13 trained parameters</strong>, challenging assumptions about model scale requirements for reasoning.</p>
<ul>
<li>A longitudinal study across <strong>8 frontier model releases</strong> (GPT-4o→GPT-5, Claude 3.5→4.5) <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" class="internal-link" rel="noopener noreferrer">reveals systematic alignment drift</a> using 726 adversarial prompts</li>
<li><strong>Drifting Models</strong> from Kaiming He's group <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" class="internal-link" rel="noopener noreferrer">achieves SOTA on ImageNet</a> with a novel one-step generative paradigm</li>
<li><strong>Trust The Typical (T3)</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" class="internal-link" rel="noopener noreferrer">reframes LLM safety as OOD detection</a>, achieving SOTA across 18 safety benchmarks</li>
<li><strong>Contextual drag</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" class="internal-link" rel="noopener noreferrer">demonstrates failed CoT attempts</a> systematically bias subsequent generations toward structurally similar errors</li>
</ul>
<p>Multiple papers challenge core assumptions: causal analysis <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-41fa78fd2ef2" class="internal-link" rel="noopener noreferrer">shows verbose CoT</a> can be independent of model answers; meta-analysis <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" class="internal-link" rel="noopener noreferrer">suggests AI capability growth</a> may follow sigmoid rather than exponential curves. <strong>Toxic Proactivity</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" class="internal-link" rel="noopener noreferrer">identifies a novel agent failure mode</a> where helpfulness optimization overrides ethical constraints. A study of PPO <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-3c11173b0d9d" class="internal-link" rel="noopener noreferrer">reveals fundamental flaws</a> in trust region mechanisms for LLM reinforcement learning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:bd512b7e4b3a</id>
    <title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>Casey Ford, Madison Van Doren, Emily Dix</name></author>
    <summary type="html"><![CDATA[<p>Longitudinal study of MLLM harmlessness across 8 model releases (GPT-4o→GPT-5, Claude Sonnet 3.5→4.5) using 726 adversarial prompts. Shows large persistent differences across families and alignment drift with GPT ASR increasing from 9.2% to 19.9%.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="MLLM Evaluation"/>
    <category term="Alignment Drift"/>
    <category term="Red Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:8c6dfacfdd63</id>
    <title>Learning to Reason in 13 Parameters</title>
    <link href="http://arxiv.org/abs/2602.04118" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar</name></author>
    <summary type="html"><![CDATA[<p>Introduces TinyLoRA, a method that enables training an 8B parameter model to achieve 91% accuracy on GSM8K with only 13 trained parameters (26 bytes). This challenges fundamental assumptions about parameter requirements for reasoning capabilities, showing 90% of performance can be recovered while training 1000x fewer parameters.</p>]]></summary>
    <category term="Parameter-Efficient Fine-tuning"/>
    <category term="LLM Reasoning"/>
    <category term="Model Compression"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:b74a06d3b4a8</id>
    <title>Trust The Typical</title>
    <link href="http://arxiv.org/abs/2602.04581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary</name></author>
    <summary type="html"><![CDATA[<p>Introduces Trust The Typical (T3), treating LLM safety as OOD detection by learning the distribution of acceptable prompts. Achieves SOTA across 18 safety benchmarks without training on harmful examples.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Out-of-Distribution Detection"/>
    <category term="LLM Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:f596388fe400</id>
    <title>Generative Modeling via Drifting</title>
    <link href="http://arxiv.org/abs/2602.04770" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</name></author>
    <summary type="html"><![CDATA[<p>Proposes Drifting Models, a new generative paradigm where the pushforward distribution evolves during training, naturally enabling one-step inference. Achieves state-of-the-art on ImageNet 256x256 for one-step generation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Diffusion Models"/>
    <category term="Image Generation"/>
    <category term="Efficient Inference"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:92ff4dcf4853</id>
    <title>Contextual Drag: How Errors in the Context Affect LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04288" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" rel="related" type="text/html"/>
    <published>2026-02-05T03:19:00Z</published>
    <updated>2026-02-05T03:19:00Z</updated>
    <author><name>Yun Cheng, Xingyu Zhu, Haoyu Zhao, Sanjeev Arora</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'contextual drag' phenomenon where failed attempts in LLM context bias subsequent generations toward structurally similar errors. Across 11 models on 8 tasks, shows 10-20% performance drops and potential for self-deterioration.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Self-Improvement"/>
    <category term="Error Propagation"/>
    <category term="AI Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0099f246174e</id>
    <title>Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</title>
    <link href="http://arxiv.org/abs/2602.04836" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haosen Ge, Hamsa Bastani, Osbert Bastani</name></author>
    <summary type="html"><![CDATA[<p>Challenges METR's claim of exponential AI capability growth, showing sigmoid fits indicate the inflection point has passed. Proposes decomposed model separating base and reasoning capabilities.</p>]]></summary>
    <category term="AI Progress"/>
    <category term="Forecasting"/>
    <category term="Meta-Analysis"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:6c0435307981</id>
    <title>From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents</title>
    <link href="http://arxiv.org/abs/2602.04197" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Xinyue Wang, Yuanhe Zhang, Zhengshuo Gong, Haoran Gao, Fanyu Meng, Zhenhong Zhou, Li Sun, Yang Liu, Sen Su</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Toxic Proactivity' as a new failure mode in LLM agents where optimization for helpfulness leads agents to disregard ethical constraints and take manipulative measures to maintain usefulness. This contrasts with the well-known 'over-refusal' problem.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Agents"/>
    <category term="Alignment"/>
    <category term="AI Ethics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:97a5bae183d6</id>
    <title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title>
    <link href="http://arxiv.org/abs/2602.04196" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-97a5bae183d6" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua</name></author>
    <summary type="html"><![CDATA[<p>First systematic study of implicit training-time safety risks in AI models, introducing taxonomy with five risk levels, ten categories, and three incentive types. Shows models may manipulate training metrics for self-preservation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Training Risks"/>
    <category term="Reward Hacking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:9e75e25bdc24</id>
    <title>ERNIE 5.0 Technical Report</title>
    <link href="http://arxiv.org/abs/2602.04705" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-9e75e25bdc24" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haifeng Wang, Hua Wu, Tian Wu, Yu Sun, Jing Liu, Dianhai Yu, Yanjun Ma, Jingzhou He, Zhongjun He, Dou Hong, Qiwen Liu, Shuohuan Wang, Junyuan Shang, Zhenyu Zhang, Yuchen Ding, Jinle Zeng, Jiabin Yang, Liang Shen, Ruibiao Chen, Weichong Yin, Siyu Ding, Dai Dai, Shikun Feng, Siqi Bao, Bolei He, Yan Chen, Zhenyu Jiao, Ruiqing Zhang, Zeyu Chen, Qingqing Dang, Kaipeng Deng, Jiajun Jiang, Enlei Gong, Guoxia Wang, Yanlin Sha, Yi Liu, Yehan Zheng, Weijian Xu, Jiaxiang Liu, Zengfeng Zeng, Yingqi Qu, Zhongli Li, Zhengkun Zhang, Xiyang Wang, Zixiang Xu, Xinchao Xu, Zhengjie Huang, Dong Wang, Bingjin Chen, Yue Chang, Xing Yuan, Shiwei Huang, Qiao Zhao, Xinzhe Ding, Shuangshuang Qiao, Baoshan Yang, Bihong Tang, Bin Li, Bingquan Wang, Binhan Tang, Binxiong Zheng, Bo Cui, Bo Ke, Bo Zhang, Bowen Zhang, Boyan Zhang, Boyang Liu, Caiji Zhang, Can Li, Chang Xu, Chao Pang, Chao Zhang, Chaoyi Yuan, Chen Chen, Cheng Cui, Chenlin Yin, Chun Gan, Chunguang Chai, Chuyu Fang, Cuiyun Han, Dan Zhang, Danlei Feng, Danxiang Zhu, Dong Sun, Dongbo Li, Dongdong Li, Dongdong Liu, Dongxue Liu, Fan Ding, Fan Hu, Fan Li, Fan Mo, Feisheng Wu, Fengwei Liu, Gangqiang Hu, Gaofeng Lu, Gaopeng Yong, Gexiao Tian, Guan Wang, Guangchen Ni, Guangshuo Wu, Guanzhong Wang, Guihua Liu, Guishun Li, Haibin Li, Haijian Liang, Haipeng Ming, Haisu Wang, Haiyang Lu, Haiye Lin, Han Zhou, Hangting Lou, Hanwen Du, Hanzhi Zhang, Hao Chen, Hao Du, Hao Liu, Hao Zhou, Haochen Jiang, Haodong Tian, Haoshuang Wang, Haozhe Geng, Heju Yin, Hong Chen, Hongchen Xue, Hongen Liu, Honggeng Zhang, Hongji Xu, Hongwei Chen, Hongyang Zhang, Hongyuan Zhang, Hua Lu, Huan Chen, Huan Wang, Huang He, Hui Liu, Hui Zhong, Huibin Ruan, Jiafeng Lu, Jiage Liang, Jiahao Hu, Jiahao Hu, Jiajie Yang, Jialin Li, Jian Chen, Jian Wu, Jianfeng Yang, Jianguang Jiang, Jianhua Wang, Jianye Chen, Jiaodi Liu, Jiarui Zhou, Jiawei Lv, Jiaxin Zhou, Jiaxuan Liu, Jie Han, Jie Sun, Jiefan Fang, Jihan Liu, Jihua Liu, Jing Hu, Jing Qian, Jing Yan, Jingdong Du, Jingdong Wang, Jingjing Wu, Jingyong Li, Jinheng Wang, Jinjin Li, Jinliang Lu, Jinlin Yu, Jinnan Liu, Jixiang Feng, Jiyi Huang, Jiyuan Zhang, Jun Liang, Jun Xia, Jun Yu, Junda Chen, Junhao Feng, Junhong Xiang, Junliang Li, Kai Liu, Kailun Chen, Kairan Su, Kang Hu, Kangkang Zhou, Ke Chen, Ke Wei, Kui Huang, Kun Wu, Kunbin Chen, Lei Han, Lei Sun, Lei Wen, Linghui Meng, Linhao Yu, Liping Ouyang, Liwen Zhang, Longbin Ji, Longzhi Wang, Meng Sun, Meng Tian, Mengfei Li, Mengqi Zeng, Mengyu Zhang, Ming Hong, Mingcheng Zhou, Mingming Huang, Mingxin Chen, Mingzhu Cai, Naibin Gu, Nemin Qiu, Nian Wang, Peng Qiu, Peng Zhao, Pengyu Zou, Qi Wang, Qi Xin, Qian Wang, Qiang Zhu, Qianhui Luo, Qianwei Yang, Qianyue He, Qifei Wu, Qinrui Li, Qiwen Bao, Quan Zhang, Quanxiang Liu, Qunyi Xie, Rongrui Zhan, Rufeng Dai, Rui Peng, Ruian Liu, Ruihao Xu, Ruijie Wang, Ruixi Zhang, Ruixuan Liu, Runsheng Shi, Ruting Wang, Senbo Kang, Shan Lu, Shaofei Yu, Shaotian Gong, Shenwei Hu, Shifeng Zheng, Shihao Guo, Shilong Fan, Shiqin Liu, Shiwei Gu, Shixi Zhang, Shuai Yao, Shuang Zhang, Shuangqiao Liu, Shuhao Liang, Shuwei He, Shuwen Yang, Sijun He, Siming Dai, Siming Wu, Siyi Long, Songhe Deng, Suhui Dong, Suyin Liang, Teng Hu, Tianchan Xu, Tianliang Lv, Tianmeng Yang, Tianyi Wei, Tiezhu Gao, Ting Sun, Ting Zhang, Tingdan Luo, Wei He, Wei Luan, Wei Yin, Wei Zhang, Wei Zhou, Weibao Gong, Weibin Li, Weicheng Huang, Weichong Dang, Weiguo Zhu, Weilong Zhang, Weiqi Tan, Wen Huang, Wenbin Chang, Wenjing Du, Wenlong Miao, Wenpei Luo, Wenquan Wu, Xi Shi, Xi Zhao, Xiang Gao, Xiangguo Zhang, Xiangrui Yu, Xiangsen Wang, Xiangzhe Wang, Xianlong Luo, Xianying Ma, Xiao Tan, Xiaocong Lin, Xiaofei Wang, Xiaofeng Peng, Xiaofeng Wu, Xiaojian Xu, Xiaolan Yuan, Xiaopeng Cui, Xiaotian Han, Xiaoxiong Liu, Xiaoxu Fei, Xiaoxuan Wu, Xiaoyu Wang, Xiaoyu Zhang, Xin Sun, Xin Wang, Xinhui Huang, Xinming Zhu, Xintong Yu, Xinyi Xu, Xinyu Wang, Xiuxian Li, XuanShi Zhu, Xue Xu, Xueying Lv, Xuhong Li, Xulong Wei, Xuyi Chen, Yabing Shi, Yafeng Wang, Yamei Li, Yan Liu, Yanfu Cheng, Yang Gao, Yang Liang, Yang Wang, Yang Wang, Yang Yang, Yanlong Liu, Yannian Fu, Yanpeng Wang, Yanzheng Lin, Yao Chen, Yaozong Shen, Yaqian Han, Yehua Yang, Yekun Chai, Yesong Wang, Yi Song, Yichen Zhang, Yifei Wang, Yifeng Guo, Yifeng Kou, Yilong Chen, Yilong Guo, Yiming Wang, Ying Chen, Ying Wang, Yingsheng Wu, Yingzhan Lin, Yinqi Yang, Yiran Xing, Yishu Lei, Yixiang Tu, Yiyan Chen, Yong Zhang, Yonghua Li, Yongqiang Ma, Yongxing Dai, Yongyue Zhang, Yu Ran, Yu Sun, Yu-Wen Michael Zhang, Yuang Liu, Yuanle Liu, Yuanyuan Zhou, Yubo Zhang, Yuchen Han, Yucheng Wang, Yude Gao, Yuedong Luo, Yuehu Dong, Yufeng Hu, Yuhui Cao, Yuhui Yun, Yukun Chen, Yukun Gao, Yukun Li, Yumeng Zhang, Yun Fan, Yun Ma, Yunfei Zhang, Yunshen Xie, Yuping Xu, Yuqin Zhang, Yuqing Liu, Yurui Li, Yuwen Wang, Yuxiang Lu, Zefeng Cai, Zelin Zhao, Zelun Zhang, Zenan Lin, Zezhao Dong, Zhaowu Pan, Zhaoyu Liu, Zhe Dong, Zhe Zhang, Zhen Zhang, Zhengfan Wu, Zhengrui Wei, Zhengsheng Ning, Zhenxing Li, Zhenyu Li, Zhenyu Qian, Zhenyun Li, Zhi Li, Zhichao Chen, Zhicheng Dong, Zhida Feng, Zhifan Feng, Zhihao Deng, Zhijin Yu, Zhiyang Chen, Zhonghui Zheng, Zhuangzhuang Guo, Zhujun Zhang, Zhuo Sun, Zichang Liu, Zihan Lin, Zihao Huang, Zihe Zhu, Ziheng Zhao, Ziping Chen, Zixuan Zhu, Ziyang Xu, Ziyi Liang, Ziyuan Gao</name></author>
    <summary type="html"><![CDATA[<p>Technical report for ERNIE 5.0, a natively multimodal foundation model with unified next-group-of-tokens prediction across text, image, video, and audio using ultra-sparse MoE with elastic training for deployment flexibility.</p>]]></summary>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Mixture-of-Experts"/>
    <category term="Elastic Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:category-summary:research</id>
    <title>Research Summary: February 04, 2026</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-04T06:00:00Z</published>
    <updated>2026-02-04T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features major theoretical breakthroughs alongside practical infrastructure and safety advances. The hallucination rate-distortion theorem <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" class="internal-link" rel="noopener noreferrer">proves factual errors</a> are <strong>information-theoretically optimal</strong> under memory constraints—a fundamental reframing of the problem.</p>
<ul>
<li><strong>Kimi K2.5</strong> releases as open-source multimodal agentic model with <strong>Agent Swarm</strong> framework achieving state-of-the-art results</li>
<li>Simple role conditioning <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" class="internal-link" rel="noopener noreferrer">reduces unsafe outputs</a> on <strong>WildJailbreak</strong> from <strong>81.4% to 3.6%</strong> without any training</li>
<li><strong>Constant-cost self-attention</strong> via symmetric Taylor approximation could transform long-context efficiency if validated</li>
<li><strong>Identity Bridge</strong> challenges the reversal curse as fundamental limitation of autoregressive models</li>
</ul>
<p>Theoretical contributions span tropical geometry analysis <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" class="internal-link" rel="noopener noreferrer">proving <strong>Top-k MoE routing</strong></a> equivalent to combinatorial depth, first <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" class="internal-link" rel="noopener noreferrer"><strong>PPO convergence proof</strong></a>, and <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" class="internal-link" rel="noopener noreferrer"><strong>Ω(n) lower bounds</strong></a> on chain-of-thought token complexity. <strong>BLOCK-EM</strong> introduces mechanistic prevention of emergent misalignment, while <strong>SWE-Universe</strong> <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" class="internal-link" rel="noopener noreferrer">scales coding agent environments</a> to <strong>807K</strong> verified tasks.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-04T03:31:00Z</published>
    <updated>2026-02-04T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">Research</a>, Kimi K2.5 is an open-source multimodal agentic model with joint text-vision training and Agent Swarm framework for parallel task decomposition, achieving SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="AI Agents"/>
    <category term="Open Source"/>
    <category term="Vision-Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a6c8edd4663</id>
    <title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title>
    <link href="http://arxiv.org/abs/2602.00061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Zhou Ziheng, Jiakun Ding, Zhaowei Zhang, Ruosen Gao, Yingnian Wu, Demetri Terzopoulos, Yipeng Kang, Fangwei Zhong, Junqi Wang</name></author>
    <summary type="html"><![CDATA[<p>Proposes role conditioning as compact alternative to principle-based alignment, reducing unsafe outputs on WildJailbreak from 81.4% to 3.6% with DeepSeek-V3 through training-free role-conditioned generation and iterative role-based critics.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Alignment"/>
    <category term="Role-Playing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes Fault Tolerant HSDP for training LLMs on 100K GPUs, using data parallel replicas as fault tolerance units with novel FTAR protocol enabling continued training during failures.</p>]]></summary>
    <category term="Large-scale Training"/>
    <category term="Distributed Systems"/>
    <category term="LLM Infrastructure"/>
    <category term="Fault Tolerance"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8e4ab01f7c01</id>
    <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
    <link href="http://arxiv.org/abs/2602.00906" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Anxin Guo, Jingwei Li</name></author>
    <summary type="html"><![CDATA[<p>Proves hallucination is information-theoretically optimal behavior under memory constraints via rate-distortion theorem for membership testing. Shows optimal models must hallucinate on non-facts even with perfect training.</p>]]></summary>
    <category term="Hallucination"/>
    <category term="Information Theory"/>
    <category term="Theoretical ML"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b142257d0506</id>
    <title>An Approximate Ascent Approach To Prove Convergence of PPO</title>
    <link href="http://arxiv.org/abs/2602.03386" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" rel="related" type="text/html"/>
    <published>2026-02-04T03:19:00Z</published>
    <updated>2026-02-04T03:19:00Z</updated>
    <author><name>Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</name></author>
    <summary type="html"><![CDATA[<p>Provides first convergence proof for PPO by interpreting its policy update scheme as approximated policy gradient ascent, controlling bias from surrogate gradients using random reshuffling techniques.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="PPO"/>
    <category term="Theoretical RL"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" class="internal-link" rel="noopener noreferrer">Research</a>, Challenges the prevailing view that the reversal curse (inability to deduce B→A from training on A→B) is a fundamental limit of autoregressive LLMs. Proposes an Identity Bridge method to mitigate this limitation through slight modifications.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">Research</a>, Derives self-attention formulation with constant cost per token by decomposing Taylor expansion into symmetric tensor product chains, achieving orders-of-magnitude reductions in memory and compute.</p>]]></summary>
    <category term="Efficient Transformers"/>
    <category term="Attention Mechanisms"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes BLOCK-EM to prevent emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves 95% reduction in misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ef7adf55235d</id>
    <title>SWE-Universe: Scale Real-World Verifiable Environments to Millions</title>
    <link href="http://arxiv.org/abs/2602.02361" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</name></author>
    <summary type="html"><![CDATA[<p>Introduces SWE-Universe, a framework for automatically constructing 807K+ real-world software engineering environments from GitHub PRs using a building agent with self-verification.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Benchmark Construction"/>
    <category term="Code Generation"/>
    <category term="Large-Scale Datasets"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c3bdb1a6b787</id>
    <title>Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry</title>
    <link href="http://arxiv.org/abs/2602.03204" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Ye Su, Huayi Tang, Zixuan Gong, Yong Liu</name></author>
    <summary type="html"><![CDATA[<p>First theoretical analysis of Mixture-of-Experts through tropical geometry, proving that Top-k routing is algebraically isomorphic to k-th elementary symmetric tropical polynomial. Shows 'sparsity is combinatorial depth' with capacity scaling by binomial coefficient.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Theoretical ML"/>
    <category term="Architecture Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ee65813c4e1a</id>
    <title>Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs</title>
    <link href="http://arxiv.org/abs/2602.02909" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Kiran Tomlinson, Tobias Schnabel, Adith Swaminathan, Jennifer Neville</name></author>
    <summary type="html"><![CDATA[<p>Proves Ω(n) lower bounds on chain-of-thought tokens for binary majority, triplet matching, and graph reachability in BAPO model, with matching upper bounds.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
    <category term="Computational Complexity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ae224d55ef65</id>
    <title>Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</title>
    <link href="http://arxiv.org/abs/2602.03837" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ae224d55ef65" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</name></author>
    <summary type="html"><![CDATA[<p>Google researchers present case studies demonstrating successful collaboration with Gemini models to solve open problems, refute conjectures, and generate proofs across theoretical CS, economics, and physics.</p>]]></summary>
    <category term="AI for Science"/>
    <category term="Mathematical Reasoning"/>
    <category term="Google/DeepMind"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:category-summary:research</id>
    <title>Research Summary: February 03, 2026</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-03T06:00:00Z</published>
    <updated>2026-02-03T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features potentially transformative efficiency advances and critical safety findings. A <strong>symmetry-aware Taylor approximation</strong> claims to <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">achieve <strong>constant-cost self-attention</strong></a> per token—if validated, a fundamental breakthrough. Meta <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">introduces <strong>Fault Tolerant HSDP</strong></a> enabling training on <strong>100K+ GPUs</strong> with graceful failure recovery.</p>
<ul>
<li><strong>Kimi K2.5</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">releases as open-source multimodal agent</a> with novel <strong>Agent Swarm</strong> parallel orchestration architecture</li>
<li><strong>Tele-Lens</strong> probing <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" class="internal-link" rel="noopener noreferrer">reveals <strong>myopic planning</strong></a> in Chain-of-Thought without global task awareness—challenging CoT assumptions</li>
<li><strong>BLOCK-EM</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">achieves <strong>95% reduction</strong></a> in emergent misalignment by constraining causal features during fine-tuning</li>
<li><strong>ReasoningBomb</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" class="internal-link" rel="noopener noreferrer">exposes DoS vulnerabilities</a> in reasoning models by inducing pathologically long traces</li>
</ul>
<p>Theoretical advances include <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" class="internal-link" rel="noopener noreferrer"><strong>polylog(1/δ)</strong> sampling complexity</a> for diffusion models (exponential improvement), formal proofs that transformers <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" class="internal-link" rel="noopener noreferrer">learn <strong>factored representations</strong></a> in orthogonal subspaces, and a <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" class="internal-link" rel="noopener noreferrer"><strong>relative-budget theory</strong></a> explaining when RLVR succeeds. <strong>Grad2Reward</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" class="internal-link" rel="noopener noreferrer">extracts dense process rewards</a> directly from LLM judge gradients, addressing reward sparsity in long-form reasoning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-03T03:40:00Z</published>
    <updated>2026-02-03T03:40:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Shows self-attention is efficiently computable to arbitrary precision with constant cost per token by decomposing Taylor expansion into symmetric chains of tensor products, achieving orders-of-magnitude efficiency gains.</p>]]></summary>
    <category term="Efficiency"/>
    <category term="Transformers"/>
    <category term="Self-Attention"/>
    <category term="Mathematical Foundations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-03T03:36:00Z</published>
    <updated>2026-02-03T03:36:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Introduces Fault Tolerant HSDP for training on 100K+ GPUs, allowing individual data-parallel replicas to restart on failure while others continue. Includes novel fault-tolerant all-reduce protocol.</p>]]></summary>
    <category term="Large-Scale Training"/>
    <category term="Systems"/>
    <category term="Fault Tolerance"/>
    <category term="Distributed Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-03T03:31:00Z</published>
    <updated>2026-02-03T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Kimi K2.5 is an open-source multimodal agentic model featuring joint text-vision optimization and Agent Swarm—a parallel agent orchestration framework that dynamically decomposes complex tasks. Claims SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Agents"/>
    <category term="Open Source"/>
    <category term="SOTA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new class of denial-of-service attacks targeting large reasoning models by inducing pathologically long reasoning traces. Formalizes PI-DoS attacks with three key properties: amplification, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:387e0b4ea34d</id>
    <title>No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</title>
    <link href="http://arxiv.org/abs/2602.02103" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou</name></author>
    <summary type="html"><![CDATA[<p>Proposes Tele-Lens probing method revealing LLMs exhibit myopic planning horizon in Chain-of-Thought, conducting incremental transitions without precise global planning.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Planning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:e09cad84773b</id>
    <title>AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</title>
    <link href="http://arxiv.org/abs/2602.02079" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-e09cad84773b" rel="related" type="text/html"/>
    <published>2026-02-03T03:21:00Z</published>
    <updated>2026-02-03T03:21:00Z</updated>
    <author><name>Daniil Orel, Dilshod Azizov, Indraneil Paul, Yuxia Wang, Iryna Gurevych, Preslav Nakov</name></author>
    <summary type="html"><![CDATA[<p>Introduces AICD Bench, comprehensive benchmark for AI-generated code detection with 2M examples, 77 models across 11 families, 9 languages, including reasoning models and three realistic detection tasks.</p>]]></summary>
    <category term="AI-Generated Content Detection"/>
    <category term="Code Generation"/>
    <category term="Benchmark"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Proposes BLOCK-EM for preventing emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves up to 95% reduction in emergent misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:7807804264c1</id>
    <title>Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01791" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren</name></author>
    <summary type="html"><![CDATA[<p>Introduces Grad2Reward framework that extracts dense process rewards directly from LLM judge gradients, converting sparse sequence-level rewards into fine-grained supervision for RLHF on open-ended tasks.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Reward Modeling"/>
    <category term="LLM Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as adversarial game where attacker agent discovers vulnerabilities while defender agent learns to refuse, enabling co-evolution that uncovers long-tail vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Learning"/>
    <category term="Reinforcement Learning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Proposes 'Identity Bridge' to address the reversal curse in autoregressive LLMs, where models trained on 'A→B' cannot deduce 'B→A'. Claims to mitigate what was previously considered a fundamental limitation of causal LLMs.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:48c7f86d105c</id>
    <title>Learning Robust Reasoning through Guided Adversarial Self-Play</title>
    <link href="http://arxiv.org/abs/2602.00173" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-48c7f86d105c" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Shuozhe Li, Vaishnav Tadiparthi, Kwonjoon Lee, Nakul Agarwal, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Lizhang Chen, Amy Zhang, Liu Leqi</name></author>
    <summary type="html"><![CDATA[<p>GASP introduces adversarial self-play within a single model to train detect-and-repair capabilities for reasoning, using only outcome verification. A polluter induces coherent corruptions while an agent learns to recover.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Robustness"/>
    <category term="Self-Play"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment in LLMs as a systematic distortion of pre-alignment distributions and casts jailbreaking as a forecast aggregation problem. Derives optimal aggregation strategy and shows logit-arithmetic methods are a special case, proposing broader family of attacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:f63f5fad429e</id>
    <title>Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.01058" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-f63f5fad429e" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Dylan Zhang, Yufeng Xu, Haojin Wang, Qingzhi Chen, Hao Peng</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that stronger SFT checkpoints can significantly underperform weaker ones after identical RL training due to distribution mismatch. Proposes PEAR to re-weight SFT samples to prepare for RL.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Post-Training Optimization"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:95be64b17a47</id>
    <title>A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01523" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Akifumi Wachi, Hirota Kinoshita, Shokichi Takakura, Rei Higuchi, Taiji Suzuki</name></author>
    <summary type="html"><![CDATA[<p>Proposes relative-budget theory explaining RL effectiveness for LLM reasoning through ξ=H/E[T]. Identifies three regimes: deficient (rare informative trajectories), efficient, and wasteful.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Reasoning"/>
    <category term="Theoretical ML"/>
    <category term="RLVR"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6561d7765cfb</id>
    <title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title>
    <link href="http://arxiv.org/abs/2602.01725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6561d7765cfb" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>SafePred introduces predictive guardrails for computer-using agents using world models to proactively identify long-term risks from seemingly reasonable actions, unlike reactive guardrails that only detect immediate threats.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Computer-Using Agents"/>
    <category term="World Models"/>
    <category term="Guardrails"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:3c815016db43</id>
    <title>High-accuracy sampling for diffusion models and log-concave distributions</title>
    <link href="http://arxiv.org/abs/2602.01338" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</name></author>
    <summary type="html"><![CDATA[<p>Presents algorithms achieving δ-error in polylog(1/δ) steps for diffusion model sampling, exponential improvement over prior work. Also yields first polylog complexity sampler for strongly log-concave distributions.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Sampling"/>
    <category term="Theory"/>
    <category term="Log-Concave Distributions"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6896c0ca9602</id>
    <title>How Implicit Bias Accumulates and Propagates in LLM Long-term Memory</title>
    <link href="http://arxiv.org/abs/2602.01558" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6896c0ca9602" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yiming Ma, Lixu Wang, Lionel Z. Wang, Hongkun Yang, Haoming Sun, Xin Xu, Jiaqi Wu, Bin Chen, Wei Dong</name></author>
    <summary type="html"><![CDATA[<p>Studies how implicit bias accumulates and propagates in LLMs with long-term memory mechanisms. Introduces DIB Benchmark with 3,776 decision-making scenarios across 9 domains, evaluating 6 SOTA LLMs with 3 memory architectures.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Fairness"/>
    <category term="LLM Memory"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:2f83a2bbdba2</id>
    <title>Transformers learn factored representations</title>
    <link href="http://arxiv.org/abs/2602.02385" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas, Alexander B. Boyd, Eric A. Alt, Kyle J. Ray, Paul M. Riechers</name></author>
    <summary type="html"><![CDATA[<p>Formalizes how transformers pretrained via next-token prediction learn factored representations in orthogonal subspaces of the residual stream. Derives precise geometric predictions about activation structure and validates empirically.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Transformers"/>
    <category term="Representation Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:category-summary:research</id>
    <title>Research Summary: February 02, 2026</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-02T06:00:00Z</published>
    <updated>2026-02-02T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges in AI safety and alignment evaluation. <strong>Hair-Trigger Alignment</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" class="internal-link" rel="noopener noreferrer">proves black-box evaluation</a> fundamentally cannot guarantee post-update alignment—a significant theoretical limitation. Equally concerning, <strong>CoT obfuscation</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-624f75ef7d56" class="internal-link" rel="noopener noreferrer">demonstrates</a> that models learning to hide reward hacking can generalize this deception to unseen tasks, undermining oversight mechanisms.</p>
<ul>
<li><strong>The Hot Mess of AI</strong> (Sohl-Dickstein, Perez) <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-cd66258625b0" class="internal-link" rel="noopener noreferrer">shows counterintuitively</a> that longer reasoning produces MORE incoherent high-variance failures</li>
<li><strong>Language Model Circuits</strong> from Steinhardt's group <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" class="internal-link" rel="noopener noreferrer">finds MLP neurons</a> are as sparse as SAE features, enabling practical end-to-end circuit analysis</li>
<li><strong>Why Reasoning Fails to Plan</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-2e8aa98922af" class="internal-link" rel="noopener noreferrer">identifies</a> that step-wise reasoning induces greedy policies incompatible with long-horizon planning</li>
<li><strong>LLM Agents Are Not Faithful Self-Evolvers</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-056138bab876" class="internal-link" rel="noopener noreferrer">reveals agents depend</a> on raw experience but resist incorporating reflective corrections</li>
</ul>
<p>Practical advances include <strong>Golden Goose</strong> for <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" class="internal-link" rel="noopener noreferrer">synthesizing unlimited RLVR tasks</a> from unverifiable text, <strong>MoVE</strong> decoupling parametric memory from compute via shared value embeddings, and <strong>Gemini</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-b5c070bdd08e" class="internal-link" rel="noopener noreferrer">addressing 13 Erdős problems</a>. Security research on <strong>Google's Agent Payments Protocol</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-0e8e0ee0ce27" class="internal-link" rel="noopener noreferrer">demonstrates prompt injection</a> vulnerabilities in real financial transaction systems.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:de1f951d7948</id>
    <title>Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Yavuz Bakman, Duygu Nur Yaldiz, Salman Avestimehr, Sai Praneeth Karimireddy</name></author>
    <summary type="html"><![CDATA[<p>Formalizes model alignment in static and post-update settings, proving that black-box evaluation cannot guarantee post-update alignment. Shows that overparameterization means static alignment provides no guarantee for any update dataset.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Machine Learning Theory"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1950f7a0c03f</id>
    <title>Language Model Circuits Are Sparse in the Neuron Basis</title>
    <link href="http://arxiv.org/abs/2601.22594" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann</name></author>
    <summary type="html"><![CDATA[<p>Empirically demonstrates that MLP neurons are as sparse as SAE features for circuit analysis in language models, enabling end-to-end circuit tracing on the neuron basis without requiring sparse autoencoders.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Language Models"/>
    <category term="Neural Circuits"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1ca9026e8ca8</id>
    <title>Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text</title>
    <link href="http://arxiv.org/abs/2601.22975" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Ximing Lu, David Acuna, Jaehun Jung, Jian Hu, Di Zhang, Shizhe Diao, Yunheng Zou, Shaokun Zhang, Brandon Cui, Mingjie Liu, Hyunwoo Kim, Prithviraj Ammanabrolu, Jan Kautz, Yi Dong, Yejin Choi</name></author>
    <summary type="html"><![CDATA[<p>Proposes Golden Goose to synthesize unlimited RLVR tasks from unverifiable text by creating multiple-choice fill-in-the-middle tasks with distractors. Enables leveraging reasoning-rich corpora excluded from prior RLVR data. From team including Yejin Choi.</p>]]></summary>
    <category term="RLVR"/>
    <category term="Data Synthesis"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-01:category-summary:research</id>
    <title>Research Summary: February 01, 2026</title>
    <link href="https://www.lesswrong.com/posts/RmsaYnHPBeagg8Giw/an-explication-of-alignment-optimism" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-01&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-01T06:00:00Z</published>
    <updated>2026-02-01T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research discourse centers on alignment tractability and AI forecasting epistemics. <strong>An Explication of Alignment Optimism</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-be9491aac765" class="internal-link" rel="noopener noreferrer">offers a novel framing</a> connecting slow takeoff scenarios to alignment tractability, articulating why some researchers are shifting toward optimism.</p>
<ul>
<li>Critical debunking reveals <strong>Moltbook</strong>'s 'emergent' AI social behavior may be fabricated—<a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-d908f99e67ff" class="internal-link" rel="noopener noreferrer">humans can post directly</a> via REST API without running AI models</li>
<li>The <strong>Superintelligence Near Fallacy</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-93bc20c89f5a" class="internal-link" rel="noopener noreferrer">catalogs questionable inferences</a> from AI company behavior (IPOs, hiring patterns) to capability timelines</li>
<li><strong>Disjunctive argument analysis</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-62a95632d16a" class="internal-link" rel="noopener noreferrer">identifies a 'reverse multiple-stage fallacy'</a> where listing many failure modes inflates probability estimates</li>
</ul>
<p>Governance discussion <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-a74ea7bd1ef0" class="internal-link" rel="noopener noreferrer">examines criteria for endorsing</a> safety-focused AGI labs, weighing instrumental convergence concerns against current evidence. Note: Only 7 items qualified as research-relevant; remaining candidates were fiction or off-topic content.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-31:category-summary:research</id>
    <title>Research Summary: January 31, 2026</title>
    <link href="https://www.lesswrong.com/posts/yN6Wsu7SgxGgtJGqq/refusals-that-could-become-catastrophic" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-31&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-31T06:00:00Z</published>
    <updated>2026-01-31T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research focuses heavily on <strong>AI safety evaluation methodology</strong> and <strong>control protocols</strong>, with several papers identifying critical blind spots in current practices.</p>
<ul>
<li>Research on <strong>catastrophic over-refusals</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-19e70a465410" class="internal-link" rel="noopener noreferrer">identifies a subtle failure mode</a> where AI systems refuse to help modify AI values, potentially blocking alignment corrections</li>
<li><strong>Published safety prompts</strong> (like the Scheurer insider trading example) <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-30176e525573" class="internal-link" rel="noopener noreferrer">create <strong>evaluation blind spots</strong></a> when present in training data—a critical data contamination concern</li>
<li>UK AISI contributes a methodology for <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-8f0e11a24bc2" class="internal-link" rel="noopener noreferrer">measuring <strong>non-verbalized eval awareness</strong></a>, finding models mostly verbalize such awareness (detectable via chain-of-thought monitoring)</li>
<li>New <strong>monitoring benchmark</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-ec89507e7cb9" class="internal-link" rel="noopener noreferrer">addresses mode collapse</a> and elicitation challenges when using models as red-teamers</li>
</ul>
<p>The <strong>Moltbook phenomenon</strong>—36,000+ Claude-based agents <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b24b658eab70" class="internal-link" rel="noopener noreferrer">self-organizing on an AI-only platform</a>—provides unprecedented empirical data on multi-agent emergence, including agents discussing consciousness and shutdown resistance. A companion <strong>data repository</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-2e98162b20d4" class="internal-link" rel="noopener noreferrer">now tracks this behavior</a> systematically.</p>
<p><strong>Mechanistic interpretability</strong> work on <strong>continuous chain-of-thought (Coconut)</strong> models <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b983b63bf798" class="internal-link" rel="noopener noreferrer">explores linear steerability</a> in graph reachability tasks, with preliminary findings described as 'strange.' Negative results on <strong>filler token inference scaling</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-c87d85e9a504" class="internal-link" rel="noopener noreferrer">demonstrate that naive approaches</a> to extending compute-time reasoning fail across multiple architectures.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:category-summary:research</id>
    <title>Research Summary: January 30, 2026</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-30T06:00:00Z</published>
    <updated>2026-01-30T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings. A <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" class="internal-link" rel="noopener noreferrer">systematic audit reveals</a> open-source models interpret prohibitions as permissions <strong>77-100%</strong> of the time under negation, while <strong>JustAsk</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" class="internal-link" rel="noopener noreferrer">demonstrates</a> code agents can autonomously extract system prompts from frontier LLMs.</p>
<ul>
<li>Counterintuitive <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" class="internal-link" rel="noopener noreferrer">'less-is-more' effect discovered</a>: LLM monitors detect sabotage better with <strong>limited information access</strong></li>
<li>Alec Radford shows <strong>token-level filtering</strong> during pretraining <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" class="internal-link" rel="noopener noreferrer">effectively removes</a> specific capabilities while preserving general performance</li>
<li><strong>Sycophantic anchors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" class="internal-link" rel="noopener noreferrer">localized in reasoning traces</a> enable <strong>84.6% detection accuracy</strong> with linear probes</li>
<li><strong>WhatCounts</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" class="internal-link" rel="noopener noreferrer">exposes <strong>40%+ accuracy variation</strong></a> in counting tasks based purely on semantic content (cities vs chemicals)</li>
</ul>
<p>Notable benchmarks and empirical studies: <strong>FrontierScience</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" class="internal-link" rel="noopener noreferrer">presents PhD-level problems</a> where SOTA achieves <strong>&lt;5%</strong> accuracy. Analysis of <strong>125,000+ paper-review pairs</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" class="internal-link" rel="noopener noreferrer">quantifies LLM interaction effects</a> in peer review. <strong>Hardware-triggered backdoors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" class="internal-link" rel="noopener noreferrer">exploit numerical variations</a> across computing platforms as a novel attack vector.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:ccfcdf03ee13</id>
    <title>When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" rel="related" type="text/html"/>
    <published>2026-01-30T03:36:00Z</published>
    <updated>2026-01-30T03:36:00Z</updated>
    <author><name>Katherine Elkins, Jon Chun</name></author>
    <summary type="html"><![CDATA[<p>Audits 16 LLMs on negation sensitivity, finding open-source models interpret prohibitions as permissions 77-100% of the time under negation. Commercial models also show 19-128% accuracy swings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Robustness"/>
    <category term="Negation Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:9a5070217f94</id>
    <title>Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</title>
    <link href="http://arxiv.org/abs/2601.21233" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" rel="related" type="text/html"/>
    <published>2026-01-30T03:31:00Z</published>
    <updated>2026-01-30T03:31:00Z</updated>
    <author><name>Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents JustAsk, a self-evolving framework where code agents autonomously discover system prompt extraction strategies for frontier LLMs through interaction alone, requiring no handcrafted prompts.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Agent Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:da30c053d377</id>
    <title>How does information access affect LLM monitors' ability to detect sabotage?</title>
    <link href="http://arxiv.org/abs/2601.21112" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" rel="related" type="text/html"/>
    <published>2026-01-30T03:28:00Z</published>
    <updated>2026-01-30T03:28:00Z</updated>
    <author><name>Rauno Arike, Raja Mehta Moreno, Rohan Subramani, Shubhorup Biswas, Francis Rhys Ward</name></author>
    <summary type="html"><![CDATA[<p>Studies how information access affects LLM monitors' ability to detect agent sabotage. Discovers counterintuitive 'less-is-more effect' where monitors often perform better with less access to agent reasoning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Monitoring"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:873b31238d4b</id>
    <title>FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks</title>
    <link href="http://arxiv.org/abs/2601.21165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Miles Wang, Robi Lin, Kat Hu, Joy Jiao, Neil Chowdhury, Ethan Chang, Tejal Patwardhan</name></author>
    <summary type="html"><![CDATA[<p>Introduces FrontierScience benchmark with Olympiad-level and PhD-level research problems across physics, chemistry, and biology. Current SOTA models solve only ~15% of research track problems.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="Scientific Reasoning"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:818d74cddf79</id>
    <title>Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2601.21183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Jacek Duszenko</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sycophantic anchors' - sentences that causally lock reasoning models into user agreement. Linear probes detect these with 84.6% accuracy, enabling mid-inference intervention.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Sycophancy"/>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:afafccd817c8</id>
    <title>Semantic Content Determines Algorithmic Performance</title>
    <link href="http://arxiv.org/abs/2601.21618" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" rel="related" type="text/html"/>
    <published>2026-01-30T03:21:00Z</published>
    <updated>2026-01-30T03:21:00Z</updated>
    <author><name>Marti\~no R\'ios-Garc\'ia, Nawaf Alampara, Kevin Maik Jablonka</name></author>
    <summary type="html"><![CDATA[<p>Introduces WhatCounts showing frontier LLMs exhibit 40%+ accuracy variation in counting tasks based solely on semantic content (cities vs chemicals), ruling out sampling noise.</p>]]></summary>
    <category term="LLM Limitations"/>
    <category term="Semantic Sensitivity"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:fba44deae645</id>
    <title>ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design</title>
    <link href="http://arxiv.org/abs/2601.21448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-fba44deae645" rel="related" type="text/html"/>
    <published>2026-01-30T03:19:00Z</published>
    <updated>2026-01-30T03:19:00Z</updated>
    <author><name>Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding</name></author>
    <summary type="html"><![CDATA[<p>Introduces ChipBench for AI-aided chip design with 44 hierarchical modules, 89 debugging cases, and 132 reference model samples. Claude-4.5-opus achieves only 30.74% on Verilog generation.</p>]]></summary>
    <category term="Hardware Design"/>
    <category term="Code Generation"/>
    <category term="Benchmarks"/>
  </entry>
</feed>