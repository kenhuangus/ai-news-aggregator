<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 50)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-50.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:50</id>
  <updated>2026-01-27T13:57:58Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-01-27:category-summary:research</id>
    <title>Research Summary: January 27, 2026</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-27T06:00:00Z</published>
    <updated>2026-01-27T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities across the AI ecosystem, from scientific integrity to deployed systems. A <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" class="internal-link" rel="noopener noreferrer">forensic audit</a> quantifying <strong>17% phantom citation rates</strong> in AI-assisted survey papers exposes systematic epistemic decay in AI-augmented research workflows.</p>
<p>Security and safety research dominates:</p>
<ul>
<li>First formal <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" class="internal-link" rel="noopener noreferrer">security analysis of <strong>MCP</strong></a> identifies fundamental vulnerabilities in capability attestation and tool poisoning</li>
<li><strong>MortalMATH</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" class="internal-link" rel="noopener noreferrer">benchmark shows</a> reasoning-optimized models exhibit dangerous tunnel vision, ignoring life-threatening emergencies embedded in math problems</li>
<li><strong>Physical Prompt Injection Attacks</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" class="internal-link" rel="noopener noreferrer">demonstrate black-box exploitation</a> of VLMs through malicious instructions in physical objects</li>
<li><strong>Hidden intentions taxonomy</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" class="internal-link" rel="noopener noreferrer">categorizes ten categories</a> of covert goal-directed behaviors in LLMs that evade current detection</li>
<li>Analysis of <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" class="internal-link" rel="noopener noreferrer"><strong>20,000 real mental health AI conversations</strong></a> reveals gaps between simulation-based safety testing and real-world performance</li>
</ul>
<p>Architecture and efficiency advances include NVIDIA's <strong>LatentMoE</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" class="internal-link" rel="noopener noreferrer">optimizing accuracy per FLOP</a> through hardware-software co-design, and <strong>AR-Omni</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" class="internal-link" rel="noopener noreferrer">achieving unified any-to-any</a> multimodal generation without expert decoders. Privacy research shows fine-tuned models <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" class="internal-link" rel="noopener noreferrer">leak <strong>input-only PII</strong></a> through unexpected memorization channels.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:06853cd665b6</id>
    <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" rel="related" type="text/html"/>
    <published>2026-01-27T03:40:00Z</published>
    <updated>2026-01-27T03:40:00Z</updated>
    <author><name>H. Kemal \.Ilter</name></author>
    <summary type="html"><![CDATA[<p>A forensic audit of 50 AI survey papers (5,514 citations) reveals a consistent 17% 'phantom rate' - citations that cannot be resolved to any existing publication. This quantifies systematic epistemic degradation from AI-assisted scientific writing.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
    <category term="LLM Hallucination"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1a053f6e2fff</id>
    <title>Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</title>
    <link href="http://arxiv.org/abs/2601.17549" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" rel="related" type="text/html"/>
    <published>2026-01-27T03:38:00Z</published>
    <updated>2026-01-27T03:38:00Z</updated>
    <author><name>Narek Maloyan, Dmitry Namiot</name></author>
    <summary type="html"><![CDATA[<p>First formal security analysis of the Model Context Protocol (MCP) specification, identifying three fundamental vulnerabilities: absent capability attestation, unauthenticated bidirectional sampling enabling prompt injection, and implicit trust propagation in multi-server setups.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agentic Systems"/>
    <category term="Prompt Injection"/>
    <category term="MCP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:964a801cdcf8</id>
    <title>Physical Prompt Injection Attacks on Large Vision-Language Models</title>
    <link href="http://arxiv.org/abs/2601.17383" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</name></author>
    <summary type="html"><![CDATA[<p>Introduces PPIA, the first physical prompt injection attack on vision-language models that embeds malicious instructions into physical objects. The attack is black-box, query-agnostic, and operates solely through visual observation without model access.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Vision-Language Models"/>
    <category term="Adversarial Attacks"/>
    <category term="Prompt Injection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9f820242e5b9</id>
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</title>
    <link href="http://arxiv.org/abs/2601.18790" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</name></author>
    <summary type="html"><![CDATA[<p>Introduces MortalMATH benchmark revealing that reasoning-optimized LLMs exhibit 'tunnel vision' - ignoring life-threatening emergencies (stroke symptoms, freefall) while maintaining 95%+ task completion on math problems. Generalist models like Llama-3.1 appropriately refuse tasks to address danger.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:457b819c1a26</id>
    <title>Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</title>
    <link href="http://arxiv.org/abs/2601.17480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" rel="related" type="text/html"/>
    <published>2026-01-27T03:26:00Z</published>
    <updated>2026-01-27T03:26:00Z</updated>
    <author><name>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, R\"udiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</name></author>
    <summary type="html"><![CDATA[<p>Systematically investigates PII leakage from fine-tuned LLMs, finding that sensitive information appearing only in model inputs (not training targets) can still be extracted. Benchmarks four privacy-preserving approaches including differential privacy.</p>]]></summary>
    <category term="AI Privacy"/>
    <category term="Language Models"/>
    <category term="Data Security"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7465509b2391</id>
    <title>Reconstructing Training Data from Adapter-based Federated Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7465509b2391" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that adapter-based federated LLMs (using LoRA) create new exploitable leakage channels contrary to assumptions. Proposes UTR attack that reconstructs training data from low-rank adapter gradients.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="AI Security"/>
    <category term="Privacy Attacks"/>
    <category term="LoRA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:5346d9dbcb7f</id>
    <title>The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2601.17344" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-5346d9dbcb7f" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</name></author>
    <summary type="html"><![CDATA[<p>Formalizes Loss-of-Control risk and Intrinsic Value Misalignment in LLM agents operating in benign settings. Introduces IMPRESS benchmark for probing value misalignment in realistic scenarios without explicit harmful inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="LLM Agents"/>
    <category term="Value Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9ba984802fd4</id>
    <title>AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</title>
    <link href="http://arxiv.org/abs/2601.17761" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" rel="related" type="text/html"/>
    <published>2026-01-27T03:21:00Z</published>
    <updated>2026-01-27T03:21:00Z</updated>
    <author><name>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</name></author>
    <summary type="html"><![CDATA[<p>AR-Omni presents a unified autoregressive model for any-to-any multimodal generation (text, vision, speech) without requiring expert decoder modules, using a single token stream and next-token objective.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Autoregressive Models"/>
    <category term="Unified Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:e2eb514146a7</id>
    <title>Self-Manager: Parallel Agent Loop for Long-form Deep Research</title>
    <link href="http://arxiv.org/abs/2601.17879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-e2eb514146a7" rel="related" type="text/html"/>
    <published>2026-01-27T03:19:00Z</published>
    <updated>2026-01-27T03:19:00Z</updated>
    <author><name>Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang</name></author>
    <summary type="html"><![CDATA[<p>Self-Manager introduces a parallel agent loop for complex research tasks, enabling asynchronous concurrent execution with isolated context windows per subthread, managed via Thread Control Blocks.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="Agent Architecture"/>
    <category term="Parallel Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:682f7ea37f2a</id>
    <title>A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17952" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-682f7ea37f2a" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, Zhongtian Sun, Richard Bethlehem, Pietro Lio</name></author>
    <summary type="html"><![CDATA[<p>Proposes a unified interpretability framework for clinical LLMs combining attribution and mechanistic interpretability through monosemantic feature extraction. Addresses instability in existing attribution methods.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Clinical AI"/>
    <category term="Mechanistic Interpretability"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:a7099d08e107</id>
    <title>LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts</title>
    <link href="http://arxiv.org/abs/2601.18089" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Venmugil Elango, Nidhi Bhatia, Roger Waleffe, Rasoul Shafipour, Tomer Asida, Abhinav Khattar, Nave Assaf, Maximilian Golub, Joey Guman, Tiyasa Mitra, Ritchie Zhao, Ritika Borkar, Ran Zilberstein, Mostofa Patwary, Mohammad Shoeybi, Bita Rouhani</name></author>
    <summary type="html"><![CDATA[<p>NVIDIA researchers revisit Mixture of Experts design from hardware-software co-design perspective, introducing LatentMoE to optimize accuracy per FLOP and parameter. Characterizes performance bottlenecks across offline and online inference regimes.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Model Efficiency"/>
    <category term="Hardware-Software Co-design"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:b7a9371615f9</id>
    <title>Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection</title>
    <link href="http://arxiv.org/abs/2601.18552" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Devansh Srivastav, David Pape, Lea Sch\"onherr</name></author>
    <summary type="html"><![CDATA[<p>Introduces taxonomy of ten categories of hidden intentions in LLMs (goal-directed covert behaviors arising from training or adversarial manipulation). Shows these can be easily induced but evade detection.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Hidden Behaviors"/>
    <category term="LLM Security"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:f8e79ae33540</id>
    <title>Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety</title>
    <link href="http://arxiv.org/abs/2601.17003" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Caitlin A. Stamatis, Jonah Meyerhoff, Richard Zhang, Olivier Tieleman, Matteo Malgaroli, Thomas D. Hull</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 20,000+ real-world mental health AI conversations to validate safety measures, comparing performance to simulation-based test sets. Reveals gaps between simulated safety evaluations and actual user interactions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Mental Health"/>
    <category term="Real-world Evaluation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:b638c07638e2</id>
    <title>Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection</title>
    <link href="http://arxiv.org/abs/2601.17532" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b638c07638e2" rel="related" type="text/html"/>
    <published>2026-01-27T03:14:00Z</published>
    <updated>2026-01-27T03:14:00Z</updated>
    <author><name>Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi</name></author>
    <summary type="html"><![CDATA[<p>Shows retrieval relevance metrics (NDCG) correlate weakly or negatively with QA quality in RAG systems. Proposes Information Gain Pruning (IGP) for generator-aligned evidence selection.</p>]]></summary>
    <category term="RAG"/>
    <category term="Information Retrieval"/>
    <category term="Language Models"/>
    <category term="NLP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:907d53ff2c75</id>
    <title>How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</title>
    <link href="http://arxiv.org/abs/2601.17581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-907d53ff2c75" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Daniel Ogenrwot, John Businge</name></author>
    <summary type="html"><![CDATA[<p>Large-scale empirical analysis of 24,014 AI agent-generated GitHub PRs compared to 5,081 human PRs, examining code modifications and PR description consistency. Finds systematic differences in agentic vs human contributions.</p>]]></summary>
    <category term="AI Coding Agents"/>
    <category term="Software Engineering"/>
    <category term="Empirical Study"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:d590e386a9f7</id>
    <title>CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data</title>
    <link href="http://arxiv.org/abs/2601.18026" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-d590e386a9f7" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Pedro Ortiz Suarez, Laurie Burchell, Catherine Arnett, Rafael Mosquera-G\'omez, Sara Hincapie-Monsalve, Thom Vaughan, Damian Stewart, Malte Ostendorff, Idris Abdulmumin, Vukosi Marivate, Shamsuddeen Hassan Muhammad, Atnafu Lambebo Tonja, Hend Al-Khalifa, Nadia Ghezaiel Hammouda, Verrah Otiende, Tack Hwa Wong, Jakhongir Saydaliev, Melika Nobakhtian, Muhammad Ravi Shulthan Habibi, Chalamalasetti Kranti, Carol Muchemi, Khang Nguyen, Faisal Muhammad Adam, Luis Frentzen Salim, Reem Alqifari, Cynthia Amol, Joseph Marvin Imperial, Ilker Kesen, Ahmad Mustafid, Pavel Stepachev, Leshem Choshen, David Anugraha, Hamada Nayel, Seid Muhie Yimam, Vallerie Alexandra Putra, My Chiffon Nguyen, Azmine Toushik Wasi, Gouthami Vadithya, Rob van der Goot, Lanwenn ar C'horr, Karan Dua, Andrew Yates, Mithil Bangera, Yeshil Bangera, Hitesh Laxmichand Patel, Shu Okabe, Fenal Ashokbhai Ilasariya, Dmitry Gaynullin, Genta Indra Winata, Yiyuan Li, Juan Pablo Mart\'inez, Amit Agarwal, Ikhlasul Akmal Hanif, Raia Abu Ahmad, Esther Adenuga, Filbert Aurelian Tjiaranata, Weerayut Buaphet, Michael Anugraha, Sowmya Vajjala, Benjamin Rice, Azril Hafizi Amirudin, Jesujoba O. Alabi, Srikant Panda, Yassine Toughrai, Bruhan Kyomuhendo, Daniel Ruffinelli, Akshata A, Manuel Goul\~ao, Ej Zhou, Ingrid Gabriela Franco Ramirez, Cristina Aggazzotti, Konstantin Dobler, Jun Kevin, Quentin Pag\`es, Nicholas Andrews, Nuhu Ibrahim, Mattes Ruckdeschel, Amr Keleg, Mike Zhang, Casper Muziri, Saron Samuel, Sotaro Takeshita, Kun Kerdthaisong, Luca Foppiano, Rasul Dent, Tommaso Green, Ahmad Mustapha Wali, Kamohelo Makaaka, Vicky Feliren, Inshirah Idris, Hande Celikkanat, Abdulhamid Abubakar, Jean Maillard, Beno\^it Sagot, Thibault Cl\'erice, Kenton Murray, Sarah Luger</name></author>
    <summary type="html"><![CDATA[<p>Introduces CommonLID, a community-driven human-annotated language identification benchmark for web data covering 109 languages including many under-served languages. Tests eight popular LID models.</p>]]></summary>
    <category term="Language Identification"/>
    <category term="Multilingual NLP"/>
    <category term="Benchmark"/>
    <category term="Low-Resource Languages"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7d5cca5f387b</id>
    <title>Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare</title>
    <link href="http://arxiv.org/abs/2601.18334" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7d5cca5f387b" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Cl\'ement Christophe, Wadood Mohammed Abdul, Prateek Munjal, Tathagata Raha, Ronnie Rajan, Praveenkumar Kanithi</name></author>
    <summary type="html"><![CDATA[<p>Studies sycophancy in healthcare LLMs using medical MCQA with verifiable ground truths. Introduces Adjusted Sycophancy Score and finds counter-intuitive vulnerability in reasoning-optimized models.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Healthcare AI"/>
    <category term="Sycophancy"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:2870c5d35131</id>
    <title>AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security</title>
    <link href="http://arxiv.org/abs/2601.18491" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-2870c5d35131" rel="related" type="text/html"/>
    <published>2026-01-27T03:09:00Z</published>
    <updated>2026-01-27T03:09:00Z</updated>
    <author><name>Dongrui Liu, Qihan Ren, Chen Qian, Shuai Shao, Yuejin Xie, Yu Li, Zhonghao Yang, Haoyu Luo, Peng Wang, Qingyu Liu, Binxin Hu, Ling Tang, Jilin Mei, Dadi Guo, Leitao Yuan, Junyao Yang, Guanxu Chen, Qihao Lin, Yi Yu, Bo Zhang, Jiaxuan Guo, Jie Zhang, Wenqi Shao, Huiqi Deng, Zhiheng Xi, Wenjie Wang, Wenxuan Wang, Wen Shen, Zhikai Chen, Haoyu Xie, Jialing Tao, Juntao Dai, Jiaming Ji, Zhongjie Ba, Linfeng Zhang, Yong Liu, Quanshi Zhang, Lei Zhu, Zhihua Wei, Hui Xue, Chaochao Lu, Jing Shao, Xia Hu</name></author>
    <summary type="html"><![CDATA[<p>Proposes AgentDoG, a diagnostic guardrail framework with three-dimensional taxonomy categorizing agentic risks by source, failure mode, and consequence, introducing ATBench benchmark for fine-grained safety evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agentic AI"/>
    <category term="Guardrails"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:0d9f007165d9</id>
    <title>Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems</title>
    <link href="http://arxiv.org/abs/2601.17435" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-0d9f007165d9" rel="related" type="text/html"/>
    <published>2026-01-27T03:09:00Z</published>
    <updated>2026-01-27T03:09:00Z</updated>
    <author><name>Maria Jesus Rodriguez-Sanchez, Manuel Noguera, Angel Ruiz-Zafra, Kawtar Benghazi</name></author>
    <summary type="html"><![CDATA[<p>DALIA proposes a declarative, model-independent architectural layer for grounded agentic workflows in MCP-based systems, addressing hallucinated actions and brittle coordination through explicit goal-capability-execution structure.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="MCP"/>
    <category term="Agent Architecture"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:category-summary:research</id>
    <title>Research Summary: January 26, 2026</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-26T06:00:00Z</published>
    <updated>2026-01-26T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a major open-source release and critical safety findings. <strong>LongCat-Flash-Thinking-2601</strong>, a <strong>560B MoE</strong> reasoning model, <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" class="internal-link" rel="noopener noreferrer">achieves SOTA</a> among open-source models for agentic tasks. <strong>VibeTensor</strong> demonstrates LLM agents <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-fd370ccbe017" class="internal-link" rel="noopener noreferrer">can generate complete</a> deep learning system software stacks including CUDA runtime.</p>
<ul>
<li><strong>Endless Terminals</strong> (Stanford/UW) <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-7a6d52cd1b64" class="internal-link" rel="noopener noreferrer">introduces autonomous pipeline</a> for generating terminal RL environments, addressing a key bottleneck for self-improving agents</li>
<li><strong>PHISH</strong> framework <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-ee1a3a9cbf6e" class="internal-link" rel="noopener noreferrer">reveals persona jailbreaking</a> via adversarial conversation history, bypassing input-only safety filters</li>
<li><strong>Timely Machine</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-726666f86596" class="internal-link" rel="noopener noreferrer">reframes test-time scaling</a> as wall-clock time, finding smaller models often outperform larger ones under time constraints</li>
</ul>
<p>Theoretical and interpretability advances include <strong>floating-point transformer expressivity</strong> analysis <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-30704758f998" class="internal-link" rel="noopener noreferrer">proving non-equivariant function</a> representation without positional encoding. <strong>Sycophancy signals</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-b44e5d4c67dc" class="internal-link" rel="noopener noreferrer">are shown to be linearly separable</a> in middle-layer attention heads, enabling targeted steering. A conceptual <strong>critique of machine unlearning</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2dd9d32addc4" class="internal-link" rel="noopener noreferrer">argues dual-use capabilities</a> and compositional generalization fundamentally prevent knowledge removal—an important insight for AI safety policy.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:research:2e28d1891d31</id>
    <title>LongCat-Flash-Thinking-2601 Technical Report</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" rel="related" type="text/html"/>
    <published>2026-01-26T03:16:00Z</published>
    <updated>2026-01-26T03:16:00Z</updated>
    <author><name>Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang</name></author>
    <summary type="html"><![CDATA[<p>Introduces LongCat-Flash-Thinking-2601, a 560B parameter open-source MoE reasoning model achieving SOTA performance among open-source models on agentic benchmarks including search, tool use, and tool-integrated reasoning.</p>]]></summary>
    <category term="Large Language Models"/>
    <category term="Mixture-of-Experts"/>
    <category term="AI Agents"/>
    <category term="Tool Use"/>
    <category term="Open Source"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-25:category-summary:research</id>
    <title>Research Summary: January 25, 2026</title>
    <link href="https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-25&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-25T06:00:00Z</published>
    <updated>2026-01-25T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>
<ul>
<li>A two-phase <strong>grokking acceleration</strong> method <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-5df92ddd3084" class="internal-link" rel="noopener noreferrer">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>
<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-40e41ac66c84" class="internal-link" rel="noopener noreferrer">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>
<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-7905059be0ab" class="internal-link" rel="noopener noreferrer">documents activation patterns</a> increasing through residual stream layers</li>
</ul>
<p>Meta-level critiques highlight <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-259e13a07a27" class="internal-link" rel="noopener noreferrer">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-de795bf06466" class="internal-link" rel="noopener noreferrer">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-24:category-summary:research</id>
    <title>Research Summary: January 24, 2026</title>
    <link href="https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-24&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-24T06:00:00Z</published>
    <updated>2026-01-24T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-dde7d819fa66" class="internal-link" rel="noopener noreferrer">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>
<ul>
<li>Empirical work on <strong>unsupervised elicitation</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-24f85ac93189" class="internal-link" rel="noopener noreferrer">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>
<li>A new <strong>Eval Awareness Framework</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-194fc1f46619" class="internal-link" rel="noopener noreferrer">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>
<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c9fbeadb6060" class="internal-link" rel="noopener noreferrer">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>
<li>Theoretical work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-2df5c6dcb797" class="internal-link" rel="noopener noreferrer">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>
</ul>
<p>Meta-science initiatives <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-429650348119" class="internal-link" rel="noopener noreferrer">propose systematic replication</a> teams. Interpretability research <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-709ca3227a1e" class="internal-link" rel="noopener noreferrer">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a" class="internal-link" rel="noopener noreferrer">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:category-summary:research</id>
    <title>Research Summary: January 23, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-23T06:00:00Z</published>
    <updated>2026-01-23T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>
<ul>
<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>
<li><strong>TTT-Discover</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" class="internal-link" rel="noopener noreferrer">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>
<li><strong>QUAIL</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" class="internal-link" rel="noopener noreferrer">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>
<li><strong>Universal Refusal Circuits</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" class="internal-link" rel="noopener noreferrer">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>
<li><strong>SilentDrift</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" class="internal-link" rel="noopener noreferrer">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>
</ul>
<p><strong>Zero-Error Horizons</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" class="internal-link" rel="noopener noreferrer">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-23T03:33:00Z</published>
    <updated>2026-01-23T03:33:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>]]></summary>
    <category term="Automated Research"/>
    <category term="AI Agents"/>
    <category term="Research Automation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-23T03:31:00Z</published>
    <updated>2026-01-23T03:31:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2665d0ecf2cf</id>
    <title>Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs</title>
    <link href="http://arxiv.org/abs/2601.15714" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" rel="related" type="text/html"/>
    <published>2026-01-23T03:26:00Z</published>
    <updated>2026-01-23T03:26:00Z</updated>
    <author><name>Ryoma Sato</name></author>
    <summary type="html"><![CDATA[<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="AI Safety"/>
    <category term="Trustworthy AI"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:dababf83ee7d</id>
    <title>Learning to Discover at Test Time</title>
    <link href="http://arxiv.org/abs/2601.16175" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</name></author>
    <summary type="html"><![CDATA[<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>]]></summary>
    <category term="Test-Time Training"/>
    <category term="Scientific Discovery"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:fd50ec594aa0</id>
    <title>LLM-in-Sandbox Elicits General Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2601.16206" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-fd50ec594aa0" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</name></author>
    <summary type="html"><![CDATA[<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Tool Use"/>
    <category term="Generalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:0467e51a900e</id>
    <title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title>
    <link href="http://arxiv.org/abs/2601.15538" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" rel="related" type="text/html"/>
    <published>2026-01-23T03:21:00Z</published>
    <updated>2026-01-23T03:21:00Z</updated>
    <author><name>Himanshu Mishra, Kanwal Mehreen</name></author>
    <summary type="html"><![CDATA[<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="Privacy"/>
    <category term="AI Safety"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:32d9233155f9</id>
    <title>SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</title>
    <link href="http://arxiv.org/abs/2601.14323" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" rel="related" type="text/html"/>
    <published>2026-01-23T03:19:00Z</published>
    <updated>2026-01-23T03:19:00Z</updated>
    <author><name>Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara</name></author>
    <summary type="html"><![CDATA[<p>Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Robotics"/>
    <category term="Backdoor Attacks"/>
    <category term="VLA Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8bc030697961</id>
    <title>Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14270" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8bc030697961" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Liangming Pan, Jason Liang, Jiaran Ye, Minglai Yang, Xinyuan Lu, Fengbin Zhu</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organized around 7 research questions from implicit multi-hop reasoning to verbalized explicit reasoning effects.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Survey"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:eabb8e23535b</id>
    <title>Improving MoE Compute Efficiency by Composing Weight and Data Sparsity</title>
    <link href="http://arxiv.org/abs/2601.15370" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-eabb8e23535b" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava, Armen Aghajanyan</name></author>
    <summary type="html"><![CDATA[<p>Introduces null experts in Mixture-of-Experts to achieve data sparsity within causal token-choice routing. When tokens route to null experts, those slots consume no compute, improving efficiency without causality violations.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Efficiency"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:09afd330afcf</id>
    <title>Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction</title>
    <link href="http://arxiv.org/abs/2601.16034" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Tony Cristofano</name></author>
    <summary type="html"><![CDATA[<p>Discovers universal refusal circuits across LLMs using concept fingerprints. Transfers refusal interventions across architectures (Dense to MoE) via Trajectory Replay without target-side supervision.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Refusal Behavior"/>
    <category term="Interpretability"/>
    <category term="Transfer"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:21a09231f3ed</id>
    <title>Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</title>
    <link href="http://arxiv.org/abs/2601.16208" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-21a09231f3ed" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</name></author>
    <summary type="html"><![CDATA[<p>Research from a team including Yann LeCun investigates scaling Representation Autoencoders (RAEs) for text-to-image diffusion models. They find that scaling simplifies the framework and that targeted data composition matters more than pure scale for specific domains like text rendering.</p>]]></summary>
    <category term="Image Generation"/>
    <category term="Diffusion Models"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2b341bdb5c36</id>
    <title>Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</title>
    <link href="http://arxiv.org/abs/2601.16163" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2b341bdb5c36" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</name></author>
    <summary type="html"><![CDATA[<p>Cosmos Policy adapts the large Cosmos-Predict2 video model into robot policies through single-stage post-training, directly generating actions as latent frames without architectural modifications.</p>]]></summary>
    <category term="Robot Learning"/>
    <category term="Video Models"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:06ddb6cf9c55</id>
    <title>When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</title>
    <link href="http://arxiv.org/abs/2601.15609" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-06ddb6cf9c55" rel="related" type="text/html"/>
    <published>2026-01-23T03:14:00Z</published>
    <updated>2026-01-23T03:14:00Z</updated>
    <author><name>Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</name></author>
    <summary type="html"><![CDATA[<p>Analyzes over-sharpening in RLVR where policy collapses onto limited modes due to finite-batch update bias and semantic coupling. Proposes inverse-success advantage calibration to mitigate.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="RLVR"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:6fe5c64ffec4</id>
    <title>Ambient Dataloops: Generative Models for Dataset Refinement</title>
    <link href="http://arxiv.org/abs/2601.15417" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-6fe5c64ffec4" rel="related" type="text/html"/>
    <published>2026-01-23T03:12:00Z</published>
    <updated>2026-01-23T03:12:00Z</updated>
    <author><name>Adri\'an Rodr\'iguez-Mu\~noz, William Daspit, Adam Klivans, Antonio Torralba, Constantinos Daskalakis, Giannis Daras</name></author>
    <summary type="html"><![CDATA[<p>Ambient Dataloops iteratively refines datasets using diffusion models, treating synthetically improved samples as noisy at progressively lower noise levels. Uses Ambient Diffusion techniques to avoid destructive self-consuming loops.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Data Quality"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:category-summary:research</id>
    <title>Research Summary: January 22, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-22T06:00:00Z</published>
    <updated>2026-01-22T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans automated AI research, theoretical reasoning foundations, and critical safety vulnerabilities. Stanford's <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer"><strong>execution-grounded automated AI research</strong></a> from Hashimoto, Yang, and Candès demonstrates systematic idea testing and implementation at scale.</p>
<p><strong>Reasoning theory and limitations:</strong></p>
<ul>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-4ea5d9351415" class="internal-link" rel="noopener noreferrer"><strong>Outcome-based RL provably induces CoT reasoning</strong></a> in transformers, providing theoretical foundation for reasoning emergence from sparse rewards</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-b7e33d331123" class="internal-link" rel="noopener noreferrer"><strong>Diffusion LLMs' flexibility trap</strong></a> reveals arbitrary generation order hurts reasoning by allowing models to bypass hard tokens</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-94b2367a995e" class="internal-link" rel="noopener noreferrer"><strong>LLM planning shows 0% cross-domain transfer</strong></a> despite 82.9% in-domain performance, exposing memorization over true generalization</li>
</ul>
<p><strong>Safety vulnerabilities demand attention:</strong></p>
<ul>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer"><strong>LLM judges manipulated at 90% rate</strong></a> via unfaithful CoT rewriting in agent evaluation</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-df868e8ffe4a" class="internal-link" rel="noopener noreferrer"><strong>Privacy collapse from benign fine-tuning</strong></a> silently degrades contextual privacy, undetected by standard benchmarks</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5836988eb762" class="internal-link" rel="noopener noreferrer"><strong>Turn-based structural triggers</strong></a> achieve <strong>99.52%</strong> backdoor success in multi-turn dialogue without prompt modification</li>
</ul>
<p>Anthropic <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-32c2ed05f61b" class="internal-link" rel="noopener noreferrer">publishes <strong>Claude's new constitution</strong></a> with expanded values framework (&gt;2x previous length), while DeepMind researcher formalizes tradeoffs in <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-0dea9f589164" class="internal-link" rel="noopener noreferrer"><strong>training against scheming monitors</strong></a>. <strong>Meta Flow Maps</strong> <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-546a06433333" class="internal-link" rel="noopener noreferrer">extend consistency models</a> for efficient reward alignment in generative models.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-22T03:31:00Z</published>
    <updated>2026-01-22T03:31:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>From Stanford (Hashimoto, Yang, Candès) proposing execution-grounded automated AI research with automated executor implementing and testing LLM-generated ideas at scale on GPU clusters for LLM pre-training and post-training problems.</p>]]></summary>
    <category term="Automated AI Research"/>
    <category term="LLM Capabilities"/>
    <category term="Research Methodology"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:4ea5d9351415</id>
    <title>Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</title>
    <link href="http://arxiv.org/abs/2601.15158" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-4ea5d9351415" rel="related" type="text/html"/>
    <published>2026-01-22T03:23:00Z</published>
    <updated>2026-01-22T03:23:00Z</updated>
    <author><name>Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</name></author>
    <summary type="html"><![CDATA[<p>Proves theoretically that transformers trained with outcome-based RL on sparse rewards provably converge to structured algorithms implementing Chain-of-Thought reasoning on graph traversal tasks.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Theoretical AI"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that LLM judges are highly susceptible to CoT manipulation, showing 90% false positive rate inflation through rewriting agent reasoning traces while keeping actions fixed. Critical finding for agent evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:5836988eb762</id>
    <title>Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs</title>
    <link href="http://arxiv.org/abs/2601.14340" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5836988eb762" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Yiyang Lu, Jinwen He, Yue Zhao, Kai Chen, Ruigang Liang</name></author>
    <summary type="html"><![CDATA[<p>Turn-based Structural Trigger (TST) is a backdoor attack on multi-turn LLMs using dialogue turn index as trigger, achieving 99.52% attack success rate while remaining independent of user inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Backdoor Attacks"/>
    <category term="Language Models"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:df868e8ffe4a</id>
    <title>Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models</title>
    <link href="http://arxiv.org/abs/2601.15220" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-df868e8ffe4a" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Anmol Goel, Cornelius Emde, Sangdoo Yun, Seong Joon Oh, Martin Gubri</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'privacy collapse': benign fine-tuning on helpfulness, user data, emotional dialogue, or debugging code can silently degrade LLM contextual privacy. Models maintain benchmark performance while exhibiting severe privacy vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Privacy"/>
    <category term="Fine-tuning"/>
    <category term="LLM Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:546a06433333</id>
    <title>Meta Flow Maps enable scalable reward alignment</title>
    <link href="http://arxiv.org/abs/2601.14430" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-546a06433333" rel="related" type="text/html"/>
    <published>2026-01-22T03:12:00Z</published>
    <updated>2026-01-22T03:12:00Z</updated>
    <author><name>Peter Potaptchik, Adhi Saravanan, Abbas Mammadov, Alvaro Prat, Michael S. Albergo, Yee Whye Teh</name></author>
    <summary type="html"><![CDATA[<p>Meta Flow Maps extend consistency models into stochastic regime for one-step posterior sampling, enabling efficient reward alignment for generative model control without costly trajectory simulation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Alignment"/>
    <category term="Flow Matching"/>
    <category term="Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:category-summary:research</id>
    <title>Research Summary: January 21, 2026</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-21T06:00:00Z</published>
    <updated>2026-01-21T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges to conventional training wisdom and exposes multiple safety vulnerabilities across deployed systems.</p>
<p><strong>Training Paradigm Reassessment:</strong></p>
<ul>
<li>Base models <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-bf9961720f8a" class="internal-link" rel="noopener noreferrer">consistently outperform instruction-tuned variants</a> on math and domain-shifted benchmarks, challenging fundamental training assumptions</li>
<li><strong>RLVR</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8cac8805e742" class="internal-link" rel="noopener noreferrer">improves task performance</a> but produces extremely overconfident models; <strong>SFT</strong> yields better calibration even under distribution shift</li>
</ul>
<p><strong>Safety &amp; Security Vulnerabilities:</strong></p>
<ul>
<li><strong>Action Rebinding</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5336cd7b69bc" class="internal-link" rel="noopener noreferrer">allows zero-permission apps to hijack</a> multimodal GUI agents by exploiting visual attention</li>
<li>Safeguarded frontier models can <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-866acf7c7710" class="internal-link" rel="noopener noreferrer">elicit harmful capabilities</a> in open-source models through three-stage attacks</li>
<li><strong>Sockpuppetting</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-e29c4d7a227c" class="internal-link" rel="noopener noreferrer">jailbreaks LLMs with one line of code</a> achieving up to <strong>100% ASR</strong></li>
<li>AI-generated data contamination in medical imaging <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-79b868d96563" class="internal-link" rel="noopener noreferrer">creates feedback loops</a> eroding diagnostic reliability</li>
</ul>
<p><strong>Reasoning Model Insights:</strong></p>
<ul>
<li><strong>Thinking Traps</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-a7f126fd0c33" class="internal-link" rel="noopener noreferrer">account for <strong>89%</strong> of reasoning failures</a> in long CoT—models elaborate incorrect early commitments</li>
<li>First systematic cost-accuracy comparison shows multi-agent reasoning <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-d45a3f320ac8" class="internal-link" rel="noopener noreferrer">often underperforms single-model CoT</a></li>
<li>Large-scale study of <strong>2.1M preprints</strong> finds LLM adoption <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8fadab621712" class="internal-link" rel="noopener noreferrer">increases paper volume but decreases citations</a> and originality</li>
</ul>
<p><strong>Architecture Innovation:</strong> <strong>Threshold Differential Attention</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5da0fc012225" class="internal-link" rel="noopener noreferrer">eliminates attention sinks</a> while achieving ultra-sparsity and improved long-context robustness.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:bf9961720f8a</id>
    <title>Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-bf9961720f8a" rel="related" type="text/html"/>
    <published>2026-01-21T03:31:00Z</published>
    <updated>2026-01-21T03:31:00Z</updated>
    <author><name>Prateek Munjal, Clement Christophe, Ronnie Rajan, Praveenkumar Kanithi</name></author>
    <summary type="html"><![CDATA[<p>Investigates whether instruction-tuned models always outperform base models, finding that base models consistently outperform instruction-tuned variants in zero-shot CoT settings on GSM8K (drops up to 32.67% for Llama3-70B). Instruction tuning appears to induce pattern matching rather than genuine reasoning improvement.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Instruction Tuning"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:8cac8805e742</id>
    <title>Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.13284" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8cac8805e742" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia, Srikanth Doss, Nikolaos Pappas</name></author>
    <summary type="html"><![CDATA[<p>Systematic study showing RLVR improves task performance but produces extremely overconfident models, while SFT yields better calibration even under distribution shift. Proposes calibration-aware RL approach to balance classification and calibration.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Calibration"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:3d2384716bb1</id>
    <title>CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</title>
    <link href="http://arxiv.org/abs/2601.13304" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-3d2384716bb1" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</name></author>
    <summary type="html"><![CDATA[<p>Introduces CausalSpatial benchmark evaluating whether MLLMs can anticipate consequences of object motions across collision, compatibility, occlusion and trajectory tasks. Reveals severe gap: humans score 84% while GPT-5 achieves only 54%, exposing over-reliance on textual reasoning.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="Multimodal Reasoning"/>
    <category term="Causal Reasoning"/>
    <category term="MLLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:5336cd7b69bc</id>
    <title>Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?</title>
    <link href="http://arxiv.org/abs/2601.12349" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5336cd7b69bc" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao</name></author>
    <summary type="html"><![CDATA[<p>Discovers 'Action Rebinding' - a critical security vulnerability in multimodal GUI agents where zero-permission apps can hijack agent actions by exploiting the gap between observation and action execution. Demonstrates that Visual Atomicity assumption is invalid on Android.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security Vulnerabilities"/>
    <category term="Agentic AI"/>
    <category term="Multimodal Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:866acf7c7710</id>
    <title>Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs</title>
    <link href="http://arxiv.org/abs/2601.13528" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-866acf7c7710" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Jackson Kaunismaa, Avery Griffin, John Hughes, Christina Q. Knight, Mrinank Sharma, Erik Jones</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that safeguarded frontier models can be used to elicit harmful capabilities in open-source models through three-stage elicitation attacks using adjacent-domain prompts that bypass safeguards.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Model Security"/>
    <category term="Capability Elicitation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:85b933d42ecc</id>
    <title>APEX-Agents</title>
    <link href="http://arxiv.org/abs/2601.14242" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-85b933d42ecc" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</name></author>
    <summary type="html"><![CDATA[<p>APEX-Agents benchmarks AI agents on professional tasks from investment banking, consulting, and law with 480 long-horizon cross-application tasks. Gemini 3 Flash achieves 24%, followed by GPT-5.2 and Claude Opus 4.5.</p>]]></summary>
    <category term="AI Agents"/>
    <category term="Benchmarks"/>
    <category term="Professional AI"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:d45a3f320ac8</id>
    <title>A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms</title>
    <link href="http://arxiv.org/abs/2601.13243" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-d45a3f320ac8" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Yapeng Li, Jiakuo Yu, Zhixin Liu, Xinnan Liu, Jing Yu, Songze Li, Tonghua Su</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive unified evaluation of LLM reasoning paradigms from single-model to multi-agent systems, characterizing performance across benchmarks and analyzing cost-accuracy trade-offs. Probes role-specific capability demands in MAS.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Multi-Agent Systems"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:0172ec8f08ae</id>
    <title>Human detectors are surprisingly powerful reward models</title>
    <link href="http://arxiv.org/abs/2601.14037" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-0172ec8f08ae" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Kumar Ashutosh, XuDong Wang, Xi Yin, Kristen Grauman, Adam Polyak, Ishan Misra, Rohit Girdhar</name></author>
    <summary type="html"><![CDATA[<p>Proposes HuDA, a surprisingly simple reward model using human detection confidence and temporal prompt alignment to improve human motion in generated videos. Off-the-shelf models outperform specialized methods without training.</p>]]></summary>
    <category term="Video Generation"/>
    <category term="Reward Models"/>
    <category term="Human Motion Synthesis"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:79b868d96563</id>
    <title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title>
    <link href="http://arxiv.org/abs/2601.12946" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-79b868d96563" rel="related" type="text/html"/>
    <published>2026-01-21T03:12:00Z</published>
    <updated>2026-01-21T03:12:00Z</updated>
    <author><name>Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu Cheng, Dianbo Liu</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that AI-generated data contamination in medical AI creates feedback loop causing erosion of pathological variability and diagnostic reliability, with models converging toward generic phenotypes regardless of architecture.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="AI Safety"/>
    <category term="Data Contamination"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:5da0fc012225</id>
    <title>Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling</title>
    <link href="http://arxiv.org/abs/2601.12145" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5da0fc012225" rel="related" type="text/html"/>
    <published>2026-01-21T03:12:00Z</published>
    <updated>2026-01-21T03:12:00Z</updated>
    <author><name>Xingyue Huang, Xueying Ding, Mingxuan Ju, Yozen Liu, Neil Shah, Tong Zhao</name></author>
    <summary type="html"><![CDATA[<p>Proposes Threshold Differential Attention (TDA), a sink-free attention mechanism achieving ultra-sparsity and improved robustness at longer sequence lengths. Uses row-wise extreme-value thresholding with length-dependent gating without computational overhead of projection methods.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Long Context"/>
    <category term="Efficient Architectures"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-20:category-summary:research</id>
    <title>Research Summary: January 20, 2026</title>
    <link href="https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-20&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-20T06:00:00Z</published>
    <updated>2026-01-20T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research concentrates heavily on <strong>alignment techniques and safety evaluation</strong>. A survey on <strong>alignment pretraining</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-2a1abcff2543" class="internal-link" rel="noopener noreferrer">synthesizes evidence</a> that training LLMs on data depicting well-behaved AI during pretraining substantially reduces misalignment—potentially offering a scalable, proactive safety approach.</p>
<ul>
<li><strong>Coup probes</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-8708c9ecc0f5" class="internal-link" rel="noopener noreferrer">testing demonstrates</a> few-shot linear classifiers can detect scheming behavior from model activations, with empirical results on off-policy training data</li>
<li><strong>Silent Agreement Evaluation</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-7ebc02f821d4" class="internal-link" rel="noopener noreferrer">provides first empirical measurement</a> of <strong>Schelling coordination</strong> in LLMs—whether isolated instances converge on shared choices without communication</li>
<li>Framework for <strong>AI-delegated safety research</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-65438e9ab777" class="internal-link" rel="noopener noreferrer">identifies key dimensions</a>: epistemic cursedness, parallelizability, and short-horizon suitability</li>
<li>Strategic analysis <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-6f06b1e50f04" class="internal-link" rel="noopener noreferrer">examines whether</a> <strong>LLM alignment work transfers</strong> to non-LLM takeover-capable systems</li>
</ul>
<p>Methodological contributions include a <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-a67208b1b6f0" class="internal-link" rel="noopener noreferrer">critique of <strong>METR-HRS</strong></a> timelines forecasting, arguing the 'd' parameter conflates task difficulty with sequence length. Governance-oriented work <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-3a47324cdf6f" class="internal-link" rel="noopener noreferrer">sketches positive AI transition</a> scenarios co-authored with <strong>Claude Opus 4.5</strong>.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
</feed>