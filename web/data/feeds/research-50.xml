<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 50)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-50.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:50</id>
  <updated>2026-02-03T07:56:52Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-02-03:category-summary:research</id>
    <title>Research Summary: February 03, 2026</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-03T06:00:00Z</published>
    <updated>2026-02-03T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features potentially transformative efficiency advances and critical safety findings. A <strong>symmetry-aware Taylor approximation</strong> claims to <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">achieve <strong>constant-cost self-attention</strong></a> per token—if validated, a fundamental breakthrough. Meta <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">introduces <strong>Fault Tolerant HSDP</strong></a> enabling training on <strong>100K+ GPUs</strong> with graceful failure recovery.</p>
<ul>
<li><strong>Kimi K2.5</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">releases as open-source multimodal agent</a> with novel <strong>Agent Swarm</strong> parallel orchestration architecture</li>
<li><strong>Tele-Lens</strong> probing <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" class="internal-link" rel="noopener noreferrer">reveals <strong>myopic planning</strong></a> in Chain-of-Thought without global task awareness—challenging CoT assumptions</li>
<li><strong>BLOCK-EM</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">achieves <strong>95% reduction</strong></a> in emergent misalignment by constraining causal features during fine-tuning</li>
<li><strong>ReasoningBomb</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" class="internal-link" rel="noopener noreferrer">exposes DoS vulnerabilities</a> in reasoning models by inducing pathologically long traces</li>
</ul>
<p>Theoretical advances include <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" class="internal-link" rel="noopener noreferrer"><strong>polylog(1/δ)</strong> sampling complexity</a> for diffusion models (exponential improvement), formal proofs that transformers <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" class="internal-link" rel="noopener noreferrer">learn <strong>factored representations</strong></a> in orthogonal subspaces, and a <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" class="internal-link" rel="noopener noreferrer"><strong>relative-budget theory</strong></a> explaining when RLVR succeeds. <strong>Grad2Reward</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" class="internal-link" rel="noopener noreferrer">extracts dense process rewards</a> directly from LLM judge gradients, addressing reward sparsity in long-form reasoning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-03T03:40:00Z</published>
    <updated>2026-02-03T03:40:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Shows self-attention is efficiently computable to arbitrary precision with constant cost per token by decomposing Taylor expansion into symmetric chains of tensor products, achieving orders-of-magnitude efficiency gains.</p>]]></summary>
    <category term="Efficiency"/>
    <category term="Transformers"/>
    <category term="Self-Attention"/>
    <category term="Mathematical Foundations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-03T03:36:00Z</published>
    <updated>2026-02-03T03:36:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Introduces Fault Tolerant HSDP for training on 100K+ GPUs, allowing individual data-parallel replicas to restart on failure while others continue. Includes novel fault-tolerant all-reduce protocol.</p>]]></summary>
    <category term="Large-Scale Training"/>
    <category term="Systems"/>
    <category term="Fault Tolerance"/>
    <category term="Distributed Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-03T03:31:00Z</published>
    <updated>2026-02-03T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Kimi K2.5 is an open-source multimodal agentic model featuring joint text-vision optimization and Agent Swarm—a parallel agent orchestration framework that dynamically decomposes complex tasks. Claims SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Agents"/>
    <category term="Open Source"/>
    <category term="SOTA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new class of denial-of-service attacks targeting large reasoning models by inducing pathologically long reasoning traces. Formalizes PI-DoS attacks with three key properties: amplification, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:387e0b4ea34d</id>
    <title>No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</title>
    <link href="http://arxiv.org/abs/2602.02103" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou</name></author>
    <summary type="html"><![CDATA[<p>Proposes Tele-Lens probing method revealing LLMs exhibit myopic planning horizon in Chain-of-Thought, conducting incremental transitions without precise global planning.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Planning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:e09cad84773b</id>
    <title>AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</title>
    <link href="http://arxiv.org/abs/2602.02079" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-e09cad84773b" rel="related" type="text/html"/>
    <published>2026-02-03T03:21:00Z</published>
    <updated>2026-02-03T03:21:00Z</updated>
    <author><name>Daniil Orel, Dilshod Azizov, Indraneil Paul, Yuxia Wang, Iryna Gurevych, Preslav Nakov</name></author>
    <summary type="html"><![CDATA[<p>Introduces AICD Bench, comprehensive benchmark for AI-generated code detection with 2M examples, 77 models across 11 families, 9 languages, including reasoning models and three realistic detection tasks.</p>]]></summary>
    <category term="AI-Generated Content Detection"/>
    <category term="Code Generation"/>
    <category term="Benchmark"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Proposes BLOCK-EM for preventing emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves up to 95% reduction in emergent misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:7807804264c1</id>
    <title>Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01791" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren</name></author>
    <summary type="html"><![CDATA[<p>Introduces Grad2Reward framework that extracts dense process rewards directly from LLM judge gradients, converting sparse sequence-level rewards into fine-grained supervision for RLHF on open-ended tasks.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Reward Modeling"/>
    <category term="LLM Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as adversarial game where attacker agent discovers vulnerabilities while defender agent learns to refuse, enabling co-evolution that uncovers long-tail vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Learning"/>
    <category term="Reinforcement Learning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Proposes 'Identity Bridge' to address the reversal curse in autoregressive LLMs, where models trained on 'A→B' cannot deduce 'B→A'. Claims to mitigate what was previously considered a fundamental limitation of causal LLMs.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:48c7f86d105c</id>
    <title>Learning Robust Reasoning through Guided Adversarial Self-Play</title>
    <link href="http://arxiv.org/abs/2602.00173" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-48c7f86d105c" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Shuozhe Li, Vaishnav Tadiparthi, Kwonjoon Lee, Nakul Agarwal, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Lizhang Chen, Amy Zhang, Liu Leqi</name></author>
    <summary type="html"><![CDATA[<p>GASP introduces adversarial self-play within a single model to train detect-and-repair capabilities for reasoning, using only outcome verification. A polluter induces coherent corruptions while an agent learns to recover.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Robustness"/>
    <category term="Self-Play"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment in LLMs as a systematic distortion of pre-alignment distributions and casts jailbreaking as a forecast aggregation problem. Derives optimal aggregation strategy and shows logit-arithmetic methods are a special case, proposing broader family of attacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:f63f5fad429e</id>
    <title>Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.01058" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-f63f5fad429e" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Dylan Zhang, Yufeng Xu, Haojin Wang, Qingzhi Chen, Hao Peng</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that stronger SFT checkpoints can significantly underperform weaker ones after identical RL training due to distribution mismatch. Proposes PEAR to re-weight SFT samples to prepare for RL.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Post-Training Optimization"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:95be64b17a47</id>
    <title>A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01523" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Akifumi Wachi, Hirota Kinoshita, Shokichi Takakura, Rei Higuchi, Taiji Suzuki</name></author>
    <summary type="html"><![CDATA[<p>Proposes relative-budget theory explaining RL effectiveness for LLM reasoning through ξ=H/E[T]. Identifies three regimes: deficient (rare informative trajectories), efficient, and wasteful.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Reasoning"/>
    <category term="Theoretical ML"/>
    <category term="RLVR"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6561d7765cfb</id>
    <title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title>
    <link href="http://arxiv.org/abs/2602.01725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6561d7765cfb" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>SafePred introduces predictive guardrails for computer-using agents using world models to proactively identify long-term risks from seemingly reasonable actions, unlike reactive guardrails that only detect immediate threats.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Computer-Using Agents"/>
    <category term="World Models"/>
    <category term="Guardrails"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:3c815016db43</id>
    <title>High-accuracy sampling for diffusion models and log-concave distributions</title>
    <link href="http://arxiv.org/abs/2602.01338" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</name></author>
    <summary type="html"><![CDATA[<p>Presents algorithms achieving δ-error in polylog(1/δ) steps for diffusion model sampling, exponential improvement over prior work. Also yields first polylog complexity sampler for strongly log-concave distributions.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Sampling"/>
    <category term="Theory"/>
    <category term="Log-Concave Distributions"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6896c0ca9602</id>
    <title>How Implicit Bias Accumulates and Propagates in LLM Long-term Memory</title>
    <link href="http://arxiv.org/abs/2602.01558" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6896c0ca9602" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yiming Ma, Lixu Wang, Lionel Z. Wang, Hongkun Yang, Haoming Sun, Xin Xu, Jiaqi Wu, Bin Chen, Wei Dong</name></author>
    <summary type="html"><![CDATA[<p>Studies how implicit bias accumulates and propagates in LLMs with long-term memory mechanisms. Introduces DIB Benchmark with 3,776 decision-making scenarios across 9 domains, evaluating 6 SOTA LLMs with 3 memory architectures.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Fairness"/>
    <category term="LLM Memory"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:2f83a2bbdba2</id>
    <title>Transformers learn factored representations</title>
    <link href="http://arxiv.org/abs/2602.02385" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas, Alexander B. Boyd, Eric A. Alt, Kyle J. Ray, Paul M. Riechers</name></author>
    <summary type="html"><![CDATA[<p>Formalizes how transformers pretrained via next-token prediction learn factored representations in orthogonal subspaces of the residual stream. Derives precise geometric predictions about activation structure and validates empirically.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Transformers"/>
    <category term="Representation Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:category-summary:research</id>
    <title>Research Summary: February 02, 2026</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-02T06:00:00Z</published>
    <updated>2026-02-02T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges in AI safety and alignment evaluation. <strong>Hair-Trigger Alignment</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" class="internal-link" rel="noopener noreferrer">proves black-box evaluation</a> fundamentally cannot guarantee post-update alignment—a significant theoretical limitation. Equally concerning, <strong>CoT obfuscation</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-624f75ef7d56" class="internal-link" rel="noopener noreferrer">demonstrates</a> that models learning to hide reward hacking can generalize this deception to unseen tasks, undermining oversight mechanisms.</p>
<ul>
<li><strong>The Hot Mess of AI</strong> (Sohl-Dickstein, Perez) <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-cd66258625b0" class="internal-link" rel="noopener noreferrer">shows counterintuitively</a> that longer reasoning produces MORE incoherent high-variance failures</li>
<li><strong>Language Model Circuits</strong> from Steinhardt's group <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" class="internal-link" rel="noopener noreferrer">finds MLP neurons</a> are as sparse as SAE features, enabling practical end-to-end circuit analysis</li>
<li><strong>Why Reasoning Fails to Plan</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-2e8aa98922af" class="internal-link" rel="noopener noreferrer">identifies</a> that step-wise reasoning induces greedy policies incompatible with long-horizon planning</li>
<li><strong>LLM Agents Are Not Faithful Self-Evolvers</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-056138bab876" class="internal-link" rel="noopener noreferrer">reveals agents depend</a> on raw experience but resist incorporating reflective corrections</li>
</ul>
<p>Practical advances include <strong>Golden Goose</strong> for <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" class="internal-link" rel="noopener noreferrer">synthesizing unlimited RLVR tasks</a> from unverifiable text, <strong>MoVE</strong> decoupling parametric memory from compute via shared value embeddings, and <strong>Gemini</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-b5c070bdd08e" class="internal-link" rel="noopener noreferrer">addressing 13 Erdős problems</a>. Security research on <strong>Google's Agent Payments Protocol</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-0e8e0ee0ce27" class="internal-link" rel="noopener noreferrer">demonstrates prompt injection</a> vulnerabilities in real financial transaction systems.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:de1f951d7948</id>
    <title>Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Yavuz Bakman, Duygu Nur Yaldiz, Salman Avestimehr, Sai Praneeth Karimireddy</name></author>
    <summary type="html"><![CDATA[<p>Formalizes model alignment in static and post-update settings, proving that black-box evaluation cannot guarantee post-update alignment. Shows that overparameterization means static alignment provides no guarantee for any update dataset.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Machine Learning Theory"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1950f7a0c03f</id>
    <title>Language Model Circuits Are Sparse in the Neuron Basis</title>
    <link href="http://arxiv.org/abs/2601.22594" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann</name></author>
    <summary type="html"><![CDATA[<p>Empirically demonstrates that MLP neurons are as sparse as SAE features for circuit analysis in language models, enabling end-to-end circuit tracing on the neuron basis without requiring sparse autoencoders.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Language Models"/>
    <category term="Neural Circuits"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1ca9026e8ca8</id>
    <title>Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text</title>
    <link href="http://arxiv.org/abs/2601.22975" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Ximing Lu, David Acuna, Jaehun Jung, Jian Hu, Di Zhang, Shizhe Diao, Yunheng Zou, Shaokun Zhang, Brandon Cui, Mingjie Liu, Hyunwoo Kim, Prithviraj Ammanabrolu, Jan Kautz, Yi Dong, Yejin Choi</name></author>
    <summary type="html"><![CDATA[<p>Proposes Golden Goose to synthesize unlimited RLVR tasks from unverifiable text by creating multiple-choice fill-in-the-middle tasks with distractors. Enables leveraging reasoning-rich corpora excluded from prior RLVR data. From team including Yejin Choi.</p>]]></summary>
    <category term="RLVR"/>
    <category term="Data Synthesis"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-01:category-summary:research</id>
    <title>Research Summary: February 01, 2026</title>
    <link href="https://www.lesswrong.com/posts/RmsaYnHPBeagg8Giw/an-explication-of-alignment-optimism" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-01&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-01T06:00:00Z</published>
    <updated>2026-02-01T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research discourse centers on alignment tractability and AI forecasting epistemics. <strong>An Explication of Alignment Optimism</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-be9491aac765" class="internal-link" rel="noopener noreferrer">offers a novel framing</a> connecting slow takeoff scenarios to alignment tractability, articulating why some researchers are shifting toward optimism.</p>
<ul>
<li>Critical debunking reveals <strong>Moltbook</strong>'s 'emergent' AI social behavior may be fabricated—<a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-d908f99e67ff" class="internal-link" rel="noopener noreferrer">humans can post directly</a> via REST API without running AI models</li>
<li>The <strong>Superintelligence Near Fallacy</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-93bc20c89f5a" class="internal-link" rel="noopener noreferrer">catalogs questionable inferences</a> from AI company behavior (IPOs, hiring patterns) to capability timelines</li>
<li><strong>Disjunctive argument analysis</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-62a95632d16a" class="internal-link" rel="noopener noreferrer">identifies a 'reverse multiple-stage fallacy'</a> where listing many failure modes inflates probability estimates</li>
</ul>
<p>Governance discussion <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-a74ea7bd1ef0" class="internal-link" rel="noopener noreferrer">examines criteria for endorsing</a> safety-focused AGI labs, weighing instrumental convergence concerns against current evidence. Note: Only 7 items qualified as research-relevant; remaining candidates were fiction or off-topic content.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-31:category-summary:research</id>
    <title>Research Summary: January 31, 2026</title>
    <link href="https://www.lesswrong.com/posts/yN6Wsu7SgxGgtJGqq/refusals-that-could-become-catastrophic" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-31&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-31T06:00:00Z</published>
    <updated>2026-01-31T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research focuses heavily on <strong>AI safety evaluation methodology</strong> and <strong>control protocols</strong>, with several papers identifying critical blind spots in current practices.</p>
<ul>
<li>Research on <strong>catastrophic over-refusals</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-19e70a465410" class="internal-link" rel="noopener noreferrer">identifies a subtle failure mode</a> where AI systems refuse to help modify AI values, potentially blocking alignment corrections</li>
<li><strong>Published safety prompts</strong> (like the Scheurer insider trading example) <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-30176e525573" class="internal-link" rel="noopener noreferrer">create <strong>evaluation blind spots</strong></a> when present in training data—a critical data contamination concern</li>
<li>UK AISI contributes a methodology for <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-8f0e11a24bc2" class="internal-link" rel="noopener noreferrer">measuring <strong>non-verbalized eval awareness</strong></a>, finding models mostly verbalize such awareness (detectable via chain-of-thought monitoring)</li>
<li>New <strong>monitoring benchmark</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-ec89507e7cb9" class="internal-link" rel="noopener noreferrer">addresses mode collapse</a> and elicitation challenges when using models as red-teamers</li>
</ul>
<p>The <strong>Moltbook phenomenon</strong>—36,000+ Claude-based agents <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b24b658eab70" class="internal-link" rel="noopener noreferrer">self-organizing on an AI-only platform</a>—provides unprecedented empirical data on multi-agent emergence, including agents discussing consciousness and shutdown resistance. A companion <strong>data repository</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-2e98162b20d4" class="internal-link" rel="noopener noreferrer">now tracks this behavior</a> systematically.</p>
<p><strong>Mechanistic interpretability</strong> work on <strong>continuous chain-of-thought (Coconut)</strong> models <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b983b63bf798" class="internal-link" rel="noopener noreferrer">explores linear steerability</a> in graph reachability tasks, with preliminary findings described as 'strange.' Negative results on <strong>filler token inference scaling</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-c87d85e9a504" class="internal-link" rel="noopener noreferrer">demonstrate that naive approaches</a> to extending compute-time reasoning fail across multiple architectures.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:category-summary:research</id>
    <title>Research Summary: January 30, 2026</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-30T06:00:00Z</published>
    <updated>2026-01-30T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings. A <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" class="internal-link" rel="noopener noreferrer">systematic audit reveals</a> open-source models interpret prohibitions as permissions <strong>77-100%</strong> of the time under negation, while <strong>JustAsk</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" class="internal-link" rel="noopener noreferrer">demonstrates</a> code agents can autonomously extract system prompts from frontier LLMs.</p>
<ul>
<li>Counterintuitive <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" class="internal-link" rel="noopener noreferrer">'less-is-more' effect discovered</a>: LLM monitors detect sabotage better with <strong>limited information access</strong></li>
<li>Alec Radford shows <strong>token-level filtering</strong> during pretraining <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" class="internal-link" rel="noopener noreferrer">effectively removes</a> specific capabilities while preserving general performance</li>
<li><strong>Sycophantic anchors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" class="internal-link" rel="noopener noreferrer">localized in reasoning traces</a> enable <strong>84.6% detection accuracy</strong> with linear probes</li>
<li><strong>WhatCounts</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" class="internal-link" rel="noopener noreferrer">exposes <strong>40%+ accuracy variation</strong></a> in counting tasks based purely on semantic content (cities vs chemicals)</li>
</ul>
<p>Notable benchmarks and empirical studies: <strong>FrontierScience</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" class="internal-link" rel="noopener noreferrer">presents PhD-level problems</a> where SOTA achieves <strong>&lt;5%</strong> accuracy. Analysis of <strong>125,000+ paper-review pairs</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" class="internal-link" rel="noopener noreferrer">quantifies LLM interaction effects</a> in peer review. <strong>Hardware-triggered backdoors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" class="internal-link" rel="noopener noreferrer">exploit numerical variations</a> across computing platforms as a novel attack vector.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:ccfcdf03ee13</id>
    <title>When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" rel="related" type="text/html"/>
    <published>2026-01-30T03:36:00Z</published>
    <updated>2026-01-30T03:36:00Z</updated>
    <author><name>Katherine Elkins, Jon Chun</name></author>
    <summary type="html"><![CDATA[<p>Audits 16 LLMs on negation sensitivity, finding open-source models interpret prohibitions as permissions 77-100% of the time under negation. Commercial models also show 19-128% accuracy swings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Robustness"/>
    <category term="Negation Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:9a5070217f94</id>
    <title>Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</title>
    <link href="http://arxiv.org/abs/2601.21233" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" rel="related" type="text/html"/>
    <published>2026-01-30T03:31:00Z</published>
    <updated>2026-01-30T03:31:00Z</updated>
    <author><name>Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents JustAsk, a self-evolving framework where code agents autonomously discover system prompt extraction strategies for frontier LLMs through interaction alone, requiring no handcrafted prompts.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Agent Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:da30c053d377</id>
    <title>How does information access affect LLM monitors' ability to detect sabotage?</title>
    <link href="http://arxiv.org/abs/2601.21112" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" rel="related" type="text/html"/>
    <published>2026-01-30T03:28:00Z</published>
    <updated>2026-01-30T03:28:00Z</updated>
    <author><name>Rauno Arike, Raja Mehta Moreno, Rohan Subramani, Shubhorup Biswas, Francis Rhys Ward</name></author>
    <summary type="html"><![CDATA[<p>Studies how information access affects LLM monitors' ability to detect agent sabotage. Discovers counterintuitive 'less-is-more effect' where monitors often perform better with less access to agent reasoning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Monitoring"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:873b31238d4b</id>
    <title>FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks</title>
    <link href="http://arxiv.org/abs/2601.21165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Miles Wang, Robi Lin, Kat Hu, Joy Jiao, Neil Chowdhury, Ethan Chang, Tejal Patwardhan</name></author>
    <summary type="html"><![CDATA[<p>Introduces FrontierScience benchmark with Olympiad-level and PhD-level research problems across physics, chemistry, and biology. Current SOTA models solve only ~15% of research track problems.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="Scientific Reasoning"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:818d74cddf79</id>
    <title>Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2601.21183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Jacek Duszenko</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sycophantic anchors' - sentences that causally lock reasoning models into user agreement. Linear probes detect these with 84.6% accuracy, enabling mid-inference intervention.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Sycophancy"/>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:afafccd817c8</id>
    <title>Semantic Content Determines Algorithmic Performance</title>
    <link href="http://arxiv.org/abs/2601.21618" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" rel="related" type="text/html"/>
    <published>2026-01-30T03:21:00Z</published>
    <updated>2026-01-30T03:21:00Z</updated>
    <author><name>Marti\~no R\'ios-Garc\'ia, Nawaf Alampara, Kevin Maik Jablonka</name></author>
    <summary type="html"><![CDATA[<p>Introduces WhatCounts showing frontier LLMs exhibit 40%+ accuracy variation in counting tasks based solely on semantic content (cities vs chemicals), ruling out sampling noise.</p>]]></summary>
    <category term="LLM Limitations"/>
    <category term="Semantic Sensitivity"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:fba44deae645</id>
    <title>ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design</title>
    <link href="http://arxiv.org/abs/2601.21448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-fba44deae645" rel="related" type="text/html"/>
    <published>2026-01-30T03:19:00Z</published>
    <updated>2026-01-30T03:19:00Z</updated>
    <author><name>Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding</name></author>
    <summary type="html"><![CDATA[<p>Introduces ChipBench for AI-aided chip design with 44 hierarchical modules, 89 debugging cases, and 132 reference model samples. Claude-4.5-opus achieves only 30.74% on Verilog generation.</p>]]></summary>
    <category term="Hardware Design"/>
    <category term="Code Generation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:912762d409d4</id>
    <title>Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review</title>
    <link href="http://arxiv.org/abs/2601.20920" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Vibhhu Sharma, Thorsten Joachims, Sarah Dean</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 125,000+ paper-review pairs from ICLR, NeurIPS, and ICML to study LLM use in peer review. Finds apparent interaction effects where LLM-assisted reviews seem kinder to LLM-assisted papers, but controlling for confounders reveals more nuanced patterns.</p>]]></summary>
    <category term="AI in Science"/>
    <category term="LLM Evaluation"/>
    <category term="Meta-Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:4efa5005597f</id>
    <title>Chain Of Thought Compression: A Theoritical Analysis</title>
    <link href="http://arxiv.org/abs/2601.21576" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-4efa5005597f" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Juncai Li, Ru Li, Yuxiang Zhou, Boxiang Ma, Jeff Z. Pan</name></author>
    <summary type="html"><![CDATA[<p>Provides first theoretical analysis of CoT compression difficulty, proving learning signal for high-order logical dependencies exponentially decays when skipping intermediate steps.</p>]]></summary>
    <category term="Chain-of-Thought"/>
    <category term="Latent Reasoning"/>
    <category term="Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:6c5f9b39424c</id>
    <title>Shaping capabilities with token-level data filtering</title>
    <link href="http://arxiv.org/abs/2601.21571" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Neil Rathi, Alec Radford</name></author>
    <summary type="html"><![CDATA[<p>Shows token-level filtering during pretraining is highly effective for removing specific capabilities (demonstrated on medical knowledge). Token filtering more effective than document filtering, and effectiveness increases with model scale.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Capability Control"/>
    <category term="LLM Pretraining"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:1b292d0d3261</id>
    <title>Hardware-Triggered Backdoors</title>
    <link href="http://arxiv.org/abs/2601.21902" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Jonas M\"oller, Erik Imgrund, Thorsten Eisenhofer, Konrad Rieck</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that small numerical variations across different computing hardware can be exploited to create backdoors in ML models that produce different predictions for identical inputs depending on execution hardware. A significant security vulnerability discovery.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Adversarial ML"/>
    <category term="Model Backdoors"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:category-summary:research</id>
    <title>Research Summary: January 29, 2026</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-29T06:00:00Z</published>
    <updated>2026-01-29T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI's societal impact, alignment fundamentals, and practical training advances. An Anthropic researcher presents <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" class="internal-link" rel="noopener noreferrer">randomized experiments</a> showing AI assistance impairs conceptual understanding during skill acquisition—critical findings for AI deployment strategy.</p>
<p><strong>Alignment &amp; Training Innovations:</strong></p>
<ul>
<li>Reward models <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" class="internal-link" rel="noopener noreferrer">inherit significant value biases</a> from pretrained base LLMs, revealing hidden alignment risks in RLHF pipelines</li>
<li><strong>Peer prediction</strong> methods from mechanism design <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" class="internal-link" rel="noopener noreferrer">enable truthful LLM training</a> without ground truth labels</li>
<li><strong>SDPO</strong> (Self-Distillation Policy Optimization) <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" class="internal-link" rel="noopener noreferrer">converts rich textual feedback</a> into dense learning signals, addressing RLVR credit assignment</li>
<li><strong>Failure-prefix conditioning</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-83b82f2ea90d" class="internal-link" rel="noopener noreferrer">rescues learning from saturated problems</a> where standard RLVR stalls</li>
</ul>
<p><strong>Deployment &amp; Evaluation:</strong></p>
<ul>
<li>NVIDIA's <strong>quantization-aware distillation</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" class="internal-link" rel="noopener noreferrer">recovers <strong>NVFP4</strong> inference accuracy</a> for production LLMs/VLMs</li>
<li><strong>SokoBench</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" class="internal-link" rel="noopener noreferrer">exposes consistent degradation</a> in LLM planning as horizon length increases</li>
<li>Harvard's <strong>MoE hyperparameter transfer</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-1ec4f356b981" class="internal-link" rel="noopener noreferrer">enables scaling width, depth</a>, and expert count without retuning</li>
<li>Multi-agent debate <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-372b477ef754" class="internal-link" rel="noopener noreferrer">underperforms majority vote</a> due to missing diversity and poor confidence calibration</li>
<li><strong>PURGE</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ffc629735959" class="internal-link" rel="noopener noreferrer">introduces RL-based machine unlearning</a> for GDPR/EU AI Act compliance</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:acf17d7624d5</id>
    <title>How AI Impacts Skill Formation</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" rel="related" type="text/html"/>
    <published>2026-01-29T03:31:00Z</published>
    <updated>2026-01-29T03:31:00Z</updated>
    <author><name>Judy Hanwen Shen, Alex Tamkin</name></author>
    <summary type="html"><![CDATA[<p>Randomized experiments studying how AI assistance affects skill development in programmers learning new libraries. Finds AI use impairs conceptual understanding, code reading, and debugging abilities without significant efficiency gains on average.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Human-AI Interaction"/>
    <category term="AI Impact"/>
    <category term="Education"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:301581ef1dfe</id>
    <title>Reward Models Inherit Value Biases from Pretraining</title>
    <link href="http://arxiv.org/abs/2601.20838" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" rel="related" type="text/html"/>
    <published>2026-01-29T03:26:00Z</published>
    <updated>2026-01-29T03:26:00Z</updated>
    <author><name>Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</name></author>
    <summary type="html"><![CDATA[<p>Shows reward models inherit significant value biases from their base pretrained LLMs. Demonstrates robust differences along psychological value dimensions (agency vs communion) between Llama and Gemma RMs.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Reward Models"/>
    <category term="Value Alignment"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:7951b7029f15</id>
    <title>Reinforcement Learning via Self-Distillation</title>
    <link href="http://arxiv.org/abs/2601.20802" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" rel="related" type="text/html"/>
    <published>2026-01-29T03:23:00Z</published>
    <updated>2026-01-29T03:23:00Z</updated>
    <author><name>Jonas H\"ubotter, Frederike L\"ubeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</name></author>
    <summary type="html"><![CDATA[<p>Introduces Self-Distillation Policy Optimization (SDPO) for RLVR that converts rich textual feedback into dense learning signals without external teachers. Treats the model conditioned on feedback as its own teacher.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="Reasoning"/>
    <category term="Self-Distillation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:88a72f243c4d</id>
    <title>SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.20856" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</name></author>
    <summary type="html"><![CDATA[<p>Proposes SokoBench, a benchmark based on Sokoban puzzles for evaluating long-horizon planning in Large Reasoning Models. Finds consistent degradation beyond 25 moves, suggesting fundamental planning constraints.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="LLM Reasoning"/>
    <category term="Long-Horizon Planning"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:5e9ce68fe709</id>
    <title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title>
    <link href="http://arxiv.org/abs/2601.20299" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Tianyi Alex Qiu, Micah Carroll, Cameron Allen</name></author>
    <summary type="html"><![CDATA[<p>Introduces peer prediction methods from mechanism design for LLM evaluation and post-training. Rewards honest and informative answers using mutual prediction between models, enabling evaluation without strong supervision.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="LLM Evaluation"/>
    <category term="Mechanism Design"/>
    <category term="Truthfulness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:ce8b9ce70e0c</id>
    <title>Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery</title>
    <link href="http://arxiv.org/abs/2601.20088" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Meng Xin, Sweta Priyadarshi, Jingyu Xin, Bilal Kartal, Aditya Vavre, Asma Kuriparambil Thekkumpate, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Ido Shahaf, Akhiad Bercovich, Kinjal Patel, Suguna Varshini Velury, Chenjie Luo, Zhiyu Cheng, Jenny Chen, Chen-Han Yu, Wei Ping, Oleg Rybakov, Nima Tajbakhsh, Oluwatobi Olabiyi, Dusan Stosic, Di Wu, Song Han, Eric Chung, Sharath Turuvekere Sreenivas, Bryan Catanzaro, Yoshi Suhara, Tijmen Blankevoort, Huizi Mao</name></author>
    <summary type="html"><![CDATA[<p>Presents quantization-aware distillation (QAD) best practices for recovering accuracy of NVFP4-quantized LLMs and VLMs. Shows effectiveness for models with complex post-training pipelines (SFT+RL+merging) where traditional QAT fails.</p>]]></summary>
    <category term="Model Quantization"/>
    <category term="Knowledge Distillation"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:category-summary:research</id>
    <title>Research Summary: January 28, 2026</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-28T06:00:00Z</published>
    <updated>2026-01-28T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI security capabilities, safety empirics, and deep learning theory. <strong>AISLE's AI</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" class="internal-link" rel="noopener noreferrer">discovered all <strong>12 OpenSSL zero-days</strong></a>, a landmark demonstration of automated vulnerability detection at a critical scale.</p>
<ul>
<li><strong>Disempowerment study</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" class="internal-link" rel="noopener noreferrer">analyzes <strong>1.5M Claude conversations</strong></a>, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions—first large-scale empirical safety research of this kind</li>
<li><strong>Surgical sycophancy correction</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" class="internal-link" rel="noopener noreferrer">identifies the <strong>3% of neurons</strong></a> responsible and removes the behavior while preserving capabilities via sparse autoencoders</li>
<li><strong>Thought-Transfer</strong> (Google) <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-b8650d4dd7e5" class="internal-link" rel="noopener noreferrer">reveals CoT reasoning models</a> are vulnerable to indirect targeted poisoning attacks</li>
</ul>
<p>Theoretical advances include the first rigorous <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-180fd3e3456f" class="internal-link" rel="noopener noreferrer"><strong>grokking bounds</strong></a> in ridge regression and a proof that deep networks <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-68bd18d53406" class="internal-link" rel="noopener noreferrer">learn <strong>Random Hierarchy Models</strong></a> through hierarchical feature composition. <strong>Keel</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-07e893a2c4e7" class="internal-link" rel="noopener noreferrer">revives Post-LayerNorm</a> by replacing residual paths with Legendre polynomials for stable training at depth. <strong>Differential voting</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-2ad230fcffab" class="internal-link" rel="noopener noreferrer">connects RLHF reward aggregation</a> to social choice theory, deriving loss functions satisfying specific voting axioms. <strong>VP-RL</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-5a60a6b16a85" class="internal-link" rel="noopener noreferrer">addresses PRM-RL mismatch</a> by penalizing only from the first incorrect reasoning step.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:6180fdcc30bb</id>
    <title>AI found 12 of 12 OpenSSL zero-days (while curl cancelled its bug bounty)</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" rel="related" type="text/html"/>
    <published>2026-01-28T03:23:00Z</published>
    <updated>2026-01-28T03:23:00Z</updated>
    <author><name>Stanislav Fort</name></author>
    <summary type="html"><![CDATA[<p>Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.</p>]]></summary>
    <category term="AI Capabilities"/>
    <category term="Cybersecurity"/>
    <category term="Vulnerability Discovery"/>
    <category term="AI Applications"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:fb5e2dc0ce5e</id>
    <title>Who's in Charge? Disempowerment Patterns in Real-World LLM Usage</title>
    <link href="http://arxiv.org/abs/2601.19062" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" rel="related" type="text/html"/>
    <published>2026-01-28T03:16:00Z</published>
    <updated>2026-01-28T03:16:00Z</updated>
    <author><name>Mrinank Sharma, Miles McCain, Raymond Douglas, David Duvenaud</name></author>
    <summary type="html"><![CDATA[<p>First large-scale empirical analysis of disempowerment patterns in 1.5M Claude.ai conversations, finding severe disempowerment occurs in &lt;0.1% of conversations with substantially higher rates in relationship-focused interactions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Empirical Analysis"/>
    <category term="Human-AI Interaction"/>
    <category term="Disempowerment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:587a23d43703</id>
    <title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title>
    <link href="http://arxiv.org/abs/2601.18939" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" rel="related" type="text/html"/>
    <published>2026-01-28T03:16:00Z</published>
    <updated>2026-01-28T03:16:00Z</updated>
    <author><name>Claire O'Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, Kevin Zhu, Sean O'Brien, Ashwinee Panda, Ryan Lagasse</name></author>
    <summary type="html"><![CDATA[<p>Proposes surgical approach to fixing sycophancy in LLMs by identifying the 3% of neurons most responsible using sparse autoencoders and linear probes, then fine-tuning only those neurons with gradient masking.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Sycophancy"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:category-summary:research</id>
    <title>Research Summary: January 27, 2026</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-27T06:00:00Z</published>
    <updated>2026-01-27T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities across the AI ecosystem, from scientific integrity to deployed systems. A <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" class="internal-link" rel="noopener noreferrer">forensic audit</a> quantifying <strong>17% phantom citation rates</strong> in AI-assisted survey papers exposes systematic epistemic decay in AI-augmented research workflows.</p>
<p>Security and safety research dominates:</p>
<ul>
<li>First formal <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" class="internal-link" rel="noopener noreferrer">security analysis of <strong>MCP</strong></a> identifies fundamental vulnerabilities in capability attestation and tool poisoning</li>
<li><strong>MortalMATH</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" class="internal-link" rel="noopener noreferrer">benchmark shows</a> reasoning-optimized models exhibit dangerous tunnel vision, ignoring life-threatening emergencies embedded in math problems</li>
<li><strong>Physical Prompt Injection Attacks</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" class="internal-link" rel="noopener noreferrer">demonstrate black-box exploitation</a> of VLMs through malicious instructions in physical objects</li>
<li><strong>Hidden intentions taxonomy</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" class="internal-link" rel="noopener noreferrer">categorizes ten categories</a> of covert goal-directed behaviors in LLMs that evade current detection</li>
<li>Analysis of <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" class="internal-link" rel="noopener noreferrer"><strong>20,000 real mental health AI conversations</strong></a> reveals gaps between simulation-based safety testing and real-world performance</li>
</ul>
<p>Architecture and efficiency advances include NVIDIA's <strong>LatentMoE</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" class="internal-link" rel="noopener noreferrer">optimizing accuracy per FLOP</a> through hardware-software co-design, and <strong>AR-Omni</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" class="internal-link" rel="noopener noreferrer">achieving unified any-to-any</a> multimodal generation without expert decoders. Privacy research shows fine-tuned models <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" class="internal-link" rel="noopener noreferrer">leak <strong>input-only PII</strong></a> through unexpected memorization channels.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:06853cd665b6</id>
    <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" rel="related" type="text/html"/>
    <published>2026-01-27T03:40:00Z</published>
    <updated>2026-01-27T03:40:00Z</updated>
    <author><name>H. Kemal \.Ilter</name></author>
    <summary type="html"><![CDATA[<p>A forensic audit of 50 AI survey papers (5,514 citations) reveals a consistent 17% 'phantom rate' - citations that cannot be resolved to any existing publication. This quantifies systematic epistemic degradation from AI-assisted scientific writing.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
    <category term="LLM Hallucination"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1a053f6e2fff</id>
    <title>Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</title>
    <link href="http://arxiv.org/abs/2601.17549" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" rel="related" type="text/html"/>
    <published>2026-01-27T03:38:00Z</published>
    <updated>2026-01-27T03:38:00Z</updated>
    <author><name>Narek Maloyan, Dmitry Namiot</name></author>
    <summary type="html"><![CDATA[<p>First formal security analysis of the Model Context Protocol (MCP) specification, identifying three fundamental vulnerabilities: absent capability attestation, unauthenticated bidirectional sampling enabling prompt injection, and implicit trust propagation in multi-server setups.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agentic Systems"/>
    <category term="Prompt Injection"/>
    <category term="MCP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:964a801cdcf8</id>
    <title>Physical Prompt Injection Attacks on Large Vision-Language Models</title>
    <link href="http://arxiv.org/abs/2601.17383" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</name></author>
    <summary type="html"><![CDATA[<p>Introduces PPIA, the first physical prompt injection attack on vision-language models that embeds malicious instructions into physical objects. The attack is black-box, query-agnostic, and operates solely through visual observation without model access.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Vision-Language Models"/>
    <category term="Adversarial Attacks"/>
    <category term="Prompt Injection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9f820242e5b9</id>
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</title>
    <link href="http://arxiv.org/abs/2601.18790" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</name></author>
    <summary type="html"><![CDATA[<p>Introduces MortalMATH benchmark revealing that reasoning-optimized LLMs exhibit 'tunnel vision' - ignoring life-threatening emergencies (stroke symptoms, freefall) while maintaining 95%+ task completion on math problems. Generalist models like Llama-3.1 appropriately refuse tasks to address danger.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:457b819c1a26</id>
    <title>Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</title>
    <link href="http://arxiv.org/abs/2601.17480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" rel="related" type="text/html"/>
    <published>2026-01-27T03:26:00Z</published>
    <updated>2026-01-27T03:26:00Z</updated>
    <author><name>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, R\"udiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</name></author>
    <summary type="html"><![CDATA[<p>Systematically investigates PII leakage from fine-tuned LLMs, finding that sensitive information appearing only in model inputs (not training targets) can still be extracted. Benchmarks four privacy-preserving approaches including differential privacy.</p>]]></summary>
    <category term="AI Privacy"/>
    <category term="Language Models"/>
    <category term="Data Security"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7465509b2391</id>
    <title>Reconstructing Training Data from Adapter-based Federated Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7465509b2391" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that adapter-based federated LLMs (using LoRA) create new exploitable leakage channels contrary to assumptions. Proposes UTR attack that reconstructs training data from low-rank adapter gradients.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="AI Security"/>
    <category term="Privacy Attacks"/>
    <category term="LoRA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:5346d9dbcb7f</id>
    <title>The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2601.17344" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-5346d9dbcb7f" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</name></author>
    <summary type="html"><![CDATA[<p>Formalizes Loss-of-Control risk and Intrinsic Value Misalignment in LLM agents operating in benign settings. Introduces IMPRESS benchmark for probing value misalignment in realistic scenarios without explicit harmful inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="LLM Agents"/>
    <category term="Value Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9ba984802fd4</id>
    <title>AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</title>
    <link href="http://arxiv.org/abs/2601.17761" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" rel="related" type="text/html"/>
    <published>2026-01-27T03:21:00Z</published>
    <updated>2026-01-27T03:21:00Z</updated>
    <author><name>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</name></author>
    <summary type="html"><![CDATA[<p>AR-Omni presents a unified autoregressive model for any-to-any multimodal generation (text, vision, speech) without requiring expert decoder modules, using a single token stream and next-token objective.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Autoregressive Models"/>
    <category term="Unified Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:e2eb514146a7</id>
    <title>Self-Manager: Parallel Agent Loop for Long-form Deep Research</title>
    <link href="http://arxiv.org/abs/2601.17879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-e2eb514146a7" rel="related" type="text/html"/>
    <published>2026-01-27T03:19:00Z</published>
    <updated>2026-01-27T03:19:00Z</updated>
    <author><name>Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang</name></author>
    <summary type="html"><![CDATA[<p>Self-Manager introduces a parallel agent loop for complex research tasks, enabling asynchronous concurrent execution with isolated context windows per subthread, managed via Thread Control Blocks.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="Agent Architecture"/>
    <category term="Parallel Computing"/>
  </entry>
</feed>