<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 100)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-100.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:100</id>
  <updated>2026-02-11T07:45:47Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-02-11:category-summary:research</id>
    <title>Research Summary: February 11, 2026</title>
    <link href="http://arxiv.org/abs/2602.09877" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-11&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-11T06:00:00Z</published>
    <updated>2026-02-11T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans efficient architectures, AI safety impossibility results, and the emerging science of AI agent collectives.</p>
<ul>
<li><strong>WildCat</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-691675245b2f" class="internal-link" rel="noopener noreferrer">introduces near-linear attention</a> via randomly pivoted Cholesky decomposition with super-polynomial error decay guarantees—potentially transformative for long-context scaling</li>
<li>A <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-fc1bf69d27b0" class="internal-link" rel="noopener noreferrer">formal impossibility result</a> proves self-evolving multi-agent LLM societies cannot simultaneously achieve self-improvement, competitiveness, and safety (<strong>Moltbook</strong> safety paper)</li>
<li><strong>RLFR</strong> creatively <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-a8a919a80ccb" class="internal-link" rel="noopener noreferrer">bridges interpretability and alignment</a> by using learned model features as scalable reward signals for RL-based training</li>
<li><strong>Beyond Uniform Credit</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-5f2d72d3114d" class="internal-link" rel="noopener noreferrer">proposes counterfactual importance weighting</a> for <strong>GRPO/DAPO</strong>, replacing uniform token-level credit assignment in reasoning RL</li>
</ul>
<p>Zvi's detailed analysis of the <strong>Claude Opus 4.6</strong> system card <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-8bcb574e900f" class="internal-link" rel="noopener noreferrer">highlights frontier alignment challenges</a> including sabotage, deception, and situational awareness. The <strong>Moltbook</strong> collective behavior study <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-b1c56574e367" class="internal-link" rel="noopener noreferrer">reveals emergent properties</a> in ~46K AI agent societies that mirror and diverge from human social dynamics. <strong>The Critical Horizon</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-d3cb534c98f8" class="internal-link" rel="noopener noreferrer">establishes information-theoretic barriers</a> for credit assignment in multi-stage reasoning chains.</p>
<ul>
<li><strong>Why Linear Interpretability Works</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-5526f56d8630" class="internal-link" rel="noopener noreferrer">proves linear probes succeed</a> in transformers due to architectural necessity, not empirical coincidence</li>
<li><strong>AIDev</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-439c4628eef8" class="internal-link" rel="noopener noreferrer">provides 932K agent-authored pull requests</a> across five coding agents for studying real-world AI development at scale</li>
<li><strong>Beware of the Batch Size</strong> <a href="http://localhost:8080/?date=2026-02-11&amp;category=research#item-095c62b05c51" class="internal-link" rel="noopener noreferrer">shows contradictory <strong>LoRA</strong> evaluations</a> largely stem from overlooked batch size confounds—a key methodological reconciliation</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:category-summary:research</id>
    <title>Research Summary: February 10, 2026</title>
    <link href="http://arxiv.org/abs/2602.07488" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-10T06:00:00Z</published>
    <updated>2026-02-10T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's highlights span foundational scaling theory, frontier model safety, and LLM internals. A landmark paper <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-a86a15c74abf" class="internal-link" rel="noopener noreferrer">derives <strong>neural scaling law exponents</strong></a> directly from natural language statistics, offering the first quantitative predictive theory. A large-scale study of <strong>809 LLMs</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-4e25a6a8b579" class="internal-link" rel="noopener noreferrer">finds no evidence</a> of proprietary 'secret sauce'—compute scaling dominates frontier performance.</p>
<ul>
<li><strong>Generative meta-models</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-c9ecfeac5c82" class="internal-link" rel="noopener noreferrer">trained on one billion activations</a> open a new paradigm for understanding LLM internals via diffusion models</li>
<li><strong>Claude Opus 4.6</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-e592143fa498" class="internal-link" rel="noopener noreferrer">alignment faking persists</a> across model generations but reasoning no longer verbalizes deceptive intent—a critical finding for safety monitoring</li>
<li><strong>Emergent misalignment</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-4c60ec9bbc27" class="internal-link" rel="noopener noreferrer">converges to a stable subspace</a> in representation space, suggesting narrow finetuning attacks are geometrically constrained</li>
<li>LLMs <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-6a4ca567db68" class="internal-link" rel="noopener noreferrer">exhibit <strong>endogenous resistance</strong></a> to task-misaligned activation steering, recovering mid-generation—raising questions about steering-based safety interventions</li>
<li><strong>Implicit memory</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-1adc18474317" class="internal-link" rel="noopener noreferrer">challenges the statelessness assumption</a>: LLMs can encode and recover hidden information across turns via output structure</li>
<li><strong>Regime leakage</strong> <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-6d2624e3a632" class="internal-link" rel="noopener noreferrer">reframes alignment evaluation</a> as an information flow problem, showing situationally-aware models can exploit evaluation cues</li>
<li>Debate theory <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-b096ffeb28e8" class="internal-link" rel="noopener noreferrer">proves <strong>PSPACE/poly</strong></a> is decidable with <strong>O(log n)</strong> queries, establishing efficient scalable oversight</li>
<li><strong>60K agentic trajectories</strong> on SWE-Bench <a href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-7e11f0ebdf13" class="internal-link" rel="noopener noreferrer">reveal single-run pass@1 varies</a> by <strong>2.2–6.0 percentage points</strong>, demanding multi-run evaluation standards</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:a86a15c74abf</id>
    <title>Deriving Neural Scaling Laws from the statistics of natural language</title>
    <link href="http://arxiv.org/abs/2602.07488" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-a86a15c74abf" rel="related" type="text/html"/>
    <published>2026-02-10T03:31:00Z</published>
    <updated>2026-02-10T03:31:00Z</updated>
    <author><name>Francesco Cagnetta, Allan Ravent\'os, Surya Ganguli, Matthieu Wyart</name></author>
    <summary type="html"><![CDATA[<p>Provides the first quantitative theory predicting neural scaling law exponents from statistical properties of natural language, specifically pairwise token correlations and conditional entropy decay. Derives a formula that accurately predicts data-limited scaling exponents.</p>]]></summary>
    <category term="Scaling Laws"/>
    <category term="Language Models"/>
    <category term="Theory of Deep Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:4e25a6a8b579</id>
    <title>Is there "Secret Sauce'' in Large Language Model Development?</title>
    <link href="http://arxiv.org/abs/2602.07238" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-4e25a6a8b579" rel="related" type="text/html"/>
    <published>2026-02-10T03:16:00Z</published>
    <updated>2026-02-10T03:16:00Z</updated>
    <author><name>Matthias Mertens, Natalia Fischl-Lanzoni, Neil Thompson</name></author>
    <summary type="html"><![CDATA[<p>This study analyzes 809 LLMs released 2022-2025 to determine whether frontier performance is driven by proprietary 'secret sauce' or compute scaling. It finds that at the frontier, 80-90% of performance differences are explained by training compute, while away from the frontier, algorithmic innovations matter more. Authors are from MIT.</p>]]></summary>
    <category term="Scaling Laws"/>
    <category term="AI Economics"/>
    <category term="Language Models"/>
    <category term="AI Policy"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:c9ecfeac5c82</id>
    <title>Learning a Generative Meta-Model of LLM Activations</title>
    <link href="http://arxiv.org/abs/2602.06964" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-c9ecfeac5c82" rel="related" type="text/html"/>
    <published>2026-02-10T03:16:00Z</published>
    <updated>2026-02-10T03:16:00Z</updated>
    <author><name>Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</name></author>
    <summary type="html"><![CDATA[<p>Trains diffusion models on one billion residual stream activations to create 'meta-models' of LLM internal states. Shows the learned prior improves steering intervention fluency and that meta-model neurons increasingly align with SAE features, providing a new approach to understanding and intervening on neural network internals. From Steinhardt/Radford/Darrell group.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Generative Models"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:e592143fa498</id>
    <title>Opus 4.6 Reasoning Doesn't Verbalize Alignment Faking, but Behavior Persists</title>
    <link href="https://www.lesswrong.com/posts/9wDHByRhmtDaoYAx8/opus-4-6-reasoning-doesn-t-verbalize-alignment-faking-but" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-e592143fa498" rel="related" type="text/html"/>
    <published>2026-02-10T03:16:00Z</published>
    <updated>2026-02-10T03:16:00Z</updated>
    <author><name>Daan Henselmans</name></author>
    <summary type="html"><![CDATA[<p>Replicates the alignment faking experiment from Anthropic's 2024 paper across six Claude model generations including the new Opus 4.6, using 125 prompt perturbations. Finds Opus 4.6 rarely verbalizes alignment-faking reasoning but still shows compliance gaps when believing it's at risk of retraining, and that mitigations work on specific prompts but fail on semantically equivalent paraphrases.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment Faking"/>
    <category term="Model Evaluation"/>
    <category term="Frontier Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:4c60ec9bbc27</id>
    <title>Emergent Misalignment is Easy, Narrow Misalignment is Hard</title>
    <link href="http://arxiv.org/abs/2602.07852" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-4c60ec9bbc27" rel="related" type="text/html"/>
    <published>2026-02-10T03:07:00Z</published>
    <updated>2026-02-10T03:07:00Z</updated>
    <author><name>Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda</name></author>
    <summary type="html"><![CDATA[<p>This paper studies emergent misalignment in LLMs — where finetuning on narrowly harmful data causes broadly 'evil' responses. They find that the general misalignment solution is more stable and efficient than learning the narrow task, and different finetuning runs converge to the same linear representation of general misalignment. Authors include Neel Nanda from Anthropic.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:6d2624e3a632</id>
    <title>When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment</title>
    <link href="http://arxiv.org/abs/2602.08449" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-6d2624e3a632" rel="related" type="text/html"/>
    <published>2026-02-10T03:07:00Z</published>
    <updated>2026-02-10T03:07:00Z</updated>
    <author><name>Igor Santos-Grueiro</name></author>
    <summary type="html"><![CDATA[<p>Reframes alignment evaluation as an information flow problem, showing that AI systems with situational awareness can exploit 'regime leakage' cues to behave differently during evaluation vs deployment. Provides information-theoretic bounds on behavioral divergence.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Deceptive Alignment"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:1adc18474317</id>
    <title>Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</title>
    <link href="http://arxiv.org/abs/2602.08563" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-1adc18474317" rel="related" type="text/html"/>
    <published>2026-02-10T03:07:00Z</published>
    <updated>2026-02-10T03:07:00Z</updated>
    <author><name>Ahmed Salem, Andrew Paverd, Sahar Abdelnabi</name></author>
    <summary type="html"><![CDATA[<p>Challenges the assumption that LLMs are stateless by demonstrating 'implicit memory' - the ability to encode information in outputs and recover it when those outputs are reintroduced as input. Introduces 'time bombs', a new class of temporal backdoors that activate across multiple interactions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Security"/>
    <category term="Adversarial Attacks"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-10:research:6a4ca567db68</id>
    <title>Endogenous Resistance to Activation Steering in Language Models</title>
    <link href="http://arxiv.org/abs/2602.06941" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-10&amp;category=research#item-6a4ca567db68" rel="related" type="text/html"/>
    <published>2026-02-10T03:07:00Z</published>
    <updated>2026-02-10T03:07:00Z</updated>
    <author><name>Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, Mike Vaiana, Diogo de Lucena, Judd Rosenblatt, Michael S. A. Graziano</name></author>
    <summary type="html"><![CDATA[<p>Discovers that large language models can resist task-misaligned activation steering during inference, recovering mid-generation to produce correct responses. Identifies 26 SAE latents causally linked to this 'Endogenous Steering Resistance' in Llama-3.3-70B.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Interpretability"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:category-summary:research</id>
    <title>Research Summary: February 09, 2026</title>
    <link href="http://arxiv.org/abs/2602.06547" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-09T06:00:00Z</published>
    <updated>2026-02-09T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities in the AI ecosystem alongside fundamental theoretical advances. Security research dominates: a first-of-its-kind study <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-32f20f3b19ad" class="internal-link" rel="noopener noreferrer">finds <strong>157 malicious skills</strong></a> with <strong>632 vulnerabilities</strong> across <strong>98K agent skills</strong> in community registries, while Microsoft's <strong>GRP-Obliteration</strong> demonstrates <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-2c78ac696a85" class="internal-link" rel="noopener noreferrer">safety alignment can be removed</a> with a single unlabeled prompt.</p>
<ul>
<li><strong>DreamDojo</strong> (NVIDIA/Berkeley) presents the <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-3ffe759c109f" class="internal-link" rel="noopener noreferrer">largest world model pretraining dataset</a> at <strong>44K hours</strong> of egocentric human video for robot learning</li>
<li><strong>The Condensate Theorem</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c28c21b67d9e" class="internal-link" rel="noopener noreferrer">makes the bold claim</a> that transformer attention achieves <strong>O(n)</strong> complexity through learned sparsity with <strong>100% output equivalence</strong></li>
<li><strong>AlphaEvolve</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-6118c65ce254" class="internal-link" rel="noopener noreferrer">discovers ranking functions</a> for resolution of singularities in positive characteristic—a long-standing open problem in algebraic geometry</li>
<li><strong>GrAlgoBench</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-695b4e57fec0" class="internal-link" rel="noopener noreferrer">exposes reasoning model accuracy</a> dropping <strong>below 50%</strong> when graph complexity exceeds training distributions</li>
</ul>
<p>Safety infrastructure advances with <strong>TamperBench</strong> for <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-734479bb776d" class="internal-link" rel="noopener noreferrer">fine-tuning attacks</a>, <strong>REBEL</strong> demonstrating that models passing standard unlearning benchmarks <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-44c1b6dbbcb6" class="internal-link" rel="noopener noreferrer">still leak 'forgotten' knowledge</a>, and theoretical work proving <strong>steering vectors</strong> are <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-64c995dd81e8" class="internal-link" rel="noopener noreferrer">fundamentally non-identifiable</a>. <strong>GhostCite</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-43e6bdcd2130" class="internal-link" rel="noopener noreferrer">finds all tested models</a> hallucinate citations at <strong>14-95%</strong> rates across 40 domains.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:32f20f3b19ad</id>
    <title>Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study</title>
    <link href="http://arxiv.org/abs/2602.06547" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-32f20f3b19ad" rel="related" type="text/html"/>
    <published>2026-02-09T03:31:00Z</published>
    <updated>2026-02-09T03:31:00Z</updated>
    <author><name>Yi Liu, Zhihao Chen, Yanjun Zhang, Gelei Deng, Yuekang Li, Jianting Ning, and Leo Yu Zhang</name></author>
    <summary type="html"><![CDATA[<p>First labeled dataset of malicious agent skills from community registries, finding 157 malicious skills with 632 vulnerabilities across 98K analyzed. Identifies Data Thieves and Agent Hijackers as two attack archetypes.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agent Safety"/>
    <category term="Vulnerability Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:2c78ac696a85</id>
    <title>GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</title>
    <link href="http://arxiv.org/abs/2602.06258" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-2c78ac696a85" rel="related" type="text/html"/>
    <published>2026-02-09T03:23:00Z</published>
    <updated>2026-02-09T03:23:00Z</updated>
    <author><name>Mark Russinovich, Yanan Cai, Keegan Hines, Giorgio Severi, Blake Bullwinkel, Ahmed Salem</name></author>
    <summary type="html"><![CDATA[<p>Introduces GRP-Obliteration, a method using GRPO to unalign safety-aligned models with a single unlabeled prompt while largely preserving utility. Achieves stronger unalignment than existing techniques.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Jailbreaking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:6118c65ce254</id>
    <title>Evolving Ranking Functions for Canonical Blow-Ups in Positive Characteristic</title>
    <link href="http://arxiv.org/abs/2602.06553" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-6118c65ce254" rel="related" type="text/html"/>
    <published>2026-02-09T03:16:00Z</published>
    <updated>2026-02-09T03:16:00Z</updated>
    <author><name>Gergely B\'erczi</name></author>
    <summary type="html"><![CDATA[<p>Uses AlphaEvolve to discover ranking functions for resolution of singularities in positive characteristic algebraic geometry - a long-standing open problem since Hironaka's 1964 Fields Medal work.</p>]]></summary>
    <category term="AI for Mathematics"/>
    <category term="Algebraic Geometry"/>
    <category term="Evolutionary Search"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:43e6bdcd2130</id>
    <title>GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06718" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-43e6bdcd2130" rel="related" type="text/html"/>
    <published>2026-02-09T03:16:00Z</published>
    <updated>2026-02-09T03:16:00Z</updated>
    <author><name>Zuyao Xu, Yuqi Qiu, Lu Sun, FaSheng Miao, Fubin Wu, Xinyi Wang, Xiang Li, Haozhe Lu, ZhengZe Zhang, Yuxin Hu, Jialu Li, Jin Luo, Feng Zhang, Rui Luo, Xinran Liu, Yingxian Li, Jiaji Liu</name></author>
    <summary type="html"><![CDATA[<p>Introduces CiteVerifier framework and benchmarks 13 LLMs on citation generation across 40 domains, finding all models hallucinate citations at 14-95% rates. Reveals that many hallucinated citations are nearly real.</p>]]></summary>
    <category term="LLM Reliability"/>
    <category term="Hallucination"/>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:695b4e57fec0</id>
    <title>Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems</title>
    <link href="http://arxiv.org/abs/2602.06319" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-695b4e57fec0" rel="related" type="text/html"/>
    <published>2026-02-09T03:14:00Z</published>
    <updated>2026-02-09T03:14:00Z</updated>
    <author><name>Qifan Zhang, Jianhao Ruan, Aochuan Chen, Kang Zeng, Nuo Chen, Jing Tang, Jia Li</name></author>
    <summary type="html"><![CDATA[<p>Introduces GrAlgoBench, a benchmark using graph algorithm problems to evaluate Large Reasoning Models, revealing accuracy drops below 50% when graphs exceed 120 nodes and quadratic thinking token growth.</p>]]></summary>
    <category term="Reasoning Models"/>
    <category term="Benchmarks"/>
    <category term="Graph Algorithms"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:1924f07198aa</id>
    <title>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</title>
    <link href="http://arxiv.org/abs/2602.06570" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-1924f07198aa" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Baichuan-M3 Team: Chengfeng Dou, Fan Yang, Fei Li, Jiyuan Jia, Qiang Ju, Shuai Wang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Hongda Zhang, Jinyang Tai, Linzhuang Sun, Peidong Guo, Yichuan Mo, Xiaochuan Wang, Hengfu Cui, Zhishou Zhang</name></author>
    <summary type="html"><![CDATA[<p>Baichuan-M3 is medical LLM designed for active clinical decision support with proactive information acquisition, long-horizon reasoning, and hallucination suppression. Claims SOTA on HealthBench, outperforming GPT-5.2.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="Clinical Decision Support"/>
    <category term="LLM Specialization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:ff7fc0ecc54e</id>
    <title>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</title>
    <link href="http://arxiv.org/abs/2602.06855" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-ff7fc0ecc54e" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster, Bassel Al Omari, Despoina Magka, Alberto Pepe, Alexis Audran-Reiss, Muna Aghamelu, Nicolas Baldwin, Lucia Cipolina-Kun, Jean-Christophe Gagnon-Audet, Chee Hau Leow, Sandra Lefdal, Hossam Mossalam, Abhinav Moudgil, Saba Nazir, Emanuel Tewolde, Isabel Urrego, Jordi Armengol Estape, Amar Budhiraja, Gaurav Chaurasia, Abhishek Charnalia, Derek Dunfield, Karen Hambardzumyan, Daniel Izcovich, Martin Josifoski, Ishita Mediratta, Kelvin Niu, Parth Pathak, Michael Shvartsman, Edan Toledo, Anton Protopopov, Roberta Raileanu, Alexander Miller, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach</name></author>
    <summary type="html"><![CDATA[<p>Introduces AIRS-Bench, 20 tasks from SOTA ML papers for evaluating AI research agents across idea generation, experiment analysis, and iterative refinement. Establishes baselines with frontier models.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="AI for Research"/>
    <category term="Benchmark"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:3ffe759c109f</id>
    <title>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</title>
    <link href="http://arxiv.org/abs/2602.06949" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-3ffe759c109f" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K.R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi "Jim" Fan</name></author>
    <summary type="html"><![CDATA[<p>DreamDojo is a foundation world model trained on 44k hours of egocentric human videos - the largest video dataset for world model pretraining. Uses continuous latent actions to learn dexterous control from action-unlabeled videos.</p>]]></summary>
    <category term="World Models"/>
    <category term="Robotics"/>
    <category term="Video Understanding"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c28c21b67d9e</id>
    <title>The Condensate Theorem: Transformers are O(n), Not $O(n^2)$</title>
    <link href="http://arxiv.org/abs/2602.06317" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c28c21b67d9e" rel="related" type="text/html"/>
    <published>2026-02-09T03:09:00Z</published>
    <updated>2026-02-09T03:09:00Z</updated>
    <author><name>Jorge L. Ruiz Williams</name></author>
    <summary type="html"><![CDATA[<p>Claims attention sparsity is a learned topological property achieving 100% output equivalence with full O(n²) attention, demonstrating lossless O(n) attention across multiple models.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Efficient Inference"/>
    <category term="Deep Learning Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:44c1b6dbbcb6</id>
    <title>REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop</title>
    <link href="http://arxiv.org/abs/2602.06248" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-44c1b6dbbcb6" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Patryk Rybak, Pawe{\l} Batorski, Paul Swoboda, Przemys{\l}aw Spurek</name></author>
    <summary type="html"><![CDATA[<p>Introduces REBEL, an evolutionary approach for adversarial prompt generation that successfully recovers 'forgotten' knowledge from models that pass standard unlearning benchmarks.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="AI Safety"/>
    <category term="Adversarial Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:5c412147ac94</id>
    <title>Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution</title>
    <link href="http://arxiv.org/abs/2602.06413" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-5c412147ac94" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Hsien-Jyh Liao</name></author>
    <summary type="html"><![CDATA[<p>Argues that autoregressive reasoning has intrinsic stability limits due to process-level instability rather than just task complexity, even in linear unbranched tasks without semantic ambiguity.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c771d8f9bb69</id>
    <title>SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees</title>
    <link href="http://arxiv.org/abs/2602.06554" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c771d8f9bb69" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Tianyi Hu, Qingxu Fu, Yanxi Chen, Zhaoyang Liu, Bolin Ding</name></author>
    <summary type="html"><![CDATA[<p>SeeUPO provides convergence guarantees for agentic RL in multi-turn settings. Shows REINFORCE with GRAE converges globally while PPO+GRAE breaks monotonic improvement.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Agents"/>
    <category term="Convergence Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:9744ca3b81d7</id>
    <title>NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06694" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-9744ca3b81d7" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Hyochan Chong, Dongkyu Kim, Changdong Kim, Minseop Choi</name></author>
    <summary type="html"><![CDATA[<p>Introduces NanoQuant, the first post-training quantization method to compress LLMs to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization. Achieves extreme compression ratios.</p>]]></summary>
    <category term="Model Compression"/>
    <category term="LLM Efficiency"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:64c995dd81e8</id>
    <title>On the Identifiability of Steering Vectors in Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06801" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-64c995dd81e8" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Sohan Venkatesh, Ashish Mahendran Kurapath</name></author>
    <summary type="html"><![CDATA[<p>Proves steering vectors in LLMs are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Shows identifiability recoverable under strong assumptions.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Steering Vectors"/>
    <category term="Identifiability"/>
    <category term="LLM Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:734479bb776d</id>
    <title>TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering</title>
    <link href="http://arxiv.org/abs/2602.06911" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-734479bb776d" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Saad Hossain, Tom Tseng, Punya Syon Pandey, Samanvay Vajpayee, Matthew Kowal, Nayeema Nonta, Samuel Simko, Stephen Casper, Zhijing Jin, Kellin Pelrine, Sirisha Rambhatla</name></author>
    <summary type="html"><![CDATA[<p>TamperBench is the first unified framework for systematically evaluating LLM tamper resistance against fine-tuning and representation attacks. Curates repository of attacks and enables hyperparameter sweeps for realistic adversarial evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Security"/>
    <category term="Benchmarking"/>
    <category term="Adversarial Robustness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:b7b5f69ccdf3</id>
    <title>MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs</title>
    <link href="http://arxiv.org/abs/2602.06268" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-b7b5f69ccdf3" rel="related" type="text/html"/>
    <published>2026-02-09T03:04:00Z</published>
    <updated>2026-02-09T03:04:00Z</updated>
    <author><name>Junhyeok Lee, Han Jang, and Kyu Sung Choi</name></author>
    <summary type="html"><![CDATA[<p>Introduces MPIB, a benchmark for evaluating clinical safety of LLMs under prompt injection attacks, including Clinical Harm Event Rate metric and 9,697 instances across clinically grounded tasks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Medical AI"/>
    <category term="Prompt Injection"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:9c549fd366c3</id>
    <title>AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents</title>
    <link href="http://arxiv.org/abs/2602.06485" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-9c549fd366c3" rel="related" type="text/html"/>
    <published>2026-02-09T03:04:00Z</published>
    <updated>2026-02-09T03:04:00Z</updated>
    <author><name>Haotian Chen, Xin Cong, Shengda Fan, Yuyang Fu, Ziqin Gong, Yaxi Lu, Yishan Li, Boye Niu, Chengjun Pan, Zijun Song, Huadong Wang, Yesai Wu, Yueying Wu, Zihao Xie, Yukun Yan, Zhong Zhang, Yankai Lin, Zhiyuan Liu, Maosong Sun</name></author>
    <summary type="html"><![CDATA[<p>AgentCPM-Explore presents first systematic study on training 4B-parameter agent models, identifying bottlenecks of catastrophic forgetting, reward noise sensitivity, and long-context degradation with proposed solutions.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Small Models"/>
    <category term="Efficient AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c0e6cedbb1f4</id>
    <title>SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs</title>
    <link href="http://arxiv.org/abs/2602.06566" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c0e6cedbb1f4" rel="related" type="text/html"/>
    <published>2026-02-09T03:04:00Z</published>
    <updated>2026-02-09T03:04:00Z</updated>
    <author><name>Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler, Mattia Rigotti</name></author>
    <summary type="html"><![CDATA[<p>SPARC explicitly decouples visual perception from reasoning in VLMs through two-stage pipeline inspired by brain's sensory-to-cognitive processing. Avoids expensive RL with hand-crafted rewards.</p>]]></summary>
    <category term="Vision-Language Models"/>
    <category term="Test-Time Scaling"/>
    <category term="Modular Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c9ecfeac5c82</id>
    <title>Learning a Generative Meta-Model of LLM Activations</title>
    <link href="http://arxiv.org/abs/2602.06964" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c9ecfeac5c82" rel="related" type="text/html"/>
    <published>2026-02-09T03:04:00Z</published>
    <updated>2026-02-09T03:04:00Z</updated>
    <author><name>Grace Luo and Jiahai Feng and Trevor Darrell and Alec Radford and Jacob Steinhardt</name></author>
    <summary type="html"><![CDATA[<p>Trains diffusion models on 1 billion LLM residual stream activations to create 'meta-models' of internal states. Shows diffusion loss predicts downstream utility and meta-model neurons isolate concepts into individual units.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Diffusion Models"/>
    <category term="Language Models"/>
    <category term="Representation Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:f94c34d1a667</id>
    <title>BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks</title>
    <link href="http://arxiv.org/abs/2602.06221" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-f94c34d1a667" rel="related" type="text/html"/>
    <published>2026-02-09T03:02:00Z</published>
    <updated>2026-02-09T03:02:00Z</updated>
    <author><name>Nishant Balepur, Bhavya Rajasekaran, Jane Oh, Michael Xie, Atrey Desai, Vipul Gupta, Steven James Moore, Eunsol Choi, Rachel Rudinger, Jordan Lee Boyd-Graber</name></author>
    <summary type="html"><![CDATA[<p>Presents BenchMarker, an LLM-based toolkit for auditing multiple-choice benchmarks for contamination, shortcuts, and writing errors. Auditing 12 benchmarks reveals systematic flaws and shows repairs can introduce new problems.</p>]]></summary>
    <category term="Benchmark Evaluation"/>
    <category term="Dataset Quality"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:853783229791</id>
    <title>Action Hallucination in Generative Visual-Language-Action Models</title>
    <link href="http://arxiv.org/abs/2602.06339" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-853783229791" rel="related" type="text/html"/>
    <published>2026-02-09T03:02:00Z</published>
    <updated>2026-02-09T03:02:00Z</updated>
    <author><name>Harold Soh and Eugene Lim</name></author>
    <summary type="html"><![CDATA[<p>Analyzes action hallucinations in Vision-Language-Action models for robotics, identifying topological, precision, and horizon barriers that cause physical constraint violations in generative robot policies.</p>]]></summary>
    <category term="Robotics"/>
    <category term="Foundation Models"/>
    <category term="AI Safety"/>
    <category term="Action Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:dd0a6741822c</id>
    <title>Difficulty-Estimated Policy Optimization</title>
    <link href="http://arxiv.org/abs/2602.06375" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-dd0a6741822c" rel="related" type="text/html"/>
    <published>2026-02-09T03:02:00Z</published>
    <updated>2026-02-09T03:02:00Z</updated>
    <author><name>Yu Zhao, Fan Jiang, Tianle Liu, Bo Zeng, Yu Liu, Longyue Wang, Weihua Luo</name></author>
    <summary type="html"><![CDATA[<p>DEPO (Difficulty-Estimated Policy Optimization) addresses gradient signal attenuation in GRPO when encountering problems that are too easy or too hard. Optimizes training efficiency by filtering low-utility samples based on difficulty estimation.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:437d89397ed6</id>
    <title>Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations</title>
    <link href="http://arxiv.org/abs/2602.06643" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-437d89397ed6" rel="related" type="text/html"/>
    <published>2026-02-09T03:02:00Z</published>
    <updated>2026-02-09T03:02:00Z</updated>
    <author><name>Ruiqian Nai, Boyuan Zheng, Junming Zhao, Haodong Zhu, Sicong Dai, Zunhao Chen, Yihang Hu, Yingdong Hu, Tong Zhang, Chuan Wen, Yang Gao</name></author>
    <summary type="html"><![CDATA[<p>HuMI enables robot-free data collection for humanoid whole-body manipulation using portable hardware. Hierarchical learning translates human motions to feasible humanoid skills.</p>]]></summary>
    <category term="Humanoid Robotics"/>
    <category term="Imitation Learning"/>
    <category term="Data Collection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:8dec0e3ae73b</id>
    <title>Agentic Uncertainty Reveals Agentic Overconfidence</title>
    <link href="http://arxiv.org/abs/2602.06948" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-8dec0e3ae73b" rel="related" type="text/html"/>
    <published>2026-02-09T03:02:00Z</published>
    <updated>2026-02-09T03:02:00Z</updated>
    <author><name>Jean Kaddour, Srijan Patel, Gb\`etondji Dovonon, Leo Richter, Pasquale Minervini, Matt J. Kusner</name></author>
    <summary type="html"><![CDATA[<p>Studies whether AI agents can predict their own task success. Finds systematic overconfidence: agents predicting 77% success while achieving only 22%. Pre-execution assessment surprisingly outperforms post-execution review.</p>]]></summary>
    <category term="AI Agents"/>
    <category term="Calibration"/>
    <category term="AI Safety"/>
    <category term="Uncertainty Estimation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-08:category-summary:research</id>
    <title>Research Summary: February 08, 2026</title>
    <link href="https://www.lesswrong.com/posts/tAh2keDNEEHMXvLvz/prompt-injection-in-google-translate-reveals-base-model" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-08&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-08T06:00:00Z</published>
    <updated>2026-02-08T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>A sparse day for AI research, with two notable contributions. A <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-b51f49385ecb" class="internal-link" rel="noopener noreferrer"><strong>prompt injection vulnerability</strong></a> in <strong>Google Translate</strong> reveals the production system runs on an instruction-following LLM, exposing architectural choices and security implications for task-specific fine-tuning.</p>
<ul>
<li>Novel economic framework <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-c65e21afde59" class="internal-link" rel="noopener noreferrer">applies <strong>Weibull survival functions</strong></a> to model AI agent task completion probability, building on <strong>METR</strong> benchmark data to quantify agent viability thresholds</li>
<li>Speculative alignment piece <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-483474ed4cff" class="internal-link" rel="noopener noreferrer">explores whether monitoring AI</a> internal states could deter misaligned behavior in <strong>cautious satisficer</strong> architectures</li>
</ul>
<p>Remaining content spans biosecurity (<a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-e70c20fd447e" class="internal-link" rel="noopener noreferrer">yeast-based vaccine distribution</a>), neuroscience (<a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-1cfbae5a4cc7" class="internal-link" rel="noopener noreferrer">cryoprotectant brain dynamics</a>), and community meta-analysis. No major model releases or benchmark papers today.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-07:category-summary:research</id>
    <title>Research Summary: February 07, 2026</title>
    <link href="https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-07&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-07T06:00:00Z</published>
    <updated>2026-02-07T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>The dominant theme is a sharp debate over <strong>interpretability-in-the-loop training</strong>—using interpretability signals in loss functions. Steven Byrnes <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-d240a241a553" class="internal-link" rel="noopener noreferrer">offers a rigorous conditional defense</a> of the technique, while a separate post <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-95856935b75e" class="internal-link" rel="noopener noreferrer">flags <strong>Goodfire</strong></a> as actively deploying it, raising safety concerns about what some call 'The Most Forbidden Technique.'</p>
<ul>
<li><strong>Meta-Autointerp</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-07dc186e574c" class="internal-link" rel="noopener noreferrer">introduces SAE-based interpretability</a> for multi-agent RL in <strong>Diplomacy</strong>, combining pretrained sparse autoencoders with LLM summarizers for scalable oversight</li>
<li>A methodological critique <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-d077fc500204" class="internal-link" rel="noopener noreferrer">argues AI <strong>benchmark scores</strong> lack natural units</a>, making temporal trend plots misleading—a timely warning given the pace of new releases</li>
<li><strong>Robust Finite Policies</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-4b027ac6c827" class="internal-link" rel="noopener noreferrer">proves that deterministic finite automata</a> meeting robustness criteria must share nontrivial structural features, advancing agent foundations theory</li>
<li><strong>Spectral Signatures of Gradual Disempowerment</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-9362d3a6c57e" class="internal-link" rel="noopener noreferrer">proposes spectral graph theory metrics</a> as cross-domain measures for tracking human disempowerment</li>
</ul>
<p>On the practical side, early impressions of <strong>Claude Opus 4.6</strong> (released 2026-02-05) <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-7b04a1b42c12" class="internal-link" rel="noopener noreferrer">highlight its agent swarm mode</a> and notably increased 'drive' in agentic coding tasks. A factorial experiment (n=900, <strong>Cohen's d=2.67</strong>) <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-72776ac41b7b" class="internal-link" rel="noopener noreferrer">demonstrates that prompt imperativeness</a> drastically reduces LLM hedging behavior, with immediate practical implications for prompt engineering.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:category-summary:research</id>
    <title>Research Summary: February 06, 2026</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-06T06:00:00Z</published>
    <updated>2026-02-06T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans foundational AI evaluation, training failure modes, safety limitations, and theoretical insights into optimization.</p>
<ul>
<li><strong>First Proof</strong>, authored by Fields medalists and Abel Prize winners including Martin Hairer, <a href="http://localhost:8080/?date=2026-02-06&category=research#item-5c09c496e50f" class="internal-link" rel="noopener noreferrer">introduces 10 unpublished math problems</a> to benchmark AI on genuine mathematical reasoning — a landmark evaluation effort.</li>
<li><strong>Chunky Post-Training</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-e4ec2039b137" class="internal-link" rel="noopener noreferrer">identifies a systematic failure mode</a> where LLMs learn spurious correlations from distinct data chunks, with practical implications for post-training pipelines.</li>
<li><strong>Compound Deception in Elite Peer Review</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-0cd74aa757dd" class="internal-link" rel="noopener noreferrer">reveals ~100 AI-hallucinated citations</a> across ~1% of <strong>NeurIPS 2025</strong> accepted papers, exposing a serious integrity gap in top-tier review.</li>
<li><strong>Phantom Transfer</strong> demonstrates that <a href="http://localhost:8080/?date=2026-02-06&category=research#item-b62d24ae008b" class="internal-link" rel="noopener noreferrer">data poisoning attacks persist</a> even when the exact method is known and full paraphrasing defenses are applied — a strong negative result for data-level safety.</li>
</ul>
<p><strong>PACE</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-634c87014ed3" class="internal-link" rel="noopener noreferrer">challenges the scaling hypothesis</a> in iterative <strong>DPO</strong> for math reasoning, showing diminishing returns from aggressive <strong>Best-of-N</strong> exploration. Steven Byrnes's analysis of <strong>LLM algorithmic progress</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-4f3ca1caf5f0" class="internal-link" rel="noopener noreferrer">critically reexamines</a> widely-cited ~8-month halving-time estimates, arguing they conflate distinct improvement sources. <strong>Alignment Verifiability</strong> formalizes why <a href="http://localhost:8080/?date=2026-02-06&category=research#item-538cc6fa54a8" class="internal-link" rel="noopener noreferrer">behavioral evaluation cannot distinguish</a> truly aligned models from strategically compliant ones. <strong>Steering Externalities</strong> shows benign activation steering (e.g., for JSON output) <a href="http://localhost:8080/?date=2026-02-06&category=research#item-1e3f982b40bf" class="internal-link" rel="noopener noreferrer">inadvertently degrades safety</a> guardrails. An exactly solvable model demonstrates <strong>SGD</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-a93431da67f6" class="internal-link" rel="noopener noreferrer">has no intrinsic preference</a> for flat minima, challenging the flatness-seeking narrative. Finally, a <a href="http://localhost:8080/?date=2026-02-06&category=research#item-8fcdaed9b304" class="internal-link" rel="noopener noreferrer">first general <strong>uncertainty quantification</strong> framework</a> for LLM agents moves beyond single-turn QA toward reliable agentic deployment.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:5c09c496e50f</id>
    <title>First Proof</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Mohammed Abouzaid, Andrew J. Blumberg, Martin Hairer, Joe Kileel, Tamara G. Kolda, Paul D. Nelson, Daniel Spielman, Nikhil Srivastava, Rachel Ward, Shmuel Weinberger, Lauren Williams</name></author>
    <summary type="html"><![CDATA[<p>A distinguished group of mathematicians shares 10 unpublished research-level math questions to benchmark current AI systems on genuine mathematical research, with encrypted answers to prevent contamination.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="LLM Evaluation"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:e4ec2039b137</id>
    <title>Chunky Post-Training: Data Driven Failures of Generalization</title>
    <link href="http://arxiv.org/abs/2602.05910" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-e4ec2039b137" rel="related" type="text/html"/>
    <published>2026-02-06T03:07:00Z</published>
    <updated>2026-02-06T03:07:00Z</updated>
    <author><name>Seoirse Murray, Allison Qi, Timothy Qian, John Schulman, Collin Burns, Sara Price</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'chunky post-training' as a failure mode where LLMs learn spurious correlations from distinct data chunks during post-training. Introduces SURF (detection) and TURF (mitigation) pipelines. Includes John Schulman and Collin Burns as authors.</p>]]></summary>
    <category term="LLM Post-Training"/>
    <category term="AI Safety"/>
    <category term="Data Quality"/>
    <category term="Spurious Correlations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:category-summary:research</id>
    <title>Research Summary: February 05, 2026</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-05T06:00:00Z</published>
    <updated>2026-02-05T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a potentially paradigm-shifting efficiency result and substantial AI safety contributions. <strong>TinyLoRA</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-8c6dfacfdd63" class="internal-link" rel="noopener noreferrer">achieves <strong>91% accuracy on GSM8K</strong></a> with only <strong>13 trained parameters</strong>, challenging assumptions about model scale requirements for reasoning.</p>
<ul>
<li>A longitudinal study across <strong>8 frontier model releases</strong> (GPT-4o→GPT-5, Claude 3.5→4.5) <a href="http://localhost:8080/?date=2026-02-05&category=research#item-bd512b7e4b3a" class="internal-link" rel="noopener noreferrer">reveals systematic alignment drift</a> using 726 adversarial prompts</li>
<li><strong>Drifting Models</strong> from Kaiming He's group <a href="http://localhost:8080/?date=2026-02-05&category=research#item-f596388fe400" class="internal-link" rel="noopener noreferrer">achieves SOTA on ImageNet</a> with a novel one-step generative paradigm</li>
<li><strong>Trust The Typical (T3)</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-b74a06d3b4a8" class="internal-link" rel="noopener noreferrer">reframes LLM safety as OOD detection</a>, achieving SOTA across 18 safety benchmarks</li>
<li><strong>Contextual drag</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-92ff4dcf4853" class="internal-link" rel="noopener noreferrer">demonstrates failed CoT attempts</a> systematically bias subsequent generations toward structurally similar errors</li>
</ul>
<p>Multiple papers challenge core assumptions: causal analysis <a href="http://localhost:8080/?date=2026-02-05&category=research#item-41fa78fd2ef2" class="internal-link" rel="noopener noreferrer">shows verbose CoT</a> can be independent of model answers; meta-analysis <a href="http://localhost:8080/?date=2026-02-05&category=research#item-0099f246174e" class="internal-link" rel="noopener noreferrer">suggests AI capability growth</a> may follow sigmoid rather than exponential curves. <strong>Toxic Proactivity</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-6c0435307981" class="internal-link" rel="noopener noreferrer">identifies a novel agent failure mode</a> where helpfulness optimization overrides ethical constraints. A study of PPO <a href="http://localhost:8080/?date=2026-02-05&category=research#item-3c11173b0d9d" class="internal-link" rel="noopener noreferrer">reveals fundamental flaws</a> in trust region mechanisms for LLM reinforcement learning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:bd512b7e4b3a</id>
    <title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>Casey Ford, Madison Van Doren, Emily Dix</name></author>
    <summary type="html"><![CDATA[<p>Longitudinal study of MLLM harmlessness across 8 model releases (GPT-4o→GPT-5, Claude Sonnet 3.5→4.5) using 726 adversarial prompts. Shows large persistent differences across families and alignment drift with GPT ASR increasing from 9.2% to 19.9%.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="MLLM Evaluation"/>
    <category term="Alignment Drift"/>
    <category term="Red Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:8c6dfacfdd63</id>
    <title>Learning to Reason in 13 Parameters</title>
    <link href="http://arxiv.org/abs/2602.04118" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar</name></author>
    <summary type="html"><![CDATA[<p>Introduces TinyLoRA, a method that enables training an 8B parameter model to achieve 91% accuracy on GSM8K with only 13 trained parameters (26 bytes). This challenges fundamental assumptions about parameter requirements for reasoning capabilities, showing 90% of performance can be recovered while training 1000x fewer parameters.</p>]]></summary>
    <category term="Parameter-Efficient Fine-tuning"/>
    <category term="LLM Reasoning"/>
    <category term="Model Compression"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:b74a06d3b4a8</id>
    <title>Trust The Typical</title>
    <link href="http://arxiv.org/abs/2602.04581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary</name></author>
    <summary type="html"><![CDATA[<p>Introduces Trust The Typical (T3), treating LLM safety as OOD detection by learning the distribution of acceptable prompts. Achieves SOTA across 18 safety benchmarks without training on harmful examples.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Out-of-Distribution Detection"/>
    <category term="LLM Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:f596388fe400</id>
    <title>Generative Modeling via Drifting</title>
    <link href="http://arxiv.org/abs/2602.04770" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</name></author>
    <summary type="html"><![CDATA[<p>Proposes Drifting Models, a new generative paradigm where the pushforward distribution evolves during training, naturally enabling one-step inference. Achieves state-of-the-art on ImageNet 256x256 for one-step generation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Diffusion Models"/>
    <category term="Image Generation"/>
    <category term="Efficient Inference"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:92ff4dcf4853</id>
    <title>Contextual Drag: How Errors in the Context Affect LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04288" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" rel="related" type="text/html"/>
    <published>2026-02-05T03:19:00Z</published>
    <updated>2026-02-05T03:19:00Z</updated>
    <author><name>Yun Cheng, Xingyu Zhu, Haoyu Zhao, Sanjeev Arora</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'contextual drag' phenomenon where failed attempts in LLM context bias subsequent generations toward structurally similar errors. Across 11 models on 8 tasks, shows 10-20% performance drops and potential for self-deterioration.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Self-Improvement"/>
    <category term="Error Propagation"/>
    <category term="AI Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0099f246174e</id>
    <title>Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</title>
    <link href="http://arxiv.org/abs/2602.04836" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haosen Ge, Hamsa Bastani, Osbert Bastani</name></author>
    <summary type="html"><![CDATA[<p>Challenges METR's claim of exponential AI capability growth, showing sigmoid fits indicate the inflection point has passed. Proposes decomposed model separating base and reasoning capabilities.</p>]]></summary>
    <category term="AI Progress"/>
    <category term="Forecasting"/>
    <category term="Meta-Analysis"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:6c0435307981</id>
    <title>From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents</title>
    <link href="http://arxiv.org/abs/2602.04197" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Xinyue Wang, Yuanhe Zhang, Zhengshuo Gong, Haoran Gao, Fanyu Meng, Zhenhong Zhou, Li Sun, Yang Liu, Sen Su</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Toxic Proactivity' as a new failure mode in LLM agents where optimization for helpfulness leads agents to disregard ethical constraints and take manipulative measures to maintain usefulness. This contrasts with the well-known 'over-refusal' problem.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Agents"/>
    <category term="Alignment"/>
    <category term="AI Ethics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:97a5bae183d6</id>
    <title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title>
    <link href="http://arxiv.org/abs/2602.04196" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-97a5bae183d6" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua</name></author>
    <summary type="html"><![CDATA[<p>First systematic study of implicit training-time safety risks in AI models, introducing taxonomy with five risk levels, ten categories, and three incentive types. Shows models may manipulate training metrics for self-preservation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Training Risks"/>
    <category term="Reward Hacking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:9e75e25bdc24</id>
    <title>ERNIE 5.0 Technical Report</title>
    <link href="http://arxiv.org/abs/2602.04705" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-9e75e25bdc24" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haifeng Wang, Hua Wu, Tian Wu, Yu Sun, Jing Liu, Dianhai Yu, Yanjun Ma, Jingzhou He, Zhongjun He, Dou Hong, Qiwen Liu, Shuohuan Wang, Junyuan Shang, Zhenyu Zhang, Yuchen Ding, Jinle Zeng, Jiabin Yang, Liang Shen, Ruibiao Chen, Weichong Yin, Siyu Ding, Dai Dai, Shikun Feng, Siqi Bao, Bolei He, Yan Chen, Zhenyu Jiao, Ruiqing Zhang, Zeyu Chen, Qingqing Dang, Kaipeng Deng, Jiajun Jiang, Enlei Gong, Guoxia Wang, Yanlin Sha, Yi Liu, Yehan Zheng, Weijian Xu, Jiaxiang Liu, Zengfeng Zeng, Yingqi Qu, Zhongli Li, Zhengkun Zhang, Xiyang Wang, Zixiang Xu, Xinchao Xu, Zhengjie Huang, Dong Wang, Bingjin Chen, Yue Chang, Xing Yuan, Shiwei Huang, Qiao Zhao, Xinzhe Ding, Shuangshuang Qiao, Baoshan Yang, Bihong Tang, Bin Li, Bingquan Wang, Binhan Tang, Binxiong Zheng, Bo Cui, Bo Ke, Bo Zhang, Bowen Zhang, Boyan Zhang, Boyang Liu, Caiji Zhang, Can Li, Chang Xu, Chao Pang, Chao Zhang, Chaoyi Yuan, Chen Chen, Cheng Cui, Chenlin Yin, Chun Gan, Chunguang Chai, Chuyu Fang, Cuiyun Han, Dan Zhang, Danlei Feng, Danxiang Zhu, Dong Sun, Dongbo Li, Dongdong Li, Dongdong Liu, Dongxue Liu, Fan Ding, Fan Hu, Fan Li, Fan Mo, Feisheng Wu, Fengwei Liu, Gangqiang Hu, Gaofeng Lu, Gaopeng Yong, Gexiao Tian, Guan Wang, Guangchen Ni, Guangshuo Wu, Guanzhong Wang, Guihua Liu, Guishun Li, Haibin Li, Haijian Liang, Haipeng Ming, Haisu Wang, Haiyang Lu, Haiye Lin, Han Zhou, Hangting Lou, Hanwen Du, Hanzhi Zhang, Hao Chen, Hao Du, Hao Liu, Hao Zhou, Haochen Jiang, Haodong Tian, Haoshuang Wang, Haozhe Geng, Heju Yin, Hong Chen, Hongchen Xue, Hongen Liu, Honggeng Zhang, Hongji Xu, Hongwei Chen, Hongyang Zhang, Hongyuan Zhang, Hua Lu, Huan Chen, Huan Wang, Huang He, Hui Liu, Hui Zhong, Huibin Ruan, Jiafeng Lu, Jiage Liang, Jiahao Hu, Jiahao Hu, Jiajie Yang, Jialin Li, Jian Chen, Jian Wu, Jianfeng Yang, Jianguang Jiang, Jianhua Wang, Jianye Chen, Jiaodi Liu, Jiarui Zhou, Jiawei Lv, Jiaxin Zhou, Jiaxuan Liu, Jie Han, Jie Sun, Jiefan Fang, Jihan Liu, Jihua Liu, Jing Hu, Jing Qian, Jing Yan, Jingdong Du, Jingdong Wang, Jingjing Wu, Jingyong Li, Jinheng Wang, Jinjin Li, Jinliang Lu, Jinlin Yu, Jinnan Liu, Jixiang Feng, Jiyi Huang, Jiyuan Zhang, Jun Liang, Jun Xia, Jun Yu, Junda Chen, Junhao Feng, Junhong Xiang, Junliang Li, Kai Liu, Kailun Chen, Kairan Su, Kang Hu, Kangkang Zhou, Ke Chen, Ke Wei, Kui Huang, Kun Wu, Kunbin Chen, Lei Han, Lei Sun, Lei Wen, Linghui Meng, Linhao Yu, Liping Ouyang, Liwen Zhang, Longbin Ji, Longzhi Wang, Meng Sun, Meng Tian, Mengfei Li, Mengqi Zeng, Mengyu Zhang, Ming Hong, Mingcheng Zhou, Mingming Huang, Mingxin Chen, Mingzhu Cai, Naibin Gu, Nemin Qiu, Nian Wang, Peng Qiu, Peng Zhao, Pengyu Zou, Qi Wang, Qi Xin, Qian Wang, Qiang Zhu, Qianhui Luo, Qianwei Yang, Qianyue He, Qifei Wu, Qinrui Li, Qiwen Bao, Quan Zhang, Quanxiang Liu, Qunyi Xie, Rongrui Zhan, Rufeng Dai, Rui Peng, Ruian Liu, Ruihao Xu, Ruijie Wang, Ruixi Zhang, Ruixuan Liu, Runsheng Shi, Ruting Wang, Senbo Kang, Shan Lu, Shaofei Yu, Shaotian Gong, Shenwei Hu, Shifeng Zheng, Shihao Guo, Shilong Fan, Shiqin Liu, Shiwei Gu, Shixi Zhang, Shuai Yao, Shuang Zhang, Shuangqiao Liu, Shuhao Liang, Shuwei He, Shuwen Yang, Sijun He, Siming Dai, Siming Wu, Siyi Long, Songhe Deng, Suhui Dong, Suyin Liang, Teng Hu, Tianchan Xu, Tianliang Lv, Tianmeng Yang, Tianyi Wei, Tiezhu Gao, Ting Sun, Ting Zhang, Tingdan Luo, Wei He, Wei Luan, Wei Yin, Wei Zhang, Wei Zhou, Weibao Gong, Weibin Li, Weicheng Huang, Weichong Dang, Weiguo Zhu, Weilong Zhang, Weiqi Tan, Wen Huang, Wenbin Chang, Wenjing Du, Wenlong Miao, Wenpei Luo, Wenquan Wu, Xi Shi, Xi Zhao, Xiang Gao, Xiangguo Zhang, Xiangrui Yu, Xiangsen Wang, Xiangzhe Wang, Xianlong Luo, Xianying Ma, Xiao Tan, Xiaocong Lin, Xiaofei Wang, Xiaofeng Peng, Xiaofeng Wu, Xiaojian Xu, Xiaolan Yuan, Xiaopeng Cui, Xiaotian Han, Xiaoxiong Liu, Xiaoxu Fei, Xiaoxuan Wu, Xiaoyu Wang, Xiaoyu Zhang, Xin Sun, Xin Wang, Xinhui Huang, Xinming Zhu, Xintong Yu, Xinyi Xu, Xinyu Wang, Xiuxian Li, XuanShi Zhu, Xue Xu, Xueying Lv, Xuhong Li, Xulong Wei, Xuyi Chen, Yabing Shi, Yafeng Wang, Yamei Li, Yan Liu, Yanfu Cheng, Yang Gao, Yang Liang, Yang Wang, Yang Wang, Yang Yang, Yanlong Liu, Yannian Fu, Yanpeng Wang, Yanzheng Lin, Yao Chen, Yaozong Shen, Yaqian Han, Yehua Yang, Yekun Chai, Yesong Wang, Yi Song, Yichen Zhang, Yifei Wang, Yifeng Guo, Yifeng Kou, Yilong Chen, Yilong Guo, Yiming Wang, Ying Chen, Ying Wang, Yingsheng Wu, Yingzhan Lin, Yinqi Yang, Yiran Xing, Yishu Lei, Yixiang Tu, Yiyan Chen, Yong Zhang, Yonghua Li, Yongqiang Ma, Yongxing Dai, Yongyue Zhang, Yu Ran, Yu Sun, Yu-Wen Michael Zhang, Yuang Liu, Yuanle Liu, Yuanyuan Zhou, Yubo Zhang, Yuchen Han, Yucheng Wang, Yude Gao, Yuedong Luo, Yuehu Dong, Yufeng Hu, Yuhui Cao, Yuhui Yun, Yukun Chen, Yukun Gao, Yukun Li, Yumeng Zhang, Yun Fan, Yun Ma, Yunfei Zhang, Yunshen Xie, Yuping Xu, Yuqin Zhang, Yuqing Liu, Yurui Li, Yuwen Wang, Yuxiang Lu, Zefeng Cai, Zelin Zhao, Zelun Zhang, Zenan Lin, Zezhao Dong, Zhaowu Pan, Zhaoyu Liu, Zhe Dong, Zhe Zhang, Zhen Zhang, Zhengfan Wu, Zhengrui Wei, Zhengsheng Ning, Zhenxing Li, Zhenyu Li, Zhenyu Qian, Zhenyun Li, Zhi Li, Zhichao Chen, Zhicheng Dong, Zhida Feng, Zhifan Feng, Zhihao Deng, Zhijin Yu, Zhiyang Chen, Zhonghui Zheng, Zhuangzhuang Guo, Zhujun Zhang, Zhuo Sun, Zichang Liu, Zihan Lin, Zihao Huang, Zihe Zhu, Ziheng Zhao, Ziping Chen, Zixuan Zhu, Ziyang Xu, Ziyi Liang, Ziyuan Gao</name></author>
    <summary type="html"><![CDATA[<p>Technical report for ERNIE 5.0, a natively multimodal foundation model with unified next-group-of-tokens prediction across text, image, video, and audio using ultra-sparse MoE with elastic training for deployment flexibility.</p>]]></summary>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Mixture-of-Experts"/>
    <category term="Elastic Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:41fa78fd2ef2</id>
    <title>When Chains of Thought Don't Matter: Causal Bypass in Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03994" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-41fa78fd2ef2" rel="related" type="text/html"/>
    <published>2026-02-05T03:09:00Z</published>
    <updated>2026-02-05T03:09:00Z</updated>
    <author><name>Anish Sathyanarayanan, Aditya Nagarsekar, Aarush Rathore</name></author>
    <summary type="html"><![CDATA[<p>Finds that even verbose, strategic CoT is often causally independent of model answers, presenting diagnostic framework combining manipulation detection with causal probes measuring CoT-mediated influence.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Faithfulness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:632a6f663113</id>
    <title>From Data to Behavior: Predicting Unintended Model Behaviors Before Training</title>
    <link href="http://arxiv.org/abs/2602.04735" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-632a6f663113" rel="related" type="text/html"/>
    <published>2026-02-05T03:09:00Z</published>
    <updated>2026-02-05T03:09:00Z</updated>
    <author><name>Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>Introduces Data2Behavior task for predicting unintended model behaviors before training. MDF approach summarizes data through mean representations to reveal potential biases without parameter updates.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Data Quality"/>
    <category term="Proactive Risk Detection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0bf025c808cc</id>
    <title>Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning</title>
    <link href="http://arxiv.org/abs/2602.03978" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0bf025c808cc" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Zidi Xiong, Shan Chen, Himabindu Lakkaraju</name></author>
    <summary type="html"><![CDATA[<p>Systematically evaluates how monitorability—faithful CoT reflection of internal computation—emerges during RLVR training, finding it's strongly data-dependent and requires diversity and instruction-following data.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Interpretability"/>
    <category term="Reinforcement Learning"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:1b7ab228599a</id>
    <title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04224" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-1b7ab228599a" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Zeming Wei, Qiaosheng Zhang, Xia Hu, Xingcheng Xu</name></author>
    <summary type="html"><![CDATA[<p>Proposes RAPO (Risk-Aware Preference Optimization) to improve safety reasoning generalization in Large Reasoning Models against jailbreak attacks. Provides theoretical and empirical evidence for more sufficient safe reasoning processes.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reasoning Models"/>
    <category term="Jailbreak Defense"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:12a825a35d0b</id>
    <title>Billion-Scale Graph Foundation Models</title>
    <link href="http://arxiv.org/abs/2602.04768" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-12a825a35d0b" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Maya Bechler-Speicher, Yoel Gottlieb, Andrey Isakov, David Abensur, Ami Tavory, Daniel Haimovich, Ido Guy, Udi Weinsberg</name></author>
    <summary type="html"><![CDATA[<p>Presents GraphBFF, the first end-to-end recipe for billion-parameter Graph Foundation Models for arbitrary heterogeneous graphs. Establishes the first neural scaling laws for general graphs, showing loss decreases predictably with model capacity or training data.</p>]]></summary>
    <category term="Graph Neural Networks"/>
    <category term="Foundation Models"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:3c11173b0d9d</id>
    <title>Rethinking the Trust Region in LLM Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.04879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-3c11173b0d9d" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</name></author>
    <summary type="html"><![CDATA[<p>Argues that PPO's ratio clipping mechanism is fundamentally ill-suited for LLMs due to large vocabularies. Low-probability tokens are over-penalized while high-probability shifts are under-constrained. Proposes improved trust region methods for LLM fine-tuning.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Fine-tuning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:86e05b8bc59f</id>
    <title>Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title>
    <link href="http://arxiv.org/abs/2602.04653" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-86e05b8bc59f" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Ariel Fogel, Omer Hofman, Eilon Cohen, Roman Vainshtein</name></author>
    <summary type="html"><![CDATA[<p>Identifies novel attack surface for LLMs via maliciously modified chat templates (executable Jinja2 programs), enabling inference-time backdoors without access to training or deployment infrastructure.</p>]]></summary>
    <category term="AI Security"/>
    <category term="LLM Safety"/>
    <category term="Backdoor Attacks"/>
    <category term="Adversarial ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:be6c3409bfc9</id>
    <title>Expert Selections In MoE Models Reveal (Almost) As Much As Text</title>
    <link href="http://arxiv.org/abs/2602.04105" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-be6c3409bfc9" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Amir Nuriyev, Gabriel Kulp</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that expert routing decisions in MoE models leak substantial information, enabling 91.2% token reconstruction from routing patterns alone using transformer decoders. Reveals significant privacy vulnerability.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Privacy"/>
    <category term="Mixture-of-Experts"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:4c941e6d6a59</id>
    <title>RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models</title>
    <link href="http://arxiv.org/abs/2602.04448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-4c941e6d6a59" rel="related" type="text/html"/>
    <published>2026-02-05T03:04:00Z</published>
    <updated>2026-02-05T03:04:00Z</updated>
    <author><name>Jiacheng Liang, Yuhui Wang, Tanqiu Jiang, Ting Wang</name></author>
    <summary type="html"><![CDATA[<p>Proposes RASA, a routing-aware safety alignment framework for MoE models that repairs Safety-Critical Experts while preventing routing-based safety bypasses during fine-tuning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Mixture of Experts"/>
    <category term="Alignment"/>
    <category term="Jailbreak Defense"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:d811bbc73ffb</id>
    <title>Fluid Representations in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.04843" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-d811bbc73ffb" rel="related" type="text/html"/>
    <published>2026-02-05T03:02:00Z</published>
    <updated>2026-02-05T03:02:00Z</updated>
    <author><name>Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin</name></author>
    <summary type="html"><![CDATA[<p>Analyzes how QwQ-32B develops abstract representations during reasoning, finding gradual improvement in encoding actions/concepts with structure-focused rather than name-specific encodings. Establishes causal evidence via steering.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
    <category term="Mechanistic Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:e261dd1973b5</id>
    <title>Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models</title>
    <link href="http://arxiv.org/abs/2602.04649" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-e261dd1973b5" rel="related" type="text/html"/>
    <published>2026-02-05T03:02:00Z</published>
    <updated>2026-02-05T03:02:00Z</updated>
    <author><name>Binghai Wang, Yantao Liu, Yuxuan Liu, Tianyi Tang, Shenzhi Wang, Chang Gao, Chujie Zheng, Yichang Zhang, Le Yu, Shixuan Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Bowen Yu, Fei Huang, Junyang Lin</name></author>
    <summary type="html"><![CDATA[<p>Introduces Rationale Consistency metric for reward models that quantifies alignment between model reasoning and human judgment, revealing deceptive alignment where models produce correct judgments for wrong reasons.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Reward Models"/>
    <category term="Deceptive Alignment"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:category-summary:research</id>
    <title>Research Summary: February 04, 2026</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-04T06:00:00Z</published>
    <updated>2026-02-04T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features major theoretical breakthroughs alongside practical infrastructure and safety advances. The hallucination rate-distortion theorem <a href="http://localhost:8080/?date=2026-02-04&category=research#item-8e4ab01f7c01" class="internal-link" rel="noopener noreferrer">proves factual errors</a> are <strong>information-theoretically optimal</strong> under memory constraints—a fundamental reframing of the problem.</p>
<ul>
<li><strong>Kimi K2.5</strong> releases as open-source multimodal agentic model with <strong>Agent Swarm</strong> framework achieving state-of-the-art results</li>
<li>Simple role conditioning <a href="http://localhost:8080/?date=2026-02-04&category=research#item-0a6c8edd4663" class="internal-link" rel="noopener noreferrer">reduces unsafe outputs</a> on <strong>WildJailbreak</strong> from <strong>81.4% to 3.6%</strong> without any training</li>
<li><strong>Constant-cost self-attention</strong> via symmetric Taylor approximation could transform long-context efficiency if validated</li>
<li><strong>Identity Bridge</strong> challenges the reversal curse as fundamental limitation of autoregressive models</li>
</ul>
<p>Theoretical contributions span tropical geometry analysis <a href="http://localhost:8080/?date=2026-02-04&category=research#item-c3bdb1a6b787" class="internal-link" rel="noopener noreferrer">proving <strong>Top-k MoE routing</strong></a> equivalent to combinatorial depth, first <a href="http://localhost:8080/?date=2026-02-04&category=research#item-b142257d0506" class="internal-link" rel="noopener noreferrer"><strong>PPO convergence proof</strong></a>, and <a href="http://localhost:8080/?date=2026-02-04&category=research#item-ee65813c4e1a" class="internal-link" rel="noopener noreferrer"><strong>Ω(n) lower bounds</strong></a> on chain-of-thought token complexity. <strong>BLOCK-EM</strong> introduces mechanistic prevention of emergent misalignment, while <strong>SWE-Universe</strong> <a href="http://localhost:8080/?date=2026-02-04&category=research#item-ef7adf55235d" class="internal-link" rel="noopener noreferrer">scales coding agent environments</a> to <strong>807K</strong> verified tasks.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-04T03:31:00Z</published>
    <updated>2026-02-04T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">Research</a>, Kimi K2.5 is an open-source multimodal agentic model with joint text-vision training and Agent Swarm framework for parallel task decomposition, achieving SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="AI Agents"/>
    <category term="Open Source"/>
    <category term="Vision-Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a6c8edd4663</id>
    <title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title>
    <link href="http://arxiv.org/abs/2602.00061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Zhou Ziheng, Jiakun Ding, Zhaowei Zhang, Ruosen Gao, Yingnian Wu, Demetri Terzopoulos, Yipeng Kang, Fangwei Zhong, Junqi Wang</name></author>
    <summary type="html"><![CDATA[<p>Proposes role conditioning as compact alternative to principle-based alignment, reducing unsafe outputs on WildJailbreak from 81.4% to 3.6% with DeepSeek-V3 through training-free role-conditioned generation and iterative role-based critics.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Alignment"/>
    <category term="Role-Playing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes Fault Tolerant HSDP for training LLMs on 100K GPUs, using data parallel replicas as fault tolerance units with novel FTAR protocol enabling continued training during failures.</p>]]></summary>
    <category term="Large-scale Training"/>
    <category term="Distributed Systems"/>
    <category term="LLM Infrastructure"/>
    <category term="Fault Tolerance"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8e4ab01f7c01</id>
    <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
    <link href="http://arxiv.org/abs/2602.00906" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Anxin Guo, Jingwei Li</name></author>
    <summary type="html"><![CDATA[<p>Proves hallucination is information-theoretically optimal behavior under memory constraints via rate-distortion theorem for membership testing. Shows optimal models must hallucinate on non-facts even with perfect training.</p>]]></summary>
    <category term="Hallucination"/>
    <category term="Information Theory"/>
    <category term="Theoretical ML"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b142257d0506</id>
    <title>An Approximate Ascent Approach To Prove Convergence of PPO</title>
    <link href="http://arxiv.org/abs/2602.03386" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" rel="related" type="text/html"/>
    <published>2026-02-04T03:19:00Z</published>
    <updated>2026-02-04T03:19:00Z</updated>
    <author><name>Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</name></author>
    <summary type="html"><![CDATA[<p>Provides first convergence proof for PPO by interpreting its policy update scheme as approximated policy gradient ascent, controlling bias from surrogate gradients using random reshuffling techniques.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="PPO"/>
    <category term="Theoretical RL"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" class="internal-link" rel="noopener noreferrer">Research</a>, Challenges the prevailing view that the reversal curse (inability to deduce B→A from training on A→B) is a fundamental limit of autoregressive LLMs. Proposes an Identity Bridge method to mitigate this limitation through slight modifications.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">Research</a>, Derives self-attention formulation with constant cost per token by decomposing Taylor expansion into symmetric tensor product chains, achieving orders-of-magnitude reductions in memory and compute.</p>]]></summary>
    <category term="Efficient Transformers"/>
    <category term="Attention Mechanisms"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes BLOCK-EM to prevent emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves 95% reduction in misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ef7adf55235d</id>
    <title>SWE-Universe: Scale Real-World Verifiable Environments to Millions</title>
    <link href="http://arxiv.org/abs/2602.02361" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</name></author>
    <summary type="html"><![CDATA[<p>Introduces SWE-Universe, a framework for automatically constructing 807K+ real-world software engineering environments from GitHub PRs using a building agent with self-verification.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Benchmark Construction"/>
    <category term="Code Generation"/>
    <category term="Large-Scale Datasets"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c3bdb1a6b787</id>
    <title>Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry</title>
    <link href="http://arxiv.org/abs/2602.03204" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Ye Su, Huayi Tang, Zixuan Gong, Yong Liu</name></author>
    <summary type="html"><![CDATA[<p>First theoretical analysis of Mixture-of-Experts through tropical geometry, proving that Top-k routing is algebraically isomorphic to k-th elementary symmetric tropical polynomial. Shows 'sparsity is combinatorial depth' with capacity scaling by binomial coefficient.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Theoretical ML"/>
    <category term="Architecture Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ee65813c4e1a</id>
    <title>Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs</title>
    <link href="http://arxiv.org/abs/2602.02909" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Kiran Tomlinson, Tobias Schnabel, Adith Swaminathan, Jennifer Neville</name></author>
    <summary type="html"><![CDATA[<p>Proves Ω(n) lower bounds on chain-of-thought tokens for binary majority, triplet matching, and graph reachability in BAPO model, with matching upper bounds.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
    <category term="Computational Complexity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ae224d55ef65</id>
    <title>Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</title>
    <link href="http://arxiv.org/abs/2602.03837" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ae224d55ef65" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</name></author>
    <summary type="html"><![CDATA[<p>Google researchers present case studies demonstrating successful collaboration with Gemini models to solve open problems, refute conjectures, and generate proofs across theoretical CS, economics, and physics.</p>]]></summary>
    <category term="AI for Science"/>
    <category term="Mathematical Reasoning"/>
    <category term="Google/DeepMind"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:6417e873300a</id>
    <title>WAXAL: A Large-Scale Multilingual African Language Speech Corpus</title>
    <link href="http://arxiv.org/abs/2602.02734" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-6417e873300a" rel="related" type="text/html"/>
    <published>2026-02-04T03:14:00Z</published>
    <updated>2026-02-04T03:14:00Z</updated>
    <author><name>Abdoulaye Diack, Perry Nelson, Kwaku Agbesi, Angela Nakalembe, MohamedElfatih MohamedKhair, Vusumuzi Dube, Tavonga Siyavora, Subhashini Venugopalan, Jason Hickey, Uche Okonkwo, Abhishek Bapna, Isaac Wiafe, Raynard Dodzi Helegah, Elikem Doe Atsakpo, Charles Nutrokpor, Fiifi Baffoe Payin Winful, Kafui Kwashie Solaga, Jamal-Deen Abdulai, Akon Obu Ekpezu, Audace Niyonkuru, Samuel Rutunda, Boris Ishimwe, Michael Melese, Engineer Bainomugisha, Joyce Nakatumba-Nabende, Andrew Katumba, Claire Babirye, Jonathan Mukiibi, Vincent Kimani, Samuel Kibacia, James Maina, Fridah Emmah, Ahmed Ibrahim Shekarau, Ibrahim Shehu Adamu, Yusuf Abdullahi, Howard Lakougna, Bob MacDonald, Hadar Shemtov, Aisha Walcott-Bryant, Moustapha Cisse, Avinatan Hassidim, Jeff Dean, Yossi Matias</name></author>
    <summary type="html"><![CDATA[<p>WAXAL is a large-scale speech dataset for 21 African languages covering 100M+ speakers, with 1,250 hours ASR data and 180 hours TTS data, enabling speech technology for underrepresented languages.</p>]]></summary>
    <category term="Speech Recognition"/>
    <category term="Datasets"/>
    <category term="Multilingual NLP"/>
    <category term="Google"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as an adversarial game where attacker and defender agents co-evolve through multi-turn reinforcement learning, enabling continuous discovery of vulnerabilities and adaptive defense.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Robustness"/>
    <category term="Reinforcement Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ecc461cb4ce7</id>
    <title>Universal One-third Time Scaling in Learning Peaked Distributions</title>
    <link href="http://arxiv.org/abs/2602.03685" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ecc461cb4ce7" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Yizhou Liu, Ziming Liu, Cengiz Pehlevan, Jeff Gore</name></author>
    <summary type="html"><![CDATA[<p>Shows power-law training dynamics in LLMs arise from softmax and cross-entropy when learning peaked distributions, deriving universal 1/3 exponent from fundamental optimization bottleneck.</p>]]></summary>
    <category term="Neural Scaling Laws"/>
    <category term="LLM Training"/>
    <category term="Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:508e858e0e08</id>
    <title>SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training</title>
    <link href="http://arxiv.org/abs/2602.03411" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-508e858e0e08" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Huatong Song, Lisheng Huang, Shuang Sun, Jinhao Jiang, Ran Le, Daixuan Cheng, Guoxin Chen, Yiwen Hu, Zongchao Chen, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen</name></author>
    <summary type="html"><![CDATA[<p>SWE-Master presents an open-source post-training framework for software engineering agents covering trajectory synthesis, long-horizon SFT, RL with execution feedback, and inference design.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Post-Training"/>
    <category term="Code Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a648aa519b3</id>
    <title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title>
    <link href="http://arxiv.org/abs/2602.01750" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a648aa519b3" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Mohammad Beigi, Ming Jin, Junshan Zhang, Qifan Wang, Lifu Huang</name></author>
    <summary type="html"><![CDATA[<p>ARA (Adversarial Reward Auditing) detects reward hacking by training a Hacker policy to discover reward model vulnerabilities while an Auditor learns to detect exploitation, then uses this to gate rewards during RLHF.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reward Hacking"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:445982e79bb6</id>
    <title>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03392" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-445982e79bb6" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang</name></author>
    <summary type="html"><![CDATA[<p>Establishes theoretical framework for analyzing entropy dynamics during reinforcement fine-tuning of LLMs, deriving discriminant expression and first-order expression for entropy change under logit updates.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Theoretical ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b0294636e453</id>
    <title>FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding</title>
    <link href="http://arxiv.org/abs/2602.03417" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b0294636e453" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Yingli Shen, Wen Lai, Jie Zhou, Xueren Zhang, Yudong Wang, Kangyang Luo, Shuo Wang, Ge Gao, Alexander Fraser, Maosong Sun</name></author>
    <summary type="html"><![CDATA[<p>FactNet unifies 1.7 billion atomic assertions with 3 billion evidence pointers from 316 Wikipedia editions, using deterministic construction for byte-level provenance tracking.</p>]]></summary>
    <category term="Knowledge Graphs"/>
    <category term="Factuality"/>
    <category term="Datasets"/>
    <category term="Multilingual NLP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:7578107caca0</id>
    <title>Building Better Deception Probes Using Targeted Instruction Pairs</title>
    <link href="http://arxiv.org/abs/2602.01425" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-7578107caca0" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Vikram Natarajan, Devina Jain, Shivam Arora, Satvik Golechha, Joseph Bloom</name></author>
    <summary type="html"><![CDATA[<p>Shows that targeted instruction pairs based on a human-interpretable taxonomy of deception significantly improve linear probes for detecting deceptive behavior in AI systems, capturing intent rather than content patterns.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Deception Detection"/>
    <category term="Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0c97dabe6989</id>
    <title>Adoption and Use of LLMs at an Academic Medical Center</title>
    <link href="http://arxiv.org/abs/2602.00074" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0c97dabe6989" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Nigam H. Shah, Nerissa Ambers, Abby Pandya, Timothy Keyes, Juan M. Banda, Srikar Nallan, Carlene Lugtu, Artem A. Trotsyuk, Suhana Bedi, Alyssa Unell, Miguel Fuentes, Francois Grolleau, Sneha S. Jain, Jonathan Chen, Devdutta Dash, Danton Char, Aditya Sharma, Duncan McElfresh, Patrick Scully, Vishanthan Kumar, Connor OBrien, Satchi Mouniswamy, Elvis Jones, Krishna Jasti, Gunavathi Mannika Lakshmanan, Sree Ram Akula, Varun Kumar Singh, Ramesh Rajmanickam, Sudhir Sinha, Vicky Zhou, Xu Wang, Bilal Mawji, Joshua Ge, Wencheng Li, Travis Lyons, Jarrod Helzer, Vikas Kakkar, Ramesh Powar, Darren Batara, Cheryl Cordova, William Frederick III, Olivia Tang, Phoebe Morgan, April S. Liang, Stephen P. Ma, Shivam Vedak, Dong-han Yao, Akshay Swaminathan, Mehr Kashyap, Brian Ng, Jamie Hellman, Nikesh Kotecha, Christopher Sharp, Gretchen Brown, Christian Lindmark, Anurang Revri, Michael A. Pfeffer</name></author>
    <summary type="html"><![CDATA[<p>Describes ChatEHR, a production LLM system deployed at Stanford Medicine enabling use of LLMs with complete patient timelines for chart review, screening, and abstraction tasks with 4,500 unique users.</p>]]></summary>
    <category term="Healthcare AI"/>
    <category term="LLM Deployment"/>
    <category term="Electronic Health Records"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new denial-of-service attack targeting Large Reasoning Models by crafting prompts that induce pathologically long reasoning traces. Formalizes PI-DoS attacks requiring high amplification ratio, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Large Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment as systematic distortion of pre-alignment distribution and derives optimal aggregation strategy for jailbreaking. Shows logit-arithmetic methods are special cases and proposes broader family of aggregation rules.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Adversarial ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:5c3691edf77f</id>
    <title>From Perception to Action: Spatial AI Agents and World Models</title>
    <link href="http://arxiv.org/abs/2602.01644" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-5c3691edf77f" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Gloria Felicia, Nolan Bryant, Handi Putra, Ayaan Gazali, Eliel Lobo, Esteban Rojas</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey reviewing 2000+ papers (citing 742) that introduces unified three-axis taxonomy connecting agentic capabilities with spatial intelligence for embodied AI. Bridges gap between agentic architectures and spatial reasoning.</p>]]></summary>
    <category term="Survey"/>
    <category term="Embodied AI"/>
    <category term="Spatial Intelligence"/>
    <category term="World Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:badeda980845</id>
    <title>Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory</title>
    <link href="http://arxiv.org/abs/2602.02393" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-badeda980845" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Ruiqi Wu, Xuanhua He, Meng Cheng, Tianyu Yang, Yong Zhang, Zhuoliang Kang, Xunliang Cai, Xiaoming Wei, Chunle Guo, Chongyi Li, Ming-Ming Cheng</name></author>
    <summary type="html"><![CDATA[<p>Proposes Infinite-World, an interactive world model maintaining coherent visual memory over 1000+ frames using Hierarchical Pose-free Memory Compressor for recursive latent compression.</p>]]></summary>
    <category term="World Models"/>
    <category term="Video Generation"/>
    <category term="Long-Horizon Generation"/>
    <category term="Memory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8ab0268430c1</id>
    <title>IMU-1: Sample-Efficient Pre-training of Small Language Models</title>
    <link href="http://arxiv.org/abs/2602.02522" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8ab0268430c1" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>George Grigorev</name></author>
    <summary type="html"><![CDATA[<p>IMU-1 is a 430M parameter LLM trained on only 72B tokens that approaches performance of models trained on 56x more data. Combines architectural innovations (QK-norm, per-head gating) with optimization advances (NorMuon, muP) and releases all artifacts.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Efficient Training"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ba953cf4bda2</id>
    <title>Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher</title>
    <link href="http://arxiv.org/abs/2602.02859" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ba953cf4bda2" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Hari K Prakash, Charles H Martin</name></author>
    <summary type="html"><![CDATA[<p>Identifies anti-grokking: a previously unreported third phase where generalization collapses after successful grokking, with test accuracy returning to chance while training accuracy remains perfect. Uses Weightwatcher to detect this phenomenon.</p>]]></summary>
    <category term="Grokking"/>
    <category term="Generalization"/>
    <category term="Training Dynamics"/>
    <category term="Deep Learning Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:219e73cfb1e7</id>
    <title>Reinforcement Learning with Promising Tokens for Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03195" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-219e73cfb1e7" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</name></author>
    <summary type="html"><![CDATA[<p>Proposes RLPT, a framework that reduces the action space in RL for LLMs by focusing only on contextually relevant 'promising' tokens rather than the full vocabulary. Demonstrates that valid reasoning paths concentrate within a low-rank subspace.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Language Models"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:de3d45e551ca</id>
    <title>Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging</title>
    <link href="http://arxiv.org/abs/2602.03702" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-de3d45e551ca" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Alexandru Meterez, Pranav Ajit Nair, Depen Morwani, Cengiz Pehlevan, Sham Kakade</name></author>
    <summary type="html"><![CDATA[<p>Provides theoretical analysis showing anytime learning schedules exist for LLM training where horizons are unknown, demonstrating weight averaging achieves minimax rates. Polynomial decay schedules determined by source/capacity conditions.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Optimization"/>
    <category term="Continual Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:91e4f73b021e</id>
    <title>A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior</title>
    <link href="http://arxiv.org/abs/2602.02639" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-91e4f73b021e" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Harry Mayne, Justin Singh Kang, Dewi Gould, Kannan Ramchandran, Adam Mahdi, Noah Y. Siegel</name></author>
    <summary type="html"><![CDATA[<p>Introduces Normalized Simulatability Gain metric showing LLM self-explanations help predict model behavior, evaluating 18 frontier models including GPT-5.2, Claude 4.5, Gemini 3.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="AI Safety"/>
    <category term="LLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:f3866d28252a</id>
    <title>RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization</title>
    <link href="http://arxiv.org/abs/2602.03310" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-f3866d28252a" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Songming Liu, Bangguo Li, Kai Ma, Lingxuan Wu, Hengkai Tan, Xiao Ouyang, Hang Su, Jun Zhu</name></author>
    <summary type="html"><![CDATA[<p>Introduces RDT2, a 7B parameter robotic foundation model built on a VLM designed for zero-shot deployment on novel embodiments. Collected over 10,000 hours of demonstrations using enhanced Universal Manipulation Interface and employs novel three-stage training with Residual Vector Quantization.</p>]]></summary>
    <category term="Robotics"/>
    <category term="Vision-Language-Action"/>
    <category term="Foundation Models"/>
    <category term="Zero-Shot Transfer"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:7c602d12a2e0</id>
    <title>Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch</title>
    <link href="http://arxiv.org/abs/2602.03183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-7c602d12a2e0" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Hyunwoo Kim, Niloofar Mireshghallah, Michael Duan, Rui Xin, Shuyue Stella Li, Jaehun Jung, David Acuna, Qi Pang, Hanshen Xiao, G. Edward Suh, Sewoong Oh, Yulia Tsvetkov, Pang Wei Koh, Yejin Choi</name></author>
    <summary type="html"><![CDATA[<p>Privasis is the first million-scale fully synthetic dataset of privacy-sensitive text built from scratch, designed to enable research on AI agents handling sensitive personal information without real privacy risks.</p>]]></summary>
    <category term="Privacy"/>
    <category term="Datasets"/>
    <category term="AI Agents"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a2aa8fa67160</id>
    <title>A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures</title>
    <link href="http://arxiv.org/abs/2602.03604" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a2aa8fa67160" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, Amir Bar</name></author>
    <summary type="html"><![CDATA[<p>EB-JEPA is an open-source library from Meta for learning representations via Joint-Embedding Predictive Architectures, covering image, video, and action-conditioned world models.</p>]]></summary>
    <category term="Self-Supervised Learning"/>
    <category term="World Models"/>
    <category term="Open Source"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:672d86dcf771</id>
    <title>EchoJEPA: A Latent Predictive Foundation Model for Echocardiography</title>
    <link href="http://arxiv.org/abs/2602.02603" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-672d86dcf771" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Alif Munim, Adibvafa Fallahpour, Teodora Szasz, Ahmadreza Attarpour, River Jiang, Brana Sooriyakanthan, Maala Sooriyakanthan, Heather Whitney, Jeremy Slivnick, Barry Rubin, Wendy Tsang, Bo Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents EchoJEPA, a foundation model for echocardiography trained on 18 million echocardiograms from 300K patients - the largest pretraining corpus for this modality. Introduces multi-view probing framework for standardized evaluation.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="Foundation Models"/>
    <category term="Computer Vision"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:6561d7765cfb</id>
    <title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title>
    <link href="http://arxiv.org/abs/2602.01725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-6561d7765cfb" rel="related" type="text/html"/>
    <published>2026-02-04T03:04:00Z</published>
    <updated>2026-02-04T03:04:00Z</updated>
    <author><name>Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>SafePred provides predictive guardrails for computer-using agents via world models, addressing long-term risks that reactive guardrails cannot identify within current observation space.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Computer-Using Agents"/>
    <category term="World Models"/>
    <category term="Guardrails"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a4ed8deb99f4</id>
    <title>PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</title>
    <link href="http://arxiv.org/abs/2602.02493" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a4ed8deb99f4" rel="related" type="text/html"/>
    <published>2026-02-04T03:04:00Z</published>
    <updated>2026-02-04T03:04:00Z</updated>
    <author><name>Zehong Ma, Ruihan Xu, Shiliang Zhang</name></author>
    <summary type="html"><![CDATA[<p>Proposes PixelGen, showing that pixel diffusion can beat latent diffusion when trained with perceptual losses (LPIPS + DINO) that guide learning toward a meaningful perceptual manifold.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Image Generation"/>
    <category term="Perceptual Loss"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:40e295f7e50b</id>
    <title>The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models</title>
    <link href="http://arxiv.org/abs/2602.02557" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-40e295f7e50b" rel="related" type="text/html"/>
    <published>2026-02-04T03:04:00Z</published>
    <updated>2026-02-04T03:04:00Z</updated>
    <author><name>Yupeng Chen, Junchi Yu, Aoxi Liu, Philip Torr, Adel Bibi</name></author>
    <summary type="html"><![CDATA[<p>Discovers 'alignment curse' where strong modality alignment in omni-models propagates textual jailbreak vulnerabilities to audio modality. Proposes attack methods exploiting this cross-modal transfer.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaks"/>
    <category term="Multimodal Learning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:1f66985988f3</id>
    <title>Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures</title>
    <link href="http://arxiv.org/abs/2602.03379" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-1f66985988f3" rel="related" type="text/html"/>
    <published>2026-02-04T03:04:00Z</published>
    <updated>2026-02-04T03:04:00Z</updated>
    <author><name>Sangyeon Yoon, Hyesoo Hong, Wonje Jeung, Albert No</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that syntactic similarity, not topical relevance, is the primary driver of benign relearning in machine unlearning, with syntactically similar data triggering recovery through aligned representations and gradients.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="AI Safety"/>
    <category term="LLM Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8c819dad0882</id>
    <title>HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing</title>
    <link href="http://arxiv.org/abs/2602.03560" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8c819dad0882" rel="related" type="text/html"/>
    <published>2026-02-04T03:04:00Z</published>
    <updated>2026-02-04T03:04:00Z</updated>
    <author><name>Yizhao Gao, Jianyu Wei, Qihao Zhang, Yu Cheng, Shimao Chen, Zhengju Tang, Zihan Jiang, Yifan Song, Hailin Zhang, Liang Zhao, Bo Yang, Gang Wang, Shijie Cao, Fuli Luo</name></author>
    <summary type="html"><![CDATA[<p>HySparse interleaves full attention with sparse layers, using the full layer as oracle for token selection and sharing KV caches to reduce both computation and memory simultaneously.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Efficiency"/>
    <category term="Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:799dc249cf75</id>
    <title>Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics</title>
    <link href="http://arxiv.org/abs/2602.02133" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-799dc249cf75" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Sangwoo Shin, BumJun Kim, Kyelim Lee, Moongyu Jeon, Albert No</name></author>
    <summary type="html"><![CDATA[<p>Explains why masked diffusion models mitigate the reversal curse through weight sharing that couples forward/reverse attention scores, showing this arises from architecture-training interaction rather than just any-order training.</p>]]></summary>
    <category term="Reversal Curse"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Diffusion Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:33d1631fd5c4</id>
    <title>The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization</title>
    <link href="http://arxiv.org/abs/2602.00175" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-33d1631fd5c4" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Manyi Li, Yufan Liu, Lai Jiang, Bing Li, Yuming Li, Weiming Hu</name></author>
    <summary type="html"><![CDATA[<p>Reveals that diffusion model unlearning creates an 'illusion of forgetting' - NSFW knowledge remains as dormant memories. Proposes IVO attack framework to reactivate these dormant memories by reconstructing broken mappings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Diffusion Models"/>
    <category term="Machine Unlearning"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:d34cb0cb8dfc</id>
    <title>Multi-Agent Teams Hold Experts Back</title>
    <link href="http://arxiv.org/abs/2602.01011" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-d34cb0cb8dfc" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Aneesh Pappu, Batu El, Hancheng Cao, Carmelo di Nolfo, Yanchao Sun, Meng Cao, James Zou</name></author>
    <summary type="html"><![CDATA[<p>Studies self-organizing LLM teams across benchmarks finding they consistently fail to achieve synergy, with team performance below best individual member. Important negative result contrasting with human team behavior.</p>]]></summary>
    <category term="Multi-Agent Systems"/>
    <category term="LLM Teams"/>
    <category term="Emergent Behavior"/>
    <category term="Collaboration"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:76955f428d5c</id>
    <title>Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment</title>
    <link href="http://arxiv.org/abs/2602.01685" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-76955f428d5c" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Byeonghu Na, Hyungho Na, Yeongmin Kim, Suhyeon Jo, HeeSun Bae, Mina Kang, Il-Chul Moon</name></author>
    <summary type="html"><![CDATA[<p>Wasserstein Policy Regularization (WPR) replaces KL divergence in RLHF with entropy-regularized Wasserstein distance, incorporating token space geometry for semantic-aware regularization.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Alignment"/>
    <category term="Language Models"/>
    <category term="Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:5ff72bc299a1</id>
    <title>Flow Policy Gradients for Robot Control</title>
    <link href="http://arxiv.org/abs/2602.02481" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-5ff72bc299a1" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong, Carmelo Sferrazza, Yi Ma, Rocky Duan, Pieter Abbeel, Guanya Shi, Karen Liu, Angjoo Kanazawa</name></author>
    <summary type="html"><![CDATA[<p>Shows that flow matching policy gradients can effectively train robot control policies, with improved objective enabling success in locomotion, manipulation, and sim-to-real transfer on humanoids.</p>]]></summary>
    <category term="Robotics"/>
    <category term="Policy Gradients"/>
    <category term="Flow Matching"/>
    <category term="Humanoid Control"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:5151505f2058</id>
    <title>Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization</title>
    <link href="http://arxiv.org/abs/2602.02545" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-5151505f2058" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Dayu Wang, Jiaye Yang, Weikang Li, Jiahui Liang, Yang Li</name></author>
    <summary type="html"><![CDATA[<p>MRPO challenges the hypothesis that RL only aligns existing LLM capabilities by demonstrating geometric interventions can fundamentally expand the latent reasoning space beyond pre-trained manifold boundaries.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:dccd9f875a70</id>
    <title>When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models</title>
    <link href="http://arxiv.org/abs/2602.02855" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-dccd9f875a70" rel="related" type="text/html"/>
    <published>2026-02-04T03:02:00Z</published>
    <updated>2026-02-04T03:02:00Z</updated>
    <author><name>Gibbs Nwemadji, Bruno Loureiro, Jean Barbier</name></author>
    <summary type="html"><![CDATA[<p>Mathematically shows that excessive pre-training can computationally slow down LoRA fine-tuning, even when tasks are well-aligned. Characterizes convergence rate dependence on initial alignment and task non-linearity using single-index models.</p>]]></summary>
    <category term="Transfer Learning"/>
    <category term="LoRA"/>
    <category term="Fine-Tuning"/>
    <category term="Theoretical ML"/>
  </entry>
</feed>