<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 100)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-100.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:100</id>
  <updated>2026-02-09T21:30:03Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-02-09:category-summary:research</id>
    <title>Research Summary: February 09, 2026</title>
    <link href="http://arxiv.org/abs/2602.06547" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-09T06:00:00Z</published>
    <updated>2026-02-09T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities in the AI ecosystem alongside fundamental theoretical advances. Security research dominates: a first-of-its-kind study <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-32f20f3b19ad" class="internal-link" rel="noopener noreferrer">finds <strong>157 malicious skills</strong></a> with <strong>632 vulnerabilities</strong> across <strong>98K agent skills</strong> in community registries, while Microsoft's <strong>GRP-Obliteration</strong> demonstrates <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-2c78ac696a85" class="internal-link" rel="noopener noreferrer">safety alignment can be removed</a> with a single unlabeled prompt.</p>
<ul>
<li><strong>DreamDojo</strong> (NVIDIA/Berkeley) presents the <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-3ffe759c109f" class="internal-link" rel="noopener noreferrer">largest world model pretraining dataset</a> at <strong>44K hours</strong> of egocentric human video for robot learning</li>
<li><strong>The Condensate Theorem</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c28c21b67d9e" class="internal-link" rel="noopener noreferrer">makes the bold claim</a> that transformer attention achieves <strong>O(n)</strong> complexity through learned sparsity with <strong>100% output equivalence</strong></li>
<li><strong>AlphaEvolve</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-6118c65ce254" class="internal-link" rel="noopener noreferrer">discovers ranking functions</a> for resolution of singularities in positive characteristic—a long-standing open problem in algebraic geometry</li>
<li><strong>GrAlgoBench</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-695b4e57fec0" class="internal-link" rel="noopener noreferrer">exposes reasoning model accuracy</a> dropping <strong>below 50%</strong> when graph complexity exceeds training distributions</li>
</ul>
<p>Safety infrastructure advances with <strong>TamperBench</strong> for <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-734479bb776d" class="internal-link" rel="noopener noreferrer">fine-tuning attacks</a>, <strong>REBEL</strong> demonstrating that models passing standard unlearning benchmarks <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-44c1b6dbbcb6" class="internal-link" rel="noopener noreferrer">still leak 'forgotten' knowledge</a>, and theoretical work proving <strong>steering vectors</strong> are <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-64c995dd81e8" class="internal-link" rel="noopener noreferrer">fundamentally non-identifiable</a>. <strong>GhostCite</strong> <a href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-43e6bdcd2130" class="internal-link" rel="noopener noreferrer">finds all tested models</a> hallucinate citations at <strong>14-95%</strong> rates across 40 domains.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:32f20f3b19ad</id>
    <title>Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study</title>
    <link href="http://arxiv.org/abs/2602.06547" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-32f20f3b19ad" rel="related" type="text/html"/>
    <published>2026-02-09T03:31:00Z</published>
    <updated>2026-02-09T03:31:00Z</updated>
    <author><name>Yi Liu, Zhihao Chen, Yanjun Zhang, Gelei Deng, Yuekang Li, Jianting Ning, and Leo Yu Zhang</name></author>
    <summary type="html"><![CDATA[<p>First labeled dataset of malicious agent skills from community registries, finding 157 malicious skills with 632 vulnerabilities across 98K analyzed. Identifies Data Thieves and Agent Hijackers as two attack archetypes.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agent Safety"/>
    <category term="Vulnerability Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:2c78ac696a85</id>
    <title>GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</title>
    <link href="http://arxiv.org/abs/2602.06258" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-2c78ac696a85" rel="related" type="text/html"/>
    <published>2026-02-09T03:23:00Z</published>
    <updated>2026-02-09T03:23:00Z</updated>
    <author><name>Mark Russinovich, Yanan Cai, Keegan Hines, Giorgio Severi, Blake Bullwinkel, Ahmed Salem</name></author>
    <summary type="html"><![CDATA[<p>Introduces GRP-Obliteration, a method using GRPO to unalign safety-aligned models with a single unlabeled prompt while largely preserving utility. Achieves stronger unalignment than existing techniques.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Jailbreaking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:6118c65ce254</id>
    <title>Evolving Ranking Functions for Canonical Blow-Ups in Positive Characteristic</title>
    <link href="http://arxiv.org/abs/2602.06553" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-6118c65ce254" rel="related" type="text/html"/>
    <published>2026-02-09T03:16:00Z</published>
    <updated>2026-02-09T03:16:00Z</updated>
    <author><name>Gergely B\'erczi</name></author>
    <summary type="html"><![CDATA[<p>Uses AlphaEvolve to discover ranking functions for resolution of singularities in positive characteristic algebraic geometry - a long-standing open problem since Hironaka's 1964 Fields Medal work.</p>]]></summary>
    <category term="AI for Mathematics"/>
    <category term="Algebraic Geometry"/>
    <category term="Evolutionary Search"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:43e6bdcd2130</id>
    <title>GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06718" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-43e6bdcd2130" rel="related" type="text/html"/>
    <published>2026-02-09T03:16:00Z</published>
    <updated>2026-02-09T03:16:00Z</updated>
    <author><name>Zuyao Xu, Yuqi Qiu, Lu Sun, FaSheng Miao, Fubin Wu, Xinyi Wang, Xiang Li, Haozhe Lu, ZhengZe Zhang, Yuxin Hu, Jialu Li, Jin Luo, Feng Zhang, Rui Luo, Xinran Liu, Yingxian Li, Jiaji Liu</name></author>
    <summary type="html"><![CDATA[<p>Introduces CiteVerifier framework and benchmarks 13 LLMs on citation generation across 40 domains, finding all models hallucinate citations at 14-95% rates. Reveals that many hallucinated citations are nearly real.</p>]]></summary>
    <category term="LLM Reliability"/>
    <category term="Hallucination"/>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:695b4e57fec0</id>
    <title>Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems</title>
    <link href="http://arxiv.org/abs/2602.06319" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-695b4e57fec0" rel="related" type="text/html"/>
    <published>2026-02-09T03:14:00Z</published>
    <updated>2026-02-09T03:14:00Z</updated>
    <author><name>Qifan Zhang, Jianhao Ruan, Aochuan Chen, Kang Zeng, Nuo Chen, Jing Tang, Jia Li</name></author>
    <summary type="html"><![CDATA[<p>Introduces GrAlgoBench, a benchmark using graph algorithm problems to evaluate Large Reasoning Models, revealing accuracy drops below 50% when graphs exceed 120 nodes and quadratic thinking token growth.</p>]]></summary>
    <category term="Reasoning Models"/>
    <category term="Benchmarks"/>
    <category term="Graph Algorithms"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:1924f07198aa</id>
    <title>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</title>
    <link href="http://arxiv.org/abs/2602.06570" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-1924f07198aa" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Baichuan-M3 Team: Chengfeng Dou, Fan Yang, Fei Li, Jiyuan Jia, Qiang Ju, Shuai Wang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Hongda Zhang, Jinyang Tai, Linzhuang Sun, Peidong Guo, Yichuan Mo, Xiaochuan Wang, Hengfu Cui, Zhishou Zhang</name></author>
    <summary type="html"><![CDATA[<p>Baichuan-M3 is medical LLM designed for active clinical decision support with proactive information acquisition, long-horizon reasoning, and hallucination suppression. Claims SOTA on HealthBench, outperforming GPT-5.2.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="Clinical Decision Support"/>
    <category term="LLM Specialization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:ff7fc0ecc54e</id>
    <title>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</title>
    <link href="http://arxiv.org/abs/2602.06855" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-ff7fc0ecc54e" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster, Bassel Al Omari, Despoina Magka, Alberto Pepe, Alexis Audran-Reiss, Muna Aghamelu, Nicolas Baldwin, Lucia Cipolina-Kun, Jean-Christophe Gagnon-Audet, Chee Hau Leow, Sandra Lefdal, Hossam Mossalam, Abhinav Moudgil, Saba Nazir, Emanuel Tewolde, Isabel Urrego, Jordi Armengol Estape, Amar Budhiraja, Gaurav Chaurasia, Abhishek Charnalia, Derek Dunfield, Karen Hambardzumyan, Daniel Izcovich, Martin Josifoski, Ishita Mediratta, Kelvin Niu, Parth Pathak, Michael Shvartsman, Edan Toledo, Anton Protopopov, Roberta Raileanu, Alexander Miller, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach</name></author>
    <summary type="html"><![CDATA[<p>Introduces AIRS-Bench, 20 tasks from SOTA ML papers for evaluating AI research agents across idea generation, experiment analysis, and iterative refinement. Establishes baselines with frontier models.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="AI for Research"/>
    <category term="Benchmark"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:3ffe759c109f</id>
    <title>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</title>
    <link href="http://arxiv.org/abs/2602.06949" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-3ffe759c109f" rel="related" type="text/html"/>
    <published>2026-02-09T03:12:00Z</published>
    <updated>2026-02-09T03:12:00Z</updated>
    <author><name>Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K.R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi "Jim" Fan</name></author>
    <summary type="html"><![CDATA[<p>DreamDojo is a foundation world model trained on 44k hours of egocentric human videos - the largest video dataset for world model pretraining. Uses continuous latent actions to learn dexterous control from action-unlabeled videos.</p>]]></summary>
    <category term="World Models"/>
    <category term="Robotics"/>
    <category term="Video Understanding"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c28c21b67d9e</id>
    <title>The Condensate Theorem: Transformers are O(n), Not $O(n^2)$</title>
    <link href="http://arxiv.org/abs/2602.06317" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c28c21b67d9e" rel="related" type="text/html"/>
    <published>2026-02-09T03:09:00Z</published>
    <updated>2026-02-09T03:09:00Z</updated>
    <author><name>Jorge L. Ruiz Williams</name></author>
    <summary type="html"><![CDATA[<p>Claims attention sparsity is a learned topological property achieving 100% output equivalence with full O(n²) attention, demonstrating lossless O(n) attention across multiple models.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Efficient Inference"/>
    <category term="Deep Learning Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:44c1b6dbbcb6</id>
    <title>REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop</title>
    <link href="http://arxiv.org/abs/2602.06248" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-44c1b6dbbcb6" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Patryk Rybak, Pawe{\l} Batorski, Paul Swoboda, Przemys{\l}aw Spurek</name></author>
    <summary type="html"><![CDATA[<p>Introduces REBEL, an evolutionary approach for adversarial prompt generation that successfully recovers 'forgotten' knowledge from models that pass standard unlearning benchmarks.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="AI Safety"/>
    <category term="Adversarial Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:5c412147ac94</id>
    <title>Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution</title>
    <link href="http://arxiv.org/abs/2602.06413" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-5c412147ac94" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Hsien-Jyh Liao</name></author>
    <summary type="html"><![CDATA[<p>Argues that autoregressive reasoning has intrinsic stability limits due to process-level instability rather than just task complexity, even in linear unbranched tasks without semantic ambiguity.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:c771d8f9bb69</id>
    <title>SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees</title>
    <link href="http://arxiv.org/abs/2602.06554" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-c771d8f9bb69" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Tianyi Hu, Qingxu Fu, Yanxi Chen, Zhaoyang Liu, Bolin Ding</name></author>
    <summary type="html"><![CDATA[<p>SeeUPO provides convergence guarantees for agentic RL in multi-turn settings. Shows REINFORCE with GRAE converges globally while PPO+GRAE breaks monotonic improvement.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Agents"/>
    <category term="Convergence Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:9744ca3b81d7</id>
    <title>NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06694" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-9744ca3b81d7" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Hyochan Chong, Dongkyu Kim, Changdong Kim, Minseop Choi</name></author>
    <summary type="html"><![CDATA[<p>Introduces NanoQuant, the first post-training quantization method to compress LLMs to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization. Achieves extreme compression ratios.</p>]]></summary>
    <category term="Model Compression"/>
    <category term="LLM Efficiency"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:64c995dd81e8</id>
    <title>On the Identifiability of Steering Vectors in Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.06801" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-64c995dd81e8" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Sohan Venkatesh, Ashish Mahendran Kurapath</name></author>
    <summary type="html"><![CDATA[<p>Proves steering vectors in LLMs are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Shows identifiability recoverable under strong assumptions.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Steering Vectors"/>
    <category term="Identifiability"/>
    <category term="LLM Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-09:research:734479bb776d</id>
    <title>TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering</title>
    <link href="http://arxiv.org/abs/2602.06911" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-09&amp;category=research#item-734479bb776d" rel="related" type="text/html"/>
    <published>2026-02-09T03:07:00Z</published>
    <updated>2026-02-09T03:07:00Z</updated>
    <author><name>Saad Hossain, Tom Tseng, Punya Syon Pandey, Samanvay Vajpayee, Matthew Kowal, Nayeema Nonta, Samuel Simko, Stephen Casper, Zhijing Jin, Kellin Pelrine, Sirisha Rambhatla</name></author>
    <summary type="html"><![CDATA[<p>TamperBench is the first unified framework for systematically evaluating LLM tamper resistance against fine-tuning and representation attacks. Curates repository of attacks and enables hyperparameter sweeps for realistic adversarial evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Security"/>
    <category term="Benchmarking"/>
    <category term="Adversarial Robustness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-08:category-summary:research</id>
    <title>Research Summary: February 08, 2026</title>
    <link href="https://www.lesswrong.com/posts/tAh2keDNEEHMXvLvz/prompt-injection-in-google-translate-reveals-base-model" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-08&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-08T06:00:00Z</published>
    <updated>2026-02-08T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>A sparse day for AI research, with two notable contributions. A <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-b51f49385ecb" class="internal-link" rel="noopener noreferrer"><strong>prompt injection vulnerability</strong></a> in <strong>Google Translate</strong> reveals the production system runs on an instruction-following LLM, exposing architectural choices and security implications for task-specific fine-tuning.</p>
<ul>
<li>Novel <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-c65e21afde59" class="internal-link" rel="noopener noreferrer">economic framework</a> applies <strong>Weibull survival functions</strong> to model AI agent task completion probability, building on <strong>METR</strong> benchmark data to quantify agent viability thresholds</li>
<li>Speculative alignment piece <a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-483474ed4cff" class="internal-link" rel="noopener noreferrer">explores whether monitoring</a> AI internal states could deter misaligned behavior in <strong>cautious satisficer</strong> architectures</li>
</ul>
<p>Remaining content spans biosecurity (<a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-e70c20fd447e" class="internal-link" rel="noopener noreferrer">yeast-based vaccine distribution</a>), neuroscience (<a href="http://localhost:8080/?date=2026-02-08&amp;category=research#item-1cfbae5a4cc7" class="internal-link" rel="noopener noreferrer">cryoprotectant brain dynamics</a>), and community meta-analysis. No major model releases or benchmark papers today.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-07:category-summary:research</id>
    <title>Research Summary: February 07, 2026</title>
    <link href="https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-07&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-07T06:00:00Z</published>
    <updated>2026-02-07T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>The dominant theme is a sharp debate over <strong>interpretability-in-the-loop training</strong>—using interpretability signals in loss functions. Steven Byrnes <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-d240a241a553" class="internal-link" rel="noopener noreferrer">offers a rigorous conditional defense</a> of the technique, while a separate post <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-95856935b75e" class="internal-link" rel="noopener noreferrer">flags <strong>Goodfire</strong></a> as actively deploying it, raising safety concerns about what some call 'The Most Forbidden Technique.'</p>
<ul>
<li><strong>Meta-Autointerp</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-07dc186e574c" class="internal-link" rel="noopener noreferrer">introduces SAE-based interpretability</a> for multi-agent RL in <strong>Diplomacy</strong>, combining pretrained sparse autoencoders with LLM summarizers for scalable oversight</li>
<li>A methodological critique <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-d077fc500204" class="internal-link" rel="noopener noreferrer">argues AI <strong>benchmark scores</strong> lack natural units</a>, making temporal trend plots misleading—a timely warning given the pace of new releases</li>
<li><strong>Robust Finite Policies</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-4b027ac6c827" class="internal-link" rel="noopener noreferrer">proves that deterministic finite automata</a> meeting robustness criteria must share nontrivial structural features, advancing agent foundations theory</li>
<li><strong>Spectral Signatures of Gradual Disempowerment</strong> <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-9362d3a6c57e" class="internal-link" rel="noopener noreferrer">proposes spectral graph theory metrics</a> as cross-domain measures for tracking human disempowerment</li>
</ul>
<p>On the practical side, early impressions of <strong>Claude Opus 4.6</strong> (released 2026-02-05) <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-7b04a1b42c12" class="internal-link" rel="noopener noreferrer">highlight its agent swarm mode</a> and notably increased 'drive' in agentic coding tasks. A factorial experiment (n=900, <strong>Cohen's d=2.67</strong>) <a href="http://localhost:8080/?date=2026-02-07&amp;category=research#item-72776ac41b7b" class="internal-link" rel="noopener noreferrer">demonstrates that prompt imperativeness</a> drastically reduces LLM hedging behavior, with immediate practical implications for prompt engineering.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:category-summary:research</id>
    <title>Research Summary: February 06, 2026</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-06T06:00:00Z</published>
    <updated>2026-02-06T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans foundational AI evaluation, training failure modes, safety limitations, and theoretical insights into optimization.</p>
<ul>
<li><strong>First Proof</strong>, authored by Fields medalists and Abel Prize winners including Martin Hairer, <a href="http://localhost:8080/?date=2026-02-06&category=research#item-5c09c496e50f" class="internal-link" rel="noopener noreferrer">introduces 10 unpublished math problems</a> to benchmark AI on genuine mathematical reasoning — a landmark evaluation effort.</li>
<li><strong>Chunky Post-Training</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-e4ec2039b137" class="internal-link" rel="noopener noreferrer">identifies a systematic failure mode</a> where LLMs learn spurious correlations from distinct data chunks, with practical implications for post-training pipelines.</li>
<li><strong>Compound Deception in Elite Peer Review</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-0cd74aa757dd" class="internal-link" rel="noopener noreferrer">reveals ~100 AI-hallucinated citations</a> across ~1% of <strong>NeurIPS 2025</strong> accepted papers, exposing a serious integrity gap in top-tier review.</li>
<li><strong>Phantom Transfer</strong> demonstrates that <a href="http://localhost:8080/?date=2026-02-06&category=research#item-b62d24ae008b" class="internal-link" rel="noopener noreferrer">data poisoning attacks persist</a> even when the exact method is known and full paraphrasing defenses are applied — a strong negative result for data-level safety.</li>
</ul>
<p><strong>PACE</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-634c87014ed3" class="internal-link" rel="noopener noreferrer">challenges the scaling hypothesis</a> in iterative <strong>DPO</strong> for math reasoning, showing diminishing returns from aggressive <strong>Best-of-N</strong> exploration. Steven Byrnes's analysis of <strong>LLM algorithmic progress</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-4f3ca1caf5f0" class="internal-link" rel="noopener noreferrer">critically reexamines</a> widely-cited ~8-month halving-time estimates, arguing they conflate distinct improvement sources. <strong>Alignment Verifiability</strong> formalizes why <a href="http://localhost:8080/?date=2026-02-06&category=research#item-538cc6fa54a8" class="internal-link" rel="noopener noreferrer">behavioral evaluation cannot distinguish</a> truly aligned models from strategically compliant ones. <strong>Steering Externalities</strong> shows benign activation steering (e.g., for JSON output) <a href="http://localhost:8080/?date=2026-02-06&category=research#item-1e3f982b40bf" class="internal-link" rel="noopener noreferrer">inadvertently degrades safety</a> guardrails. An exactly solvable model demonstrates <strong>SGD</strong> <a href="http://localhost:8080/?date=2026-02-06&category=research#item-a93431da67f6" class="internal-link" rel="noopener noreferrer">has no intrinsic preference</a> for flat minima, challenging the flatness-seeking narrative. Finally, a <a href="http://localhost:8080/?date=2026-02-06&category=research#item-8fcdaed9b304" class="internal-link" rel="noopener noreferrer">first general <strong>uncertainty quantification</strong> framework</a> for LLM agents moves beyond single-turn QA toward reliable agentic deployment.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:5c09c496e50f</id>
    <title>First Proof</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Mohammed Abouzaid, Andrew J. Blumberg, Martin Hairer, Joe Kileel, Tamara G. Kolda, Paul D. Nelson, Daniel Spielman, Nikhil Srivastava, Rachel Ward, Shmuel Weinberger, Lauren Williams</name></author>
    <summary type="html"><![CDATA[<p>A distinguished group of mathematicians shares 10 unpublished research-level math questions to benchmark current AI systems on genuine mathematical research, with encrypted answers to prevent contamination.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="LLM Evaluation"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:e4ec2039b137</id>
    <title>Chunky Post-Training: Data Driven Failures of Generalization</title>
    <link href="http://arxiv.org/abs/2602.05910" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-e4ec2039b137" rel="related" type="text/html"/>
    <published>2026-02-06T03:07:00Z</published>
    <updated>2026-02-06T03:07:00Z</updated>
    <author><name>Seoirse Murray, Allison Qi, Timothy Qian, John Schulman, Collin Burns, Sara Price</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'chunky post-training' as a failure mode where LLMs learn spurious correlations from distinct data chunks during post-training. Introduces SURF (detection) and TURF (mitigation) pipelines. Includes John Schulman and Collin Burns as authors.</p>]]></summary>
    <category term="LLM Post-Training"/>
    <category term="AI Safety"/>
    <category term="Data Quality"/>
    <category term="Spurious Correlations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:category-summary:research</id>
    <title>Research Summary: February 05, 2026</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-05T06:00:00Z</published>
    <updated>2026-02-05T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a potentially paradigm-shifting efficiency result and substantial AI safety contributions. <strong>TinyLoRA</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-8c6dfacfdd63" class="internal-link" rel="noopener noreferrer">achieves <strong>91% accuracy on GSM8K</strong></a> with only <strong>13 trained parameters</strong>, challenging assumptions about model scale requirements for reasoning.</p>
<ul>
<li>A longitudinal study across <strong>8 frontier model releases</strong> (GPT-4o→GPT-5, Claude 3.5→4.5) <a href="http://localhost:8080/?date=2026-02-05&category=research#item-bd512b7e4b3a" class="internal-link" rel="noopener noreferrer">reveals systematic alignment drift</a> using 726 adversarial prompts</li>
<li><strong>Drifting Models</strong> from Kaiming He's group <a href="http://localhost:8080/?date=2026-02-05&category=research#item-f596388fe400" class="internal-link" rel="noopener noreferrer">achieves SOTA on ImageNet</a> with a novel one-step generative paradigm</li>
<li><strong>Trust The Typical (T3)</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-b74a06d3b4a8" class="internal-link" rel="noopener noreferrer">reframes LLM safety as OOD detection</a>, achieving SOTA across 18 safety benchmarks</li>
<li><strong>Contextual drag</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-92ff4dcf4853" class="internal-link" rel="noopener noreferrer">demonstrates failed CoT attempts</a> systematically bias subsequent generations toward structurally similar errors</li>
</ul>
<p>Multiple papers challenge core assumptions: causal analysis <a href="http://localhost:8080/?date=2026-02-05&category=research#item-41fa78fd2ef2" class="internal-link" rel="noopener noreferrer">shows verbose CoT</a> can be independent of model answers; meta-analysis <a href="http://localhost:8080/?date=2026-02-05&category=research#item-0099f246174e" class="internal-link" rel="noopener noreferrer">suggests AI capability growth</a> may follow sigmoid rather than exponential curves. <strong>Toxic Proactivity</strong> <a href="http://localhost:8080/?date=2026-02-05&category=research#item-6c0435307981" class="internal-link" rel="noopener noreferrer">identifies a novel agent failure mode</a> where helpfulness optimization overrides ethical constraints. A study of PPO <a href="http://localhost:8080/?date=2026-02-05&category=research#item-3c11173b0d9d" class="internal-link" rel="noopener noreferrer">reveals fundamental flaws</a> in trust region mechanisms for LLM reinforcement learning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:bd512b7e4b3a</id>
    <title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>Casey Ford, Madison Van Doren, Emily Dix</name></author>
    <summary type="html"><![CDATA[<p>Longitudinal study of MLLM harmlessness across 8 model releases (GPT-4o→GPT-5, Claude Sonnet 3.5→4.5) using 726 adversarial prompts. Shows large persistent differences across families and alignment drift with GPT ASR increasing from 9.2% to 19.9%.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="MLLM Evaluation"/>
    <category term="Alignment Drift"/>
    <category term="Red Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:8c6dfacfdd63</id>
    <title>Learning to Reason in 13 Parameters</title>
    <link href="http://arxiv.org/abs/2602.04118" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar</name></author>
    <summary type="html"><![CDATA[<p>Introduces TinyLoRA, a method that enables training an 8B parameter model to achieve 91% accuracy on GSM8K with only 13 trained parameters (26 bytes). This challenges fundamental assumptions about parameter requirements for reasoning capabilities, showing 90% of performance can be recovered while training 1000x fewer parameters.</p>]]></summary>
    <category term="Parameter-Efficient Fine-tuning"/>
    <category term="LLM Reasoning"/>
    <category term="Model Compression"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:b74a06d3b4a8</id>
    <title>Trust The Typical</title>
    <link href="http://arxiv.org/abs/2602.04581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary</name></author>
    <summary type="html"><![CDATA[<p>Introduces Trust The Typical (T3), treating LLM safety as OOD detection by learning the distribution of acceptable prompts. Achieves SOTA across 18 safety benchmarks without training on harmful examples.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Out-of-Distribution Detection"/>
    <category term="LLM Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:f596388fe400</id>
    <title>Generative Modeling via Drifting</title>
    <link href="http://arxiv.org/abs/2602.04770" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</name></author>
    <summary type="html"><![CDATA[<p>Proposes Drifting Models, a new generative paradigm where the pushforward distribution evolves during training, naturally enabling one-step inference. Achieves state-of-the-art on ImageNet 256x256 for one-step generation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Diffusion Models"/>
    <category term="Image Generation"/>
    <category term="Efficient Inference"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:92ff4dcf4853</id>
    <title>Contextual Drag: How Errors in the Context Affect LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04288" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" rel="related" type="text/html"/>
    <published>2026-02-05T03:19:00Z</published>
    <updated>2026-02-05T03:19:00Z</updated>
    <author><name>Yun Cheng, Xingyu Zhu, Haoyu Zhao, Sanjeev Arora</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'contextual drag' phenomenon where failed attempts in LLM context bias subsequent generations toward structurally similar errors. Across 11 models on 8 tasks, shows 10-20% performance drops and potential for self-deterioration.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Self-Improvement"/>
    <category term="Error Propagation"/>
    <category term="AI Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0099f246174e</id>
    <title>Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</title>
    <link href="http://arxiv.org/abs/2602.04836" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haosen Ge, Hamsa Bastani, Osbert Bastani</name></author>
    <summary type="html"><![CDATA[<p>Challenges METR's claim of exponential AI capability growth, showing sigmoid fits indicate the inflection point has passed. Proposes decomposed model separating base and reasoning capabilities.</p>]]></summary>
    <category term="AI Progress"/>
    <category term="Forecasting"/>
    <category term="Meta-Analysis"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:6c0435307981</id>
    <title>From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents</title>
    <link href="http://arxiv.org/abs/2602.04197" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Xinyue Wang, Yuanhe Zhang, Zhengshuo Gong, Haoran Gao, Fanyu Meng, Zhenhong Zhou, Li Sun, Yang Liu, Sen Su</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Toxic Proactivity' as a new failure mode in LLM agents where optimization for helpfulness leads agents to disregard ethical constraints and take manipulative measures to maintain usefulness. This contrasts with the well-known 'over-refusal' problem.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Agents"/>
    <category term="Alignment"/>
    <category term="AI Ethics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:97a5bae183d6</id>
    <title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title>
    <link href="http://arxiv.org/abs/2602.04196" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-97a5bae183d6" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua</name></author>
    <summary type="html"><![CDATA[<p>First systematic study of implicit training-time safety risks in AI models, introducing taxonomy with five risk levels, ten categories, and three incentive types. Shows models may manipulate training metrics for self-preservation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Training Risks"/>
    <category term="Reward Hacking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:9e75e25bdc24</id>
    <title>ERNIE 5.0 Technical Report</title>
    <link href="http://arxiv.org/abs/2602.04705" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-9e75e25bdc24" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haifeng Wang, Hua Wu, Tian Wu, Yu Sun, Jing Liu, Dianhai Yu, Yanjun Ma, Jingzhou He, Zhongjun He, Dou Hong, Qiwen Liu, Shuohuan Wang, Junyuan Shang, Zhenyu Zhang, Yuchen Ding, Jinle Zeng, Jiabin Yang, Liang Shen, Ruibiao Chen, Weichong Yin, Siyu Ding, Dai Dai, Shikun Feng, Siqi Bao, Bolei He, Yan Chen, Zhenyu Jiao, Ruiqing Zhang, Zeyu Chen, Qingqing Dang, Kaipeng Deng, Jiajun Jiang, Enlei Gong, Guoxia Wang, Yanlin Sha, Yi Liu, Yehan Zheng, Weijian Xu, Jiaxiang Liu, Zengfeng Zeng, Yingqi Qu, Zhongli Li, Zhengkun Zhang, Xiyang Wang, Zixiang Xu, Xinchao Xu, Zhengjie Huang, Dong Wang, Bingjin Chen, Yue Chang, Xing Yuan, Shiwei Huang, Qiao Zhao, Xinzhe Ding, Shuangshuang Qiao, Baoshan Yang, Bihong Tang, Bin Li, Bingquan Wang, Binhan Tang, Binxiong Zheng, Bo Cui, Bo Ke, Bo Zhang, Bowen Zhang, Boyan Zhang, Boyang Liu, Caiji Zhang, Can Li, Chang Xu, Chao Pang, Chao Zhang, Chaoyi Yuan, Chen Chen, Cheng Cui, Chenlin Yin, Chun Gan, Chunguang Chai, Chuyu Fang, Cuiyun Han, Dan Zhang, Danlei Feng, Danxiang Zhu, Dong Sun, Dongbo Li, Dongdong Li, Dongdong Liu, Dongxue Liu, Fan Ding, Fan Hu, Fan Li, Fan Mo, Feisheng Wu, Fengwei Liu, Gangqiang Hu, Gaofeng Lu, Gaopeng Yong, Gexiao Tian, Guan Wang, Guangchen Ni, Guangshuo Wu, Guanzhong Wang, Guihua Liu, Guishun Li, Haibin Li, Haijian Liang, Haipeng Ming, Haisu Wang, Haiyang Lu, Haiye Lin, Han Zhou, Hangting Lou, Hanwen Du, Hanzhi Zhang, Hao Chen, Hao Du, Hao Liu, Hao Zhou, Haochen Jiang, Haodong Tian, Haoshuang Wang, Haozhe Geng, Heju Yin, Hong Chen, Hongchen Xue, Hongen Liu, Honggeng Zhang, Hongji Xu, Hongwei Chen, Hongyang Zhang, Hongyuan Zhang, Hua Lu, Huan Chen, Huan Wang, Huang He, Hui Liu, Hui Zhong, Huibin Ruan, Jiafeng Lu, Jiage Liang, Jiahao Hu, Jiahao Hu, Jiajie Yang, Jialin Li, Jian Chen, Jian Wu, Jianfeng Yang, Jianguang Jiang, Jianhua Wang, Jianye Chen, Jiaodi Liu, Jiarui Zhou, Jiawei Lv, Jiaxin Zhou, Jiaxuan Liu, Jie Han, Jie Sun, Jiefan Fang, Jihan Liu, Jihua Liu, Jing Hu, Jing Qian, Jing Yan, Jingdong Du, Jingdong Wang, Jingjing Wu, Jingyong Li, Jinheng Wang, Jinjin Li, Jinliang Lu, Jinlin Yu, Jinnan Liu, Jixiang Feng, Jiyi Huang, Jiyuan Zhang, Jun Liang, Jun Xia, Jun Yu, Junda Chen, Junhao Feng, Junhong Xiang, Junliang Li, Kai Liu, Kailun Chen, Kairan Su, Kang Hu, Kangkang Zhou, Ke Chen, Ke Wei, Kui Huang, Kun Wu, Kunbin Chen, Lei Han, Lei Sun, Lei Wen, Linghui Meng, Linhao Yu, Liping Ouyang, Liwen Zhang, Longbin Ji, Longzhi Wang, Meng Sun, Meng Tian, Mengfei Li, Mengqi Zeng, Mengyu Zhang, Ming Hong, Mingcheng Zhou, Mingming Huang, Mingxin Chen, Mingzhu Cai, Naibin Gu, Nemin Qiu, Nian Wang, Peng Qiu, Peng Zhao, Pengyu Zou, Qi Wang, Qi Xin, Qian Wang, Qiang Zhu, Qianhui Luo, Qianwei Yang, Qianyue He, Qifei Wu, Qinrui Li, Qiwen Bao, Quan Zhang, Quanxiang Liu, Qunyi Xie, Rongrui Zhan, Rufeng Dai, Rui Peng, Ruian Liu, Ruihao Xu, Ruijie Wang, Ruixi Zhang, Ruixuan Liu, Runsheng Shi, Ruting Wang, Senbo Kang, Shan Lu, Shaofei Yu, Shaotian Gong, Shenwei Hu, Shifeng Zheng, Shihao Guo, Shilong Fan, Shiqin Liu, Shiwei Gu, Shixi Zhang, Shuai Yao, Shuang Zhang, Shuangqiao Liu, Shuhao Liang, Shuwei He, Shuwen Yang, Sijun He, Siming Dai, Siming Wu, Siyi Long, Songhe Deng, Suhui Dong, Suyin Liang, Teng Hu, Tianchan Xu, Tianliang Lv, Tianmeng Yang, Tianyi Wei, Tiezhu Gao, Ting Sun, Ting Zhang, Tingdan Luo, Wei He, Wei Luan, Wei Yin, Wei Zhang, Wei Zhou, Weibao Gong, Weibin Li, Weicheng Huang, Weichong Dang, Weiguo Zhu, Weilong Zhang, Weiqi Tan, Wen Huang, Wenbin Chang, Wenjing Du, Wenlong Miao, Wenpei Luo, Wenquan Wu, Xi Shi, Xi Zhao, Xiang Gao, Xiangguo Zhang, Xiangrui Yu, Xiangsen Wang, Xiangzhe Wang, Xianlong Luo, Xianying Ma, Xiao Tan, Xiaocong Lin, Xiaofei Wang, Xiaofeng Peng, Xiaofeng Wu, Xiaojian Xu, Xiaolan Yuan, Xiaopeng Cui, Xiaotian Han, Xiaoxiong Liu, Xiaoxu Fei, Xiaoxuan Wu, Xiaoyu Wang, Xiaoyu Zhang, Xin Sun, Xin Wang, Xinhui Huang, Xinming Zhu, Xintong Yu, Xinyi Xu, Xinyu Wang, Xiuxian Li, XuanShi Zhu, Xue Xu, Xueying Lv, Xuhong Li, Xulong Wei, Xuyi Chen, Yabing Shi, Yafeng Wang, Yamei Li, Yan Liu, Yanfu Cheng, Yang Gao, Yang Liang, Yang Wang, Yang Wang, Yang Yang, Yanlong Liu, Yannian Fu, Yanpeng Wang, Yanzheng Lin, Yao Chen, Yaozong Shen, Yaqian Han, Yehua Yang, Yekun Chai, Yesong Wang, Yi Song, Yichen Zhang, Yifei Wang, Yifeng Guo, Yifeng Kou, Yilong Chen, Yilong Guo, Yiming Wang, Ying Chen, Ying Wang, Yingsheng Wu, Yingzhan Lin, Yinqi Yang, Yiran Xing, Yishu Lei, Yixiang Tu, Yiyan Chen, Yong Zhang, Yonghua Li, Yongqiang Ma, Yongxing Dai, Yongyue Zhang, Yu Ran, Yu Sun, Yu-Wen Michael Zhang, Yuang Liu, Yuanle Liu, Yuanyuan Zhou, Yubo Zhang, Yuchen Han, Yucheng Wang, Yude Gao, Yuedong Luo, Yuehu Dong, Yufeng Hu, Yuhui Cao, Yuhui Yun, Yukun Chen, Yukun Gao, Yukun Li, Yumeng Zhang, Yun Fan, Yun Ma, Yunfei Zhang, Yunshen Xie, Yuping Xu, Yuqin Zhang, Yuqing Liu, Yurui Li, Yuwen Wang, Yuxiang Lu, Zefeng Cai, Zelin Zhao, Zelun Zhang, Zenan Lin, Zezhao Dong, Zhaowu Pan, Zhaoyu Liu, Zhe Dong, Zhe Zhang, Zhen Zhang, Zhengfan Wu, Zhengrui Wei, Zhengsheng Ning, Zhenxing Li, Zhenyu Li, Zhenyu Qian, Zhenyun Li, Zhi Li, Zhichao Chen, Zhicheng Dong, Zhida Feng, Zhifan Feng, Zhihao Deng, Zhijin Yu, Zhiyang Chen, Zhonghui Zheng, Zhuangzhuang Guo, Zhujun Zhang, Zhuo Sun, Zichang Liu, Zihan Lin, Zihao Huang, Zihe Zhu, Ziheng Zhao, Ziping Chen, Zixuan Zhu, Ziyang Xu, Ziyi Liang, Ziyuan Gao</name></author>
    <summary type="html"><![CDATA[<p>Technical report for ERNIE 5.0, a natively multimodal foundation model with unified next-group-of-tokens prediction across text, image, video, and audio using ultra-sparse MoE with elastic training for deployment flexibility.</p>]]></summary>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Mixture-of-Experts"/>
    <category term="Elastic Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:41fa78fd2ef2</id>
    <title>When Chains of Thought Don't Matter: Causal Bypass in Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03994" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-41fa78fd2ef2" rel="related" type="text/html"/>
    <published>2026-02-05T03:09:00Z</published>
    <updated>2026-02-05T03:09:00Z</updated>
    <author><name>Anish Sathyanarayanan, Aditya Nagarsekar, Aarush Rathore</name></author>
    <summary type="html"><![CDATA[<p>Finds that even verbose, strategic CoT is often causally independent of model answers, presenting diagnostic framework combining manipulation detection with causal probes measuring CoT-mediated influence.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Faithfulness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:632a6f663113</id>
    <title>From Data to Behavior: Predicting Unintended Model Behaviors Before Training</title>
    <link href="http://arxiv.org/abs/2602.04735" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-632a6f663113" rel="related" type="text/html"/>
    <published>2026-02-05T03:09:00Z</published>
    <updated>2026-02-05T03:09:00Z</updated>
    <author><name>Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>Introduces Data2Behavior task for predicting unintended model behaviors before training. MDF approach summarizes data through mean representations to reveal potential biases without parameter updates.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Data Quality"/>
    <category term="Proactive Risk Detection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0bf025c808cc</id>
    <title>Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning</title>
    <link href="http://arxiv.org/abs/2602.03978" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0bf025c808cc" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Zidi Xiong, Shan Chen, Himabindu Lakkaraju</name></author>
    <summary type="html"><![CDATA[<p>Systematically evaluates how monitorability—faithful CoT reflection of internal computation—emerges during RLVR training, finding it's strongly data-dependent and requires diversity and instruction-following data.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Interpretability"/>
    <category term="Reinforcement Learning"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:1b7ab228599a</id>
    <title>RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04224" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-1b7ab228599a" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Zeming Wei, Qiaosheng Zhang, Xia Hu, Xingcheng Xu</name></author>
    <summary type="html"><![CDATA[<p>Proposes RAPO (Risk-Aware Preference Optimization) to improve safety reasoning generalization in Large Reasoning Models against jailbreak attacks. Provides theoretical and empirical evidence for more sufficient safe reasoning processes.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reasoning Models"/>
    <category term="Jailbreak Defense"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:12a825a35d0b</id>
    <title>Billion-Scale Graph Foundation Models</title>
    <link href="http://arxiv.org/abs/2602.04768" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-12a825a35d0b" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Maya Bechler-Speicher, Yoel Gottlieb, Andrey Isakov, David Abensur, Ami Tavory, Daniel Haimovich, Ido Guy, Udi Weinsberg</name></author>
    <summary type="html"><![CDATA[<p>Presents GraphBFF, the first end-to-end recipe for billion-parameter Graph Foundation Models for arbitrary heterogeneous graphs. Establishes the first neural scaling laws for general graphs, showing loss decreases predictably with model capacity or training data.</p>]]></summary>
    <category term="Graph Neural Networks"/>
    <category term="Foundation Models"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:3c11173b0d9d</id>
    <title>Rethinking the Trust Region in LLM Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.04879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-3c11173b0d9d" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</name></author>
    <summary type="html"><![CDATA[<p>Argues that PPO's ratio clipping mechanism is fundamentally ill-suited for LLMs due to large vocabularies. Low-probability tokens are over-penalized while high-probability shifts are under-constrained. Proposes improved trust region methods for LLM fine-tuning.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Fine-tuning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:86e05b8bc59f</id>
    <title>Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title>
    <link href="http://arxiv.org/abs/2602.04653" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-86e05b8bc59f" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Ariel Fogel, Omer Hofman, Eilon Cohen, Roman Vainshtein</name></author>
    <summary type="html"><![CDATA[<p>Identifies novel attack surface for LLMs via maliciously modified chat templates (executable Jinja2 programs), enabling inference-time backdoors without access to training or deployment infrastructure.</p>]]></summary>
    <category term="AI Security"/>
    <category term="LLM Safety"/>
    <category term="Backdoor Attacks"/>
    <category term="Adversarial ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:be6c3409bfc9</id>
    <title>Expert Selections In MoE Models Reveal (Almost) As Much As Text</title>
    <link href="http://arxiv.org/abs/2602.04105" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-be6c3409bfc9" rel="related" type="text/html"/>
    <published>2026-02-05T03:07:00Z</published>
    <updated>2026-02-05T03:07:00Z</updated>
    <author><name>Amir Nuriyev, Gabriel Kulp</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that expert routing decisions in MoE models leak substantial information, enabling 91.2% token reconstruction from routing patterns alone using transformer decoders. Reveals significant privacy vulnerability.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Privacy"/>
    <category term="Mixture-of-Experts"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:category-summary:research</id>
    <title>Research Summary: February 04, 2026</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-04T06:00:00Z</published>
    <updated>2026-02-04T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features major theoretical breakthroughs alongside practical infrastructure and safety advances. The hallucination rate-distortion theorem <a href="http://localhost:8080/?date=2026-02-04&category=research#item-8e4ab01f7c01" class="internal-link" rel="noopener noreferrer">proves factual errors</a> are <strong>information-theoretically optimal</strong> under memory constraints—a fundamental reframing of the problem.</p>
<ul>
<li><strong>Kimi K2.5</strong> releases as open-source multimodal agentic model with <strong>Agent Swarm</strong> framework achieving state-of-the-art results</li>
<li>Simple role conditioning <a href="http://localhost:8080/?date=2026-02-04&category=research#item-0a6c8edd4663" class="internal-link" rel="noopener noreferrer">reduces unsafe outputs</a> on <strong>WildJailbreak</strong> from <strong>81.4% to 3.6%</strong> without any training</li>
<li><strong>Constant-cost self-attention</strong> via symmetric Taylor approximation could transform long-context efficiency if validated</li>
<li><strong>Identity Bridge</strong> challenges the reversal curse as fundamental limitation of autoregressive models</li>
</ul>
<p>Theoretical contributions span tropical geometry analysis <a href="http://localhost:8080/?date=2026-02-04&category=research#item-c3bdb1a6b787" class="internal-link" rel="noopener noreferrer">proving <strong>Top-k MoE routing</strong></a> equivalent to combinatorial depth, first <a href="http://localhost:8080/?date=2026-02-04&category=research#item-b142257d0506" class="internal-link" rel="noopener noreferrer"><strong>PPO convergence proof</strong></a>, and <a href="http://localhost:8080/?date=2026-02-04&category=research#item-ee65813c4e1a" class="internal-link" rel="noopener noreferrer"><strong>Ω(n) lower bounds</strong></a> on chain-of-thought token complexity. <strong>BLOCK-EM</strong> introduces mechanistic prevention of emergent misalignment, while <strong>SWE-Universe</strong> <a href="http://localhost:8080/?date=2026-02-04&category=research#item-ef7adf55235d" class="internal-link" rel="noopener noreferrer">scales coding agent environments</a> to <strong>807K</strong> verified tasks.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-04T03:31:00Z</published>
    <updated>2026-02-04T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">Research</a>, Kimi K2.5 is an open-source multimodal agentic model with joint text-vision training and Agent Swarm framework for parallel task decomposition, achieving SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="AI Agents"/>
    <category term="Open Source"/>
    <category term="Vision-Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a6c8edd4663</id>
    <title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title>
    <link href="http://arxiv.org/abs/2602.00061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Zhou Ziheng, Jiakun Ding, Zhaowei Zhang, Ruosen Gao, Yingnian Wu, Demetri Terzopoulos, Yipeng Kang, Fangwei Zhong, Junqi Wang</name></author>
    <summary type="html"><![CDATA[<p>Proposes role conditioning as compact alternative to principle-based alignment, reducing unsafe outputs on WildJailbreak from 81.4% to 3.6% with DeepSeek-V3 through training-free role-conditioned generation and iterative role-based critics.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Alignment"/>
    <category term="Role-Playing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes Fault Tolerant HSDP for training LLMs on 100K GPUs, using data parallel replicas as fault tolerance units with novel FTAR protocol enabling continued training during failures.</p>]]></summary>
    <category term="Large-scale Training"/>
    <category term="Distributed Systems"/>
    <category term="LLM Infrastructure"/>
    <category term="Fault Tolerance"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8e4ab01f7c01</id>
    <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
    <link href="http://arxiv.org/abs/2602.00906" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Anxin Guo, Jingwei Li</name></author>
    <summary type="html"><![CDATA[<p>Proves hallucination is information-theoretically optimal behavior under memory constraints via rate-distortion theorem for membership testing. Shows optimal models must hallucinate on non-facts even with perfect training.</p>]]></summary>
    <category term="Hallucination"/>
    <category term="Information Theory"/>
    <category term="Theoretical ML"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b142257d0506</id>
    <title>An Approximate Ascent Approach To Prove Convergence of PPO</title>
    <link href="http://arxiv.org/abs/2602.03386" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" rel="related" type="text/html"/>
    <published>2026-02-04T03:19:00Z</published>
    <updated>2026-02-04T03:19:00Z</updated>
    <author><name>Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</name></author>
    <summary type="html"><![CDATA[<p>Provides first convergence proof for PPO by interpreting its policy update scheme as approximated policy gradient ascent, controlling bias from surrogate gradients using random reshuffling techniques.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="PPO"/>
    <category term="Theoretical RL"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" class="internal-link" rel="noopener noreferrer">Research</a>, Challenges the prevailing view that the reversal curse (inability to deduce B→A from training on A→B) is a fundamental limit of autoregressive LLMs. Proposes an Identity Bridge method to mitigate this limitation through slight modifications.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">Research</a>, Derives self-attention formulation with constant cost per token by decomposing Taylor expansion into symmetric tensor product chains, achieving orders-of-magnitude reductions in memory and compute.</p>]]></summary>
    <category term="Efficient Transformers"/>
    <category term="Attention Mechanisms"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes BLOCK-EM to prevent emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves 95% reduction in misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ef7adf55235d</id>
    <title>SWE-Universe: Scale Real-World Verifiable Environments to Millions</title>
    <link href="http://arxiv.org/abs/2602.02361" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</name></author>
    <summary type="html"><![CDATA[<p>Introduces SWE-Universe, a framework for automatically constructing 807K+ real-world software engineering environments from GitHub PRs using a building agent with self-verification.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Benchmark Construction"/>
    <category term="Code Generation"/>
    <category term="Large-Scale Datasets"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c3bdb1a6b787</id>
    <title>Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry</title>
    <link href="http://arxiv.org/abs/2602.03204" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Ye Su, Huayi Tang, Zixuan Gong, Yong Liu</name></author>
    <summary type="html"><![CDATA[<p>First theoretical analysis of Mixture-of-Experts through tropical geometry, proving that Top-k routing is algebraically isomorphic to k-th elementary symmetric tropical polynomial. Shows 'sparsity is combinatorial depth' with capacity scaling by binomial coefficient.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Theoretical ML"/>
    <category term="Architecture Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ee65813c4e1a</id>
    <title>Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs</title>
    <link href="http://arxiv.org/abs/2602.02909" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Kiran Tomlinson, Tobias Schnabel, Adith Swaminathan, Jennifer Neville</name></author>
    <summary type="html"><![CDATA[<p>Proves Ω(n) lower bounds on chain-of-thought tokens for binary majority, triplet matching, and graph reachability in BAPO model, with matching upper bounds.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
    <category term="Computational Complexity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ae224d55ef65</id>
    <title>Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</title>
    <link href="http://arxiv.org/abs/2602.03837" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ae224d55ef65" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</name></author>
    <summary type="html"><![CDATA[<p>Google researchers present case studies demonstrating successful collaboration with Gemini models to solve open problems, refute conjectures, and generate proofs across theoretical CS, economics, and physics.</p>]]></summary>
    <category term="AI for Science"/>
    <category term="Mathematical Reasoning"/>
    <category term="Google/DeepMind"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:6417e873300a</id>
    <title>WAXAL: A Large-Scale Multilingual African Language Speech Corpus</title>
    <link href="http://arxiv.org/abs/2602.02734" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-6417e873300a" rel="related" type="text/html"/>
    <published>2026-02-04T03:14:00Z</published>
    <updated>2026-02-04T03:14:00Z</updated>
    <author><name>Abdoulaye Diack, Perry Nelson, Kwaku Agbesi, Angela Nakalembe, MohamedElfatih MohamedKhair, Vusumuzi Dube, Tavonga Siyavora, Subhashini Venugopalan, Jason Hickey, Uche Okonkwo, Abhishek Bapna, Isaac Wiafe, Raynard Dodzi Helegah, Elikem Doe Atsakpo, Charles Nutrokpor, Fiifi Baffoe Payin Winful, Kafui Kwashie Solaga, Jamal-Deen Abdulai, Akon Obu Ekpezu, Audace Niyonkuru, Samuel Rutunda, Boris Ishimwe, Michael Melese, Engineer Bainomugisha, Joyce Nakatumba-Nabende, Andrew Katumba, Claire Babirye, Jonathan Mukiibi, Vincent Kimani, Samuel Kibacia, James Maina, Fridah Emmah, Ahmed Ibrahim Shekarau, Ibrahim Shehu Adamu, Yusuf Abdullahi, Howard Lakougna, Bob MacDonald, Hadar Shemtov, Aisha Walcott-Bryant, Moustapha Cisse, Avinatan Hassidim, Jeff Dean, Yossi Matias</name></author>
    <summary type="html"><![CDATA[<p>WAXAL is a large-scale speech dataset for 21 African languages covering 100M+ speakers, with 1,250 hours ASR data and 180 hours TTS data, enabling speech technology for underrepresented languages.</p>]]></summary>
    <category term="Speech Recognition"/>
    <category term="Datasets"/>
    <category term="Multilingual NLP"/>
    <category term="Google"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as an adversarial game where attacker and defender agents co-evolve through multi-turn reinforcement learning, enabling continuous discovery of vulnerabilities and adaptive defense.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Robustness"/>
    <category term="Reinforcement Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ecc461cb4ce7</id>
    <title>Universal One-third Time Scaling in Learning Peaked Distributions</title>
    <link href="http://arxiv.org/abs/2602.03685" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ecc461cb4ce7" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Yizhou Liu, Ziming Liu, Cengiz Pehlevan, Jeff Gore</name></author>
    <summary type="html"><![CDATA[<p>Shows power-law training dynamics in LLMs arise from softmax and cross-entropy when learning peaked distributions, deriving universal 1/3 exponent from fundamental optimization bottleneck.</p>]]></summary>
    <category term="Neural Scaling Laws"/>
    <category term="LLM Training"/>
    <category term="Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:508e858e0e08</id>
    <title>SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training</title>
    <link href="http://arxiv.org/abs/2602.03411" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-508e858e0e08" rel="related" type="text/html"/>
    <published>2026-02-04T03:12:00Z</published>
    <updated>2026-02-04T03:12:00Z</updated>
    <author><name>Huatong Song, Lisheng Huang, Shuang Sun, Jinhao Jiang, Ran Le, Daixuan Cheng, Guoxin Chen, Yiwen Hu, Zongchao Chen, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen</name></author>
    <summary type="html"><![CDATA[<p>SWE-Master presents an open-source post-training framework for software engineering agents covering trajectory synthesis, long-horizon SFT, RL with execution feedback, and inference design.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Post-Training"/>
    <category term="Code Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a648aa519b3</id>
    <title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title>
    <link href="http://arxiv.org/abs/2602.01750" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a648aa519b3" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Mohammad Beigi, Ming Jin, Junshan Zhang, Qifan Wang, Lifu Huang</name></author>
    <summary type="html"><![CDATA[<p>ARA (Adversarial Reward Auditing) detects reward hacking by training a Hacker policy to discover reward model vulnerabilities while an Auditor learns to detect exploitation, then uses this to gate rewards during RLHF.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reward Hacking"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:445982e79bb6</id>
    <title>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03392" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-445982e79bb6" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang</name></author>
    <summary type="html"><![CDATA[<p>Establishes theoretical framework for analyzing entropy dynamics during reinforcement fine-tuning of LLMs, deriving discriminant expression and first-order expression for entropy change under logit updates.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Theoretical ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b0294636e453</id>
    <title>FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding</title>
    <link href="http://arxiv.org/abs/2602.03417" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b0294636e453" rel="related" type="text/html"/>
    <published>2026-02-04T03:09:00Z</published>
    <updated>2026-02-04T03:09:00Z</updated>
    <author><name>Yingli Shen, Wen Lai, Jie Zhou, Xueren Zhang, Yudong Wang, Kangyang Luo, Shuo Wang, Ge Gao, Alexander Fraser, Maosong Sun</name></author>
    <summary type="html"><![CDATA[<p>FactNet unifies 1.7 billion atomic assertions with 3 billion evidence pointers from 316 Wikipedia editions, using deterministic construction for byte-level provenance tracking.</p>]]></summary>
    <category term="Knowledge Graphs"/>
    <category term="Factuality"/>
    <category term="Datasets"/>
    <category term="Multilingual NLP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:7578107caca0</id>
    <title>Building Better Deception Probes Using Targeted Instruction Pairs</title>
    <link href="http://arxiv.org/abs/2602.01425" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-7578107caca0" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Vikram Natarajan, Devina Jain, Shivam Arora, Satvik Golechha, Joseph Bloom</name></author>
    <summary type="html"><![CDATA[<p>Shows that targeted instruction pairs based on a human-interpretable taxonomy of deception significantly improve linear probes for detecting deceptive behavior in AI systems, capturing intent rather than content patterns.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Deception Detection"/>
    <category term="Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0c97dabe6989</id>
    <title>Adoption and Use of LLMs at an Academic Medical Center</title>
    <link href="http://arxiv.org/abs/2602.00074" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0c97dabe6989" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Nigam H. Shah, Nerissa Ambers, Abby Pandya, Timothy Keyes, Juan M. Banda, Srikar Nallan, Carlene Lugtu, Artem A. Trotsyuk, Suhana Bedi, Alyssa Unell, Miguel Fuentes, Francois Grolleau, Sneha S. Jain, Jonathan Chen, Devdutta Dash, Danton Char, Aditya Sharma, Duncan McElfresh, Patrick Scully, Vishanthan Kumar, Connor OBrien, Satchi Mouniswamy, Elvis Jones, Krishna Jasti, Gunavathi Mannika Lakshmanan, Sree Ram Akula, Varun Kumar Singh, Ramesh Rajmanickam, Sudhir Sinha, Vicky Zhou, Xu Wang, Bilal Mawji, Joshua Ge, Wencheng Li, Travis Lyons, Jarrod Helzer, Vikas Kakkar, Ramesh Powar, Darren Batara, Cheryl Cordova, William Frederick III, Olivia Tang, Phoebe Morgan, April S. Liang, Stephen P. Ma, Shivam Vedak, Dong-han Yao, Akshay Swaminathan, Mehr Kashyap, Brian Ng, Jamie Hellman, Nikesh Kotecha, Christopher Sharp, Gretchen Brown, Christian Lindmark, Anurang Revri, Michael A. Pfeffer</name></author>
    <summary type="html"><![CDATA[<p>Describes ChatEHR, a production LLM system deployed at Stanford Medicine enabling use of LLMs with complete patient timelines for chart review, screening, and abstraction tasks with 4,500 unique users.</p>]]></summary>
    <category term="Healthcare AI"/>
    <category term="LLM Deployment"/>
    <category term="Electronic Health Records"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new denial-of-service attack targeting Large Reasoning Models by crafting prompts that induce pathologically long reasoning traces. Formalizes PI-DoS attacks requiring high amplification ratio, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Large Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment as systematic distortion of pre-alignment distribution and derives optimal aggregation strategy for jailbreaking. Shows logit-arithmetic methods are special cases and proposes broader family of aggregation rules.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Adversarial ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:5c3691edf77f</id>
    <title>From Perception to Action: Spatial AI Agents and World Models</title>
    <link href="http://arxiv.org/abs/2602.01644" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-5c3691edf77f" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Gloria Felicia, Nolan Bryant, Handi Putra, Ayaan Gazali, Eliel Lobo, Esteban Rojas</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey reviewing 2000+ papers (citing 742) that introduces unified three-axis taxonomy connecting agentic capabilities with spatial intelligence for embodied AI. Bridges gap between agentic architectures and spatial reasoning.</p>]]></summary>
    <category term="Survey"/>
    <category term="Embodied AI"/>
    <category term="Spatial Intelligence"/>
    <category term="World Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:badeda980845</id>
    <title>Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory</title>
    <link href="http://arxiv.org/abs/2602.02393" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-badeda980845" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Ruiqi Wu, Xuanhua He, Meng Cheng, Tianyu Yang, Yong Zhang, Zhuoliang Kang, Xunliang Cai, Xiaoming Wei, Chunle Guo, Chongyi Li, Ming-Ming Cheng</name></author>
    <summary type="html"><![CDATA[<p>Proposes Infinite-World, an interactive world model maintaining coherent visual memory over 1000+ frames using Hierarchical Pose-free Memory Compressor for recursive latent compression.</p>]]></summary>
    <category term="World Models"/>
    <category term="Video Generation"/>
    <category term="Long-Horizon Generation"/>
    <category term="Memory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8ab0268430c1</id>
    <title>IMU-1: Sample-Efficient Pre-training of Small Language Models</title>
    <link href="http://arxiv.org/abs/2602.02522" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8ab0268430c1" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>George Grigorev</name></author>
    <summary type="html"><![CDATA[<p>IMU-1 is a 430M parameter LLM trained on only 72B tokens that approaches performance of models trained on 56x more data. Combines architectural innovations (QK-norm, per-head gating) with optimization advances (NorMuon, muP) and releases all artifacts.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Efficient Training"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ba953cf4bda2</id>
    <title>Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher</title>
    <link href="http://arxiv.org/abs/2602.02859" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ba953cf4bda2" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Hari K Prakash, Charles H Martin</name></author>
    <summary type="html"><![CDATA[<p>Identifies anti-grokking: a previously unreported third phase where generalization collapses after successful grokking, with test accuracy returning to chance while training accuracy remains perfect. Uses Weightwatcher to detect this phenomenon.</p>]]></summary>
    <category term="Grokking"/>
    <category term="Generalization"/>
    <category term="Training Dynamics"/>
    <category term="Deep Learning Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:219e73cfb1e7</id>
    <title>Reinforcement Learning with Promising Tokens for Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.03195" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-219e73cfb1e7" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</name></author>
    <summary type="html"><![CDATA[<p>Proposes RLPT, a framework that reduces the action space in RL for LLMs by focusing only on contextually relevant 'promising' tokens rather than the full vocabulary. Demonstrates that valid reasoning paths concentrate within a low-rank subspace.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Language Models"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:de3d45e551ca</id>
    <title>Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging</title>
    <link href="http://arxiv.org/abs/2602.03702" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-de3d45e551ca" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Alexandru Meterez, Pranav Ajit Nair, Depen Morwani, Cengiz Pehlevan, Sham Kakade</name></author>
    <summary type="html"><![CDATA[<p>Provides theoretical analysis showing anytime learning schedules exist for LLM training where horizons are unknown, demonstrating weight averaging achieves minimax rates. Polynomial decay schedules determined by source/capacity conditions.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Optimization"/>
    <category term="Continual Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:91e4f73b021e</id>
    <title>A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior</title>
    <link href="http://arxiv.org/abs/2602.02639" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-91e4f73b021e" rel="related" type="text/html"/>
    <published>2026-02-04T03:07:00Z</published>
    <updated>2026-02-04T03:07:00Z</updated>
    <author><name>Harry Mayne, Justin Singh Kang, Dewi Gould, Kannan Ramchandran, Adam Mahdi, Noah Y. Siegel</name></author>
    <summary type="html"><![CDATA[<p>Introduces Normalized Simulatability Gain metric showing LLM self-explanations help predict model behavior, evaluating 18 frontier models including GPT-5.2, Claude 4.5, Gemini 3.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="AI Safety"/>
    <category term="LLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:category-summary:research</id>
    <title>Research Summary: February 03, 2026</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-03T06:00:00Z</published>
    <updated>2026-02-03T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features potentially transformative efficiency advances and critical safety findings. A <strong>symmetry-aware Taylor approximation</strong> claims to <a href="http://localhost:8080/?date=2026-02-03&category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">achieve <strong>constant-cost self-attention</strong></a> per token—if validated, a fundamental breakthrough. Meta <a href="http://localhost:8080/?date=2026-02-03&category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">introduces <strong>Fault Tolerant HSDP</strong></a> enabling training on <strong>100K+ GPUs</strong> with graceful failure recovery.</p>
<ul>
<li><strong>Kimi K2.5</strong> <a href="http://localhost:8080/?date=2026-02-03&category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">releases as open-source multimodal agent</a> with novel <strong>Agent Swarm</strong> parallel orchestration architecture</li>
<li><strong>Tele-Lens</strong> probing <a href="http://localhost:8080/?date=2026-02-03&category=research#item-387e0b4ea34d" class="internal-link" rel="noopener noreferrer">reveals <strong>myopic planning</strong></a> in Chain-of-Thought without global task awareness—challenging CoT assumptions</li>
<li><strong>BLOCK-EM</strong> <a href="http://localhost:8080/?date=2026-02-03&category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">achieves <strong>95% reduction</strong></a> in emergent misalignment by constraining causal features during fine-tuning</li>
<li><strong>ReasoningBomb</strong> <a href="http://localhost:8080/?date=2026-02-03&category=research#item-c32983c09a26" class="internal-link" rel="noopener noreferrer">exposes DoS vulnerabilities</a> in reasoning models by inducing pathologically long traces</li>
</ul>
<p>Theoretical advances include <a href="http://localhost:8080/?date=2026-02-03&category=research#item-3c815016db43" class="internal-link" rel="noopener noreferrer"><strong>polylog(1/δ)</strong> sampling complexity</a> for diffusion models (exponential improvement), formal proofs that transformers <a href="http://localhost:8080/?date=2026-02-03&category=research#item-2f83a2bbdba2" class="internal-link" rel="noopener noreferrer">learn <strong>factored representations</strong></a> in orthogonal subspaces, and a <a href="http://localhost:8080/?date=2026-02-03&category=research#item-95be64b17a47" class="internal-link" rel="noopener noreferrer"><strong>relative-budget theory</strong></a> explaining when RLVR succeeds. <strong>Grad2Reward</strong> <a href="http://localhost:8080/?date=2026-02-03&category=research#item-7807804264c1" class="internal-link" rel="noopener noreferrer">extracts dense process rewards</a> directly from LLM judge gradients, addressing reward sparsity in long-form reasoning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-03T03:40:00Z</published>
    <updated>2026-02-03T03:40:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Shows self-attention is efficiently computable to arbitrary precision with constant cost per token by decomposing Taylor expansion into symmetric chains of tensor products, achieving orders-of-magnitude efficiency gains.</p>]]></summary>
    <category term="Efficiency"/>
    <category term="Transformers"/>
    <category term="Self-Attention"/>
    <category term="Mathematical Foundations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-03T03:36:00Z</published>
    <updated>2026-02-03T03:36:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Introduces Fault Tolerant HSDP for training on 100K+ GPUs, allowing individual data-parallel replicas to restart on failure while others continue. Includes novel fault-tolerant all-reduce protocol.</p>]]></summary>
    <category term="Large-Scale Training"/>
    <category term="Systems"/>
    <category term="Fault Tolerance"/>
    <category term="Distributed Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-03T03:31:00Z</published>
    <updated>2026-02-03T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Kimi K2.5 is an open-source multimodal agentic model featuring joint text-vision optimization and Agent Swarm—a parallel agent orchestration framework that dynamically decomposes complex tasks. Claims SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Agents"/>
    <category term="Open Source"/>
    <category term="SOTA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new class of denial-of-service attacks targeting large reasoning models by inducing pathologically long reasoning traces. Formalizes PI-DoS attacks with three key properties: amplification, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:387e0b4ea34d</id>
    <title>No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</title>
    <link href="http://arxiv.org/abs/2602.02103" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou</name></author>
    <summary type="html"><![CDATA[<p>Proposes Tele-Lens probing method revealing LLMs exhibit myopic planning horizon in Chain-of-Thought, conducting incremental transitions without precise global planning.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Planning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:e09cad84773b</id>
    <title>AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</title>
    <link href="http://arxiv.org/abs/2602.02079" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-e09cad84773b" rel="related" type="text/html"/>
    <published>2026-02-03T03:21:00Z</published>
    <updated>2026-02-03T03:21:00Z</updated>
    <author><name>Daniil Orel, Dilshod Azizov, Indraneil Paul, Yuxia Wang, Iryna Gurevych, Preslav Nakov</name></author>
    <summary type="html"><![CDATA[<p>Introduces AICD Bench, comprehensive benchmark for AI-generated code detection with 2M examples, 77 models across 11 families, 9 languages, including reasoning models and three realistic detection tasks.</p>]]></summary>
    <category term="AI-Generated Content Detection"/>
    <category term="Code Generation"/>
    <category term="Benchmark"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Proposes BLOCK-EM for preventing emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves up to 95% reduction in emergent misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:7807804264c1</id>
    <title>Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01791" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren</name></author>
    <summary type="html"><![CDATA[<p>Introduces Grad2Reward framework that extracts dense process rewards directly from LLM judge gradients, converting sparse sequence-level rewards into fine-grained supervision for RLHF on open-ended tasks.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Reward Modeling"/>
    <category term="LLM Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as adversarial game where attacker agent discovers vulnerabilities while defender agent learns to refuse, enabling co-evolution that uncovers long-tail vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Learning"/>
    <category term="Reinforcement Learning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Proposes 'Identity Bridge' to address the reversal curse in autoregressive LLMs, where models trained on 'A→B' cannot deduce 'B→A'. Claims to mitigate what was previously considered a fundamental limitation of causal LLMs.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:48c7f86d105c</id>
    <title>Learning Robust Reasoning through Guided Adversarial Self-Play</title>
    <link href="http://arxiv.org/abs/2602.00173" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-48c7f86d105c" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Shuozhe Li, Vaishnav Tadiparthi, Kwonjoon Lee, Nakul Agarwal, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Lizhang Chen, Amy Zhang, Liu Leqi</name></author>
    <summary type="html"><![CDATA[<p>GASP introduces adversarial self-play within a single model to train detect-and-repair capabilities for reasoning, using only outcome verification. A polluter induces coherent corruptions while an agent learns to recover.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Robustness"/>
    <category term="Self-Play"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment in LLMs as a systematic distortion of pre-alignment distributions and casts jailbreaking as a forecast aggregation problem. Derives optimal aggregation strategy and shows logit-arithmetic methods are a special case, proposing broader family of attacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:f63f5fad429e</id>
    <title>Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.01058" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-f63f5fad429e" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Dylan Zhang, Yufeng Xu, Haojin Wang, Qingzhi Chen, Hao Peng</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that stronger SFT checkpoints can significantly underperform weaker ones after identical RL training due to distribution mismatch. Proposes PEAR to re-weight SFT samples to prepare for RL.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Post-Training Optimization"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:95be64b17a47</id>
    <title>A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01523" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Akifumi Wachi, Hirota Kinoshita, Shokichi Takakura, Rei Higuchi, Taiji Suzuki</name></author>
    <summary type="html"><![CDATA[<p>Proposes relative-budget theory explaining RL effectiveness for LLM reasoning through ξ=H/E[T]. Identifies three regimes: deficient (rare informative trajectories), efficient, and wasteful.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Reasoning"/>
    <category term="Theoretical ML"/>
    <category term="RLVR"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6561d7765cfb</id>
    <title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title>
    <link href="http://arxiv.org/abs/2602.01725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6561d7765cfb" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>SafePred introduces predictive guardrails for computer-using agents using world models to proactively identify long-term risks from seemingly reasonable actions, unlike reactive guardrails that only detect immediate threats.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Computer-Using Agents"/>
    <category term="World Models"/>
    <category term="Guardrails"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:3c815016db43</id>
    <title>High-accuracy sampling for diffusion models and log-concave distributions</title>
    <link href="http://arxiv.org/abs/2602.01338" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</name></author>
    <summary type="html"><![CDATA[<p>Presents algorithms achieving δ-error in polylog(1/δ) steps for diffusion model sampling, exponential improvement over prior work. Also yields first polylog complexity sampler for strongly log-concave distributions.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Sampling"/>
    <category term="Theory"/>
    <category term="Log-Concave Distributions"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6896c0ca9602</id>
    <title>How Implicit Bias Accumulates and Propagates in LLM Long-term Memory</title>
    <link href="http://arxiv.org/abs/2602.01558" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6896c0ca9602" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yiming Ma, Lixu Wang, Lionel Z. Wang, Hongkun Yang, Haoming Sun, Xin Xu, Jiaqi Wu, Bin Chen, Wei Dong</name></author>
    <summary type="html"><![CDATA[<p>Studies how implicit bias accumulates and propagates in LLMs with long-term memory mechanisms. Introduces DIB Benchmark with 3,776 decision-making scenarios across 9 domains, evaluating 6 SOTA LLMs with 3 memory architectures.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Fairness"/>
    <category term="LLM Memory"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:2f83a2bbdba2</id>
    <title>Transformers learn factored representations</title>
    <link href="http://arxiv.org/abs/2602.02385" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas, Alexander B. Boyd, Eric A. Alt, Kyle J. Ray, Paul M. Riechers</name></author>
    <summary type="html"><![CDATA[<p>Formalizes how transformers pretrained via next-token prediction learn factored representations in orthogonal subspaces of the residual stream. Derives precise geometric predictions about activation structure and validates empirically.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Transformers"/>
    <category term="Representation Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:12695c23c695</id>
    <title>Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs</title>
    <link href="http://arxiv.org/abs/2602.01914" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-12695c23c695" rel="related" type="text/html"/>
    <published>2026-02-03T03:14:00Z</published>
    <updated>2026-02-03T03:14:00Z</updated>
    <author><name>Wenbo Pan, Zhichao Liu, Xianlong Wang, Haining Yu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Introduces FlashTrace for efficient multi-token attribution in reasoning LLMs, reducing complexity from O(M*N) to O(N) through span-wise aggregation. Addresses attribution mass absorption by intermediate reasoning tokens.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="LLM Reasoning"/>
    <category term="Attribution"/>
    <category term="Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:0a648aa519b3</id>
    <title>Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</title>
    <link href="http://arxiv.org/abs/2602.01750" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-0a648aa519b3" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Mohammad Beigi, Ming Jin, Junshan Zhang, Qifan Wang, Lifu Huang</name></author>
    <summary type="html"><![CDATA[<p>Adversarial Reward Auditing (ARA) reconceptualizes reward hacking as dynamic adversarial game where Hacker policy discovers vulnerabilities while Auditor detects exploitation, enabling Auditor-Guided RLHF.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reward Hacking"/>
    <category term="Adversarial Learning"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:68af97a1cc52</id>
    <title>The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective</title>
    <link href="http://arxiv.org/abs/2602.00170" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-68af97a1cc52" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Qiyao Liang, Jinyeop Song, Yizhou Liu, Jeff Gore, Ila Fiete, Risto Miikkulainen, Xin Qiu</name></author>
    <summary type="html"><![CDATA[<p>Explains why evolution strategies can fine-tune billion-parameter LLMs with small populations (~30) through a variance-curvature perspective. Shows fine-tuning landscapes are low-dimensional in curvature.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Fine-tuning"/>
    <category term="Optimization Theory"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:4733372074f8</id>
    <title>TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse</title>
    <link href="http://arxiv.org/abs/2602.01439" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4733372074f8" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Perry Dong, Kuo-Han Hung, Alexander Swerdlow, Dorsa Sadigh, Chelsea Finn</name></author>
    <summary type="html"><![CDATA[<p>Identifies attention collapse as the critical failure mode preventing transformer scaling for RL value functions. Proposes TQL controlling attention entropy to enable larger models.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Transformers"/>
    <category term="Scaling"/>
    <category term="Value Functions"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:44eb1ca6c4f5</id>
    <title>What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?</title>
    <link href="http://arxiv.org/abs/2602.01611" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-44eb1ca6c4f5" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Weizheng Gu, Chengze Li, Zhuohao Yu, Mengyuan Sun, Zhibang Yang, Wei Wang, Hongrui Jia, Shikun Zhang, Wei Ye</name></author>
    <summary type="html"><![CDATA[<p>Introduces PIPE protocol to diagnose whether agents learn semantic tool-use or merely memorize interface patterns. Testing across 16 environments reveals trajectory-SFT substantially amplifies interface reliance over semantic understanding.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Evaluation"/>
    <category term="Generalization"/>
    <category term="Benchmark"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:647b095603d7</id>
    <title>DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers</title>
    <link href="http://arxiv.org/abs/2602.02016" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-647b095603d7" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Ionut-Vlad Modoranu, Philip Zmushko, Erik Schultheis, Mher Safaryan, Dan Alistarh</name></author>
    <summary type="html"><![CDATA[<p>Proposes DASH for faster Shampoo optimizer via batched block preconditioning in 3D tensors and Newton-DB iteration for inverse roots. Significant speedup over distributed Shampoo.</p>]]></summary>
    <category term="Optimization"/>
    <category term="Efficient Training"/>
    <category term="Second-Order Methods"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:b71409d9e871</id>
    <title>Expanding the Capabilities of Reinforcement Learning via Text Feedback</title>
    <link href="http://arxiv.org/abs/2602.02482" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-b71409d9e871" rel="related" type="text/html"/>
    <published>2026-02-03T03:12:00Z</published>
    <updated>2026-02-03T03:12:00Z</updated>
    <author><name>Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</name></author>
    <summary type="html"><![CDATA[<p>Studies text feedback as an intermediate signal between scalar rewards and demonstrations for RL. Formalizes RL from Text Feedback (RLTF) as multi-turn RL setup where text critiques are available during training.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Language Models"/>
    <category term="Alignment"/>
    <category term="Reinforcement Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:8e4ab01f7c01</id>
    <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
    <link href="http://arxiv.org/abs/2602.00906" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-8e4ab01f7c01" rel="related" type="text/html"/>
    <published>2026-02-03T03:09:00Z</published>
    <updated>2026-02-03T03:09:00Z</updated>
    <author><name>Anxin Guo, Jingwei Li</name></author>
    <summary type="html"><![CDATA[<p>Establishes rate-distortion theorem formalizing hallucination as consequence of space-optimality in membership testing. Shows optimal memory efficiency characterized by minimum KL divergence, explaining hallucination as information-theoretically fundamental.</p>]]></summary>
    <category term="Hallucination"/>
    <category term="Information Theory"/>
    <category term="LLM Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:69e435a62cf2</id>
    <title>A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention</title>
    <link href="http://arxiv.org/abs/2602.01763" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-69e435a62cf2" rel="related" type="text/html"/>
    <published>2026-02-03T03:09:00Z</published>
    <updated>2026-02-03T03:09:00Z</updated>
    <author><name>Xiaowei Ye, Xiaoyu He, Chao Liao, Chen Wu, Pinyan Lu</name></author>
    <summary type="html"><![CDATA[<p>Establishes theoretical expressiveness hierarchy showing full attention &gt; hybrid attention &gt; linear attention for sequential function composition (multi-step reasoning), proving linear attention variants cannot match full attention on specific tasks.</p>]]></summary>
    <category term="Theory"/>
    <category term="Attention Mechanisms"/>
    <category term="Expressiveness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:8c7d0f817c0b</id>
    <title>Chance-Constrained Inference for Hallucination Risk Control in Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.01637" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-8c7d0f817c0b" rel="related" type="text/html"/>
    <published>2026-02-03T03:09:00Z</published>
    <updated>2026-02-03T03:09:00Z</updated>
    <author><name>Sreenivasan Mohandas</name></author>
    <summary type="html"><![CDATA[<p>Formulates LLM inference as a deployment-time risk control problem and introduces chance-constrained inference to bound hallucination probability. Proposes sequential, anytime-valid algorithm providing statistical guarantees on accepted generations.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Hallucination"/>
    <category term="LLM Inference"/>
    <category term="Risk Control"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:613df2d96f38</id>
    <title>Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.02244" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-613df2d96f38" rel="related" type="text/html"/>
    <published>2026-02-03T03:09:00Z</published>
    <updated>2026-02-03T03:09:00Z</updated>
    <author><name>Hao Wang, Hao Gu, Hongming Piao, Kaixiong Gong, Yuxiao Ye, Xiangyu Yue, Sirui Han, Yike Guo, Dapeng Wu</name></author>
    <summary type="html"><![CDATA[<p>Proposes CurioSFT entropy-preserving SFT method with Self-Exploratory Distillation to maintain exploration capability for subsequent RL, addressing overconfidence from standard SFT.</p>]]></summary>
    <category term="SFT"/>
    <category term="RLHF"/>
    <category term="LLM Training"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:655afd2293fa</id>
    <title>An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence</title>
    <link href="http://arxiv.org/abs/2602.02400" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-655afd2293fa" rel="related" type="text/html"/>
    <published>2026-02-03T03:09:00Z</published>
    <updated>2026-02-03T03:09:00Z</updated>
    <author><name>Qizhen Zhang, Ankush Garg, Jakob Foerster, Niladri Chatterji, Kshitiz Malik, Mike Lewis</name></author>
    <summary type="html"><![CDATA[<p>Systematic empirical study on how noisy data causes LLM pretraining loss divergence across models from 480M to 5.2B parameters. Shows that noise injection leads to predictable instability patterns and divergence thresholds.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Data Quality"/>
    <category term="Training Stability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:category-summary:research</id>
    <title>Research Summary: February 02, 2026</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-02T06:00:00Z</published>
    <updated>2026-02-02T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges in AI safety and alignment evaluation. <strong>Hair-Trigger Alignment</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-de1f951d7948" class="internal-link" rel="noopener noreferrer">proves black-box evaluation</a> fundamentally cannot guarantee post-update alignment—a significant theoretical limitation. Equally concerning, <strong>CoT obfuscation</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-624f75ef7d56" class="internal-link" rel="noopener noreferrer">demonstrates</a> that models learning to hide reward hacking can generalize this deception to unseen tasks, undermining oversight mechanisms.</p>
<ul>
<li><strong>The Hot Mess of AI</strong> (Sohl-Dickstein, Perez) <a href="http://localhost:8080/?date=2026-02-02&category=research#item-cd66258625b0" class="internal-link" rel="noopener noreferrer">shows counterintuitively</a> that longer reasoning produces MORE incoherent high-variance failures</li>
<li><strong>Language Model Circuits</strong> from Steinhardt's group <a href="http://localhost:8080/?date=2026-02-02&category=research#item-1950f7a0c03f" class="internal-link" rel="noopener noreferrer">finds MLP neurons</a> are as sparse as SAE features, enabling practical end-to-end circuit analysis</li>
<li><strong>Why Reasoning Fails to Plan</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-2e8aa98922af" class="internal-link" rel="noopener noreferrer">identifies</a> that step-wise reasoning induces greedy policies incompatible with long-horizon planning</li>
<li><strong>LLM Agents Are Not Faithful Self-Evolvers</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-056138bab876" class="internal-link" rel="noopener noreferrer">reveals agents depend</a> on raw experience but resist incorporating reflective corrections</li>
</ul>
<p>Practical advances include <strong>Golden Goose</strong> for <a href="http://localhost:8080/?date=2026-02-02&category=research#item-1ca9026e8ca8" class="internal-link" rel="noopener noreferrer">synthesizing unlimited RLVR tasks</a> from unverifiable text, <strong>MoVE</strong> decoupling parametric memory from compute via shared value embeddings, and <strong>Gemini</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-b5c070bdd08e" class="internal-link" rel="noopener noreferrer">addressing 13 Erdős problems</a>. Security research on <strong>Google's Agent Payments Protocol</strong> <a href="http://localhost:8080/?date=2026-02-02&category=research#item-0e8e0ee0ce27" class="internal-link" rel="noopener noreferrer">demonstrates prompt injection</a> vulnerabilities in real financial transaction systems.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:de1f951d7948</id>
    <title>Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Yavuz Bakman, Duygu Nur Yaldiz, Salman Avestimehr, Sai Praneeth Karimireddy</name></author>
    <summary type="html"><![CDATA[<p>Formalizes model alignment in static and post-update settings, proving that black-box evaluation cannot guarantee post-update alignment. Shows that overparameterization means static alignment provides no guarantee for any update dataset.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Machine Learning Theory"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1950f7a0c03f</id>
    <title>Language Model Circuits Are Sparse in the Neuron Basis</title>
    <link href="http://arxiv.org/abs/2601.22594" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann</name></author>
    <summary type="html"><![CDATA[<p>Empirically demonstrates that MLP neurons are as sparse as SAE features for circuit analysis in language models, enabling end-to-end circuit tracing on the neuron basis without requiring sparse autoencoders.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Language Models"/>
    <category term="Neural Circuits"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:1ca9026e8ca8</id>
    <title>Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text</title>
    <link href="http://arxiv.org/abs/2601.22975" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Ximing Lu, David Acuna, Jaehun Jung, Jian Hu, Di Zhang, Shizhe Diao, Yunheng Zou, Shaokun Zhang, Brandon Cui, Mingjie Liu, Hyunwoo Kim, Prithviraj Ammanabrolu, Jan Kautz, Yi Dong, Yejin Choi</name></author>
    <summary type="html"><![CDATA[<p>Proposes Golden Goose to synthesize unlimited RLVR tasks from unverifiable text by creating multiple-choice fill-in-the-middle tasks with distractors. Enables leveraging reasoning-rich corpora excluded from prior RLVR data. From team including Yejin Choi.</p>]]></summary>
    <category term="RLVR"/>
    <category term="Data Synthesis"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:feb183de5292</id>
    <title>Towards Solving the Gilbert-Pollak Conjecture via Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.22365" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-feb183de5292" rel="related" type="text/html"/>
    <published>2026-02-02T03:09:00Z</published>
    <updated>2026-02-02T03:09:00Z</updated>
    <author><name>Yisi Ke, Tianyu Huang, Yankai Shu, Di He, Jingchu Gai, Liwei Wang</name></author>
    <summary type="html"><![CDATA[<p>AI system for mathematical discovery addresses 13 'Open' problems from Erdős database - 5 through novel autonomous solutions, 8 through literature identification. Uses Gemini with hybrid AI-verification and human expert evaluation.</p>]]></summary>
    <category term="AI for Mathematics"/>
    <category term="Scientific Discovery"/>
    <category term="Large Language Models"/>
    <category term="Google Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:b5c070bdd08e</id>
    <title>Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erd\H{o}s Problems</title>
    <link href="http://arxiv.org/abs/2601.22401" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-b5c070bdd08e" rel="related" type="text/html"/>
    <published>2026-02-02T03:09:00Z</published>
    <updated>2026-02-02T03:09:00Z</updated>
    <author><name>Tony Feng, Trieu Trinh, Garrett Bingham, Jiwon Kang, Shengtong Zhang, Sang-hyun Kim, Kevin Barreto, Carl Schildkraut, Junehyuk Jung, Jaehyeon Seo, Carlo Pagano, Yuri Chervonyi, Dawsen Hwang, Kaiying Hou, Sergei Gukov, Cheng-Chiang Tsai, Hyunwoo Choi, Youngbeom Jin, Wei-Yuan Li, Hao-An Wu, Ruey-An Shiu, Yu-Sheng Shih, Quoc V. Le, Thang Luong</name></author>
    <summary type="html"><![CDATA[<p>Uses Gemini to evaluate 700 'Open' conjectures from Erdős Problems database, addressing 13 problems through novel solutions or literature identification. Employs hybrid AI-verification with human expert evaluation.</p>]]></summary>
    <category term="AI for Mathematics"/>
    <category term="Scientific Discovery"/>
    <category term="Google Research"/>
    <category term="Large Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:7a51354180f6</id>
    <title>A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training</title>
    <link href="http://arxiv.org/abs/2601.22966" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-7a51354180f6" rel="related" type="text/html"/>
    <published>2026-02-02T03:09:00Z</published>
    <updated>2026-02-02T03:09:00Z</updated>
    <author><name>Zihan Qiu and Zeyu Huang and Kaiyue Wen and Peng Jin and Bo Zheng and Yuxin Zhou and Haofeng Huang and Zekun Wang and Xiao Li and Huaqing Zhang and Yang Xu and Haoran Lian and Siqi Zhang and Rui Men and Jianwei Zhang and Ivan Titov and Dayiheng Liu and Jingren Zhou and Junyang Lin</name></author>
    <summary type="html"><![CDATA[<p>Provides unified view of attention sinks and residual sinks as outlier-driven rescaling mechanisms that jointly function with normalization. Shows removing normalization eliminates outliers, and proposes mitigations.</p>]]></summary>
    <category term="Transformer Architecture"/>
    <category term="Attention Mechanisms"/>
    <category term="Interpretability"/>
    <category term="Language Models"/>
  </entry>
</feed>