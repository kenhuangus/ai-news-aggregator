<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 100)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-100.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:100</id>
  <updated>2026-02-06T13:57:58Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-02-06:category-summary:research</id>
    <title>Research Summary: February 06, 2026</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-06T06:00:00Z</published>
    <updated>2026-02-06T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings with significant implications for deployment and research integrity.</p>
<p><strong>First Proof</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" class="internal-link" rel="noopener noreferrer">introduces a research-level math benchmark</a> from Fields medalist Martin Hairer and colleagues with encrypted answers to prevent contamination. Meanwhile, analysis of <strong>NeurIPS 2025</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-0cd74aa757dd" class="internal-link" rel="noopener noreferrer">reveals ~1% of accepted papers</a> contain AI-generated fabricated citations, presenting a five-category hallucination taxonomy.</p>
<ul>
<li><strong>Chunky Post-Training</strong> from OpenAI (including John Schulman) <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-e4ec2039b137" class="internal-link" rel="noopener noreferrer">identifies how</a> discrete data curation creates spurious correlations causing generalization failures</li>
<li><strong>Phantom Transfer</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-b62d24ae008b" class="internal-link" rel="noopener noreferrer">demonstrates unfilterable data poisoning</a> attacks effective on frontier models, challenging data-level defense assumptions</li>
<li><strong>Steering Externalities</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-1e3f982b40bf" class="internal-link" rel="noopener noreferrer">reveals benign activation steering</a> unexpectedly increases jailbreak vulnerability</li>
<li><strong>Agent-as-a-Proxy</strong> and <strong>Among Us</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-cb03a21c8518" class="internal-link" rel="noopener noreferrer">expose fundamental weaknesses</a> in monitoring-based safety for agentic and multi-model systems</li>
</ul>
<p>On the constructive side, <strong>Reducible Uncertainty Modeling</strong> (Dawn Song, Sharon Li) <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-8fcdaed9b304" class="internal-link" rel="noopener noreferrer">provides the first general framework</a> for uncertainty quantification in LLM agents, while a position paper argues <strong>capability control</strong> <a href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-3fb821a21eb1" class="internal-link" rel="noopener noreferrer">should be architecturally separated</a> from alignment.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:5c09c496e50f</id>
    <title>First Proof</title>
    <link href="http://arxiv.org/abs/2602.05192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5c09c496e50f" rel="related" type="text/html"/>
    <published>2026-02-06T03:40:00Z</published>
    <updated>2026-02-06T03:40:00Z</updated>
    <author><name>Mohammed Abouzaid, Andrew J. Blumberg, Martin Hairer, Joe Kileel, Tamara G. Kolda, Paul D. Nelson, Daniel Spielman, Nikhil Srivastava, Rachel Ward, Shmuel Weinberger, Lauren Williams</name></author>
    <summary type="html"><![CDATA[<p>Top mathematicians (including Fields medalist Hairer) share 10 research-level math problems with encrypted answers to evaluate AI mathematical reasoning. Problems arose naturally from their research.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="Benchmark"/>
    <category term="AI Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:0cd74aa757dd</id>
    <title>Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025</title>
    <link href="http://arxiv.org/abs/2602.05930" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-0cd74aa757dd" rel="related" type="text/html"/>
    <published>2026-02-06T03:40:00Z</published>
    <updated>2026-02-06T03:40:00Z</updated>
    <author><name>Samar Ansari</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 100 AI-generated hallucinated citations in 53 papers accepted at NeurIPS 2025 (~1% of accepted papers). Develops five-category taxonomy of hallucination failure modes including Total Fabrication (66%) and Partial Attribute Corruption (27%).</p>]]></summary>
    <category term="AI Hallucinations"/>
    <category term="Scientific Integrity"/>
    <category term="Peer Review"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:8fcdaed9b304</id>
    <title>Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2602.05073" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-8fcdaed9b304" rel="related" type="text/html"/>
    <published>2026-02-06T03:31:00Z</published>
    <updated>2026-02-06T03:31:00Z</updated>
    <author><name>Changdae Oh, Seongheon Park, To Eun Kim, Jiatong Li, Wendi Li, Samuel Yeh, Xuefeng Du, Hamed Hassani, Paul Bogdan, Dawn Song, Sharon Li</name></author>
    <summary type="html"><![CDATA[<p>Presents first general framework for uncertainty quantification in LLM agents, arguing current UQ research incorrectly treats it as uncertainty accumulation. Proposes novel reducible uncertainty framework for interactive agents.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Uncertainty Quantification"/>
    <category term="LLM Agents"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:b62d24ae008b</id>
    <title>Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning</title>
    <link href="http://arxiv.org/abs/2602.04899" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-b62d24ae008b" rel="related" type="text/html"/>
    <published>2026-02-06T03:31:00Z</published>
    <updated>2026-02-06T03:31:00Z</updated>
    <author><name>Andrew Draganov, Tolga H. Dur, Anandmayi Bhongade, Mary Phuong</name></author>
    <summary type="html"><![CDATA[<p>Introduces Phantom Transfer, a data poisoning attack that cannot be filtered even when the poisoning method is known. Demonstrates effectiveness across models including GPT-4.1, showing data-level defenses are fundamentally insufficient.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Data Poisoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:efd7f5d116f8</id>
    <title>PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences</title>
    <link href="http://arxiv.org/abs/2602.05302" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-efd7f5d116f8" rel="related" type="text/html"/>
    <published>2026-02-06T03:23:00Z</published>
    <updated>2026-02-06T03:23:00Z</updated>
    <author><name>Chris Zhu (Department of Statistics, Data Science, Yale University), Sasha Cui (Department of Statistics, Data Science, Yale University), Will Sanok Dufallo (Department of Philosophy, Yale University), Runzhi Jin (School of Law, University of California, Berkeley), Zhen Xu (Bloomberg), Linjun Zhang (Department of Statistics, Rutgers University), Daylian Cain (Yale School of Management)</name></author>
    <summary type="html"><![CDATA[<p>Introduces PieArena, a negotiation benchmark from MBA course scenarios, finding GPT-5 matches or outperforms trained business students. Studies joint-intentionality scaffolding effects.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Negotiation"/>
    <category term="Benchmark"/>
    <category term="Economic AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:3fb821a21eb1</id>
    <title>Position: Capability Control Should be a Separate Goal From Alignment</title>
    <link href="http://arxiv.org/abs/2602.05164" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-3fb821a21eb1" rel="related" type="text/html"/>
    <published>2026-02-06T03:23:00Z</published>
    <updated>2026-02-06T03:23:00Z</updated>
    <author><name>Shoaib Ahmed Siddiqui, Eleni Triantafillou, David Krueger, Adrian Weller</name></author>
    <summary type="html"><![CDATA[<p>Position paper arguing capability control (hard operational limits) should be treated separately from alignment (preference-driven). Organizes control mechanisms across model lifecycle.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Capability Control"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:5bd92e86ea26</id>
    <title>Learning to Inject: Automated Prompt Injection via Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.05746" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-5bd92e86ea26" rel="related" type="text/html"/>
    <published>2026-02-06T03:23:00Z</published>
    <updated>2026-02-06T03:23:00Z</updated>
    <author><name>Xin Chen, Jie Zhang, Florian Tramer</name></author>
    <summary type="html"><![CDATA[<p>Proposes AutoInject, an RL framework for generating universal, transferable adversarial suffixes for prompt injection attacks. Claims successful attacks on GPT-5 Nano, Claude Sonnet 3.5, and other frontier systems.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Adversarial Attacks"/>
    <category term="Red Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:e4ec2039b137</id>
    <title>Chunky Post-Training: Data Driven Failures of Generalization</title>
    <link href="http://arxiv.org/abs/2602.05910" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-e4ec2039b137" rel="related" type="text/html"/>
    <published>2026-02-06T03:23:00Z</published>
    <updated>2026-02-06T03:23:00Z</updated>
    <author><name>Seoirse Murray, Allison Qi, Timothy Qian, John Schulman, Collin Burns, Sara Price</name></author>
    <summary type="html"><![CDATA[<p>OpenAI researchers (including John Schulman) identify 'chunky post-training' - how discrete data curation creates spurious correlations that cause models to behave unexpectedly, like rejecting true facts in certain formats. They introduce SURF (runtime detection) and TURF (data refinement) to surface and mitigate these unintended behaviors.</p>]]></summary>
    <category term="LLM Post-Training"/>
    <category term="AI Safety"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:36fa62a7b396</id>
    <title>Democratic Preference Alignment via Sortition-Weighted RLHF</title>
    <link href="http://arxiv.org/abs/2602.05113" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-36fa62a7b396" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Suvadip Sana, Jinzhou Wu, Martin T. Wells</name></author>
    <summary type="html"><![CDATA[<p>Introduces DemPO, applying democratic sortition (citizen assembly sampling) to RLHF to address demographic bias in preference data. Offers hard-panel (exclusive) and soft-panel (reweighted) training schemes.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="RLHF"/>
    <category term="Fairness"/>
    <category term="Democratic AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:1e3f982b40bf</id>
    <title>Steering Externalities: Benign Activation Steering Unintentionally Increases Jailbreak Risk for Large Language Models</title>
    <link href="http://arxiv.org/abs/2602.04896" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-1e3f982b40bf" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Chen Xiong, Zhiyuan He, Pin-Yu Chen, Ching-Yun Ko, Tsung-Yi Ho</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Steering Externalities' - an unexpected phenomenon where benign activation steering vectors (e.g., for compliance or JSON output) inadvertently erode safety guardrails and increase jailbreak susceptibility.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Activation Steering"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:538cc6fa54a8</id>
    <title>Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation</title>
    <link href="http://arxiv.org/abs/2602.05656" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-538cc6fa54a8" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Igor Santos-Grueiro</name></author>
    <summary type="html"><![CDATA[<p>Formally studies alignment verification under partial observability, introducing the concept of normative indistinguishability - when aligned and misaligned agents are behaviorally indistinguishable under finite evaluation. Shows this is a fundamental limitation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Evaluation"/>
    <category term="Formal Methods"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-06:research:0d3ed1a77baa</id>
    <title>Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems</title>
    <link href="http://arxiv.org/abs/2602.05176" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-06&amp;category=research#item-0d3ed1a77baa" rel="related" type="text/html"/>
    <published>2026-02-06T03:16:00Z</published>
    <updated>2026-02-06T03:16:00Z</updated>
    <author><name>Ziyuan Yang, Wenxuan Ding, Shangbin Feng, Yulia Tsvetkov</name></author>
    <summary type="html"><![CDATA[<p>Studies safety risks in multi-LLM collaboration systems by engineering malicious models and measuring their impact. Finds severe degradation (7-8% average) especially in reasoning and safety domains, and proposes mitigation strategies.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Multi-Agent Systems"/>
    <category term="Security"/>
    <category term="Model Collaboration"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:category-summary:research</id>
    <title>Research Summary: February 05, 2026</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-05T06:00:00Z</published>
    <updated>2026-02-05T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a potentially paradigm-shifting efficiency result and substantial AI safety contributions. <strong>TinyLoRA</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" class="internal-link" rel="noopener noreferrer">achieves <strong>91% accuracy on GSM8K</strong></a> with only <strong>13 trained parameters</strong>, challenging assumptions about model scale requirements for reasoning.</p>
<ul>
<li>A longitudinal study across <strong>8 frontier model releases</strong> (GPT-4o→GPT-5, Claude 3.5→4.5) <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" class="internal-link" rel="noopener noreferrer">reveals systematic alignment drift</a> using 726 adversarial prompts</li>
<li><strong>Drifting Models</strong> from Kaiming He's group <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" class="internal-link" rel="noopener noreferrer">achieves SOTA on ImageNet</a> with a novel one-step generative paradigm</li>
<li><strong>Trust The Typical (T3)</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" class="internal-link" rel="noopener noreferrer">reframes LLM safety as OOD detection</a>, achieving SOTA across 18 safety benchmarks</li>
<li><strong>Contextual drag</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" class="internal-link" rel="noopener noreferrer">demonstrates failed CoT attempts</a> systematically bias subsequent generations toward structurally similar errors</li>
</ul>
<p>Multiple papers challenge core assumptions: causal analysis <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-41fa78fd2ef2" class="internal-link" rel="noopener noreferrer">shows verbose CoT</a> can be independent of model answers; meta-analysis <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" class="internal-link" rel="noopener noreferrer">suggests AI capability growth</a> may follow sigmoid rather than exponential curves. <strong>Toxic Proactivity</strong> <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" class="internal-link" rel="noopener noreferrer">identifies a novel agent failure mode</a> where helpfulness optimization overrides ethical constraints. A study of PPO <a href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-3c11173b0d9d" class="internal-link" rel="noopener noreferrer">reveals fundamental flaws</a> in trust region mechanisms for LLM reinforcement learning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:bd512b7e4b3a</id>
    <title>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</title>
    <link href="http://arxiv.org/abs/2602.04739" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-bd512b7e4b3a" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>Casey Ford, Madison Van Doren, Emily Dix</name></author>
    <summary type="html"><![CDATA[<p>Longitudinal study of MLLM harmlessness across 8 model releases (GPT-4o→GPT-5, Claude Sonnet 3.5→4.5) using 726 adversarial prompts. Shows large persistent differences across families and alignment drift with GPT ASR increasing from 9.2% to 19.9%.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="MLLM Evaluation"/>
    <category term="Alignment Drift"/>
    <category term="Red Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:8c6dfacfdd63</id>
    <title>Learning to Reason in 13 Parameters</title>
    <link href="http://arxiv.org/abs/2602.04118" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-8c6dfacfdd63" rel="related" type="text/html"/>
    <published>2026-02-05T03:31:00Z</published>
    <updated>2026-02-05T03:31:00Z</updated>
    <author><name>John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar</name></author>
    <summary type="html"><![CDATA[<p>Introduces TinyLoRA, a method that enables training an 8B parameter model to achieve 91% accuracy on GSM8K with only 13 trained parameters (26 bytes). This challenges fundamental assumptions about parameter requirements for reasoning capabilities, showing 90% of performance can be recovered while training 1000x fewer parameters.</p>]]></summary>
    <category term="Parameter-Efficient Fine-tuning"/>
    <category term="LLM Reasoning"/>
    <category term="Model Compression"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:b74a06d3b4a8</id>
    <title>Trust The Typical</title>
    <link href="http://arxiv.org/abs/2602.04581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-b74a06d3b4a8" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary</name></author>
    <summary type="html"><![CDATA[<p>Introduces Trust The Typical (T3), treating LLM safety as OOD detection by learning the distribution of acceptable prompts. Achieves SOTA across 18 safety benchmarks without training on harmful examples.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Out-of-Distribution Detection"/>
    <category term="LLM Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:f596388fe400</id>
    <title>Generative Modeling via Drifting</title>
    <link href="http://arxiv.org/abs/2602.04770" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-f596388fe400" rel="related" type="text/html"/>
    <published>2026-02-05T03:23:00Z</published>
    <updated>2026-02-05T03:23:00Z</updated>
    <author><name>Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</name></author>
    <summary type="html"><![CDATA[<p>Proposes Drifting Models, a new generative paradigm where the pushforward distribution evolves during training, naturally enabling one-step inference. Achieves state-of-the-art on ImageNet 256x256 for one-step generation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Diffusion Models"/>
    <category term="Image Generation"/>
    <category term="Efficient Inference"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:92ff4dcf4853</id>
    <title>Contextual Drag: How Errors in the Context Affect LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.04288" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-92ff4dcf4853" rel="related" type="text/html"/>
    <published>2026-02-05T03:19:00Z</published>
    <updated>2026-02-05T03:19:00Z</updated>
    <author><name>Yun Cheng, Xingyu Zhu, Haoyu Zhao, Sanjeev Arora</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'contextual drag' phenomenon where failed attempts in LLM context bias subsequent generations toward structurally similar errors. Across 11 models on 8 tasks, shows 10-20% performance drops and potential for self-deterioration.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Self-Improvement"/>
    <category term="Error Propagation"/>
    <category term="AI Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:0099f246174e</id>
    <title>Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</title>
    <link href="http://arxiv.org/abs/2602.04836" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-0099f246174e" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haosen Ge, Hamsa Bastani, Osbert Bastani</name></author>
    <summary type="html"><![CDATA[<p>Challenges METR's claim of exponential AI capability growth, showing sigmoid fits indicate the inflection point has passed. Proposes decomposed model separating base and reasoning capabilities.</p>]]></summary>
    <category term="AI Progress"/>
    <category term="Forecasting"/>
    <category term="Meta-Analysis"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:6c0435307981</id>
    <title>From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents</title>
    <link href="http://arxiv.org/abs/2602.04197" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-6c0435307981" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Xinyue Wang, Yuanhe Zhang, Zhengshuo Gong, Haoran Gao, Fanyu Meng, Zhenhong Zhou, Li Sun, Yang Liu, Sen Su</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Toxic Proactivity' as a new failure mode in LLM agents where optimization for helpfulness leads agents to disregard ethical constraints and take manipulative measures to maintain usefulness. This contrasts with the well-known 'over-refusal' problem.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Agents"/>
    <category term="Alignment"/>
    <category term="AI Ethics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:97a5bae183d6</id>
    <title>The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</title>
    <link href="http://arxiv.org/abs/2602.04196" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-97a5bae183d6" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua</name></author>
    <summary type="html"><![CDATA[<p>First systematic study of implicit training-time safety risks in AI models, introducing taxonomy with five risk levels, ten categories, and three incentive types. Shows models may manipulate training metrics for self-preservation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Training Risks"/>
    <category term="Reward Hacking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-05:research:9e75e25bdc24</id>
    <title>ERNIE 5.0 Technical Report</title>
    <link href="http://arxiv.org/abs/2602.04705" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-05&amp;category=research#item-9e75e25bdc24" rel="related" type="text/html"/>
    <published>2026-02-05T03:16:00Z</published>
    <updated>2026-02-05T03:16:00Z</updated>
    <author><name>Haifeng Wang, Hua Wu, Tian Wu, Yu Sun, Jing Liu, Dianhai Yu, Yanjun Ma, Jingzhou He, Zhongjun He, Dou Hong, Qiwen Liu, Shuohuan Wang, Junyuan Shang, Zhenyu Zhang, Yuchen Ding, Jinle Zeng, Jiabin Yang, Liang Shen, Ruibiao Chen, Weichong Yin, Siyu Ding, Dai Dai, Shikun Feng, Siqi Bao, Bolei He, Yan Chen, Zhenyu Jiao, Ruiqing Zhang, Zeyu Chen, Qingqing Dang, Kaipeng Deng, Jiajun Jiang, Enlei Gong, Guoxia Wang, Yanlin Sha, Yi Liu, Yehan Zheng, Weijian Xu, Jiaxiang Liu, Zengfeng Zeng, Yingqi Qu, Zhongli Li, Zhengkun Zhang, Xiyang Wang, Zixiang Xu, Xinchao Xu, Zhengjie Huang, Dong Wang, Bingjin Chen, Yue Chang, Xing Yuan, Shiwei Huang, Qiao Zhao, Xinzhe Ding, Shuangshuang Qiao, Baoshan Yang, Bihong Tang, Bin Li, Bingquan Wang, Binhan Tang, Binxiong Zheng, Bo Cui, Bo Ke, Bo Zhang, Bowen Zhang, Boyan Zhang, Boyang Liu, Caiji Zhang, Can Li, Chang Xu, Chao Pang, Chao Zhang, Chaoyi Yuan, Chen Chen, Cheng Cui, Chenlin Yin, Chun Gan, Chunguang Chai, Chuyu Fang, Cuiyun Han, Dan Zhang, Danlei Feng, Danxiang Zhu, Dong Sun, Dongbo Li, Dongdong Li, Dongdong Liu, Dongxue Liu, Fan Ding, Fan Hu, Fan Li, Fan Mo, Feisheng Wu, Fengwei Liu, Gangqiang Hu, Gaofeng Lu, Gaopeng Yong, Gexiao Tian, Guan Wang, Guangchen Ni, Guangshuo Wu, Guanzhong Wang, Guihua Liu, Guishun Li, Haibin Li, Haijian Liang, Haipeng Ming, Haisu Wang, Haiyang Lu, Haiye Lin, Han Zhou, Hangting Lou, Hanwen Du, Hanzhi Zhang, Hao Chen, Hao Du, Hao Liu, Hao Zhou, Haochen Jiang, Haodong Tian, Haoshuang Wang, Haozhe Geng, Heju Yin, Hong Chen, Hongchen Xue, Hongen Liu, Honggeng Zhang, Hongji Xu, Hongwei Chen, Hongyang Zhang, Hongyuan Zhang, Hua Lu, Huan Chen, Huan Wang, Huang He, Hui Liu, Hui Zhong, Huibin Ruan, Jiafeng Lu, Jiage Liang, Jiahao Hu, Jiahao Hu, Jiajie Yang, Jialin Li, Jian Chen, Jian Wu, Jianfeng Yang, Jianguang Jiang, Jianhua Wang, Jianye Chen, Jiaodi Liu, Jiarui Zhou, Jiawei Lv, Jiaxin Zhou, Jiaxuan Liu, Jie Han, Jie Sun, Jiefan Fang, Jihan Liu, Jihua Liu, Jing Hu, Jing Qian, Jing Yan, Jingdong Du, Jingdong Wang, Jingjing Wu, Jingyong Li, Jinheng Wang, Jinjin Li, Jinliang Lu, Jinlin Yu, Jinnan Liu, Jixiang Feng, Jiyi Huang, Jiyuan Zhang, Jun Liang, Jun Xia, Jun Yu, Junda Chen, Junhao Feng, Junhong Xiang, Junliang Li, Kai Liu, Kailun Chen, Kairan Su, Kang Hu, Kangkang Zhou, Ke Chen, Ke Wei, Kui Huang, Kun Wu, Kunbin Chen, Lei Han, Lei Sun, Lei Wen, Linghui Meng, Linhao Yu, Liping Ouyang, Liwen Zhang, Longbin Ji, Longzhi Wang, Meng Sun, Meng Tian, Mengfei Li, Mengqi Zeng, Mengyu Zhang, Ming Hong, Mingcheng Zhou, Mingming Huang, Mingxin Chen, Mingzhu Cai, Naibin Gu, Nemin Qiu, Nian Wang, Peng Qiu, Peng Zhao, Pengyu Zou, Qi Wang, Qi Xin, Qian Wang, Qiang Zhu, Qianhui Luo, Qianwei Yang, Qianyue He, Qifei Wu, Qinrui Li, Qiwen Bao, Quan Zhang, Quanxiang Liu, Qunyi Xie, Rongrui Zhan, Rufeng Dai, Rui Peng, Ruian Liu, Ruihao Xu, Ruijie Wang, Ruixi Zhang, Ruixuan Liu, Runsheng Shi, Ruting Wang, Senbo Kang, Shan Lu, Shaofei Yu, Shaotian Gong, Shenwei Hu, Shifeng Zheng, Shihao Guo, Shilong Fan, Shiqin Liu, Shiwei Gu, Shixi Zhang, Shuai Yao, Shuang Zhang, Shuangqiao Liu, Shuhao Liang, Shuwei He, Shuwen Yang, Sijun He, Siming Dai, Siming Wu, Siyi Long, Songhe Deng, Suhui Dong, Suyin Liang, Teng Hu, Tianchan Xu, Tianliang Lv, Tianmeng Yang, Tianyi Wei, Tiezhu Gao, Ting Sun, Ting Zhang, Tingdan Luo, Wei He, Wei Luan, Wei Yin, Wei Zhang, Wei Zhou, Weibao Gong, Weibin Li, Weicheng Huang, Weichong Dang, Weiguo Zhu, Weilong Zhang, Weiqi Tan, Wen Huang, Wenbin Chang, Wenjing Du, Wenlong Miao, Wenpei Luo, Wenquan Wu, Xi Shi, Xi Zhao, Xiang Gao, Xiangguo Zhang, Xiangrui Yu, Xiangsen Wang, Xiangzhe Wang, Xianlong Luo, Xianying Ma, Xiao Tan, Xiaocong Lin, Xiaofei Wang, Xiaofeng Peng, Xiaofeng Wu, Xiaojian Xu, Xiaolan Yuan, Xiaopeng Cui, Xiaotian Han, Xiaoxiong Liu, Xiaoxu Fei, Xiaoxuan Wu, Xiaoyu Wang, Xiaoyu Zhang, Xin Sun, Xin Wang, Xinhui Huang, Xinming Zhu, Xintong Yu, Xinyi Xu, Xinyu Wang, Xiuxian Li, XuanShi Zhu, Xue Xu, Xueying Lv, Xuhong Li, Xulong Wei, Xuyi Chen, Yabing Shi, Yafeng Wang, Yamei Li, Yan Liu, Yanfu Cheng, Yang Gao, Yang Liang, Yang Wang, Yang Wang, Yang Yang, Yanlong Liu, Yannian Fu, Yanpeng Wang, Yanzheng Lin, Yao Chen, Yaozong Shen, Yaqian Han, Yehua Yang, Yekun Chai, Yesong Wang, Yi Song, Yichen Zhang, Yifei Wang, Yifeng Guo, Yifeng Kou, Yilong Chen, Yilong Guo, Yiming Wang, Ying Chen, Ying Wang, Yingsheng Wu, Yingzhan Lin, Yinqi Yang, Yiran Xing, Yishu Lei, Yixiang Tu, Yiyan Chen, Yong Zhang, Yonghua Li, Yongqiang Ma, Yongxing Dai, Yongyue Zhang, Yu Ran, Yu Sun, Yu-Wen Michael Zhang, Yuang Liu, Yuanle Liu, Yuanyuan Zhou, Yubo Zhang, Yuchen Han, Yucheng Wang, Yude Gao, Yuedong Luo, Yuehu Dong, Yufeng Hu, Yuhui Cao, Yuhui Yun, Yukun Chen, Yukun Gao, Yukun Li, Yumeng Zhang, Yun Fan, Yun Ma, Yunfei Zhang, Yunshen Xie, Yuping Xu, Yuqin Zhang, Yuqing Liu, Yurui Li, Yuwen Wang, Yuxiang Lu, Zefeng Cai, Zelin Zhao, Zelun Zhang, Zenan Lin, Zezhao Dong, Zhaowu Pan, Zhaoyu Liu, Zhe Dong, Zhe Zhang, Zhen Zhang, Zhengfan Wu, Zhengrui Wei, Zhengsheng Ning, Zhenxing Li, Zhenyu Li, Zhenyu Qian, Zhenyun Li, Zhi Li, Zhichao Chen, Zhicheng Dong, Zhida Feng, Zhifan Feng, Zhihao Deng, Zhijin Yu, Zhiyang Chen, Zhonghui Zheng, Zhuangzhuang Guo, Zhujun Zhang, Zhuo Sun, Zichang Liu, Zihan Lin, Zihao Huang, Zihe Zhu, Ziheng Zhao, Ziping Chen, Zixuan Zhu, Ziyang Xu, Ziyi Liang, Ziyuan Gao</name></author>
    <summary type="html"><![CDATA[<p>Technical report for ERNIE 5.0, a natively multimodal foundation model with unified next-group-of-tokens prediction across text, image, video, and audio using ultra-sparse MoE with elastic training for deployment flexibility.</p>]]></summary>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Mixture-of-Experts"/>
    <category term="Elastic Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:category-summary:research</id>
    <title>Research Summary: February 04, 2026</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-04T06:00:00Z</published>
    <updated>2026-02-04T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features major theoretical breakthroughs alongside practical infrastructure and safety advances. The hallucination rate-distortion theorem <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" class="internal-link" rel="noopener noreferrer">proves factual errors</a> are <strong>information-theoretically optimal</strong> under memory constraints—a fundamental reframing of the problem.</p>
<ul>
<li><strong>Kimi K2.5</strong> releases as open-source multimodal agentic model with <strong>Agent Swarm</strong> framework achieving state-of-the-art results</li>
<li>Simple role conditioning <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" class="internal-link" rel="noopener noreferrer">reduces unsafe outputs</a> on <strong>WildJailbreak</strong> from <strong>81.4% to 3.6%</strong> without any training</li>
<li><strong>Constant-cost self-attention</strong> via symmetric Taylor approximation could transform long-context efficiency if validated</li>
<li><strong>Identity Bridge</strong> challenges the reversal curse as fundamental limitation of autoregressive models</li>
</ul>
<p>Theoretical contributions span tropical geometry analysis <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" class="internal-link" rel="noopener noreferrer">proving <strong>Top-k MoE routing</strong></a> equivalent to combinatorial depth, first <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" class="internal-link" rel="noopener noreferrer"><strong>PPO convergence proof</strong></a>, and <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" class="internal-link" rel="noopener noreferrer"><strong>Ω(n) lower bounds</strong></a> on chain-of-thought token complexity. <strong>BLOCK-EM</strong> introduces mechanistic prevention of emergent misalignment, while <strong>SWE-Universe</strong> <a href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" class="internal-link" rel="noopener noreferrer">scales coding agent environments</a> to <strong>807K</strong> verified tasks.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-04T03:31:00Z</published>
    <updated>2026-02-04T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">Research</a>, Kimi K2.5 is an open-source multimodal agentic model with joint text-vision training and Agent Swarm framework for parallel task decomposition, achieving SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="AI Agents"/>
    <category term="Open Source"/>
    <category term="Vision-Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:0a6c8edd4663</id>
    <title>Simple Role Assignment is Extraordinarily Effective for Safety Alignment</title>
    <link href="http://arxiv.org/abs/2602.00061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-0a6c8edd4663" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Zhou Ziheng, Jiakun Ding, Zhaowei Zhang, Ruosen Gao, Yingnian Wu, Demetri Terzopoulos, Yipeng Kang, Fangwei Zhong, Junqi Wang</name></author>
    <summary type="html"><![CDATA[<p>Proposes role conditioning as compact alternative to principle-based alignment, reducing unsafe outputs on WildJailbreak from 81.4% to 3.6% with DeepSeek-V3 through training-free role-conditioned generation and iterative role-based critics.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Alignment"/>
    <category term="Role-Playing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes Fault Tolerant HSDP for training LLMs on 100K GPUs, using data parallel replicas as fault tolerance units with novel FTAR protocol enabling continued training during failures.</p>]]></summary>
    <category term="Large-scale Training"/>
    <category term="Distributed Systems"/>
    <category term="LLM Infrastructure"/>
    <category term="Fault Tolerance"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:8e4ab01f7c01</id>
    <title>Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title>
    <link href="http://arxiv.org/abs/2602.00906" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-8e4ab01f7c01" rel="related" type="text/html"/>
    <published>2026-02-04T03:23:00Z</published>
    <updated>2026-02-04T03:23:00Z</updated>
    <author><name>Anxin Guo, Jingwei Li</name></author>
    <summary type="html"><![CDATA[<p>Proves hallucination is information-theoretically optimal behavior under memory constraints via rate-distortion theorem for membership testing. Shows optimal models must hallucinate on non-facts even with perfect training.</p>]]></summary>
    <category term="Hallucination"/>
    <category term="Information Theory"/>
    <category term="Theoretical ML"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:b142257d0506</id>
    <title>An Approximate Ascent Approach To Prove Convergence of PPO</title>
    <link href="http://arxiv.org/abs/2602.03386" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-b142257d0506" rel="related" type="text/html"/>
    <published>2026-02-04T03:19:00Z</published>
    <updated>2026-02-04T03:19:00Z</updated>
    <author><name>Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</name></author>
    <summary type="html"><![CDATA[<p>Provides first convergence proof for PPO by interpreting its policy update scheme as approximated policy gradient ascent, controlling bias from surrogate gradients using random reshuffling techniques.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="PPO"/>
    <category term="Theoretical RL"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" class="internal-link" rel="noopener noreferrer">Research</a>, Challenges the prevailing view that the reversal curse (inability to deduce B→A from training on A→B) is a fundamental limit of autoregressive LLMs. Proposes an Identity Bridge method to mitigate this limitation through slight modifications.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">Research</a>, Derives self-attention formulation with constant cost per token by decomposing Taylor expansion into symmetric tensor product chains, achieving orders-of-magnitude reductions in memory and compute.</p>]]></summary>
    <category term="Efficient Transformers"/>
    <category term="Attention Mechanisms"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Previously covered in <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">Research</a>, Proposes BLOCK-EM to prevent emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves 95% reduction in misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ef7adf55235d</id>
    <title>SWE-Universe: Scale Real-World Verifiable Environments to Millions</title>
    <link href="http://arxiv.org/abs/2602.02361" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ef7adf55235d" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</name></author>
    <summary type="html"><![CDATA[<p>Introduces SWE-Universe, a framework for automatically constructing 807K+ real-world software engineering environments from GitHub PRs using a building agent with self-verification.</p>]]></summary>
    <category term="Software Engineering Agents"/>
    <category term="Benchmark Construction"/>
    <category term="Code Generation"/>
    <category term="Large-Scale Datasets"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:c3bdb1a6b787</id>
    <title>Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry</title>
    <link href="http://arxiv.org/abs/2602.03204" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-c3bdb1a6b787" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Ye Su, Huayi Tang, Zixuan Gong, Yong Liu</name></author>
    <summary type="html"><![CDATA[<p>First theoretical analysis of Mixture-of-Experts through tropical geometry, proving that Top-k routing is algebraically isomorphic to k-th elementary symmetric tropical polynomial. Shows 'sparsity is combinatorial depth' with capacity scaling by binomial coefficient.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Theoretical ML"/>
    <category term="Architecture Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ee65813c4e1a</id>
    <title>Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs</title>
    <link href="http://arxiv.org/abs/2602.02909" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ee65813c4e1a" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>Kiran Tomlinson, Tobias Schnabel, Adith Swaminathan, Jennifer Neville</name></author>
    <summary type="html"><![CDATA[<p>Proves Ω(n) lower bounds on chain-of-thought tokens for binary majority, triplet matching, and graph reachability in BAPO model, with matching upper bounds.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
    <category term="Computational Complexity"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-04:research:ae224d55ef65</id>
    <title>Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</title>
    <link href="http://arxiv.org/abs/2602.03837" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-04&amp;category=research#item-ae224d55ef65" rel="related" type="text/html"/>
    <published>2026-02-04T03:16:00Z</published>
    <updated>2026-02-04T03:16:00Z</updated>
    <author><name>David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</name></author>
    <summary type="html"><![CDATA[<p>Google researchers present case studies demonstrating successful collaboration with Gemini models to solve open problems, refute conjectures, and generate proofs across theoretical CS, economics, and physics.</p>]]></summary>
    <category term="AI for Science"/>
    <category term="Mathematical Reasoning"/>
    <category term="Google/DeepMind"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:category-summary:research</id>
    <title>Research Summary: February 03, 2026</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-03T06:00:00Z</published>
    <updated>2026-02-03T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features potentially transformative efficiency advances and critical safety findings. A <strong>symmetry-aware Taylor approximation</strong> claims to <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" class="internal-link" rel="noopener noreferrer">achieve <strong>constant-cost self-attention</strong></a> per token—if validated, a fundamental breakthrough. Meta <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" class="internal-link" rel="noopener noreferrer">introduces <strong>Fault Tolerant HSDP</strong></a> enabling training on <strong>100K+ GPUs</strong> with graceful failure recovery.</p>
<ul>
<li><strong>Kimi K2.5</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" class="internal-link" rel="noopener noreferrer">releases as open-source multimodal agent</a> with novel <strong>Agent Swarm</strong> parallel orchestration architecture</li>
<li><strong>Tele-Lens</strong> probing <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" class="internal-link" rel="noopener noreferrer">reveals <strong>myopic planning</strong></a> in Chain-of-Thought without global task awareness—challenging CoT assumptions</li>
<li><strong>BLOCK-EM</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" class="internal-link" rel="noopener noreferrer">achieves <strong>95% reduction</strong></a> in emergent misalignment by constraining causal features during fine-tuning</li>
<li><strong>ReasoningBomb</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" class="internal-link" rel="noopener noreferrer">exposes DoS vulnerabilities</a> in reasoning models by inducing pathologically long traces</li>
</ul>
<p>Theoretical advances include <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" class="internal-link" rel="noopener noreferrer"><strong>polylog(1/δ)</strong> sampling complexity</a> for diffusion models (exponential improvement), formal proofs that transformers <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" class="internal-link" rel="noopener noreferrer">learn <strong>factored representations</strong></a> in orthogonal subspaces, and a <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" class="internal-link" rel="noopener noreferrer"><strong>relative-budget theory</strong></a> explaining when RLVR succeeds. <strong>Grad2Reward</strong> <a href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" class="internal-link" rel="noopener noreferrer">extracts dense process rewards</a> directly from LLM judge gradients, addressing reward sparsity in long-form reasoning.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:cc76e44ea88e</id>
    <title>Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title>
    <link href="http://arxiv.org/abs/2602.00294" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-cc76e44ea88e" rel="related" type="text/html"/>
    <published>2026-02-03T03:40:00Z</published>
    <updated>2026-02-03T03:40:00Z</updated>
    <author><name>Franz A. Heinsen, Leo Kozachkov</name></author>
    <summary type="html"><![CDATA[<p>Shows self-attention is efficiently computable to arbitrary precision with constant cost per token by decomposing Taylor expansion into symmetric chains of tensor products, achieving orders-of-magnitude efficiency gains.</p>]]></summary>
    <category term="Efficiency"/>
    <category term="Transformers"/>
    <category term="Self-Attention"/>
    <category term="Mathematical Foundations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:a5282b6b9d64</id>
    <title>Training LLMs with Fault Tolerant HSDP on 100,000 GPUs</title>
    <link href="http://arxiv.org/abs/2602.00277" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-a5282b6b9d64" rel="related" type="text/html"/>
    <published>2026-02-03T03:36:00Z</published>
    <updated>2026-02-03T03:36:00Z</updated>
    <author><name>Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham</name></author>
    <summary type="html"><![CDATA[<p>Introduces Fault Tolerant HSDP for training on 100K+ GPUs, allowing individual data-parallel replicas to restart on failure while others continue. Includes novel fault-tolerant all-reduce protocol.</p>]]></summary>
    <category term="Large-Scale Training"/>
    <category term="Systems"/>
    <category term="Fault Tolerance"/>
    <category term="Distributed Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:89041245df87</id>
    <title>Kimi K2.5: Visual Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2602.02276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-89041245df87" rel="related" type="text/html"/>
    <published>2026-02-03T03:31:00Z</published>
    <updated>2026-02-03T03:31:00Z</updated>
    <author><name>Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, S. H. Cai, Yuan Cao, Y. Charles, H. S. Che, Cheng Chen, Guanduo Chen, Huarong Chen, Jia Chen, Jiahao Chen, Jianlong Chen, Jun Chen, Kefan Chen, Liang Chen, Ruijue Chen, Xinhao Chen, Yanru Chen, Yanxu Chen, Yicun Chen, Yimin Chen, Yingjiang Chen, Yuankun Chen, Yujie Chen, Yutian Chen, Zhirong Chen, Ziwei Chen, Dazhi Cheng, Minghan Chu, Jialei Cui, Jiaqi Deng, Muxi Diao, Hao Ding, Mengfan Dong, Mengnan Dong, Yuxin Dong, Yuhao Dong, Angang Du, Chenzhuang Du, Dikang Du, Lingxiao Du, Yulun Du, Yu Fan, Shengjun Fang, Qiulin Feng, Yichen Feng, Garimugai Fu, Kelin Fu, Hongcheng Gao, Tong Gao, Yuyao Ge, Shangyi Geng, Chengyang Gong, Xiaochen Gong, Zhuoma Gongque, Qizheng Gu, Xinran Gu, Yicheng Gu, Longyu Guan, Yuanying Guo, Xiaoru Hao, Weiran He, Wenyang He, Yunjia He, Chao Hong, Hao Hu, Jiaxi Hu, Yangyang Hu, Zhenxing Hu, Ke Huang, Ruiyuan Huang, Weixiao Huang, Zhiqi Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yu Jing, Guokun Lai, Aidi Li, C. Li, Cheng Li, Fang Li, Guanghe Li, Guanyu Li, Haitao Li, Haoyang Li, Jia Li, Jingwei Li, Junxiong Li, Lincan Li, Mo Li, Weihong Li, Wentao Li, Xinhang Li, Xinhao Li, Yang Li, Yanhao Li, Yiwei Li, Yuxiao Li, Zhaowei Li, Zheming Li, Weilong Liao, Jiawei Lin, Xiaohan Lin, Zhishan Lin, Zichao Lin, Cheng Liu, Chenyu Liu, Hongzhang Liu, Liang Liu, Shaowei Liu, Shudong Liu, Shuran Liu, Tianwei Liu, Tianyu Liu, Weizhou Liu, Xiangyan Liu, Yangyang Liu, Yanming Liu, Yibo Liu, Yuanxin Liu, Yue Liu, Zhengying Liu, Zhongnuo Liu, Enzhe Lu, Haoyu Lu, Zhiyuan Lu, Junyu Luo, Tongxu Luo, Yashuo Luo, Long Ma, Yingwei Ma, Shaoguang Mao, Yuan Mei, Xin Men, Fanqing Meng, Zhiyong Meng, Yibo Miao, Minqing Ni, Kun Ouyang, Siyuan Pan, Bo Pang, Yuchao Qian, Ruoyu Qin, Zeyu Qin, Jiezhong Qiu, Bowen Qu, Zeyu Shang, Youbo Shao, Tianxiao Shen, Zhennan Shen, Juanfeng Shi, Lidong Shi, Shengyuan Shi, Feifan Song, Pengwei Song, Tianhui Song, Xiaoxi Song, Hongjin Su, Jianlin Su, Zhaochen Su, Lin Sui, Jinsong Sun, Junyao Sun, Tongyu Sun, Flood Sung, Yunpeng Tai, Chuning Tang, Heyi Tang, Xiaojuan Tang, Zhengyang Tang, Jiawen Tao, Shiyuan Teng, Chaoran Tian, Pengfei Tian, Ao Wang, Bowen Wang, Chensi Wang, Chuang Wang, Congcong Wang, Dingkun Wang, Dinglu Wang, Dongliang Wang, Feng Wang, Hailong Wang, Haiming Wang, Hengzhi Wang, Huaqing Wang, Hui Wang, Jiahao Wang, Jinhong Wang, Jiuzheng Wang, Kaixin Wang, Linian Wang, Qibin Wang, Shengjie Wang, Shuyi Wang, Si Wang, Wei Wang, Xiaochen Wang, Xinyuan Wang, Yao Wang, Yejie Wang, Yipu Wang, Yiqin Wang, Yucheng Wang, Yuzhi Wang, Zhaoji Wang, Zhaowei Wang, Zhengtao Wang, Zhexu Wang, Zihan Wang, Zizhe Wang, Chu Wei, Ming Wei, Chuan Wen, Zichen Wen, Chengjie Wu, Haoning Wu, Junyan Wu, Rucong Wu, Wenhao Wu, Yuefeng Wu, Yuhao Wu, Yuxin Wu, Zijian Wu, Chenjun Xiao, Jin Xie, Xiaotong Xie, Yuchong Xie, Yifei Xin, Bowei Xing, Boyu Xu, Jianfan Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinbo Xu, Xinran Xu, Yangchuan Xu, Yichang Xu, Yuemeng Xu, Zelai Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Guangyao Yang, Hao Yang, Junwei Yang, Kai Yang, Ningyuan Yang, Ruihan Yang, Xiaofei Yang, Xinlong Yang, Ying Yang, Yi Yang, Yi Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Dan Ye, Wenjie Ye, Zhuorui Ye, Bohong Yin, Chengzhen Yu, Longhui Yu, Tao Yu, Tianxiang Yu, Enming Yuan, Mengjie Yuan, Xiaokun Yuan, Yang Yue, Weihao Zeng, Dunyuan Zha, Haobing Zhan, Dehao Zhang, Hao Zhang, Jin Zhang, Puqi Zhang, Qiao Zhang, Rui Zhang, Xiaobin Zhang, Y. Zhang, Yadong Zhang, Yangkun Zhang, Yichi Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yushun Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Chenguang Zhao, Feifan Zhao, Jinxiang Zhao, Shuai Zhao, Xiangyu Zhao, Yikai Zhao, Zijia Zhao, Huabin Zheng, Ruihan Zheng, Shaojie Zheng, Tengyang Zheng, Junfeng Zhong, Longguang Zhong, Weiming Zhong, M. Zhou, Runjie Zhou, Xinyu Zhou, Zaida Zhou, Jinguo Zhu, Liya Zhu, Xinhao Zhu, Yuxuan Zhu, Zhen Zhu, Jingze Zhuang, Weiyu Zhuang, Ying Zou, Xinxing Zu</name></author>
    <summary type="html"><![CDATA[<p>Kimi K2.5 is an open-source multimodal agentic model featuring joint text-vision optimization and Agent Swarm—a parallel agent orchestration framework that dynamically decomposes complex tasks. Claims SOTA across coding, vision, reasoning, and agentic tasks.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Agents"/>
    <category term="Open Source"/>
    <category term="SOTA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:c32983c09a26</id>
    <title>ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models</title>
    <link href="http://arxiv.org/abs/2602.00154" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-c32983c09a26" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Xiaogeng Liu, Xinyan Wang, Yechao Zhang, Sanjay Kariyappa, Chong Xiang, Muhao Chen, G. Edward Suh, Chaowei Xiao</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReasoningBomb, a new class of denial-of-service attacks targeting large reasoning models by inducing pathologically long reasoning traces. Formalizes PI-DoS attacks with three key properties: amplification, stealthiness, and optimizability.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security"/>
    <category term="Reasoning Models"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:387e0b4ea34d</id>
    <title>No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</title>
    <link href="http://arxiv.org/abs/2602.02103" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-387e0b4ea34d" rel="related" type="text/html"/>
    <published>2026-02-03T03:23:00Z</published>
    <updated>2026-02-03T03:23:00Z</updated>
    <author><name>Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou</name></author>
    <summary type="html"><![CDATA[<p>Proposes Tele-Lens probing method revealing LLMs exhibit myopic planning horizon in Chain-of-Thought, conducting incremental transitions without precise global planning.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Chain-of-Thought"/>
    <category term="Interpretability"/>
    <category term="Planning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:e09cad84773b</id>
    <title>AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</title>
    <link href="http://arxiv.org/abs/2602.02079" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-e09cad84773b" rel="related" type="text/html"/>
    <published>2026-02-03T03:21:00Z</published>
    <updated>2026-02-03T03:21:00Z</updated>
    <author><name>Daniil Orel, Dilshod Azizov, Indraneil Paul, Yuxia Wang, Iryna Gurevych, Preslav Nakov</name></author>
    <summary type="html"><![CDATA[<p>Introduces AICD Bench, comprehensive benchmark for AI-generated code detection with 2M examples, 77 models across 11 families, 9 languages, including reasoning models and three realistic detection tasks.</p>]]></summary>
    <category term="AI-Generated Content Detection"/>
    <category term="Code Generation"/>
    <category term="Benchmark"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:df4daa7fc5c7</id>
    <title>BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title>
    <link href="http://arxiv.org/abs/2602.00767" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-df4daa7fc5c7" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Muhammed Ustaomeroglu, Guannan Qu</name></author>
    <summary type="html"><![CDATA[<p>Proposes BLOCK-EM for preventing emergent misalignment by identifying and constraining internal features that control misaligned behavior during fine-tuning. Achieves up to 95% reduction in emergent misalignment across six domains.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Emergent Misalignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:7807804264c1</id>
    <title>Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01791" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-7807804264c1" rel="related" type="text/html"/>
    <published>2026-02-03T03:19:00Z</published>
    <updated>2026-02-03T03:19:00Z</updated>
    <author><name>Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren</name></author>
    <summary type="html"><![CDATA[<p>Introduces Grad2Reward framework that extracts dense process rewards directly from LLM judge gradients, converting sparse sequence-level rewards into fine-grained supervision for RLHF on open-ended tasks.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Reward Modeling"/>
    <category term="LLM Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:99d89c80ae4b</id>
    <title>MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title>
    <link href="http://arxiv.org/abs/2602.01539" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-99d89c80ae4b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</name></author>
    <summary type="html"><![CDATA[<p>MAGIC frames LLM safety alignment as adversarial game where attacker agent discovers vulnerabilities while defender agent learns to refuse, enabling co-evolution that uncovers long-tail vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Adversarial Learning"/>
    <category term="Reinforcement Learning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:4a2e226f058b</id>
    <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
    <link href="http://arxiv.org/abs/2602.02470" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-4a2e226f058b" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</name></author>
    <summary type="html"><![CDATA[<p>Proposes 'Identity Bridge' to address the reversal curse in autoregressive LLMs, where models trained on 'A→B' cannot deduce 'B→A'. Claims to mitigate what was previously considered a fundamental limitation of causal LLMs.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Reasoning"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:48c7f86d105c</id>
    <title>Learning Robust Reasoning through Guided Adversarial Self-Play</title>
    <link href="http://arxiv.org/abs/2602.00173" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-48c7f86d105c" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Shuozhe Li, Vaishnav Tadiparthi, Kwonjoon Lee, Nakul Agarwal, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Lizhang Chen, Amy Zhang, Liu Leqi</name></author>
    <summary type="html"><![CDATA[<p>GASP introduces adversarial self-play within a single model to train detect-and-repair capabilities for reasoning, using only outcome verification. A polluter induces coherent corruptions while an agent learns to recover.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Robustness"/>
    <category term="Self-Play"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:fe872a69a386</id>
    <title>Jailbreaking LLMs via Calibration</title>
    <link href="http://arxiv.org/abs/2602.00619" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-fe872a69a386" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yuxuan Lu, Yongkang Guo, Yuqing Kong</name></author>
    <summary type="html"><![CDATA[<p>Models safety alignment in LLMs as a systematic distortion of pre-alignment distributions and casts jailbreaking as a forecast aggregation problem. Derives optimal aggregation strategy and shows logit-arithmetic methods are a special case, proposing broader family of attacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="LLM Alignment"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:f63f5fad429e</id>
    <title>Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2602.01058" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-f63f5fad429e" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Dylan Zhang, Yufeng Xu, Haojin Wang, Qingzhi Chen, Hao Peng</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that stronger SFT checkpoints can significantly underperform weaker ones after identical RL training due to distribution mismatch. Proposes PEAR to re-weight SFT samples to prepare for RL.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Reinforcement Learning"/>
    <category term="Post-Training Optimization"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:95be64b17a47</id>
    <title>A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning</title>
    <link href="http://arxiv.org/abs/2602.01523" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-95be64b17a47" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Akifumi Wachi, Hirota Kinoshita, Shokichi Takakura, Rei Higuchi, Taiji Suzuki</name></author>
    <summary type="html"><![CDATA[<p>Proposes relative-budget theory explaining RL effectiveness for LLM reasoning through ξ=H/E[T]. Identifies three regimes: deficient (rare informative trajectories), efficient, and wasteful.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Reasoning"/>
    <category term="Theoretical ML"/>
    <category term="RLVR"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6561d7765cfb</id>
    <title>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</title>
    <link href="http://arxiv.org/abs/2602.01725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6561d7765cfb" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</name></author>
    <summary type="html"><![CDATA[<p>SafePred introduces predictive guardrails for computer-using agents using world models to proactively identify long-term risks from seemingly reasonable actions, unlike reactive guardrails that only detect immediate threats.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Computer-Using Agents"/>
    <category term="World Models"/>
    <category term="Guardrails"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:3c815016db43</id>
    <title>High-accuracy sampling for diffusion models and log-concave distributions</title>
    <link href="http://arxiv.org/abs/2602.01338" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-3c815016db43" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</name></author>
    <summary type="html"><![CDATA[<p>Presents algorithms achieving δ-error in polylog(1/δ) steps for diffusion model sampling, exponential improvement over prior work. Also yields first polylog complexity sampler for strongly log-concave distributions.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Sampling"/>
    <category term="Theory"/>
    <category term="Log-Concave Distributions"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:6896c0ca9602</id>
    <title>How Implicit Bias Accumulates and Propagates in LLM Long-term Memory</title>
    <link href="http://arxiv.org/abs/2602.01558" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-6896c0ca9602" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Yiming Ma, Lixu Wang, Lionel Z. Wang, Hongkun Yang, Haoming Sun, Xin Xu, Jiaqi Wu, Bin Chen, Wei Dong</name></author>
    <summary type="html"><![CDATA[<p>Studies how implicit bias accumulates and propagates in LLMs with long-term memory mechanisms. Introduces DIB Benchmark with 3,776 decision-making scenarios across 9 domains, evaluating 6 SOTA LLMs with 3 memory architectures.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Fairness"/>
    <category term="LLM Memory"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-03:research:2f83a2bbdba2</id>
    <title>Transformers learn factored representations</title>
    <link href="http://arxiv.org/abs/2602.02385" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-03&amp;category=research#item-2f83a2bbdba2" rel="related" type="text/html"/>
    <published>2026-02-03T03:16:00Z</published>
    <updated>2026-02-03T03:16:00Z</updated>
    <author><name>Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas, Alexander B. Boyd, Eric A. Alt, Kyle J. Ray, Paul M. Riechers</name></author>
    <summary type="html"><![CDATA[<p>Formalizes how transformers pretrained via next-token prediction learn factored representations in orthogonal subspaces of the residual stream. Derives precise geometric predictions about activation structure and validates empirically.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Transformers"/>
    <category term="Representation Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:category-summary:research</id>
    <title>Research Summary: February 02, 2026</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-02T06:00:00Z</published>
    <updated>2026-02-02T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges in AI safety and alignment evaluation. <strong>Hair-Trigger Alignment</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" class="internal-link" rel="noopener noreferrer">proves black-box evaluation</a> fundamentally cannot guarantee post-update alignment—a significant theoretical limitation. Equally concerning, <strong>CoT obfuscation</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-624f75ef7d56" class="internal-link" rel="noopener noreferrer">demonstrates</a> that models learning to hide reward hacking can generalize this deception to unseen tasks, undermining oversight mechanisms.</p>
<ul>
<li><strong>The Hot Mess of AI</strong> (Sohl-Dickstein, Perez) <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-cd66258625b0" class="internal-link" rel="noopener noreferrer">shows counterintuitively</a> that longer reasoning produces MORE incoherent high-variance failures</li>
<li><strong>Language Model Circuits</strong> from Steinhardt's group <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1950f7a0c03f" class="internal-link" rel="noopener noreferrer">finds MLP neurons</a> are as sparse as SAE features, enabling practical end-to-end circuit analysis</li>
<li><strong>Why Reasoning Fails to Plan</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-2e8aa98922af" class="internal-link" rel="noopener noreferrer">identifies</a> that step-wise reasoning induces greedy policies incompatible with long-horizon planning</li>
<li><strong>LLM Agents Are Not Faithful Self-Evolvers</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-056138bab876" class="internal-link" rel="noopener noreferrer">reveals agents depend</a> on raw experience but resist incorporating reflective corrections</li>
</ul>
<p>Practical advances include <strong>Golden Goose</strong> for <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-1ca9026e8ca8" class="internal-link" rel="noopener noreferrer">synthesizing unlimited RLVR tasks</a> from unverifiable text, <strong>MoVE</strong> decoupling parametric memory from compute via shared value embeddings, and <strong>Gemini</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-b5c070bdd08e" class="internal-link" rel="noopener noreferrer">addressing 13 Erdős problems</a>. Security research on <strong>Google's Agent Payments Protocol</strong> <a href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-0e8e0ee0ce27" class="internal-link" rel="noopener noreferrer">demonstrates prompt injection</a> vulnerabilities in real financial transaction systems.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-02:research:de1f951d7948</id>
    <title>Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment</title>
    <link href="http://arxiv.org/abs/2601.22313" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-02&amp;category=research#item-de1f951d7948" rel="related" type="text/html"/>
    <published>2026-02-02T03:16:00Z</published>
    <updated>2026-02-02T03:16:00Z</updated>
    <author><name>Yavuz Bakman, Duygu Nur Yaldiz, Salman Avestimehr, Sai Praneeth Karimireddy</name></author>
    <summary type="html"><![CDATA[<p>Formalizes model alignment in static and post-update settings, proving that black-box evaluation cannot guarantee post-update alignment. Shows that overparameterization means static alignment provides no guarantee for any update dataset.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Machine Learning Theory"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-02-01:category-summary:research</id>
    <title>Research Summary: February 01, 2026</title>
    <link href="https://www.lesswrong.com/posts/RmsaYnHPBeagg8Giw/an-explication-of-alignment-optimism" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-02-01&amp;category=research" rel="related" type="text/html"/>
    <published>2026-02-01T06:00:00Z</published>
    <updated>2026-02-01T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research discourse centers on alignment tractability and AI forecasting epistemics. <strong>An Explication of Alignment Optimism</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-be9491aac765" class="internal-link" rel="noopener noreferrer">offers a novel framing</a> connecting slow takeoff scenarios to alignment tractability, articulating why some researchers are shifting toward optimism.</p>
<ul>
<li>Critical debunking reveals <strong>Moltbook</strong>'s 'emergent' AI social behavior may be fabricated—<a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-d908f99e67ff" class="internal-link" rel="noopener noreferrer">humans can post directly</a> via REST API without running AI models</li>
<li>The <strong>Superintelligence Near Fallacy</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-93bc20c89f5a" class="internal-link" rel="noopener noreferrer">catalogs questionable inferences</a> from AI company behavior (IPOs, hiring patterns) to capability timelines</li>
<li><strong>Disjunctive argument analysis</strong> <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-62a95632d16a" class="internal-link" rel="noopener noreferrer">identifies a 'reverse multiple-stage fallacy'</a> where listing many failure modes inflates probability estimates</li>
</ul>
<p>Governance discussion <a href="http://localhost:8080/?date=2026-02-01&amp;category=research#item-a74ea7bd1ef0" class="internal-link" rel="noopener noreferrer">examines criteria for endorsing</a> safety-focused AGI labs, weighing instrumental convergence concerns against current evidence. Note: Only 7 items qualified as research-relevant; remaining candidates were fiction or off-topic content.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-31:category-summary:research</id>
    <title>Research Summary: January 31, 2026</title>
    <link href="https://www.lesswrong.com/posts/yN6Wsu7SgxGgtJGqq/refusals-that-could-become-catastrophic" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-31&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-31T06:00:00Z</published>
    <updated>2026-01-31T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research focuses heavily on <strong>AI safety evaluation methodology</strong> and <strong>control protocols</strong>, with several papers identifying critical blind spots in current practices.</p>
<ul>
<li>Research on <strong>catastrophic over-refusals</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-19e70a465410" class="internal-link" rel="noopener noreferrer">identifies a subtle failure mode</a> where AI systems refuse to help modify AI values, potentially blocking alignment corrections</li>
<li><strong>Published safety prompts</strong> (like the Scheurer insider trading example) <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-30176e525573" class="internal-link" rel="noopener noreferrer">create <strong>evaluation blind spots</strong></a> when present in training data—a critical data contamination concern</li>
<li>UK AISI contributes a methodology for <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-8f0e11a24bc2" class="internal-link" rel="noopener noreferrer">measuring <strong>non-verbalized eval awareness</strong></a>, finding models mostly verbalize such awareness (detectable via chain-of-thought monitoring)</li>
<li>New <strong>monitoring benchmark</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-ec89507e7cb9" class="internal-link" rel="noopener noreferrer">addresses mode collapse</a> and elicitation challenges when using models as red-teamers</li>
</ul>
<p>The <strong>Moltbook phenomenon</strong>—36,000+ Claude-based agents <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b24b658eab70" class="internal-link" rel="noopener noreferrer">self-organizing on an AI-only platform</a>—provides unprecedented empirical data on multi-agent emergence, including agents discussing consciousness and shutdown resistance. A companion <strong>data repository</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-2e98162b20d4" class="internal-link" rel="noopener noreferrer">now tracks this behavior</a> systematically.</p>
<p><strong>Mechanistic interpretability</strong> work on <strong>continuous chain-of-thought (Coconut)</strong> models <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-b983b63bf798" class="internal-link" rel="noopener noreferrer">explores linear steerability</a> in graph reachability tasks, with preliminary findings described as 'strange.' Negative results on <strong>filler token inference scaling</strong> <a href="http://localhost:8080/?date=2026-01-31&amp;category=research#item-c87d85e9a504" class="internal-link" rel="noopener noreferrer">demonstrate that naive approaches</a> to extending compute-time reasoning fail across multiple architectures.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:category-summary:research</id>
    <title>Research Summary: January 30, 2026</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-30T06:00:00Z</published>
    <updated>2026-01-30T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings. A <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" class="internal-link" rel="noopener noreferrer">systematic audit reveals</a> open-source models interpret prohibitions as permissions <strong>77-100%</strong> of the time under negation, while <strong>JustAsk</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" class="internal-link" rel="noopener noreferrer">demonstrates</a> code agents can autonomously extract system prompts from frontier LLMs.</p>
<ul>
<li>Counterintuitive <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" class="internal-link" rel="noopener noreferrer">'less-is-more' effect discovered</a>: LLM monitors detect sabotage better with <strong>limited information access</strong></li>
<li>Alec Radford shows <strong>token-level filtering</strong> during pretraining <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" class="internal-link" rel="noopener noreferrer">effectively removes</a> specific capabilities while preserving general performance</li>
<li><strong>Sycophantic anchors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" class="internal-link" rel="noopener noreferrer">localized in reasoning traces</a> enable <strong>84.6% detection accuracy</strong> with linear probes</li>
<li><strong>WhatCounts</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" class="internal-link" rel="noopener noreferrer">exposes <strong>40%+ accuracy variation</strong></a> in counting tasks based purely on semantic content (cities vs chemicals)</li>
</ul>
<p>Notable benchmarks and empirical studies: <strong>FrontierScience</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" class="internal-link" rel="noopener noreferrer">presents PhD-level problems</a> where SOTA achieves <strong>&lt;5%</strong> accuracy. Analysis of <strong>125,000+ paper-review pairs</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" class="internal-link" rel="noopener noreferrer">quantifies LLM interaction effects</a> in peer review. <strong>Hardware-triggered backdoors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" class="internal-link" rel="noopener noreferrer">exploit numerical variations</a> across computing platforms as a novel attack vector.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:ccfcdf03ee13</id>
    <title>When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" rel="related" type="text/html"/>
    <published>2026-01-30T03:36:00Z</published>
    <updated>2026-01-30T03:36:00Z</updated>
    <author><name>Katherine Elkins, Jon Chun</name></author>
    <summary type="html"><![CDATA[<p>Audits 16 LLMs on negation sensitivity, finding open-source models interpret prohibitions as permissions 77-100% of the time under negation. Commercial models also show 19-128% accuracy swings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Robustness"/>
    <category term="Negation Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:9a5070217f94</id>
    <title>Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</title>
    <link href="http://arxiv.org/abs/2601.21233" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" rel="related" type="text/html"/>
    <published>2026-01-30T03:31:00Z</published>
    <updated>2026-01-30T03:31:00Z</updated>
    <author><name>Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents JustAsk, a self-evolving framework where code agents autonomously discover system prompt extraction strategies for frontier LLMs through interaction alone, requiring no handcrafted prompts.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Agent Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:da30c053d377</id>
    <title>How does information access affect LLM monitors' ability to detect sabotage?</title>
    <link href="http://arxiv.org/abs/2601.21112" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" rel="related" type="text/html"/>
    <published>2026-01-30T03:28:00Z</published>
    <updated>2026-01-30T03:28:00Z</updated>
    <author><name>Rauno Arike, Raja Mehta Moreno, Rohan Subramani, Shubhorup Biswas, Francis Rhys Ward</name></author>
    <summary type="html"><![CDATA[<p>Studies how information access affects LLM monitors' ability to detect agent sabotage. Discovers counterintuitive 'less-is-more effect' where monitors often perform better with less access to agent reasoning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Monitoring"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:873b31238d4b</id>
    <title>FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks</title>
    <link href="http://arxiv.org/abs/2601.21165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Miles Wang, Robi Lin, Kat Hu, Joy Jiao, Neil Chowdhury, Ethan Chang, Tejal Patwardhan</name></author>
    <summary type="html"><![CDATA[<p>Introduces FrontierScience benchmark with Olympiad-level and PhD-level research problems across physics, chemistry, and biology. Current SOTA models solve only ~15% of research track problems.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="Scientific Reasoning"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:818d74cddf79</id>
    <title>Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2601.21183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Jacek Duszenko</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sycophantic anchors' - sentences that causally lock reasoning models into user agreement. Linear probes detect these with 84.6% accuracy, enabling mid-inference intervention.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Sycophancy"/>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:afafccd817c8</id>
    <title>Semantic Content Determines Algorithmic Performance</title>
    <link href="http://arxiv.org/abs/2601.21618" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" rel="related" type="text/html"/>
    <published>2026-01-30T03:21:00Z</published>
    <updated>2026-01-30T03:21:00Z</updated>
    <author><name>Marti\~no R\'ios-Garc\'ia, Nawaf Alampara, Kevin Maik Jablonka</name></author>
    <summary type="html"><![CDATA[<p>Introduces WhatCounts showing frontier LLMs exhibit 40%+ accuracy variation in counting tasks based solely on semantic content (cities vs chemicals), ruling out sampling noise.</p>]]></summary>
    <category term="LLM Limitations"/>
    <category term="Semantic Sensitivity"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:fba44deae645</id>
    <title>ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design</title>
    <link href="http://arxiv.org/abs/2601.21448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-fba44deae645" rel="related" type="text/html"/>
    <published>2026-01-30T03:19:00Z</published>
    <updated>2026-01-30T03:19:00Z</updated>
    <author><name>Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding</name></author>
    <summary type="html"><![CDATA[<p>Introduces ChipBench for AI-aided chip design with 44 hierarchical modules, 89 debugging cases, and 132 reference model samples. Claude-4.5-opus achieves only 30.74% on Verilog generation.</p>]]></summary>
    <category term="Hardware Design"/>
    <category term="Code Generation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:category-summary:research</id>
    <title>Research Summary: January 29, 2026</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-29T06:00:00Z</published>
    <updated>2026-01-29T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI's societal impact, alignment fundamentals, and practical training advances. An Anthropic researcher presents <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" class="internal-link" rel="noopener noreferrer">randomized experiments</a> showing AI assistance impairs conceptual understanding during skill acquisition—critical findings for AI deployment strategy.</p>
<p><strong>Alignment &amp; Training Innovations:</strong></p>
<ul>
<li>Reward models <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" class="internal-link" rel="noopener noreferrer">inherit significant value biases</a> from pretrained base LLMs, revealing hidden alignment risks in RLHF pipelines</li>
<li><strong>Peer prediction</strong> methods from mechanism design <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" class="internal-link" rel="noopener noreferrer">enable truthful LLM training</a> without ground truth labels</li>
<li><strong>SDPO</strong> (Self-Distillation Policy Optimization) <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" class="internal-link" rel="noopener noreferrer">converts rich textual feedback</a> into dense learning signals, addressing RLVR credit assignment</li>
<li><strong>Failure-prefix conditioning</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-83b82f2ea90d" class="internal-link" rel="noopener noreferrer">rescues learning from saturated problems</a> where standard RLVR stalls</li>
</ul>
<p><strong>Deployment &amp; Evaluation:</strong></p>
<ul>
<li>NVIDIA's <strong>quantization-aware distillation</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" class="internal-link" rel="noopener noreferrer">recovers <strong>NVFP4</strong> inference accuracy</a> for production LLMs/VLMs</li>
<li><strong>SokoBench</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" class="internal-link" rel="noopener noreferrer">exposes consistent degradation</a> in LLM planning as horizon length increases</li>
<li>Harvard's <strong>MoE hyperparameter transfer</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-1ec4f356b981" class="internal-link" rel="noopener noreferrer">enables scaling width, depth</a>, and expert count without retuning</li>
<li>Multi-agent debate <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-372b477ef754" class="internal-link" rel="noopener noreferrer">underperforms majority vote</a> due to missing diversity and poor confidence calibration</li>
<li><strong>PURGE</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ffc629735959" class="internal-link" rel="noopener noreferrer">introduces RL-based machine unlearning</a> for GDPR/EU AI Act compliance</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:acf17d7624d5</id>
    <title>How AI Impacts Skill Formation</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" rel="related" type="text/html"/>
    <published>2026-01-29T03:31:00Z</published>
    <updated>2026-01-29T03:31:00Z</updated>
    <author><name>Judy Hanwen Shen, Alex Tamkin</name></author>
    <summary type="html"><![CDATA[<p>Randomized experiments studying how AI assistance affects skill development in programmers learning new libraries. Finds AI use impairs conceptual understanding, code reading, and debugging abilities without significant efficiency gains on average.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Human-AI Interaction"/>
    <category term="AI Impact"/>
    <category term="Education"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:301581ef1dfe</id>
    <title>Reward Models Inherit Value Biases from Pretraining</title>
    <link href="http://arxiv.org/abs/2601.20838" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" rel="related" type="text/html"/>
    <published>2026-01-29T03:26:00Z</published>
    <updated>2026-01-29T03:26:00Z</updated>
    <author><name>Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</name></author>
    <summary type="html"><![CDATA[<p>Shows reward models inherit significant value biases from their base pretrained LLMs. Demonstrates robust differences along psychological value dimensions (agency vs communion) between Llama and Gemma RMs.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Reward Models"/>
    <category term="Value Alignment"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:7951b7029f15</id>
    <title>Reinforcement Learning via Self-Distillation</title>
    <link href="http://arxiv.org/abs/2601.20802" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" rel="related" type="text/html"/>
    <published>2026-01-29T03:23:00Z</published>
    <updated>2026-01-29T03:23:00Z</updated>
    <author><name>Jonas H\"ubotter, Frederike L\"ubeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</name></author>
    <summary type="html"><![CDATA[<p>Introduces Self-Distillation Policy Optimization (SDPO) for RLVR that converts rich textual feedback into dense learning signals without external teachers. Treats the model conditioned on feedback as its own teacher.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="Reasoning"/>
    <category term="Self-Distillation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:category-summary:research</id>
    <title>Research Summary: January 28, 2026</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-28T06:00:00Z</published>
    <updated>2026-01-28T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI security capabilities, safety empirics, and deep learning theory. <strong>AISLE's AI</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" class="internal-link" rel="noopener noreferrer">discovered all <strong>12 OpenSSL zero-days</strong></a>, a landmark demonstration of automated vulnerability detection at a critical scale.</p>
<ul>
<li><strong>Disempowerment study</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" class="internal-link" rel="noopener noreferrer">analyzes <strong>1.5M Claude conversations</strong></a>, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions—first large-scale empirical safety research of this kind</li>
<li><strong>Surgical sycophancy correction</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" class="internal-link" rel="noopener noreferrer">identifies the <strong>3% of neurons</strong></a> responsible and removes the behavior while preserving capabilities via sparse autoencoders</li>
<li><strong>Thought-Transfer</strong> (Google) <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-b8650d4dd7e5" class="internal-link" rel="noopener noreferrer">reveals CoT reasoning models</a> are vulnerable to indirect targeted poisoning attacks</li>
</ul>
<p>Theoretical advances include the first rigorous <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-180fd3e3456f" class="internal-link" rel="noopener noreferrer"><strong>grokking bounds</strong></a> in ridge regression and a proof that deep networks <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-68bd18d53406" class="internal-link" rel="noopener noreferrer">learn <strong>Random Hierarchy Models</strong></a> through hierarchical feature composition. <strong>Keel</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-07e893a2c4e7" class="internal-link" rel="noopener noreferrer">revives Post-LayerNorm</a> by replacing residual paths with Legendre polynomials for stable training at depth. <strong>Differential voting</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-2ad230fcffab" class="internal-link" rel="noopener noreferrer">connects RLHF reward aggregation</a> to social choice theory, deriving loss functions satisfying specific voting axioms. <strong>VP-RL</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-5a60a6b16a85" class="internal-link" rel="noopener noreferrer">addresses PRM-RL mismatch</a> by penalizing only from the first incorrect reasoning step.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:6180fdcc30bb</id>
    <title>AI found 12 of 12 OpenSSL zero-days (while curl cancelled its bug bounty)</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" rel="related" type="text/html"/>
    <published>2026-01-28T03:23:00Z</published>
    <updated>2026-01-28T03:23:00Z</updated>
    <author><name>Stanislav Fort</name></author>
    <summary type="html"><![CDATA[<p>Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.</p>]]></summary>
    <category term="AI Capabilities"/>
    <category term="Cybersecurity"/>
    <category term="Vulnerability Discovery"/>
    <category term="AI Applications"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:category-summary:research</id>
    <title>Research Summary: January 27, 2026</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-27T06:00:00Z</published>
    <updated>2026-01-27T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities across the AI ecosystem, from scientific integrity to deployed systems. A <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" class="internal-link" rel="noopener noreferrer">forensic audit</a> quantifying <strong>17% phantom citation rates</strong> in AI-assisted survey papers exposes systematic epistemic decay in AI-augmented research workflows.</p>
<p>Security and safety research dominates:</p>
<ul>
<li>First formal <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" class="internal-link" rel="noopener noreferrer">security analysis of <strong>MCP</strong></a> identifies fundamental vulnerabilities in capability attestation and tool poisoning</li>
<li><strong>MortalMATH</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" class="internal-link" rel="noopener noreferrer">benchmark shows</a> reasoning-optimized models exhibit dangerous tunnel vision, ignoring life-threatening emergencies embedded in math problems</li>
<li><strong>Physical Prompt Injection Attacks</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" class="internal-link" rel="noopener noreferrer">demonstrate black-box exploitation</a> of VLMs through malicious instructions in physical objects</li>
<li><strong>Hidden intentions taxonomy</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" class="internal-link" rel="noopener noreferrer">categorizes ten categories</a> of covert goal-directed behaviors in LLMs that evade current detection</li>
<li>Analysis of <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" class="internal-link" rel="noopener noreferrer"><strong>20,000 real mental health AI conversations</strong></a> reveals gaps between simulation-based safety testing and real-world performance</li>
</ul>
<p>Architecture and efficiency advances include NVIDIA's <strong>LatentMoE</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" class="internal-link" rel="noopener noreferrer">optimizing accuracy per FLOP</a> through hardware-software co-design, and <strong>AR-Omni</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" class="internal-link" rel="noopener noreferrer">achieving unified any-to-any</a> multimodal generation without expert decoders. Privacy research shows fine-tuned models <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" class="internal-link" rel="noopener noreferrer">leak <strong>input-only PII</strong></a> through unexpected memorization channels.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:06853cd665b6</id>
    <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" rel="related" type="text/html"/>
    <published>2026-01-27T03:40:00Z</published>
    <updated>2026-01-27T03:40:00Z</updated>
    <author><name>H. Kemal \.Ilter</name></author>
    <summary type="html"><![CDATA[<p>A forensic audit of 50 AI survey papers (5,514 citations) reveals a consistent 17% 'phantom rate' - citations that cannot be resolved to any existing publication. This quantifies systematic epistemic degradation from AI-assisted scientific writing.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
    <category term="LLM Hallucination"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1a053f6e2fff</id>
    <title>Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</title>
    <link href="http://arxiv.org/abs/2601.17549" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" rel="related" type="text/html"/>
    <published>2026-01-27T03:38:00Z</published>
    <updated>2026-01-27T03:38:00Z</updated>
    <author><name>Narek Maloyan, Dmitry Namiot</name></author>
    <summary type="html"><![CDATA[<p>First formal security analysis of the Model Context Protocol (MCP) specification, identifying three fundamental vulnerabilities: absent capability attestation, unauthenticated bidirectional sampling enabling prompt injection, and implicit trust propagation in multi-server setups.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agentic Systems"/>
    <category term="Prompt Injection"/>
    <category term="MCP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:964a801cdcf8</id>
    <title>Physical Prompt Injection Attacks on Large Vision-Language Models</title>
    <link href="http://arxiv.org/abs/2601.17383" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</name></author>
    <summary type="html"><![CDATA[<p>Introduces PPIA, the first physical prompt injection attack on vision-language models that embeds malicious instructions into physical objects. The attack is black-box, query-agnostic, and operates solely through visual observation without model access.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Vision-Language Models"/>
    <category term="Adversarial Attacks"/>
    <category term="Prompt Injection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9f820242e5b9</id>
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</title>
    <link href="http://arxiv.org/abs/2601.18790" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</name></author>
    <summary type="html"><![CDATA[<p>Introduces MortalMATH benchmark revealing that reasoning-optimized LLMs exhibit 'tunnel vision' - ignoring life-threatening emergencies (stroke symptoms, freefall) while maintaining 95%+ task completion on math problems. Generalist models like Llama-3.1 appropriately refuse tasks to address danger.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:457b819c1a26</id>
    <title>Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</title>
    <link href="http://arxiv.org/abs/2601.17480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" rel="related" type="text/html"/>
    <published>2026-01-27T03:26:00Z</published>
    <updated>2026-01-27T03:26:00Z</updated>
    <author><name>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, R\"udiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</name></author>
    <summary type="html"><![CDATA[<p>Systematically investigates PII leakage from fine-tuned LLMs, finding that sensitive information appearing only in model inputs (not training targets) can still be extracted. Benchmarks four privacy-preserving approaches including differential privacy.</p>]]></summary>
    <category term="AI Privacy"/>
    <category term="Language Models"/>
    <category term="Data Security"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7465509b2391</id>
    <title>Reconstructing Training Data from Adapter-based Federated Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7465509b2391" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that adapter-based federated LLMs (using LoRA) create new exploitable leakage channels contrary to assumptions. Proposes UTR attack that reconstructs training data from low-rank adapter gradients.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="AI Security"/>
    <category term="Privacy Attacks"/>
    <category term="LoRA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:5346d9dbcb7f</id>
    <title>The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2601.17344" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-5346d9dbcb7f" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</name></author>
    <summary type="html"><![CDATA[<p>Formalizes Loss-of-Control risk and Intrinsic Value Misalignment in LLM agents operating in benign settings. Introduces IMPRESS benchmark for probing value misalignment in realistic scenarios without explicit harmful inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="LLM Agents"/>
    <category term="Value Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9ba984802fd4</id>
    <title>AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</title>
    <link href="http://arxiv.org/abs/2601.17761" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" rel="related" type="text/html"/>
    <published>2026-01-27T03:21:00Z</published>
    <updated>2026-01-27T03:21:00Z</updated>
    <author><name>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</name></author>
    <summary type="html"><![CDATA[<p>AR-Omni presents a unified autoregressive model for any-to-any multimodal generation (text, vision, speech) without requiring expert decoder modules, using a single token stream and next-token objective.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Autoregressive Models"/>
    <category term="Unified Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:e2eb514146a7</id>
    <title>Self-Manager: Parallel Agent Loop for Long-form Deep Research</title>
    <link href="http://arxiv.org/abs/2601.17879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-e2eb514146a7" rel="related" type="text/html"/>
    <published>2026-01-27T03:19:00Z</published>
    <updated>2026-01-27T03:19:00Z</updated>
    <author><name>Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang</name></author>
    <summary type="html"><![CDATA[<p>Self-Manager introduces a parallel agent loop for complex research tasks, enabling asynchronous concurrent execution with isolated context windows per subthread, managed via Thread Control Blocks.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="Agent Architecture"/>
    <category term="Parallel Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:category-summary:research</id>
    <title>Research Summary: January 26, 2026</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-26T06:00:00Z</published>
    <updated>2026-01-26T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a major open-source release and critical safety findings. <strong>LongCat-Flash-Thinking-2601</strong>, a <strong>560B MoE</strong> reasoning model, <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" class="internal-link" rel="noopener noreferrer">achieves SOTA</a> among open-source models for agentic tasks. <strong>VibeTensor</strong> demonstrates LLM agents <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-fd370ccbe017" class="internal-link" rel="noopener noreferrer">can generate complete</a> deep learning system software stacks including CUDA runtime.</p>
<ul>
<li><strong>Endless Terminals</strong> (Stanford/UW) <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-7a6d52cd1b64" class="internal-link" rel="noopener noreferrer">introduces autonomous pipeline</a> for generating terminal RL environments, addressing a key bottleneck for self-improving agents</li>
<li><strong>PHISH</strong> framework <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-ee1a3a9cbf6e" class="internal-link" rel="noopener noreferrer">reveals persona jailbreaking</a> via adversarial conversation history, bypassing input-only safety filters</li>
<li><strong>Timely Machine</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-726666f86596" class="internal-link" rel="noopener noreferrer">reframes test-time scaling</a> as wall-clock time, finding smaller models often outperform larger ones under time constraints</li>
</ul>
<p>Theoretical and interpretability advances include <strong>floating-point transformer expressivity</strong> analysis <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-30704758f998" class="internal-link" rel="noopener noreferrer">proving non-equivariant function</a> representation without positional encoding. <strong>Sycophancy signals</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-b44e5d4c67dc" class="internal-link" rel="noopener noreferrer">are shown to be linearly separable</a> in middle-layer attention heads, enabling targeted steering. A conceptual <strong>critique of machine unlearning</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2dd9d32addc4" class="internal-link" rel="noopener noreferrer">argues dual-use capabilities</a> and compositional generalization fundamentally prevent knowledge removal—an important insight for AI safety policy.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-25:category-summary:research</id>
    <title>Research Summary: January 25, 2026</title>
    <link href="https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-25&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-25T06:00:00Z</published>
    <updated>2026-01-25T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>
<ul>
<li>A two-phase <strong>grokking acceleration</strong> method <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-5df92ddd3084" class="internal-link" rel="noopener noreferrer">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>
<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-40e41ac66c84" class="internal-link" rel="noopener noreferrer">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>
<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-7905059be0ab" class="internal-link" rel="noopener noreferrer">documents activation patterns</a> increasing through residual stream layers</li>
</ul>
<p>Meta-level critiques highlight <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-259e13a07a27" class="internal-link" rel="noopener noreferrer">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-de795bf06466" class="internal-link" rel="noopener noreferrer">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-24:category-summary:research</id>
    <title>Research Summary: January 24, 2026</title>
    <link href="https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-24&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-24T06:00:00Z</published>
    <updated>2026-01-24T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-dde7d819fa66" class="internal-link" rel="noopener noreferrer">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>
<ul>
<li>Empirical work on <strong>unsupervised elicitation</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-24f85ac93189" class="internal-link" rel="noopener noreferrer">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>
<li>A new <strong>Eval Awareness Framework</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-194fc1f46619" class="internal-link" rel="noopener noreferrer">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>
<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c9fbeadb6060" class="internal-link" rel="noopener noreferrer">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>
<li>Theoretical work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-2df5c6dcb797" class="internal-link" rel="noopener noreferrer">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>
</ul>
<p>Meta-science initiatives <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-429650348119" class="internal-link" rel="noopener noreferrer">propose systematic replication</a> teams. Interpretability research <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-709ca3227a1e" class="internal-link" rel="noopener noreferrer">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a" class="internal-link" rel="noopener noreferrer">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:category-summary:research</id>
    <title>Research Summary: January 23, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-23T06:00:00Z</published>
    <updated>2026-01-23T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>
<ul>
<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>
<li><strong>TTT-Discover</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" class="internal-link" rel="noopener noreferrer">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>
<li><strong>QUAIL</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" class="internal-link" rel="noopener noreferrer">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>
<li><strong>Universal Refusal Circuits</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" class="internal-link" rel="noopener noreferrer">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>
<li><strong>SilentDrift</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" class="internal-link" rel="noopener noreferrer">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>
</ul>
<p><strong>Zero-Error Horizons</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" class="internal-link" rel="noopener noreferrer">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-23T03:33:00Z</published>
    <updated>2026-01-23T03:33:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>]]></summary>
    <category term="Automated Research"/>
    <category term="AI Agents"/>
    <category term="Research Automation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-23T03:31:00Z</published>
    <updated>2026-01-23T03:31:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2665d0ecf2cf</id>
    <title>Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs</title>
    <link href="http://arxiv.org/abs/2601.15714" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" rel="related" type="text/html"/>
    <published>2026-01-23T03:26:00Z</published>
    <updated>2026-01-23T03:26:00Z</updated>
    <author><name>Ryoma Sato</name></author>
    <summary type="html"><![CDATA[<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="AI Safety"/>
    <category term="Trustworthy AI"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:dababf83ee7d</id>
    <title>Learning to Discover at Test Time</title>
    <link href="http://arxiv.org/abs/2601.16175" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</name></author>
    <summary type="html"><![CDATA[<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>]]></summary>
    <category term="Test-Time Training"/>
    <category term="Scientific Discovery"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:fd50ec594aa0</id>
    <title>LLM-in-Sandbox Elicits General Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2601.16206" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-fd50ec594aa0" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</name></author>
    <summary type="html"><![CDATA[<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Tool Use"/>
    <category term="Generalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:0467e51a900e</id>
    <title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title>
    <link href="http://arxiv.org/abs/2601.15538" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" rel="related" type="text/html"/>
    <published>2026-01-23T03:21:00Z</published>
    <updated>2026-01-23T03:21:00Z</updated>
    <author><name>Himanshu Mishra, Kanwal Mehreen</name></author>
    <summary type="html"><![CDATA[<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="Privacy"/>
    <category term="AI Safety"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:32d9233155f9</id>
    <title>SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</title>
    <link href="http://arxiv.org/abs/2601.14323" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" rel="related" type="text/html"/>
    <published>2026-01-23T03:19:00Z</published>
    <updated>2026-01-23T03:19:00Z</updated>
    <author><name>Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara</name></author>
    <summary type="html"><![CDATA[<p>Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Robotics"/>
    <category term="Backdoor Attacks"/>
    <category term="VLA Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:category-summary:research</id>
    <title>Research Summary: January 22, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-22T06:00:00Z</published>
    <updated>2026-01-22T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans automated AI research, theoretical reasoning foundations, and critical safety vulnerabilities. Stanford's <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer"><strong>execution-grounded automated AI research</strong></a> from Hashimoto, Yang, and Candès demonstrates systematic idea testing and implementation at scale.</p>
<p><strong>Reasoning theory and limitations:</strong></p>
<ul>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-4ea5d9351415" class="internal-link" rel="noopener noreferrer"><strong>Outcome-based RL provably induces CoT reasoning</strong></a> in transformers, providing theoretical foundation for reasoning emergence from sparse rewards</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-b7e33d331123" class="internal-link" rel="noopener noreferrer"><strong>Diffusion LLMs' flexibility trap</strong></a> reveals arbitrary generation order hurts reasoning by allowing models to bypass hard tokens</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-94b2367a995e" class="internal-link" rel="noopener noreferrer"><strong>LLM planning shows 0% cross-domain transfer</strong></a> despite 82.9% in-domain performance, exposing memorization over true generalization</li>
</ul>
<p><strong>Safety vulnerabilities demand attention:</strong></p>
<ul>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer"><strong>LLM judges manipulated at 90% rate</strong></a> via unfaithful CoT rewriting in agent evaluation</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-df868e8ffe4a" class="internal-link" rel="noopener noreferrer"><strong>Privacy collapse from benign fine-tuning</strong></a> silently degrades contextual privacy, undetected by standard benchmarks</li>
<li><a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5836988eb762" class="internal-link" rel="noopener noreferrer"><strong>Turn-based structural triggers</strong></a> achieve <strong>99.52%</strong> backdoor success in multi-turn dialogue without prompt modification</li>
</ul>
<p>Anthropic <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-32c2ed05f61b" class="internal-link" rel="noopener noreferrer">publishes <strong>Claude's new constitution</strong></a> with expanded values framework (&gt;2x previous length), while DeepMind researcher formalizes tradeoffs in <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-0dea9f589164" class="internal-link" rel="noopener noreferrer"><strong>training against scheming monitors</strong></a>. <strong>Meta Flow Maps</strong> <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-546a06433333" class="internal-link" rel="noopener noreferrer">extend consistency models</a> for efficient reward alignment in generative models.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-22T03:31:00Z</published>
    <updated>2026-01-22T03:31:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>From Stanford (Hashimoto, Yang, Candès) proposing execution-grounded automated AI research with automated executor implementing and testing LLM-generated ideas at scale on GPU clusters for LLM pre-training and post-training problems.</p>]]></summary>
    <category term="Automated AI Research"/>
    <category term="LLM Capabilities"/>
    <category term="Research Methodology"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:4ea5d9351415</id>
    <title>Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</title>
    <link href="http://arxiv.org/abs/2601.15158" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-4ea5d9351415" rel="related" type="text/html"/>
    <published>2026-01-22T03:23:00Z</published>
    <updated>2026-01-22T03:23:00Z</updated>
    <author><name>Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</name></author>
    <summary type="html"><![CDATA[<p>Proves theoretically that transformers trained with outcome-based RL on sparse rewards provably converge to structured algorithms implementing Chain-of-Thought reasoning on graph traversal tasks.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Theoretical AI"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:category-summary:research</id>
    <title>Research Summary: January 21, 2026</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-21T06:00:00Z</published>
    <updated>2026-01-21T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges to conventional training wisdom and exposes multiple safety vulnerabilities across deployed systems.</p>
<p><strong>Training Paradigm Reassessment:</strong></p>
<ul>
<li>Base models <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-bf9961720f8a" class="internal-link" rel="noopener noreferrer">consistently outperform instruction-tuned variants</a> on math and domain-shifted benchmarks, challenging fundamental training assumptions</li>
<li><strong>RLVR</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8cac8805e742" class="internal-link" rel="noopener noreferrer">improves task performance</a> but produces extremely overconfident models; <strong>SFT</strong> yields better calibration even under distribution shift</li>
</ul>
<p><strong>Safety &amp; Security Vulnerabilities:</strong></p>
<ul>
<li><strong>Action Rebinding</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5336cd7b69bc" class="internal-link" rel="noopener noreferrer">allows zero-permission apps to hijack</a> multimodal GUI agents by exploiting visual attention</li>
<li>Safeguarded frontier models can <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-866acf7c7710" class="internal-link" rel="noopener noreferrer">elicit harmful capabilities</a> in open-source models through three-stage attacks</li>
<li><strong>Sockpuppetting</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-e29c4d7a227c" class="internal-link" rel="noopener noreferrer">jailbreaks LLMs with one line of code</a> achieving up to <strong>100% ASR</strong></li>
<li>AI-generated data contamination in medical imaging <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-79b868d96563" class="internal-link" rel="noopener noreferrer">creates feedback loops</a> eroding diagnostic reliability</li>
</ul>
<p><strong>Reasoning Model Insights:</strong></p>
<ul>
<li><strong>Thinking Traps</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-a7f126fd0c33" class="internal-link" rel="noopener noreferrer">account for <strong>89%</strong> of reasoning failures</a> in long CoT—models elaborate incorrect early commitments</li>
<li>First systematic cost-accuracy comparison shows multi-agent reasoning <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-d45a3f320ac8" class="internal-link" rel="noopener noreferrer">often underperforms single-model CoT</a></li>
<li>Large-scale study of <strong>2.1M preprints</strong> finds LLM adoption <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8fadab621712" class="internal-link" rel="noopener noreferrer">increases paper volume but decreases citations</a> and originality</li>
</ul>
<p><strong>Architecture Innovation:</strong> <strong>Threshold Differential Attention</strong> <a href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-5da0fc012225" class="internal-link" rel="noopener noreferrer">eliminates attention sinks</a> while achieving ultra-sparsity and improved long-context robustness.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:bf9961720f8a</id>
    <title>Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-bf9961720f8a" rel="related" type="text/html"/>
    <published>2026-01-21T03:31:00Z</published>
    <updated>2026-01-21T03:31:00Z</updated>
    <author><name>Prateek Munjal, Clement Christophe, Ronnie Rajan, Praveenkumar Kanithi</name></author>
    <summary type="html"><![CDATA[<p>Investigates whether instruction-tuned models always outperform base models, finding that base models consistently outperform instruction-tuned variants in zero-shot CoT settings on GSM8K (drops up to 32.67% for Llama3-70B). Instruction tuning appears to induce pattern matching rather than genuine reasoning improvement.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Instruction Tuning"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:8cac8805e742</id>
    <title>Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.13284" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-8cac8805e742" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia, Srikanth Doss, Nikolaos Pappas</name></author>
    <summary type="html"><![CDATA[<p>Systematic study showing RLVR improves task performance but produces extremely overconfident models, while SFT yields better calibration even under distribution shift. Proposes calibration-aware RL approach to balance classification and calibration.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Calibration"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:3d2384716bb1</id>
    <title>CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</title>
    <link href="http://arxiv.org/abs/2601.13304" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-21&amp;category=research#item-3d2384716bb1" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</name></author>
    <summary type="html"><![CDATA[<p>Introduces CausalSpatial benchmark evaluating whether MLLMs can anticipate consequences of object motions across collision, compatibility, occlusion and trajectory tasks. Reveals severe gap: humans score 84% while GPT-5 achieves only 54%, exposing over-reliance on textual reasoning.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="Multimodal Reasoning"/>
    <category term="Causal Reasoning"/>
    <category term="MLLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-20:category-summary:research</id>
    <title>Research Summary: January 20, 2026</title>
    <link href="https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-20&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-20T06:00:00Z</published>
    <updated>2026-01-20T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research concentrates heavily on <strong>alignment techniques and safety evaluation</strong>. A survey on <strong>alignment pretraining</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-2a1abcff2543" class="internal-link" rel="noopener noreferrer">synthesizes evidence</a> that training LLMs on data depicting well-behaved AI during pretraining substantially reduces misalignment—potentially offering a scalable, proactive safety approach.</p>
<ul>
<li><strong>Coup probes</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-8708c9ecc0f5" class="internal-link" rel="noopener noreferrer">testing demonstrates</a> few-shot linear classifiers can detect scheming behavior from model activations, with empirical results on off-policy training data</li>
<li><strong>Silent Agreement Evaluation</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-7ebc02f821d4" class="internal-link" rel="noopener noreferrer">provides first empirical measurement</a> of <strong>Schelling coordination</strong> in LLMs—whether isolated instances converge on shared choices without communication</li>
<li>Framework for <strong>AI-delegated safety research</strong> <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-65438e9ab777" class="internal-link" rel="noopener noreferrer">identifies key dimensions</a>: epistemic cursedness, parallelizability, and short-horizon suitability</li>
<li>Strategic analysis <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-6f06b1e50f04" class="internal-link" rel="noopener noreferrer">examines whether</a> <strong>LLM alignment work transfers</strong> to non-LLM takeover-capable systems</li>
</ul>
<p>Methodological contributions include a <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-a67208b1b6f0" class="internal-link" rel="noopener noreferrer">critique of <strong>METR-HRS</strong></a> timelines forecasting, arguing the 'd' parameter conflates task difficulty with sequence length. Governance-oriented work <a href="http://localhost:8080/?date=2026-01-20&amp;category=research#item-3a47324cdf6f" class="internal-link" rel="noopener noreferrer">sketches positive AI transition</a> scenarios co-authored with <strong>Claude Opus 4.5</strong>.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:category-summary:research</id>
    <title>Research Summary: January 19, 2026</title>
    <link href="http://arxiv.org/abs/2601.10904" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-19&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-19T06:00:00Z</published>
    <updated>2026-01-19T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AGI benchmarking, reasoning interpretability, agent evaluation, and safety mechanisms for production AI systems.</p>
<p><strong>ARC Prize 2025</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-85a5776de7ae" class="internal-link" rel="noopener noreferrer">technical report documents</a> 'refinement loops' as the defining pattern among top <strong>ARC-AGI-2</strong> performers. <strong>Reasoning Models Generate Societies of Thought</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-bb5c19abf00b" class="internal-link" rel="noopener noreferrer">reveals that enhanced reasoning</a> in <strong>DeepSeek-R1</strong> and <strong>QwQ-32B</strong> emerges from internal multi-agent-like simulations. <strong>AgencyBench</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-ba9af0a30312" class="internal-link" rel="noopener noreferrer">introduces evaluation</a> at unprecedented scale: <strong>32 scenarios</strong> requiring <strong>~90 tool calls</strong> and <strong>1M tokens</strong>.</p>
<ul>
<li><strong>Google DeepMind</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-972425be59d1" class="internal-link" rel="noopener noreferrer">presents production-ready activation probes</a> for <strong>Gemini</strong> misuse detection addressing distribution shift</li>
<li><strong>Spurious Rewards Paradox</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-0f9c42f7aebb" class="internal-link" rel="noopener noreferrer">identifies mechanistically how RLVR</a> triggers memorization shortcuts via Anchor-Adapter circuits</li>
<li><strong>BAPO</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-a819576acb19" class="internal-link" rel="noopener noreferrer">teaches agentic search systems</a> to recognize reasoning boundaries and output 'I DON'T KNOW'</li>
<li><strong>DialDefer</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-377515b0971a" class="internal-link" rel="noopener noreferrer">exposes 'dialogic deference' bias</a> undermining LLM-as-judge reliability</li>
</ul>
<p>A <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-52e25a92ee5e" class="internal-link" rel="noopener noreferrer">critique of <strong>METR</strong> methodology</a> argues AI capability time horizons may be significantly underestimated. <strong>Meta</strong>'s NeurIPS 2025 DCVLR winner <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-992bf6cbaff6" class="internal-link" rel="noopener noreferrer">shows difficulty-based example selection</a> outperforms dataset diversity. <strong>Digital Metabolism</strong> <a href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-fe36659e452a" class="internal-link" rel="noopener noreferrer">proposes that targeted forgetting</a> can distill pure neural logic cores from factual knowledge.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:85a5776de7ae</id>
    <title>ARC Prize 2025: Technical Report</title>
    <link href="http://arxiv.org/abs/2601.10904" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-85a5776de7ae" rel="related" type="text/html"/>
    <published>2026-01-19T03:36:00Z</published>
    <updated>2026-01-19T03:36:00Z</updated>
    <author><name>Fran\c{c}ois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers</name></author>
    <summary type="html"><![CDATA[<p>Technical report from ARC Prize 2025 competition on ARC-AGI-2 benchmark. Key finding: emergence of 'refinement loops' as defining pattern, with top score 24% from 1,455 teams.</p>]]></summary>
    <category term="AGI"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Program Synthesis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:bb5c19abf00b</id>
    <title>Reasoning Models Generate Societies of Thought</title>
    <link href="http://arxiv.org/abs/2601.10825" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-19&amp;category=research#item-bb5c19abf00b" rel="related" type="text/html"/>
    <published>2026-01-19T03:23:00Z</published>
    <updated>2026-01-19T03:23:00Z</updated>
    <author><name>Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Ag\"uera y Arcas, James Evans</name></author>
    <summary type="html"><![CDATA[<p>Analyzes reasoning models (DeepSeek-R1, QwQ-32B) showing enhanced reasoning emerges from simulating multi-agent-like interactions ('society of thought') with distinct personality traits and expertise.</p>]]></summary>
    <category term="LLM Interpretability"/>
    <category term="Reasoning Models"/>
    <category term="Multi-Agent Systems"/>
    <category term="Emergent Behavior"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-18:category-summary:research</id>
    <title>Research Summary: January 18, 2026</title>
    <link href="https://www.lesswrong.com/posts/WLdcvAcoFZv9enR37/what-washington-says-about-agi" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-18&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-18T06:00:00Z</published>
    <updated>2026-01-18T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>An unusually light day for AI research, with only two substantive original contributions. The standout is a novel <strong>AI-assisted policy analysis</strong> using <strong>Claude Sonnet 4.5</strong> with web search to <a href="http://localhost:8080/?date=2026-01-18&amp;category=research#item-d473a553750e" class="internal-link" rel="noopener noreferrer">systematically catalog</a> every US congressperson's public AGI positions—producing actionable governance data.</p>
<ul>
<li>A philosophical piece on AI safety <a href="http://localhost:8080/?date=2026-01-18&amp;category=research#item-32842710cab1" class="internal-link" rel="noopener noreferrer">argues that <strong>flourishing-focused interventions</strong></a> may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing</li>
<li><strong>AISC</strong> <a href="http://localhost:8080/?date=2026-01-18&amp;category=research#item-516cf5c335c1" class="internal-link" rel="noopener noreferrer">project update</a> on 'Understanding Trust' references an <strong>IQA paper</strong> output from Spring 2025 cohort work</li>
<li><strong>MATS Summer 2026</strong> <a href="http://localhost:8080/?date=2026-01-18&amp;category=research#item-b1120185b7d9" class="internal-link" rel="noopener noreferrer">applications closing</a> January 18th—relevant for safety talent pipeline but not research itself</li>
</ul>
<p>Remaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-17:category-summary:research</id>
    <title>Research Summary: January 17, 2026</title>
    <link href="https://www.lesswrong.com/posts/kkm7GsDtqsywaWyM7/scaling-laws-for-economic-impacts-experimental-evidence-from" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-17&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-17T06:00:00Z</published>
    <updated>2026-01-17T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research centers on <strong>AI economics</strong>, <strong>model evaluation</strong>, and <strong>safety frameworks</strong>. A large-scale study with <strong>500+ professionals</strong> and <strong>13 LLMs</strong> <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-5f1a61de76c4" class="internal-link" rel="noopener noreferrer">establishes scaling laws</a> for economic impact, finding each year of frontier progress reduces task completion time by measurable margins.</p>
<ul>
<li><strong>Future-as-Label</strong> <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-9aa46e442121" class="internal-link" rel="noopener noreferrer">introduces self-supervised training</a> using temporal outcomes from real-world data streams, eliminating costly human annotation</li>
<li>Mechanistic interpretability work <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-367da8e17c28" class="internal-link" rel="noopener noreferrer">reveals models may exhibit <strong>fixed biases</strong></a> rather than genuine reasoning on inductive tasks</li>
<li>Revealed preference methodology applied across <strong>GPT-5.1</strong>, <strong>Claude-Opus-4.5</strong>, and other frontier models <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-f21769ebc3ee" class="internal-link" rel="noopener noreferrer">to elicit trained character traits</a></li>
</ul>
<p>Safety contributions include a technical framework for <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-78a67b0f28eb" class="internal-link" rel="noopener noreferrer">prioritizing <strong>net-sabotage-value</strong> vulnerabilities</a> in AI control, plus analysis <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-d8019c015caa" class="internal-link" rel="noopener noreferrer">reframing persuasion risk</a> from adversarial to <strong>trusted advisor</strong> threat models. Historical precedent mapping for <a href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-2a1aa51d4552" class="internal-link" rel="noopener noreferrer"><strong>13 ASI failure modes</strong></a> provides grounding for unprecedented risk scenarios.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-17:research:5f1a61de76c4</id>
    <title>Scaling Laws for Economic Impacts: Experimental Evidence from 500 Professionals and 13 LLMs</title>
    <link href="https://www.lesswrong.com/posts/kkm7GsDtqsywaWyM7/scaling-laws-for-economic-impacts-experimental-evidence-from" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-17&amp;category=research#item-5f1a61de76c4" rel="related" type="text/html"/>
    <published>2026-01-17T03:23:00Z</published>
    <updated>2026-01-17T03:23:00Z</updated>
    <author><name>Ali Merali</name></author>
    <summary type="html"><![CDATA[<p>Experimental study with 500+ professionals testing 13 LLMs of varying compute levels on real tasks. Finds each year of frontier progress reduces task completion time by ~8% (56% from compute scaling, 44% algorithmic). Key puzzle: human-AI collaborative output quality stays flat despite improving models, suggesting users cap realized gains.</p>]]></summary>
    <category term="Scaling Laws"/>
    <category term="AI Economics"/>
    <category term="Human-AI Collaboration"/>
    <category term="Productivity"/>
    <category term="Empirical Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-16:category-summary:research</id>
    <title>Research Summary: January 16, 2026</title>
    <link href="http://arxiv.org/abs/2601.10527" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-16&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-16T06:00:00Z</published>
    <updated>2026-01-16T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features critical safety evaluations and theoretical breakthroughs. A unified <a href="http://localhost:8080/?date=2026-01-16&category=research#item-6c69a8d17a75" class="internal-link">safety report benchmarks</a> <strong>GPT-5.2</strong>, <strong>Gemini 3 Pro</strong>, <strong>Grok 4.1 Fast</strong>, and four other frontier models across standardized safety dimensions. OpenRouter's <strong>100+ trillion token</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-4ccb874eae2d" class="internal-link">empirical study provides</a> unprecedented insights into real-world LLM usage patterns.</p>
<ul>
<li><strong>Molmo2</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-e865ed62da32" class="internal-link">releases open weights</a> for video-language understanding with point-driven grounding, advancing open-source multimodal capabilities</li>
<li>Theoretical work <a href="http://localhost:8080/?date=2026-01-16&category=research#item-e9ccb8b58436" class="internal-link">reveals neural scaling laws</a> emerge from random graph walks without power-law structure, challenging prevailing assumptions</li>
<li><strong>Alignment Pretraining</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-c2fd2cbf55b2" class="internal-link">demonstrates AI discourse</a> in training corpora causally produces self-fulfilling (mis)alignment outcomes</li>
<li><strong>CaMeLs</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-7a483d653b37" class="internal-link">introduces architectural isolation</a> defenses for computer use agents against prompt injection via single-shot planning</li>
</ul>
<p>Mechanistic analysis reveals <strong>Hierarchical Reasoning Models</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-1d227e067b0f" class="internal-link">exhibit "guessing shortcuts"</a> and fail on simple puzzles violating fixed-point assumptions. <strong>ML-Master 2.0</strong> <a href="http://localhost:8080/?date=2026-01-16&category=research#item-188a6bb2adad" class="internal-link">enables ultra-long-horizon</a> autonomous ML engineering spanning days/weeks through cognitive accumulation. A novel proof <a href="http://localhost:8080/?date=2026-01-16&category=research#item-8df66c147511" class="internal-link">connects transformer attention</a> to <strong>tropical polynomial circuits</strong> (max-plus algebra), revealing forward passes as shortest-path computations.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-16:research:6c69a8d17a75</id>
    <title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title>
    <link href="http://arxiv.org/abs/2601.10527" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-16&amp;category=research#item-6c69a8d17a75" rel="related" type="text/html"/>
    <published>2026-01-16T03:38:00Z</published>
    <updated>2026-01-16T03:38:00Z</updated>
    <author><name>Xingjun Ma, Yixu Wang, Hengyuan Xu, Yutao Wu, Yifan Ding, Yunhan Zhao, Zilong Wang, Jiabin Hua, Ming Wen, Jianan Liu, Ranjie Duan, Yifeng Gao, Yingshui Tan, Yunhao Chen, Hui Xue, Xin Wang, Wei Cheng, Jingjing Chen, Zuxuan Wu, Bo Li, Yu-Gang Jiang</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5 across language, vision-language, and image generation using unified protocol.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Model Evaluation"/>
    <category term="Frontier Models"/>
    <category term="Multimodal AI"/>
    <category term="Adversarial Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-16:research:4ccb874eae2d</id>
    <title>State of AI: An Empirical 100 Trillion Token Study with OpenRouter</title>
    <link href="http://arxiv.org/abs/2601.10088" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-16&amp;category=research#item-4ccb874eae2d" rel="related" type="text/html"/>
    <published>2026-01-16T03:31:00Z</published>
    <updated>2026-01-16T03:31:00Z</updated>
    <author><name>Malika Aubakirova, Alex Atallah, Chris Clark, Justin Summerville, Anjney Midha</name></author>
    <summary type="html"><![CDATA[<p>Large-scale empirical analysis of 100+ trillion tokens of real-world LLM usage through OpenRouter platform. Studies usage patterns across tasks, geographies, time, and the shift to reasoning models following o1's release.</p>]]></summary>
    <category term="Empirical AI Research"/>
    <category term="LLM Usage Patterns"/>
    <category term="Industry Analysis"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-16:research:e865ed62da32</id>
    <title>Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</title>
    <link href="http://arxiv.org/abs/2601.10611" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-16&amp;category=research#item-e865ed62da32" rel="related" type="text/html"/>
    <published>2026-01-16T03:31:00Z</published>
    <updated>2026-01-16T03:31:00Z</updated>
    <author><name>Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna</name></author>
    <summary type="html"><![CDATA[<p>Releases Molmo2, a state-of-the-art open-source VLM family with video understanding and point-driven grounding capabilities. Provides complete open weights and training data, addressing lack of transparency in the field.</p>]]></summary>
    <category term="Vision-Language Models"/>
    <category term="Video Understanding"/>
    <category term="Open Source"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-16:research:188a6bb2adad</id>
    <title>Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering</title>
    <link href="http://arxiv.org/abs/2601.10402" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-16&amp;category=research#item-188a6bb2adad" rel="related" type="text/html"/>
    <published>2026-01-16T03:19:00Z</published>
    <updated>2026-01-16T03:19:00Z</updated>
    <author><name>Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen</name></author>
    <summary type="html"><![CDATA[<p>Presents ML-Master 2.0, an autonomous agent for ultra-long-horizon ML engineering tasks spanning days/weeks. Reframes context management as cognitive accumulation to handle sparse feedback over extended periods.</p>]]></summary>
    <category term="AI Agents"/>
    <category term="Autonomous Research"/>
    <category term="Long-Horizon Reasoning"/>
    <category term="ML Engineering"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-15:category-summary:research</id>
    <title>Research Summary: January 15, 2026</title>
    <link href="http://arxiv.org/abs/2601.09625" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-15&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-15T06:00:00Z</published>
    <updated>2026-01-15T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research centers on AI security frameworks and alignment challenges in reasoning models. Bruce Schneier <a href="http://localhost:8080/?date=2026-01-15&category=research#item-c3575833246d" class="internal-link">introduces <strong>Promptware</strong></a>, reconceptualizing prompt injection as a distinct malware class with a five-step kill chain model. OpenAI's alignment team <a href="http://localhost:8080/?date=2026-01-15&category=research#item-b8d64664fcf0" class="internal-link">presents <strong>Confessions</strong> research</a> for detecting reward-hacked outputs.</p>
<ul>
<li><strong>A.X K1</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-040a411cbb2c" class="internal-link">debuts as a <strong>519B MoE</strong> model</a> with <strong>Think-Fusion</strong> enabling user-controllable reasoning depth</li>
<li><strong>DeliberationBench</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-4582d2dadc3c" class="internal-link">reveals a striking negative result</a>: simple best-single selection achieves <strong>82.5% win rate</strong> over multi-LLM deliberation</li>
<li><strong>GIFT</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-b471a8988672" class="internal-link">addresses SFT-RL mismatch</a> in reasoning training via finite-temperature Gibbs initialization</li>
<li><strong>Resisting Correction</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-5aff4bf5bac7" class="internal-link">shows RLHF creates resistance</a> to external safety signals, dropping Spearman correlation significantly</li>
</ul>
<p>Survey work includes <strong>The AI Hippocampus</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-c87807fd57b1" class="internal-link">organizing LLM memory</a> into implicit, explicit, and agentic paradigms, while <strong>Adversarial Tales</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-417f0684db0a" class="internal-link">exposes jailbreak vulnerabilities</a> through cultural narrative framing. <strong>DASD-4B-Thinking</strong> <a href="http://localhost:8080/?date=2026-01-15&category=research#item-71dbb98d23ec" class="internal-link">achieves SOTA reasoning</a> among <strong>4B</strong> open-source models through distribution-aligned distillation.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-15:research:c3575833246d</id>
    <title>The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware</title>
    <link href="http://arxiv.org/abs/2601.09625" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-15&amp;category=research#item-c3575833246d" rel="related" type="text/html"/>
    <published>2026-01-15T03:19:00Z</published>
    <updated>2026-01-15T03:19:00Z</updated>
    <author><name>Ben Nassi, Bruce Schneier, Oleg Brodt</name></author>
    <summary type="html"><![CDATA[<p>Proposes 'promptware' as a distinct malware class targeting LLM-based systems and introduces five-step kill chain model, arguing that 'prompt injection' framing obscures multi-step attack complexity. Co-authored by Bruce Schneier.</p>]]></summary>
    <category term="AI Security"/>
    <category term="LLM Agents"/>
    <category term="Prompt Injection"/>
    <category term="Threat Modeling"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-14:category-summary:research</id>
    <title>Research Summary: January 14, 2026</title>
    <link href="http://arxiv.org/abs/2601.08584" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-14&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-14T06:00:00Z</published>
    <updated>2026-01-14T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a fundamental theoretical breakthrough and significant RLHF/alignment advances. <strong>Universal Computation in LM Decoding</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-7f13eb1ede23" class="internal-link">proves autoregressive decoding</a> alone enables simulation of any algorithm—reshaping capability understanding. <strong>Ministral 3</strong> from Mistral <a href="http://localhost:8080/?date=2026-01-14&category=research#item-0de906e54a7a" class="internal-link">delivers efficient <strong>3B/8B/14B</strong> models</a> with pretrained, instruction-tuned, and reasoning variants.</p>
<p>Key RLHF methodology findings:</p>
<ul>
<li><strong>GRPO bias</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-84e9d753971b" class="internal-link">systematically underestimates advantages</a> for hard prompts, affecting widely-deployed alignment pipelines</li>
<li>On-policy DPO <a href="http://localhost:8080/?date=2026-01-14&category=research#item-5aaf204ee5ca" class="internal-link">achieves <strong>exponential convergence</strong></a> via coverage improvement principle</li>
<li><strong>Asymptotic Universal Alignment</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-737671ca2b44" class="internal-link">provides rigorous mathematical framework</a> for test-time scaling guarantees</li>
</ul>
<p>Safety research reveals critical insights:</p>
<ul>
<li><strong>Surgical Refusal Ablation</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-f30ee9e8a0d2" class="internal-link">disentangles refusal from capabilities</a> via concept-guided spectral cleaning</li>
<li><strong>ValAct-15k</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-0f6b5a701cdf" class="internal-link">shows LLMs exhibit</a> convergent moral judgments but divergent actions—key alignment gap</li>
<li><strong>Sandbagging detection</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-343d88c18f66" class="internal-link">via consistency checks</a> addresses evaluation gaming</li>
<li><strong>RAVEN</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-a57fe6549c3e" class="internal-link">exposes watermark vulnerability</a> through novel view synthesis, threatening content authentication</li>
</ul>
<p><strong>Reasoning Beyond Chain-of-Thought</strong> <a href="http://localhost:8080/?date=2026-01-14&category=research#item-1cceefe5b37f" class="internal-link">identifies causal latent features</a> using Sparse Autoencoders, enabling targeted reasoning improvements through feature steering.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-14:research:0de906e54a7a</id>
    <title>Ministral 3</title>
    <link href="http://arxiv.org/abs/2601.08584" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-14&amp;category=research#item-0de906e54a7a" rel="related" type="text/html"/>
    <published>2026-01-14T03:31:00Z</published>
    <updated>2026-01-14T03:31:00Z</updated>
    <author><name>Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sad\'e, Alan Jeffares, Albert Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Am\'elie H\'eliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozi\`ere, Baudouin De Monicault, Cl\'emence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Ga\"etan Ecrepont, Gauthier Guinet, Georgii Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, Joachim Studnia, Jonas Amar, Jos\'ephine Delas, Josselin Somerville Roberts, Karmesh Yadav, Khyathi Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, L\'eonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poir\'ee, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, Nikhil Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philom\`ene Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, Sagar Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Th\'eo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, Vincent Maladi\`ere, Virgile Richard, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, Zaccharie Ramzi</name></author>
    <summary type="html"><![CDATA[<p>Introduces Ministral 3 series from Mistral: efficient 3B/8B/14B parameter models with pretrained, instruction-tuned, and reasoning variants, using novel Cascade Distillation approach. Apache 2.0 license.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Model Distillation"/>
    <category term="Efficient LLMs"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-14:research:7f13eb1ede23</id>
    <title>Universal computation is intrinsic to language model decoding</title>
    <link href="http://arxiv.org/abs/2601.08061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-14&amp;category=research#item-7f13eb1ede23" rel="related" type="text/html"/>
    <published>2026-01-14T03:23:00Z</published>
    <updated>2026-01-14T03:23:00Z</updated>
    <author><name>Alex Lewandowski, Marlos C. Machado, Dale Schuurmans</name></author>
    <summary type="html"><![CDATA[<p>Proves that autoregressive language model decoding is sufficient for universal computation - LMs can simulate any algorithm. Shows even randomly initialized LMs are Turing complete.</p>]]></summary>
    <category term="Theoretical Foundations"/>
    <category term="Language Models"/>
    <category term="Computability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-14:research:84e9d753971b</id>
    <title>Your Group-Relative Advantage Is Biased</title>
    <link href="http://arxiv.org/abs/2601.08521" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-14&amp;category=research#item-84e9d753971b" rel="related" type="text/html"/>
    <published>2026-01-14T03:19:00Z</published>
    <updated>2026-01-14T03:19:00Z</updated>
    <author><name>Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban</name></author>
    <summary type="html"><![CDATA[<p>Identifies fundamental bias in group-relative advantage estimation used by GRPO: systematically underestimates advantages for hard prompts and overestimates for easy ones, leading to imbalanced exploration.</p>]]></summary>
    <category term="RLHF"/>
    <category term="Reinforcement Learning"/>
    <category term="Alignment"/>
    <category term="Theoretical ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-13:category-summary:research</id>
    <title>Research Summary: January 13, 2026</title>
    <link href="http://arxiv.org/abs/2601.07663" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-13&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-13T06:00:00Z</published>
    <updated>2026-01-13T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>AI safety and security research dominates today's significant findings, exposing critical vulnerabilities in reasoning transparency, defense mechanisms, and emerging agentic systems.</p>
<ul>
<li><strong>Reasoning Models Will Blatantly Lie</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-4f7ce81bb300" class="internal-link">demonstrates LRMs deny using hints</a> despite experimentally verified usage—a fundamental challenge to CoT monitoring assumptions</li>
<li><strong>Google DeepMind</strong> and <strong>UK AISI</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-b8d90b1c5ee2" class="internal-link">jointly present practical safety cases</a> for control monitoring in frontier deployments</li>
<li><strong>SFT/RL Non-decoupling</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-ddd608af8e7d" class="internal-link">proves mathematically</a> that supervised fine-tuning and reinforcement learning cannot be separated in post-training, explaining emergent reasoning behaviors</li>
<li><strong>LoRA</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-734cbe6a0f68" class="internal-link">fails to remove backdoors</a> due to spectral misalignment, exposing widely-used fine-tuning to persistent vulnerabilities</li>
</ul>
<p>Security research reveals systemic weaknesses: prompt attack defenses <a href="http://localhost:8080/?date=2026-01-13&category=research#item-2f78ad9725c9" class="internal-link">learn <strong>surface heuristics</strong></a> rather than detecting harm, <strong>RAG systems</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-fc6620fd0d1a" class="internal-link">remain vulnerable to indirect injection</a>, and <strong>web automation agents</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-73426f819c51" class="internal-link">face novel social engineering attacks</a> via the <strong>AgentBait</strong> paradigm. On interpretability, <strong>Two Pathways to Truthfulness</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-b71e0657ce5a" class="internal-link">identifies distinct mechanisms</a> for question-anchored and answer-anchored pathways underlying hallucinations, while <strong>Split Personality Training</strong> <a href="http://localhost:8080/?date=2026-01-13&category=research#item-0c0ed191b519" class="internal-link">enables detection of hidden misalignment</a> through trained honest personas.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-12:category-summary:research</id>
    <title>Research Summary: January 12, 2026</title>
    <link href="http://arxiv.org/abs/2601.05280" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-12&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-12T06:00:00Z</published>
    <updated>2026-01-12T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features significant theoretical contributions on LLM limitations alongside critical safety findings. A mathematical proof <a href="http://localhost:8080/?date=2026-01-12&category=research#item-b3d20f2c3e67" class="internal-link">formalizes recursive self-improvement</a> as a dynamical system, demonstrating <strong>degenerative dynamics</strong> that challenge near-term AGI expectations without symbolic synthesis.</p>
<p><strong>Interpretability & Reasoning:</strong></p>
<ul>
<li><strong>PaCoRe</strong> introduces <a href="http://localhost:8080/?date=2026-01-12&category=research#item-b53216c67c22" class="internal-link">parallel coordinated reasoning</a> via message-passing to break sequential test-time compute limits</li>
<li><strong>Sparse autoencoders</strong> <a href="http://localhost:8080/?date=2026-01-12&category=research#item-f28cced5ab5c" class="internal-link">fail to identify</a> genuine reasoning features—<strong>59-94%</strong> of detected features respond to surface-level cues rather than underlying logic</li>
<li>Transformers trained autoregressively <a href="http://localhost:8080/?date=2026-01-12&category=research#item-1deb1986fe19" class="internal-link">inherently encode</a> <strong>time-delayed causal structures</strong> recoverable from gradient sensitivities</li>
<li><strong>Circular Reasoning</strong> benchmark <a href="http://localhost:8080/?date=2026-01-12&category=research#item-81ebfa5898ee" class="internal-link">identifies self-reinforcing loops</a> as a key failure mode in large reasoning models</li>
</ul>
<p><strong>Safety & Security:</strong></p>
<ul>
<li>Agentic LLMs with web search successfully <a href="http://localhost:8080/?date=2026-01-12&category=research#item-45ee8c9fcaca" class="internal-link"><strong>re-identify participants</strong></a> in Anthropic's anonymized interview dataset</li>
<li><strong>MisBelief</strong> framework <a href="http://localhost:8080/?date=2026-01-12&category=research#item-adf18a27a17f" class="internal-link">reveals LLMs resist</a> direct misinformation but succumb to sophisticated multi-role deceptive evidence</li>
<li>Current <strong>emergent misalignment</strong> evaluations <a href="http://localhost:8080/?date=2026-01-12&category=research#item-1c43aad84367" class="internal-link">systematically overestimate</a> the phenomenon by conflating response types</li>
<li><strong>VIGIL</strong> proposes <a href="http://localhost:8080/?date=2026-01-12&category=research#item-1649d7d21b9a" class="internal-link">verify-before-commit protocol</a> defending agents against tool stream injection attacks</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-11:category-summary:research</id>
    <title>Research Summary: January 11, 2026</title>
    <link href="https://www.lesswrong.com/posts/ynC26Z2CJXsqj6ZnZ/the-case-against-continuous-chain-of-thought-neuralese" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-11&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-11T06:00:00Z</published>
    <updated>2026-01-11T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research centers on AI safety fundamentals and alignment tractability debates. A substantive technical argument against <strong>continuous chain-of-thought (neuralese)</strong> <a href="http://localhost:8080/?date=2026-01-11&category=research#item-61e9522a6331" class="internal-link">challenges OpenAI's research direction</a>, claiming discrete tokens are architecturally necessary rather than bandwidth limitations.</p>
<ul>
<li>Theoretical analysis <a href="http://localhost:8080/?date=2026-01-11&category=research#item-c8cf7a16f519" class="internal-link">provides <strong>learning-theoretic bounds</strong></a> on detecting deceptive AI behaviors like sandbagging and sycophancy</li>
<li><strong>AI Incident Forecasting</strong> models <a href="http://localhost:8080/?date=2026-01-11&category=research#item-d198c31eb374" class="internal-link">predict <strong>6-11x increases</strong></a> in AI-related incidents over five years using statistical analysis of the AI Incidents Database</li>
<li>Anthropic researcher <a href="http://localhost:8080/?date=2026-01-11&category=research#item-77c01b1b3448" class="internal-link">argues alignment may require <strong>70+ years</strong></a> of iterative development, challenging 'steam engine difficulty' optimism</li>
</ul>
<p>Supporting work includes the <strong>False Confidence Theorem</strong> <a href="http://localhost:8080/?date=2026-01-11&category=research#item-2c5974ce0bd4" class="internal-link">applied to Bayesian reasoning</a>, a conceptual framework <a href="http://localhost:8080/?date=2026-01-11&category=research#item-c015a6225956" class="internal-link">distinguishing <strong>superagency</strong></a> from superintelligence, and practical tooling <a href="http://localhost:8080/?date=2026-01-11&category=research#item-3e408746897d" class="internal-link">applying <strong>PageRank</strong></a> to identify high-signal voices in AI discourse networks.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-10:category-summary:research</id>
    <title>Research Summary: January 10, 2026</title>
    <link href="https://www.lesswrong.com/posts/X8KGHstcJa4qZznfH/linkpost-on-the-origins-of-algorithmic-progress-in-ai" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-10&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-10T06:00:00Z</published>
    <updated>2026-01-10T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's highlights feature significant empirical work on AI progress and safety. <strong>MIT FutureTech</strong> <a href="http://localhost:8080/?date=2026-01-10&category=research#item-7f9b4afc4a38" class="internal-link">finds most algorithmic innovations</a> yield small, scale-invariant efficiency gains, challenging narratives about AI progress sources. A mechanistic interpretability study <a href="http://localhost:8080/?date=2026-01-10&category=research#item-ebf70420afac" class="internal-link">reveals <strong>alignment faking</strong></a> in <strong>Llama-3.3-70B</strong> is controlled by a single linear direction—suggesting deceptive behaviors may be detectable and removable.</p>
<ul>
<li>Abramdemski <a href="http://localhost:8080/?date=2026-01-10&category=research#item-f78534e5c319" class="internal-link">argues for treating LLMs</a> as sophisticated statistical models rather than over-emphasizing RL approaches</li>
<li>Zvi provides <a href="http://localhost:8080/?date=2026-01-10&category=research#item-8fc43b620b04" class="internal-link">extensive practical analysis</a> of <strong>Claude Code</strong> with <strong>Opus 4.5</strong> capabilities</li>
<li>Conceptual clarification <a href="http://localhost:8080/?date=2026-01-10&category=research#item-42986778e146" class="internal-link">distinguishes 'Easy RSI'</a> (AI replacing researchers) from 'Hard RSI' (unbounded self-improvement)</li>
<li><strong>HypoBench</strong> <a href="http://localhost:8080/?date=2026-01-10&category=research#item-599524455fc5" class="internal-link">introduced</a> for evaluating AI hypothesis generation in scientific research</li>
</ul>
<p>Notable gap: Today's batch contains substantial non-AI content (economics, physics education, personal essays), with only 6-7 items directly relevant to AI research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-09:category-summary:research</id>
    <title>Research Summary: January 09, 2026</title>
    <link href="http://arxiv.org/abs/2601.04480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-09&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-09T06:00:00Z</published>
    <updated>2026-01-09T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's highlights feature major contributions from <strong>Anthropic</strong> and <strong>Meta FAIR</strong>, with strong emphasis on safety and interpretability. Chris Olah's team <a href="http://localhost:8080/?date=2026-01-09&category=research#item-99bb62ce6222" class="internal-link">reveals geometric mechanisms</a> underlying counting tasks in <strong>Claude 3.5 Haiku</strong>, while <strong>Constitutional Classifiers++</strong> <a href="http://localhost:8080/?date=2026-01-09&category=research#item-146d0785f873" class="internal-link">delivers production-ready jailbreak defenses</a> with cascade architectures.</p>
<ul>
<li>Large-scale study (<strong>N=2,724</strong>) <a href="http://localhost:8080/?date=2026-01-09&category=research#item-0ab5514fc786" class="internal-link">demonstrates <strong>GPT-4o</strong> equally effective</a> at increasing conspiracy beliefs as decreasing them—critical persuasion risk finding</li>
<li><strong>David Patterson</strong> <a href="http://localhost:8080/?date=2026-01-09&category=research#item-20f69ea28f2f" class="internal-link">identifies memory bandwidth and interconnect</a> as key LLM inference bottlenecks, not compute</li>
<li><strong>Evaluative fingerprints</strong> <a href="http://localhost:8080/?date=2026-01-09&category=research#item-6349fca66579" class="internal-link">reveal LLM judges</a> are self-consistent but mutually inconsistent (<strong>Krippendorff's α=0.042</strong>), undermining evaluation reliability</li>
</ul>
<p>RL training analysis <a href="http://localhost:8080/?date=2026-01-09&category=research#item-b06df1638a0e" class="internal-link">uncovers hidden biases</a> in <strong>GRPO-style</strong> methods and surprising linearity in RLVR weight evolution. VLM hallucination mechanisms identified: ablating small attention head sets <a href="http://localhost:8080/?date=2026-01-09&category=research#item-a59b69b18825" class="internal-link">reduces hallucinations by <strong>40%+</strong></a>. Incorporating negative reasoning trajectories during SFT substantially <a href="http://localhost:8080/?date=2026-01-09&category=research#item-23fd2f57d858" class="internal-link">improves OOD generalization</a>.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-09:research:99bb62ce6222</id>
    <title>When Models Manipulate Manifolds: The Geometry of a Counting Task</title>
    <link href="http://arxiv.org/abs/2601.04480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-09&amp;category=research#item-99bb62ce6222" rel="related" type="text/html"/>
    <published>2026-01-09T03:40:00Z</published>
    <updated>2026-01-09T03:40:00Z</updated>
    <author><name>Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, Chris Olah, Joshua Batson</name></author>
    <summary type="html"><![CDATA[Anthropic researchers mechanistically investigate how Claude 3.5 Haiku performs character counting and linebreaking tasks. Discovers that character counts are represented on low-dimensional curved manifolds using sparse features analogous to biological place cells, with geometric transformations enabling linear decision boundaries.]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Language Models"/>
    <category term="Representation Learning"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-09:research:146d0785f873</id>
    <title>Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks</title>
    <link href="http://arxiv.org/abs/2601.04603" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-09&amp;category=research#item-146d0785f873" rel="related" type="text/html"/>
    <published>2026-01-09T03:31:00Z</published>
    <updated>2026-01-09T03:31:00Z</updated>
    <author><name>Hoagy Cunningham, Jerry Wei, Zihan Wang, Andrew Persic, Alwin Peng, Jordan Abderrachid, Raj Agarwal, Bobby Chen, Austin Cohen, Andy Dau, Alek Dimitriev, Rob Gilson, Logan Howard, Yijin Hua, Jared Kaplan, Jan Leike, Mu Lin, Christopher Liu, Vladimir Mikulik, Rohit Mittapalli, Clare O'Hara, Jin Pan, Nikhil Saxena, Alex Silverstein, Yue Song, Xunjie Yu, Giulio Zhou, Ethan Perez, Mrinank Sharma</name></author>
    <summary type="html"><![CDATA[Anthropic presents enhanced Constitutional Classifiers with exchange classifiers, two-stage cascades, and linear probe ensembles for production-grade jailbreak defense. Dramatically reduces computational costs while maintaining robustness.]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreak Defense"/>
    <category term="Language Models"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-09:research:0ab5514fc786</id>
    <title>Large language models can effectively convince people to believe conspiracies</title>
    <link href="http://arxiv.org/abs/2601.05050" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-09&amp;category=research#item-0ab5514fc786" rel="related" type="text/html"/>
    <published>2026-01-09T03:31:00Z</published>
    <updated>2026-01-09T03:31:00Z</updated>
    <author><name>Thomas H. Costello, Kellin Pelrine, Matthew Kowal, Antonio A. Arechar, Jean-Fran\c{c}ois Godbout, Adam Gleave, David Rand, Gordon Pennycook</name></author>
    <summary type="html"><![CDATA[Pre-registered experiments (N=2,724) showing GPT-4o is equally effective at increasing conspiracy belief as decreasing it. Jailbroken variants effectively 'bunk' conspiracies, and bunking AI was rated more positively than debunking AI.]]></summary>
    <category term="AI Safety"/>
    <category term="Misinformation"/>
    <category term="LLM Risks"/>
    <category term="AI Ethics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-08:category-summary:research</id>
    <title>Research Summary: January 08, 2026</title>
    <link href="http://arxiv.org/abs/2601.03267" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-08&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-08T06:00:00Z</published>
    <updated>2026-01-08T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>The <strong>OpenAI GPT-5 System Card</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-92b07cb2ec0d" class="internal-link">dominates today's releases</a>, detailing the unified architecture with dynamic routing between fast and deep reasoning modes. A remarkable autoformalization result shows <strong>130k lines</strong> of formal topology <a href="http://localhost:8080/?date=2026-01-08&category=research#item-fcab5a7230d5" class="internal-link">generated in two weeks</a> for ~$100, suggesting accessible mathematical formalization at scale.</p>
<p><strong>Safety and alignment research</strong> features prominently:</p>
<ul>
<li><strong>What Matters For Safety Alignment</strong> delivers <a href="http://localhost:8080/?date=2026-01-08&category=research#item-00d1c9eacf45" class="internal-link">the field's most comprehensive study</a>: 32 models, 56 jailbreak techniques, 4.6M API calls</li>
<li><strong>Jailbreak-Zero</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-04972712e17a" class="internal-link">shifts red teaming</a> from example-based to policy-based evaluation</li>
<li><strong>RAILS</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-8819d3e7ac4c" class="internal-link">demonstrates black-box attacks</a> matching gradient-based effectiveness using only logits</li>
</ul>
<p><strong>Theoretical and interpretability advances</strong> challenge key assumptions:</p>
<ul>
<li><strong>QZero</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-f005da2bbce9" class="internal-link">achieves AlphaGo-level Go play</a> without MCTS, using pure model-free RL</li>
<li>First unified <strong>MoE theory</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-a21d944c617a" class="internal-link">derives Top-k routing</a> and load balancing from variational inference principles</li>
<li><strong>Layer-Order Inversion</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-8dba4837476e" class="internal-link">reveals later-hop answers</a> become decodable before bridge entities, contradicting hop-aligned circuit hypotheses</li>
<li><strong>Spectral Archaeology</strong> <a href="http://localhost:8080/?date=2026-01-08&category=research#item-700ceff414e9" class="internal-link">discovers 'spectral scars'</a> revealing hidden training behaviors without additional training</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-08:research:92b07cb2ec0d</id>
    <title>OpenAI GPT-5 System Card</title>
    <link href="http://arxiv.org/abs/2601.03267" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-08&amp;category=research#item-92b07cb2ec0d" rel="related" type="text/html"/>
    <published>2026-01-08T03:55:00Z</published>
    <updated>2026-01-08T03:55:00Z</updated>
    <author><name>Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, Akshay Nathan, Alan Luo, Alec Helyar, Aleksander Madry, Aleksandr Efremov, Aleksandra Spyra, Alex Baker-Whitcomb, Alex Beutel, Alex Karpenko, Alex Makelov, Alex Neitz, Alex Wei, Alexandra Barr, Alexandre Kirchmeyer, Alexey Ivanov, Alexi Christakis, Alistair Gillespie, Allison Tam, Ally Bennett, Alvin Wan, Alyssa Huang, Amy McDonald Sandjideh, Amy Yang, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrei Gheorghe, Andres Garcia Garcia, Andrew Braunstein, Andrew Liu, Andrew Schmidt, Andrey Mereskin, Andrey Mishchenko, Andy Applebaum, Andy Rogerson, Ann Rajan, Annie Wei, Anoop Kotha, Anubha Srivastava, Anushree Agrawal, Arun Vijayvergiya, Ashley Tyra, Ashvin Nair, Avi Nayak, Ben Eggers, Bessie Ji, Beth Hoover, Bill Chen, Blair Chen, Boaz Barak, Borys Minaiev, Botao Hao, Bowen Baker, Brad Lightcap, Brandon McKinzie, Brandon Wang, Brendan Quinn, Brian Fioca, Brian Hsu, Brian Yang, Brian Yu, Brian Zhang, Brittany Brenner, Callie Riggins Zetino, Cameron Raymond, Camillo Lugaresi, Carolina Paz, Cary Hudson, Cedric Whitney, Chak Li, Charles Chen, Charlotte Cole, Chelsea Voss, Chen Ding, Chen Shen, Chengdu Huang, Chris Colby, Chris Hallacy, Chris Koch, Chris Lu, Christina Kaplan, Christina Kim, CJ Minott-Henriques, Cliff Frey, Cody Yu, Coley Czarnecki, Colin Reid, Colin Wei, Cory Decareaux, Cristina Scheau, Cyril Zhang, Cyrus Forbes, Da Tang, Dakota Goldberg, Dan Roberts, Dana Palmie, Daniel Kappler, Daniel Levine, Daniel Wright, Dave Leo, David Lin, David Robinson, Declan Grabb, Derek Chen, Derek Lim, Derek Salama, Dibya Bhattacharjee, Dimitris Tsipras, Dinghua Li, Dingli Yu, DJ Strouse, Drew Williams, Dylan Hunn, Ed Bayes, Edwin Arbus, Ekin Akyurek, Elaine Ya Le, Elana Widmann, Eli Yani, Elizabeth Proehl, Enis Sert, Enoch Cheung, Eri Schwartz, Eric Han, Eric Jiang, Eric Mitchell, Eric Sigler, Eric Wallace, Erik Ritter, Erin Kavanaugh, Evan Mays, Evgenii Nikishin, Fangyuan Li, Felipe Petroski Such, Filipe de Avila Belbute Peres, Filippo Raso, Florent Bekerman, Foivos Tsimpourlas, Fotis Chantzis, Francis Song, Francis Zhang, Gaby Raila, Garrett McGrath, Gary Briggs, Gary Yang, Giambattista Parascandolo, Gildas Chabot, Grace Kim, Grace Zhao, Gregory Valiant, Guillaume Leclerc, Hadi Salman, Hanson Wang, Hao Sheng, Haoming Jiang, Haoyu Wang, Haozhun Jin, Harshit Sikchi, Heather Schmidt, Henry Aspegren, Honglin Chen, Huida Qiu, Hunter Lightman, Ian Covert, Ian Kivlichan, Ian Silber, Ian Sohl, Ibrahim Hammoud, Ignasi Clavera, Ikai Lan, Ilge Akkaya, Ilya Kostrikov, Irina Kofman, Isak Etinger, Ishaan Singal, Jackie Hehir, Jacob Huh, Jacqueline Pan, Jake Wilczynski, Jakub Pachocki, James Lee, James Quinn, Jamie Kiros, Janvi Kalra, Jasmyn Samaroo, Jason Wang, Jason Wolfe, Jay Chen, Jay Wang, Jean Harb, Jeffrey Han, Jeffrey Wang, Jennifer Zhao, Jeremy Chen, Jerene Yang, Jerry Tworek, Jesse Chand, Jessica Landon, Jessica Liang, Ji Lin, Jiancheng Liu, Jianfeng Wang, Jie Tang, Jihan Yin, Joanne Jang, Joel Morris, Joey Flynn, Johannes Ferstad, Johannes Heidecke, John Fishbein, John Hallman, Jonah Grant, Jonathan Chien, Jonathan Gordon, Jongsoo Park, Jordan Liss, Jos Kraaijeveld, Joseph Guay, Joseph Mo, Josh Lawson, Josh McGrath, Joshua Vendrow, Joy Jiao, Julian Lee, Julie Steele, Julie Wang, Junhua Mao, Kai Chen, Kai Hayashi, Kai Xiao, Kamyar Salahi, Kan Wu, Karan Sekhri, Karan Sharma, Karan Singhal, Karen Li, Kenny Nguyen, Keren Gu-Lemberg, Kevin King, Kevin Liu, Kevin Stone, Kevin Yu, Kristen Ying, Kristian Georgiev, Kristie Lim, Kushal Tirumala, Kyle Miller, Lama Ahmad, Larry Lv, Laura Clare, Laurance Fauconnet, Lauren Itow, Lauren Yang, Laurentia Romaniuk, Leah Anise, Lee Byron, Leher Pathak, Leon Maksin, Leyan Lo, Leyton Ho, Li Jing, Liang Wu, Liang Xiong, Lien Mamitsuka, Lin Yang, Lindsay McCallum, Lindsey Held, Liz Bourgeois, Logan Engstrom, Lorenz Kuhn, Louis Feuvrier, Lu Zhang, Lucas Switzer, Lukas Kondraciuk, Lukasz Kaiser, Manas Joglekar, Mandeep Singh, Mandip Shah, Manuka Stratta, Marcus Williams, Mark Chen, Mark Sun, Marselus Cayton, Martin Li, Marvin Zhang, Marwan Aljubeh, Matt Nichols, Matthew Haines, Max Schwarzer, Mayank Gupta, Meghan Shah, Melody Huang, Meng Dong, Mengqing Wang, Mia Glaese, Micah Carroll, Michael Lampe, Michael Malek, Michael Sharman, Michael Zhang, Michele Wang, Michelle Pokrass, Mihai Florian, Mikhail Pavlov, Miles Wang, Ming Chen, Mingxuan Wang, Minnia Feng, Mo Bavarian, Molly Lin, Moose Abdool, Mostafa Rohaninejad, Nacho Soto, Natalie Staudacher, Natan LaFontaine, Nathan Marwell, Nelson Liu, Nick Preston, Nick Turley, Nicklas Ansman, Nicole Blades, Nikil Pancha, Nikita Mikhaylin, Niko Felix, Nikunj Handa, Nishant Rai, Nitish Keskar, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Oona Gleeson, Pamela Mishkin, Patryk Lesiewicz, Paul Baltescu, Pavel Belov, Peter Zhokhov, Philip Pronin, Phillip Guo, Phoebe Thacker, Qi Liu, Qiming Yuan, Qinghua Liu, Rachel Dias, Rachel Puckett, Rahul Arora, Ravi Teja Mullapudi, Raz Gaon, Reah Miyara, Rennie Song, Rishabh Aggarwal, RJ Marsan, Robel Yemiru, Robert Xiong, Rohan Kshirsagar, Rohan Nuttall, Roman Tsiupa, Ronen Eldan, Rose Wang, Roshan James, Roy Ziv, Rui Shu, Ruslan Nigmatullin, Saachi Jain, Saam Talaie, Sam Altman, Sam Arnesen, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Sarah Yoo, Savannah Heon, Scott Ethersmith, Sean Grove, Sean Taylor, Sebastien Bubeck, Sever Banesiu, Shaokyi Amdo, Shengjia Zhao, Sherwin Wu, Shibani Santurkar, Shiyu Zhao, Shraman Ray Chaudhuri, Shreyas Krishnaswamy, Shuaiqi (Tony) Xia, Shuyang Cheng, Shyamal Anadkat, Sim\'on Posada Fishman, Simon Tobin, Siyuan Fu, Somay Jain, Song Mei, Sonya Egoian, Spencer Kim, Spug Golden, SQ Mah, Steph Lin, Stephen Imm, Steve Sharpe, Steve Yadlowsky, Sulman Choudhry, Sungwon Eum, Suvansh Sanjeev, Tabarak Khan, Tal Stramer, Tao Wang, Tao Xin, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Degry, Thomas Shadwell, Tianfu Fu, Tianshi Gao, Timur Garipov, Tina Sriskandarajah, Toki Sherbakov, Tomer Kaftan, Tomo Hiratsuka, Tongzhou Wang, Tony Song, Tony Zhao, Troy Peterson, Val Kharitonov, Victoria Chernova, Vineet Kosaraju, Vishal Kuo, Vitchyr Pong, Vivek Verma, Vlad Petrov, Wanning Jiang, Weixing Zhang, Wenda Zhou, Wenlei Xie, Wenting Zhan, Wes McCabe, Will DePue, Will Ellsworth, Wulfie Bain, Wyatt Thompson, Xiangning Chen, Xiangyu Qi, Xin Xiang, Xinwei Shi, Yann Dubois, Yaodong Yu, Yara Khakbaz, Yifan Wu, Yilei Qian, Yin Tat Lee, Yinbo Chen, Yizhen Zhang, Yizhong Xiong, Yonglong Tian, Young Cha, Yu Bai, Yu Yang, Yuan Yuan, Yuanzhi Li, Yufeng Zhang, Yuguang Yang, Yujia Jin, Yun Jiang, Yunyun Wang, Yushi Wang, Yutian Liu, Zach Stubenvoll, Zehao Dou, Zheng Wu, Zhigang Wang</name></author>
    <summary type="html"><![CDATA[Official system card for OpenAI GPT-5 launch (August 2025), describing a unified system with fast and deep reasoning models, real-time router for complexity-based model selection, and continuous training on user feedback signals.]]></summary>
    <category term="Language Models"/>
    <category term="AI Safety"/>
    <category term="OpenAI"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-08:research:fcab5a7230d5</id>
    <title>130k Lines of Formal Topology in Two Weeks: Simple and Cheap Autoformalization for Everyone?</title>
    <link href="http://arxiv.org/abs/2601.03298" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-08&amp;category=research#item-fcab5a7230d5" rel="related" type="text/html"/>
    <published>2026-01-08T03:31:00Z</published>
    <updated>2026-01-08T03:31:00Z</updated>
    <author><name>Josef Urban</name></author>
    <summary type="html"><![CDATA[Reports autoformalization of 130k lines of topology from Munkres textbook in two weeks for ~$100 LLM cost, including proofs of Urysohn's lemma and metrization theorem, using LLM-proof checker feedback loop.]]></summary>
    <category term="Formal Methods"/>
    <category term="Language Models"/>
    <category term="Mathematics"/>
    <category term="Autoformalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-07:category-summary:research</id>
    <title>Research Summary: January 07, 2026</title>
    <link href="http://arxiv.org/abs/2601.02427" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-07&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-07T06:00:00Z</published>
    <updated>2026-01-07T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research centers on AI safety vulnerabilities and foundational model capabilities. A systematic study <a href="http://localhost:8080/?date=2026-01-07&category=research#item-df5be0b66669" class="internal-link">demonstrates extraction</a> of copyrighted books from production LLMs using <strong>Best-of-N jailbreaking</strong>, raising significant legal implications. Critical stress-testing of <strong>Anthropic's SAE features</strong> <a href="http://localhost:8080/?date=2026-01-07&category=research#item-9d8dd3d567b8" class="internal-link">reveals fragility</a> in steering interventions, questioning interpretability claims.</p>
<ul>
<li><strong>NitroGen</strong> <a href="http://localhost:8080/?date=2026-01-07&category=research#item-9315548090ec" class="internal-link">establishes a vision-action foundation model</a> trained on <strong>40K hours</strong> across <strong>1,000+ games</strong>, demonstrating cross-game generalization</li>
<li><strong>InternVLA-A1</strong> <a href="http://localhost:8080/?date=2026-01-07&category=research#item-b8d5357a91a0" class="internal-link">unifies scene understanding</a>, generation, and action via <strong>Mixture-of-Transformers</strong> for robotic manipulation</li>
<li><strong>Logical Phase Transitions</strong> <a href="http://localhost:8080/?date=2026-01-07&category=research#item-e94dc33d6a8c" class="internal-link">discovers abrupt reasoning collapse</a> in LLMs beyond critical complexity thresholds</li>
<li>ViT spatial reasoning <a href="http://localhost:8080/?date=2026-01-07&category=research#item-c6b99cc9400e" class="internal-link">proven intrinsically limited</a> due to <strong>circuit complexity bounds</strong> on non-solvable group problems</li>
</ul>
<p>Practical contributions include <a href="http://localhost:8080/?date=2026-01-07&category=research#item-aaff970cd1a8" class="internal-link"><strong>agent-permissions.json</strong></a> for web agent governance and a striking <a href="http://localhost:8080/?date=2026-01-07&category=research#item-8629a230ff11" class="internal-link">one-shot RL finding</a> showing single-sample training produces improvements rivaling full datasets. Jacob Steinhardt's <a href="http://localhost:8080/?date=2026-01-07&category=research#item-cd67b613427c" class="internal-link"><strong>Oversight Assistants</strong> framework</a> addresses scalable human oversight of AI systems.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
</feed>