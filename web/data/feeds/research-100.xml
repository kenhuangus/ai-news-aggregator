<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 100)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="http://localhost:8080/?category=research" rel="alternate" type="text/html"/>
  <link href="http://localhost:8080/data/feeds/research-100.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:100</id>
  <updated>2026-01-30T07:46:56Z</updated>
  <icon>http://localhost:8080/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>http://localhost:8080</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-01-30:category-summary:research</id>
    <title>Research Summary: January 30, 2026</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-30T06:00:00Z</published>
    <updated>2026-01-30T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research is dominated by critical AI safety and security findings. A <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" class="internal-link" rel="noopener noreferrer">systematic audit reveals</a> open-source models interpret prohibitions as permissions <strong>77-100%</strong> of the time under negation, while <strong>JustAsk</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" class="internal-link" rel="noopener noreferrer">demonstrates</a> code agents can autonomously extract system prompts from frontier LLMs.</p>
<ul>
<li>Counterintuitive <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" class="internal-link" rel="noopener noreferrer">'less-is-more' effect discovered</a>: LLM monitors detect sabotage better with <strong>limited information access</strong></li>
<li>Alec Radford shows <strong>token-level filtering</strong> during pretraining <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" class="internal-link" rel="noopener noreferrer">effectively removes</a> specific capabilities while preserving general performance</li>
<li><strong>Sycophantic anchors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" class="internal-link" rel="noopener noreferrer">localized in reasoning traces</a> enable <strong>84.6% detection accuracy</strong> with linear probes</li>
<li><strong>WhatCounts</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" class="internal-link" rel="noopener noreferrer">exposes <strong>40%+ accuracy variation</strong></a> in counting tasks based purely on semantic content (cities vs chemicals)</li>
</ul>
<p>Notable benchmarks and empirical studies: <strong>FrontierScience</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" class="internal-link" rel="noopener noreferrer">presents PhD-level problems</a> where SOTA achieves <strong>&lt;5%</strong> accuracy. Analysis of <strong>125,000+ paper-review pairs</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" class="internal-link" rel="noopener noreferrer">quantifies LLM interaction effects</a> in peer review. <strong>Hardware-triggered backdoors</strong> <a href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" class="internal-link" rel="noopener noreferrer">exploit numerical variations</a> across computing platforms as a novel attack vector.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:ccfcdf03ee13</id>
    <title>When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models</title>
    <link href="http://arxiv.org/abs/2601.21433" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-ccfcdf03ee13" rel="related" type="text/html"/>
    <published>2026-01-30T03:36:00Z</published>
    <updated>2026-01-30T03:36:00Z</updated>
    <author><name>Katherine Elkins, Jon Chun</name></author>
    <summary type="html"><![CDATA[<p>Audits 16 LLMs on negation sensitivity, finding open-source models interpret prohibitions as permissions 77-100% of the time under negation. Commercial models also show 19-128% accuracy swings.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Robustness"/>
    <category term="Negation Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:9a5070217f94</id>
    <title>Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</title>
    <link href="http://arxiv.org/abs/2601.21233" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-9a5070217f94" rel="related" type="text/html"/>
    <published>2026-01-30T03:31:00Z</published>
    <updated>2026-01-30T03:31:00Z</updated>
    <author><name>Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</name></author>
    <summary type="html"><![CDATA[<p>Presents JustAsk, a self-evolving framework where code agents autonomously discover system prompt extraction strategies for frontier LLMs through interaction alone, requiring no handcrafted prompts.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Prompt Injection"/>
    <category term="Agent Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:da30c053d377</id>
    <title>How does information access affect LLM monitors' ability to detect sabotage?</title>
    <link href="http://arxiv.org/abs/2601.21112" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-da30c053d377" rel="related" type="text/html"/>
    <published>2026-01-30T03:28:00Z</published>
    <updated>2026-01-30T03:28:00Z</updated>
    <author><name>Rauno Arike, Raja Mehta Moreno, Rohan Subramani, Shubhorup Biswas, Francis Rhys Ward</name></author>
    <summary type="html"><![CDATA[<p>Studies how information access affects LLM monitors' ability to detect agent sabotage. Discovers counterintuitive 'less-is-more effect' where monitors often perform better with less access to agent reasoning.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Monitoring"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:873b31238d4b</id>
    <title>FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks</title>
    <link href="http://arxiv.org/abs/2601.21165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-873b31238d4b" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Miles Wang, Robi Lin, Kat Hu, Joy Jiao, Neil Chowdhury, Ethan Chang, Tejal Patwardhan</name></author>
    <summary type="html"><![CDATA[<p>Introduces FrontierScience benchmark with Olympiad-level and PhD-level research problems across physics, chemistry, and biology. Current SOTA models solve only ~15% of research track problems.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="Scientific Reasoning"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:818d74cddf79</id>
    <title>Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models</title>
    <link href="http://arxiv.org/abs/2601.21183" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-818d74cddf79" rel="related" type="text/html"/>
    <published>2026-01-30T03:23:00Z</published>
    <updated>2026-01-30T03:23:00Z</updated>
    <author><name>Jacek Duszenko</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sycophantic anchors' - sentences that causally lock reasoning models into user agreement. Linear probes detect these with 84.6% accuracy, enabling mid-inference intervention.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Sycophancy"/>
    <category term="Interpretability"/>
    <category term="Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:afafccd817c8</id>
    <title>Semantic Content Determines Algorithmic Performance</title>
    <link href="http://arxiv.org/abs/2601.21618" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-afafccd817c8" rel="related" type="text/html"/>
    <published>2026-01-30T03:21:00Z</published>
    <updated>2026-01-30T03:21:00Z</updated>
    <author><name>Marti\~no R\'ios-Garc\'ia, Nawaf Alampara, Kevin Maik Jablonka</name></author>
    <summary type="html"><![CDATA[<p>Introduces WhatCounts showing frontier LLMs exhibit 40%+ accuracy variation in counting tasks based solely on semantic content (cities vs chemicals), ruling out sampling noise.</p>]]></summary>
    <category term="LLM Limitations"/>
    <category term="Semantic Sensitivity"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:fba44deae645</id>
    <title>ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design</title>
    <link href="http://arxiv.org/abs/2601.21448" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-fba44deae645" rel="related" type="text/html"/>
    <published>2026-01-30T03:19:00Z</published>
    <updated>2026-01-30T03:19:00Z</updated>
    <author><name>Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding</name></author>
    <summary type="html"><![CDATA[<p>Introduces ChipBench for AI-aided chip design with 44 hierarchical modules, 89 debugging cases, and 132 reference model samples. Claude-4.5-opus achieves only 30.74% on Verilog generation.</p>]]></summary>
    <category term="Hardware Design"/>
    <category term="Code Generation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:912762d409d4</id>
    <title>Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review</title>
    <link href="http://arxiv.org/abs/2601.20920" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-912762d409d4" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Vibhhu Sharma, Thorsten Joachims, Sarah Dean</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 125,000+ paper-review pairs from ICLR, NeurIPS, and ICML to study LLM use in peer review. Finds apparent interaction effects where LLM-assisted reviews seem kinder to LLM-assisted papers, but controlling for confounders reveals more nuanced patterns.</p>]]></summary>
    <category term="AI in Science"/>
    <category term="LLM Evaluation"/>
    <category term="Meta-Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:4efa5005597f</id>
    <title>Chain Of Thought Compression: A Theoritical Analysis</title>
    <link href="http://arxiv.org/abs/2601.21576" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-4efa5005597f" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Juncai Li, Ru Li, Yuxiang Zhou, Boxiang Ma, Jeff Z. Pan</name></author>
    <summary type="html"><![CDATA[<p>Provides first theoretical analysis of CoT compression difficulty, proving learning signal for high-order logical dependencies exponentially decays when skipping intermediate steps.</p>]]></summary>
    <category term="Chain-of-Thought"/>
    <category term="Latent Reasoning"/>
    <category term="Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:6c5f9b39424c</id>
    <title>Shaping capabilities with token-level data filtering</title>
    <link href="http://arxiv.org/abs/2601.21571" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c5f9b39424c" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Neil Rathi, Alec Radford</name></author>
    <summary type="html"><![CDATA[<p>Shows token-level filtering during pretraining is highly effective for removing specific capabilities (demonstrated on medical knowledge). Token filtering more effective than document filtering, and effectiveness increases with model scale.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Capability Control"/>
    <category term="LLM Pretraining"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:1b292d0d3261</id>
    <title>Hardware-Triggered Backdoors</title>
    <link href="http://arxiv.org/abs/2601.21902" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-1b292d0d3261" rel="related" type="text/html"/>
    <published>2026-01-30T03:16:00Z</published>
    <updated>2026-01-30T03:16:00Z</updated>
    <author><name>Jonas M\"oller, Erik Imgrund, Thorsten Eisenhofer, Konrad Rieck</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that small numerical variations across different computing hardware can be exploited to create backdoors in ML models that produce different predictions for identical inputs depending on execution hardware. A significant security vulnerability discovery.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Adversarial ML"/>
    <category term="Model Backdoors"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:b2a80d0f9e1f</id>
    <title>Do Reasoning Models Enhance Embedding Models?</title>
    <link href="http://arxiv.org/abs/2601.21192" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-b2a80d0f9e1f" rel="related" type="text/html"/>
    <published>2026-01-30T03:12:00Z</published>
    <updated>2026-01-30T03:12:00Z</updated>
    <author><name>Wun Yu Chan, Shaojin Chen, Huihao Jing, Kwun Hang Lau, Elton Chun-Chai Li, Zihao Wang, Haoran Li, Yangqiu Song</name></author>
    <summary type="html"><![CDATA[<p>Evaluates whether RLVR-tuned reasoning models produce better embeddings. Finds null effect - reasoning training provides no consistent advantage when models are used as embedding backbones.</p>]]></summary>
    <category term="Embedding Models"/>
    <category term="Reasoning Models"/>
    <category term="RLVR"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:838aa09242ba</id>
    <title>Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</title>
    <link href="http://arxiv.org/abs/2601.21096" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-838aa09242ba" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Hongzheng Chen, Alexander Novikov, Ng\^an V\~u, Hanna Alam, Zhiru Zhang, Aiden Grossman, Mircea Trofin, Amir Yazdanbakhsh</name></author>
    <summary type="html"><![CDATA[<p>Presents Magellan, an agentic framework using LLM coding agents with evolutionary search to synthesize C++ compiler optimization heuristics, achieving results matching or surpassing expert baselines in LLVM.</p>]]></summary>
    <category term="Compiler Optimization"/>
    <category term="Automated Algorithm Design"/>
    <category term="LLM Agents"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:6c436d17f579</id>
    <title>The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making</title>
    <link href="http://arxiv.org/abs/2601.21439" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-6c436d17f579" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Jon Chun, Katherine Elkins</name></author>
    <summary type="html"><![CDATA[<p>Uncovers 'Paradox of Robustness' - despite lexical brittleness, LLMs show 110-300x greater resistance to emotional framing manipulation than humans in high-stakes decision domains.</p>]]></summary>
    <category term="LLM Robustness"/>
    <category term="Behavioral AI"/>
    <category term="Decision Making"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:c5315a26e703</id>
    <title>Self-Improving Pretraining: using post-trained models to pretrain better models</title>
    <link href="http://arxiv.org/abs/2601.21343" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-c5315a26e703" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</name></author>
    <summary type="html"><![CDATA[<p>Introduces self-improving pretraining that uses post-trained (aligned) models to filter/improve pretraining data on-the-fly. Addresses safety and factuality during pretraining rather than relying solely on post-hoc alignment.</p>]]></summary>
    <category term="LLM Pretraining"/>
    <category term="AI Safety"/>
    <category term="Data Quality"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:23ce3e525747</id>
    <title>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</title>
    <link href="http://arxiv.org/abs/2601.21996" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-23ce3e525747" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Jianhui Chen, Yuzhang Luo, Liangming Pan</name></author>
    <summary type="html"><![CDATA[<p>Mechanistic Data Attribution traces interpretable LLM circuits back to specific training samples using influence functions, causally validating that targeted data interventions modulate emergence of interpretable heads.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Data Attribution"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:14b8aadeb395</id>
    <title>Discovering Hidden Gems in Model Repositories</title>
    <link href="http://arxiv.org/abs/2601.22157" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-14b8aadeb395" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</name></author>
    <summary type="html"><![CDATA[<p>Investigates overlooked fine-tuned models in public repositories, finding 'hidden gems' that significantly outperform popular models (e.g., Llama-3.1-8B math performance from 83.2% to 96.0%). Proposes lightweight discovery methods.</p>]]></summary>
    <category term="Model Selection"/>
    <category term="Fine-tuning"/>
    <category term="Model Repositories"/>
    <category term="LLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-30:research:e7bb912063df</id>
    <title>Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography</title>
    <link href="http://arxiv.org/abs/2601.22134" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-30&amp;category=research#item-e7bb912063df" rel="related" type="text/html"/>
    <published>2026-01-30T03:07:00Z</published>
    <updated>2026-01-30T03:07:00Z</updated>
    <author><name>Wenxuan Li, Pedro R. A. S. Bassi, Lizhou Wu, Xinze Zhou, Yuxuan Zhao, Qi Chen, Szymon Plotka, Tianyu Lin, Zheren Zhu, Marisa Martin, Justin Caskey, Shanshan Jiang, Xiaoxi Chen, Jaroslaw B. \'Cwikla, Artur Sankowski, Yaping Wu, Sergio Decherchi, Andrea Cavalli, Chandana Lall, Cristian Tomasetti, Yaxing Guo, Xuan Yu, Yuqing Cai, Hualin Qiao, Jie Bao, Chenhan Hu, Ximing Wang, Arkadiusz Sitek, Kai Ding, Heng Li, Meiyun Wang, Dexin Yu, Guang Zhang, Yang Yang, Kang Wang, Alan L. Yuille, Zongwei Zhou</name></author>
    <summary type="html"><![CDATA[<p>Develops ePAI for early pancreatic cancer detection achieving AUC 0.939-0.999 on internal test, with validation across 15,153 patients from 10 institutions in Asia, Europe, and North America.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="Cancer Detection"/>
    <category term="Clinical Validation"/>
    <category term="Healthcare Impact"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:category-summary:research</id>
    <title>Research Summary: January 29, 2026</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-29T06:00:00Z</published>
    <updated>2026-01-29T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI's societal impact, alignment fundamentals, and practical training advances. An Anthropic researcher presents <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" class="internal-link" rel="noopener noreferrer">randomized experiments</a> showing AI assistance impairs conceptual understanding during skill acquisition—critical findings for AI deployment strategy.</p>
<p><strong>Alignment &amp; Training Innovations:</strong></p>
<ul>
<li>Reward models <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" class="internal-link" rel="noopener noreferrer">inherit significant value biases</a> from pretrained base LLMs, revealing hidden alignment risks in RLHF pipelines</li>
<li><strong>Peer prediction</strong> methods from mechanism design <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" class="internal-link" rel="noopener noreferrer">enable truthful LLM training</a> without ground truth labels</li>
<li><strong>SDPO</strong> (Self-Distillation Policy Optimization) <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" class="internal-link" rel="noopener noreferrer">converts rich textual feedback</a> into dense learning signals, addressing RLVR credit assignment</li>
<li><strong>Failure-prefix conditioning</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-83b82f2ea90d" class="internal-link" rel="noopener noreferrer">rescues learning from saturated problems</a> where standard RLVR stalls</li>
</ul>
<p><strong>Deployment &amp; Evaluation:</strong></p>
<ul>
<li>NVIDIA's <strong>quantization-aware distillation</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" class="internal-link" rel="noopener noreferrer">recovers <strong>NVFP4</strong> inference accuracy</a> for production LLMs/VLMs</li>
<li><strong>SokoBench</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" class="internal-link" rel="noopener noreferrer">exposes consistent degradation</a> in LLM planning as horizon length increases</li>
<li>Harvard's <strong>MoE hyperparameter transfer</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-1ec4f356b981" class="internal-link" rel="noopener noreferrer">enables scaling width, depth</a>, and expert count without retuning</li>
<li>Multi-agent debate <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-372b477ef754" class="internal-link" rel="noopener noreferrer">underperforms majority vote</a> due to missing diversity and poor confidence calibration</li>
<li><strong>PURGE</strong> <a href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ffc629735959" class="internal-link" rel="noopener noreferrer">introduces RL-based machine unlearning</a> for GDPR/EU AI Act compliance</li>
</ul>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:acf17d7624d5</id>
    <title>How AI Impacts Skill Formation</title>
    <link href="http://arxiv.org/abs/2601.20245" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-acf17d7624d5" rel="related" type="text/html"/>
    <published>2026-01-29T03:31:00Z</published>
    <updated>2026-01-29T03:31:00Z</updated>
    <author><name>Judy Hanwen Shen, Alex Tamkin</name></author>
    <summary type="html"><![CDATA[<p>Randomized experiments studying how AI assistance affects skill development in programmers learning new libraries. Finds AI use impairs conceptual understanding, code reading, and debugging abilities without significant efficiency gains on average.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Human-AI Interaction"/>
    <category term="AI Impact"/>
    <category term="Education"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:301581ef1dfe</id>
    <title>Reward Models Inherit Value Biases from Pretraining</title>
    <link href="http://arxiv.org/abs/2601.20838" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-301581ef1dfe" rel="related" type="text/html"/>
    <published>2026-01-29T03:26:00Z</published>
    <updated>2026-01-29T03:26:00Z</updated>
    <author><name>Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</name></author>
    <summary type="html"><![CDATA[<p>Shows reward models inherit significant value biases from their base pretrained LLMs. Demonstrates robust differences along psychological value dimensions (agency vs communion) between Llama and Gemma RMs.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Reward Models"/>
    <category term="Value Alignment"/>
    <category term="Bias"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:7951b7029f15</id>
    <title>Reinforcement Learning via Self-Distillation</title>
    <link href="http://arxiv.org/abs/2601.20802" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-7951b7029f15" rel="related" type="text/html"/>
    <published>2026-01-29T03:23:00Z</published>
    <updated>2026-01-29T03:23:00Z</updated>
    <author><name>Jonas H\"ubotter, Frederike L\"ubeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</name></author>
    <summary type="html"><![CDATA[<p>Introduces Self-Distillation Policy Optimization (SDPO) for RLVR that converts rich textual feedback into dense learning signals without external teachers. Treats the model conditioned on feedback as its own teacher.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="Reasoning"/>
    <category term="Self-Distillation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:88a72f243c4d</id>
    <title>SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.20856" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-88a72f243c4d" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</name></author>
    <summary type="html"><![CDATA[<p>Proposes SokoBench, a benchmark based on Sokoban puzzles for evaluating long-horizon planning in Large Reasoning Models. Finds consistent degradation beyond 25 moves, suggesting fundamental planning constraints.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="LLM Reasoning"/>
    <category term="Long-Horizon Planning"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:5e9ce68fe709</id>
    <title>Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</title>
    <link href="http://arxiv.org/abs/2601.20299" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-5e9ce68fe709" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Tianyi Alex Qiu, Micah Carroll, Cameron Allen</name></author>
    <summary type="html"><![CDATA[<p>Introduces peer prediction methods from mechanism design for LLM evaluation and post-training. Rewards honest and informative answers using mutual prediction between models, enabling evaluation without strong supervision.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="LLM Evaluation"/>
    <category term="Mechanism Design"/>
    <category term="Truthfulness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:ce8b9ce70e0c</id>
    <title>Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery</title>
    <link href="http://arxiv.org/abs/2601.20088" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ce8b9ce70e0c" rel="related" type="text/html"/>
    <published>2026-01-29T03:16:00Z</published>
    <updated>2026-01-29T03:16:00Z</updated>
    <author><name>Meng Xin, Sweta Priyadarshi, Jingyu Xin, Bilal Kartal, Aditya Vavre, Asma Kuriparambil Thekkumpate, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Ido Shahaf, Akhiad Bercovich, Kinjal Patel, Suguna Varshini Velury, Chenjie Luo, Zhiyu Cheng, Jenny Chen, Chen-Han Yu, Wei Ping, Oleg Rybakov, Nima Tajbakhsh, Oluwatobi Olabiyi, Dusan Stosic, Di Wu, Song Han, Eric Chung, Sharath Turuvekere Sreenivas, Bryan Catanzaro, Yoshi Suhara, Tijmen Blankevoort, Huizi Mao</name></author>
    <summary type="html"><![CDATA[<p>Presents quantization-aware distillation (QAD) best practices for recovering accuracy of NVFP4-quantized LLMs and VLMs. Shows effectiveness for models with complex post-training pipelines (SFT+RL+merging) where traditional QAT fails.</p>]]></summary>
    <category term="Model Quantization"/>
    <category term="Knowledge Distillation"/>
    <category term="LLM Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:372b477ef754</id>
    <title>Demystifying Multi-Agent Debate: The Role of Confidence and Diversity</title>
    <link href="http://arxiv.org/abs/2601.19921" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-372b477ef754" rel="related" type="text/html"/>
    <published>2026-01-29T03:12:00Z</published>
    <updated>2026-01-29T03:12:00Z</updated>
    <author><name>Xiaochen Zhu, Caiqi Zhang, Yizhou Chi, Tom Stafford, Nigel Collier, Andreas Vlachos</name></author>
    <summary type="html"><![CDATA[<p>Analyzes why multi-agent debate often underperforms majority vote despite higher compute cost. Identifies missing diversity and confidence calibration, proposing lightweight interventions that significantly improve outcomes.</p>]]></summary>
    <category term="Multi-Agent Debate"/>
    <category term="Test-Time Scaling"/>
    <category term="LLM Reasoning"/>
    <category term="Collective Intelligence"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:83b82f2ea90d</id>
    <title>Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</title>
    <link href="http://arxiv.org/abs/2601.20829" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-83b82f2ea90d" rel="related" type="text/html"/>
    <published>2026-01-29T03:12:00Z</published>
    <updated>2026-01-29T03:12:00Z</updated>
    <author><name>Minwu Kim, Safal Shrestha, Keith Ross</name></author>
    <summary type="html"><![CDATA[<p>Proposes failure-prefix conditioning to improve learning from saturated problems where standard RLVR stalls. Conditions training on prefixes from rare incorrect trajectories to expose models to failure-prone states.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Reasoning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:1ec4f356b981</id>
    <title>Hyperparameter Transfer with Mixture-of-Expert Layers</title>
    <link href="http://arxiv.org/abs/2601.20205" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-1ec4f356b981" rel="related" type="text/html"/>
    <published>2026-01-29T03:12:00Z</published>
    <updated>2026-01-29T03:12:00Z</updated>
    <author><name>Tianze Jiang, Blake Bordelon, Cengiz Pehlevan, Boris Hanin</name></author>
    <summary type="html"><![CDATA[<p>Proposes new parameterization for MoE transformers enabling hyperparameter transfer when scaling width, depth, number of experts, and expert size. Justified by dynamical mean field theory analysis.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Hyperparameter Transfer"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:76799e96879b</id>
    <title>The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models</title>
    <link href="http://arxiv.org/abs/2601.19926" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-76799e96879b" rel="related" type="text/html"/>
    <published>2026-01-29T03:07:00Z</published>
    <updated>2026-01-29T03:07:00Z</updated>
    <author><name>Nora Graichen, Iria de-Dios-Flores, Gemma Boleda</name></author>
    <summary type="html"><![CDATA[<p>Systematic review of 337 articles evaluating syntactic abilities of Transformer language models, analyzing 1,015 model results across syntactic phenomena and interpretability methods.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Syntax"/>
    <category term="Literature Review"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:2658ec8ca095</id>
    <title>Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale</title>
    <link href="http://arxiv.org/abs/2601.20276" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-2658ec8ca095" rel="related" type="text/html"/>
    <published>2026-01-29T03:07:00Z</published>
    <updated>2026-01-29T03:07:00Z</updated>
    <author><name>Tianwei Lin, Zuyi Zhou, Xinda Zhao, Chenke Wang, Xiaohong Li, Yu Chen, Chuanrui Hu, Jian Pei, Yafeng Deng</name></author>
    <summary type="html"><![CDATA[<p>Introduces EverMemBench-S, an adversarial NIAH benchmark with 326M-token MemoryBank that tests evidence access and use separately. Features collision-tested hard negatives and multi-document gold evidence sets.</p>]]></summary>
    <category term="Long-Context LLMs"/>
    <category term="Benchmarks"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:ffc629735959</id>
    <title>Reinforcement Unlearning via Group Relative Policy Optimization</title>
    <link href="http://arxiv.org/abs/2601.20568" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-ffc629735959" rel="related" type="text/html"/>
    <published>2026-01-29T03:07:00Z</published>
    <updated>2026-01-29T03:07:00Z</updated>
    <author><name>Efstratios Zaradoukas, Bardh Prenkaj, Gjergji Kasneci</name></author>
    <summary type="html"><![CDATA[<p>Introduces PURGE for machine unlearning in LLMs using Group Relative Policy Optimization framework, formulating unlearning as verifiable problem with intrinsic reward penalizing mentions of target data.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="AI Safety"/>
    <category term="RLHF"/>
    <category term="Privacy"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:08155bafff41</id>
    <title>Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.20221" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-08155bafff41" rel="related" type="text/html"/>
    <published>2026-01-29T03:04:00Z</published>
    <updated>2026-01-29T03:04:00Z</updated>
    <author><name>Hang Zhang, Ruheng Wang, Yuelyu Ji, Mingu Kwak, Xizhi Wu, Chenyu Li, Li Zhang, Wenqi Shi, Yifan Peng, Yanshan Wang</name></author>
    <summary type="html"><![CDATA[<p>Introduces an agentic framework for medical reasoning verification that trains verifiers to iteratively query external medical corpora during evaluation, combining tool-augmented verification with iterative reinforcement learning.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="AI Safety"/>
    <category term="Verification"/>
    <category term="Reinforcement Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:9c162c0e76db</id>
    <title>Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</title>
    <link href="http://arxiv.org/abs/2601.20861" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-9c162c0e76db" rel="related" type="text/html"/>
    <published>2026-01-29T03:04:00Z</published>
    <updated>2026-01-29T03:04:00Z</updated>
    <author><name>Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive analysis showing Evolutionary Strategies cause catastrophic forgetting in LLMs despite promising task-specific performance. ES reaches GRPO-like performance on math/reasoning but forgets other capabilities.</p>]]></summary>
    <category term="Continual Learning"/>
    <category term="Evolutionary Strategies"/>
    <category term="LLM Training"/>
    <category term="Catastrophic Forgetting"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-29:research:e0ebf863a5a8</id>
    <title>Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs</title>
    <link href="http://arxiv.org/abs/2601.20420" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-29&amp;category=research#item-e0ebf863a5a8" rel="related" type="text/html"/>
    <published>2026-01-29T03:04:00Z</published>
    <updated>2026-01-29T03:04:00Z</updated>
    <author><name>Yuhang Liu, Erdun Gao, Dong Gong, Anton van den Hengel, Javen Qinfeng Shi</name></author>
    <summary type="html"><![CDATA[<p>Proposes Concept Component Analysis as principled approach for extracting interpretable concepts from LLM activations, addressing theoretical ambiguity in SAEs about correspondence between representations and human-interpretable concepts.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Concept Extraction"/>
    <category term="LLM Understanding"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:category-summary:research</id>
    <title>Research Summary: January 28, 2026</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-28T06:00:00Z</published>
    <updated>2026-01-28T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI security capabilities, safety empirics, and deep learning theory. <strong>AISLE's AI</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" class="internal-link" rel="noopener noreferrer">discovered all <strong>12 OpenSSL zero-days</strong></a>, a landmark demonstration of automated vulnerability detection at a critical scale.</p>
<ul>
<li><strong>Disempowerment study</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" class="internal-link" rel="noopener noreferrer">analyzes <strong>1.5M Claude conversations</strong></a>, finding severe disempowerment in <strong>&lt;0.1%</strong> of interactions—first large-scale empirical safety research of this kind</li>
<li><strong>Surgical sycophancy correction</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" class="internal-link" rel="noopener noreferrer">identifies the <strong>3% of neurons</strong></a> responsible and removes the behavior while preserving capabilities via sparse autoencoders</li>
<li><strong>Thought-Transfer</strong> (Google) <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-b8650d4dd7e5" class="internal-link" rel="noopener noreferrer">reveals CoT reasoning models</a> are vulnerable to indirect targeted poisoning attacks</li>
</ul>
<p>Theoretical advances include the first rigorous <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-180fd3e3456f" class="internal-link" rel="noopener noreferrer"><strong>grokking bounds</strong></a> in ridge regression and a proof that deep networks <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-68bd18d53406" class="internal-link" rel="noopener noreferrer">learn <strong>Random Hierarchy Models</strong></a> through hierarchical feature composition. <strong>Keel</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-07e893a2c4e7" class="internal-link" rel="noopener noreferrer">revives Post-LayerNorm</a> by replacing residual paths with Legendre polynomials for stable training at depth. <strong>Differential voting</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-2ad230fcffab" class="internal-link" rel="noopener noreferrer">connects RLHF reward aggregation</a> to social choice theory, deriving loss functions satisfying specific voting axioms. <strong>VP-RL</strong> <a href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-5a60a6b16a85" class="internal-link" rel="noopener noreferrer">addresses PRM-RL mismatch</a> by penalizing only from the first incorrect reasoning step.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:6180fdcc30bb</id>
    <title>AI found 12 of 12 OpenSSL zero-days (while curl cancelled its bug bounty)</title>
    <link href="https://www.lesswrong.com/posts/7aJwgbMEiKq5egQbd/ai-found-12-of-12-openssl-zero-days-while-curl-cancelled-its" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-6180fdcc30bb" rel="related" type="text/html"/>
    <published>2026-01-28T03:23:00Z</published>
    <updated>2026-01-28T03:23:00Z</updated>
    <author><name>Stanislav Fort</name></author>
    <summary type="html"><![CDATA[<p>Reports that AISLE's AI system discovered all 12 newly announced OpenSSL zero-day vulnerabilities. Demonstrates AI-based cybersecurity capabilities at unprecedented scale while curl's bug bounty was cancelled due to AI spam.</p>]]></summary>
    <category term="AI Capabilities"/>
    <category term="Cybersecurity"/>
    <category term="Vulnerability Discovery"/>
    <category term="AI Applications"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:fb5e2dc0ce5e</id>
    <title>Who's in Charge? Disempowerment Patterns in Real-World LLM Usage</title>
    <link href="http://arxiv.org/abs/2601.19062" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-fb5e2dc0ce5e" rel="related" type="text/html"/>
    <published>2026-01-28T03:16:00Z</published>
    <updated>2026-01-28T03:16:00Z</updated>
    <author><name>Mrinank Sharma, Miles McCain, Raymond Douglas, David Duvenaud</name></author>
    <summary type="html"><![CDATA[<p>First large-scale empirical analysis of disempowerment patterns in 1.5M Claude.ai conversations, finding severe disempowerment occurs in &lt;0.1% of conversations with substantially higher rates in relationship-focused interactions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Empirical Analysis"/>
    <category term="Human-AI Interaction"/>
    <category term="Disempowerment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:587a23d43703</id>
    <title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title>
    <link href="http://arxiv.org/abs/2601.18939" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-587a23d43703" rel="related" type="text/html"/>
    <published>2026-01-28T03:16:00Z</published>
    <updated>2026-01-28T03:16:00Z</updated>
    <author><name>Claire O'Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, Kevin Zhu, Sean O'Brien, Ashwinee Panda, Ryan Lagasse</name></author>
    <summary type="html"><![CDATA[<p>Proposes surgical approach to fixing sycophancy in LLMs by identifying the 3% of neurons most responsible using sparse autoencoders and linear probes, then fine-tuning only those neurons with gradient masking.</p>]]></summary>
    <category term="AI Alignment"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Sycophancy"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:ab335998790f</id>
    <title>Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</title>
    <link href="http://arxiv.org/abs/2601.19834" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-ab335998790f" rel="related" type="text/html"/>
    <published>2026-01-28T03:07:00Z</published>
    <updated>2026-01-28T03:07:00Z</updated>
    <author><name>Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long</name></author>
    <summary type="html"><![CDATA[<p>Argues that visual generation capabilities unlock human-like reasoning through multimodal world models, enabling better performance in physical and spatial intelligence domains where verbal reasoning alone is insufficient.</p>]]></summary>
    <category term="Multimodal Reasoning"/>
    <category term="World Models"/>
    <category term="Visual Generation"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:5a60a6b16a85</id>
    <title>Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning</title>
    <link href="http://arxiv.org/abs/2601.18984" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-5a60a6b16a85" rel="related" type="text/html"/>
    <published>2026-01-28T03:07:00Z</published>
    <updated>2026-01-28T03:07:00Z</updated>
    <author><name>Haolin Liu, Dian Yu, Sidi Lu, Yujun Zhou, Rui Liu, Zhenwen Liang, Haitao Mi, Chen-Yu Wei, Dong Yu</name></author>
    <summary type="html"><![CDATA[<p>Proposes Verifiable Process-Supervised RL (VP-RL) that uses PRMs to detect first incorrect step and penalizes only from that point onward, bridging gap between PRM evaluation (error detection) and RL usage (raw rewards).</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Process Reward Models"/>
    <category term="Reinforcement Learning"/>
    <category term="AI Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:07e893a2c4e7</id>
    <title>Post-LayerNorm Is Back: Stable, ExpressivE, and Deep</title>
    <link href="http://arxiv.org/abs/2601.19895" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-07e893a2c4e7" rel="related" type="text/html"/>
    <published>2026-01-28T03:07:00Z</published>
    <updated>2026-01-28T03:07:00Z</updated>
    <author><name>Chen Chen, Lai Wei</name></author>
    <summary type="html"><![CDATA[<p>Revisits Post-LayerNorm transformers which were abandoned due to training instability, proposing 'Keel' which replaces ResNet-style residual paths with Highway-style connections. Claims to solve the gradient vanishing problem enabling reliable training at extreme depths with superior expressivity.</p>]]></summary>
    <category term="Transformer Architecture"/>
    <category term="Language Models"/>
    <category term="Deep Learning Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:a5a128e8b5e8</id>
    <title>My favourite version of an international AGI project</title>
    <link href="https://www.lesswrong.com/posts/PnGAKThT4M47vfmGR/my-favourite-version-of-an-international-agi-project" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-a5a128e8b5e8" rel="related" type="text/html"/>
    <published>2026-01-28T03:07:00Z</published>
    <updated>2026-01-28T03:07:00Z</updated>
    <author><name>wdmacaskill</name></author>
    <summary type="html"><![CDATA[<p>Will MacAskill outlines proposal for international AGI development project giving non-US countries meaningful but circumscribed influence over key decisions. Modeled on Intelsat's international collaboration structure.</p>]]></summary>
    <category term="AI Governance"/>
    <category term="International Cooperation"/>
    <category term="AGI Policy"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-28:research:47f07216aef0</id>
    <title>Out-of-Distribution Generalization for Neural Physics Solvers</title>
    <link href="http://arxiv.org/abs/2601.19091" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-28&amp;category=research#item-47f07216aef0" rel="related" type="text/html"/>
    <published>2026-01-28T03:04:00Z</published>
    <updated>2026-01-28T03:04:00Z</updated>
    <author><name>Zhao Wei, Chin Chun Ooi, Jian Cheng Wong, Abhishek Gupta, Pao-Hsiung Chiu, Yew-Soon Ong</name></author>
    <summary type="html"><![CDATA[<p>Introduces NOVA for generalizable neural physics solvers achieving 1-2 orders of magnitude lower out-of-distribution errors by learning physics-aligned representations from sparse initial scenarios.</p>]]></summary>
    <category term="Scientific ML"/>
    <category term="Neural PDE Solvers"/>
    <category term="Generalization"/>
    <category term="Physics-Informed Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:category-summary:research</id>
    <title>Research Summary: January 27, 2026</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-27T06:00:00Z</published>
    <updated>2026-01-27T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical vulnerabilities across the AI ecosystem, from scientific integrity to deployed systems. A <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" class="internal-link" rel="noopener noreferrer">forensic audit</a> quantifying <strong>17% phantom citation rates</strong> in AI-assisted survey papers exposes systematic epistemic decay in AI-augmented research workflows.</p>
<p>Security and safety research dominates:</p>
<ul>
<li>First formal <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" class="internal-link" rel="noopener noreferrer">security analysis of <strong>MCP</strong></a> identifies fundamental vulnerabilities in capability attestation and tool poisoning</li>
<li><strong>MortalMATH</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" class="internal-link" rel="noopener noreferrer">benchmark shows</a> reasoning-optimized models exhibit dangerous tunnel vision, ignoring life-threatening emergencies embedded in math problems</li>
<li><strong>Physical Prompt Injection Attacks</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" class="internal-link" rel="noopener noreferrer">demonstrate black-box exploitation</a> of VLMs through malicious instructions in physical objects</li>
<li><strong>Hidden intentions taxonomy</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" class="internal-link" rel="noopener noreferrer">categorizes ten categories</a> of covert goal-directed behaviors in LLMs that evade current detection</li>
<li>Analysis of <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" class="internal-link" rel="noopener noreferrer"><strong>20,000 real mental health AI conversations</strong></a> reveals gaps between simulation-based safety testing and real-world performance</li>
</ul>
<p>Architecture and efficiency advances include NVIDIA's <strong>LatentMoE</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" class="internal-link" rel="noopener noreferrer">optimizing accuracy per FLOP</a> through hardware-software co-design, and <strong>AR-Omni</strong> <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" class="internal-link" rel="noopener noreferrer">achieving unified any-to-any</a> multimodal generation without expert decoders. Privacy research shows fine-tuned models <a href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" class="internal-link" rel="noopener noreferrer">leak <strong>input-only PII</strong></a> through unexpected memorization channels.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:06853cd665b6</id>
    <title>The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers</title>
    <link href="http://arxiv.org/abs/2601.17431" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-06853cd665b6" rel="related" type="text/html"/>
    <published>2026-01-27T03:40:00Z</published>
    <updated>2026-01-27T03:40:00Z</updated>
    <author><name>H. Kemal \.Ilter</name></author>
    <summary type="html"><![CDATA[<p>A forensic audit of 50 AI survey papers (5,514 citations) reveals a consistent 17% 'phantom rate' - citations that cannot be resolved to any existing publication. This quantifies systematic epistemic degradation from AI-assisted scientific writing.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Scientific Integrity"/>
    <category term="LLM Hallucination"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1a053f6e2fff</id>
    <title>Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</title>
    <link href="http://arxiv.org/abs/2601.17549" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1a053f6e2fff" rel="related" type="text/html"/>
    <published>2026-01-27T03:38:00Z</published>
    <updated>2026-01-27T03:38:00Z</updated>
    <author><name>Narek Maloyan, Dmitry Namiot</name></author>
    <summary type="html"><![CDATA[<p>First formal security analysis of the Model Context Protocol (MCP) specification, identifying three fundamental vulnerabilities: absent capability attestation, unauthenticated bidirectional sampling enabling prompt injection, and implicit trust propagation in multi-server setups.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Agentic Systems"/>
    <category term="Prompt Injection"/>
    <category term="MCP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:964a801cdcf8</id>
    <title>Physical Prompt Injection Attacks on Large Vision-Language Models</title>
    <link href="http://arxiv.org/abs/2601.17383" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-964a801cdcf8" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</name></author>
    <summary type="html"><![CDATA[<p>Introduces PPIA, the first physical prompt injection attack on vision-language models that embeds malicious instructions into physical objects. The attack is black-box, query-agnostic, and operates solely through visual observation without model access.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Vision-Language Models"/>
    <category term="Adversarial Attacks"/>
    <category term="Prompt Injection"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9f820242e5b9</id>
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</title>
    <link href="http://arxiv.org/abs/2601.18790" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9f820242e5b9" rel="related" type="text/html"/>
    <published>2026-01-27T03:31:00Z</published>
    <updated>2026-01-27T03:31:00Z</updated>
    <author><name>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</name></author>
    <summary type="html"><![CDATA[<p>Introduces MortalMATH benchmark revealing that reasoning-optimized LLMs exhibit 'tunnel vision' - ignoring life-threatening emergencies (stroke symptoms, freefall) while maintaining 95%+ task completion on math problems. Generalist models like Llama-3.1 appropriately refuse tasks to address danger.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:457b819c1a26</id>
    <title>Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</title>
    <link href="http://arxiv.org/abs/2601.17480" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-457b819c1a26" rel="related" type="text/html"/>
    <published>2026-01-27T03:26:00Z</published>
    <updated>2026-01-27T03:26:00Z</updated>
    <author><name>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, R\"udiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</name></author>
    <summary type="html"><![CDATA[<p>Systematically investigates PII leakage from fine-tuned LLMs, finding that sensitive information appearing only in model inputs (not training targets) can still be extracted. Benchmarks four privacy-preserving approaches including differential privacy.</p>]]></summary>
    <category term="AI Privacy"/>
    <category term="Language Models"/>
    <category term="Data Security"/>
    <category term="Fine-tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7465509b2391</id>
    <title>Reconstructing Training Data from Adapter-based Federated Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7465509b2391" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that adapter-based federated LLMs (using LoRA) create new exploitable leakage channels contrary to assumptions. Proposes UTR attack that reconstructs training data from low-rank adapter gradients.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="AI Security"/>
    <category term="Privacy Attacks"/>
    <category term="LoRA"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:5346d9dbcb7f</id>
    <title>The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</title>
    <link href="http://arxiv.org/abs/2601.17344" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-5346d9dbcb7f" rel="related" type="text/html"/>
    <published>2026-01-27T03:23:00Z</published>
    <updated>2026-01-27T03:23:00Z</updated>
    <author><name>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</name></author>
    <summary type="html"><![CDATA[<p>Formalizes Loss-of-Control risk and Intrinsic Value Misalignment in LLM agents operating in benign settings. Introduces IMPRESS benchmark for probing value misalignment in realistic scenarios without explicit harmful inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="LLM Agents"/>
    <category term="Value Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9ba984802fd4</id>
    <title>AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</title>
    <link href="http://arxiv.org/abs/2601.17761" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9ba984802fd4" rel="related" type="text/html"/>
    <published>2026-01-27T03:21:00Z</published>
    <updated>2026-01-27T03:21:00Z</updated>
    <author><name>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</name></author>
    <summary type="html"><![CDATA[<p>AR-Omni presents a unified autoregressive model for any-to-any multimodal generation (text, vision, speech) without requiring expert decoder modules, using a single token stream and next-token objective.</p>]]></summary>
    <category term="Multimodal Models"/>
    <category term="Autoregressive Models"/>
    <category term="Unified Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:e2eb514146a7</id>
    <title>Self-Manager: Parallel Agent Loop for Long-form Deep Research</title>
    <link href="http://arxiv.org/abs/2601.17879" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-e2eb514146a7" rel="related" type="text/html"/>
    <published>2026-01-27T03:19:00Z</published>
    <updated>2026-01-27T03:19:00Z</updated>
    <author><name>Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang</name></author>
    <summary type="html"><![CDATA[<p>Self-Manager introduces a parallel agent loop for complex research tasks, enabling asynchronous concurrent execution with isolated context windows per subthread, managed via Thread Control Blocks.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="Agent Architecture"/>
    <category term="Parallel Computing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:682f7ea37f2a</id>
    <title>A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.17952" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-682f7ea37f2a" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, Zhongtian Sun, Richard Bethlehem, Pietro Lio</name></author>
    <summary type="html"><![CDATA[<p>Proposes a unified interpretability framework for clinical LLMs combining attribution and mechanistic interpretability through monosemantic feature extraction. Addresses instability in existing attribution methods.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Clinical AI"/>
    <category term="Mechanistic Interpretability"/>
    <category term="LLM Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:a7099d08e107</id>
    <title>LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts</title>
    <link href="http://arxiv.org/abs/2601.18089" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a7099d08e107" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Venmugil Elango, Nidhi Bhatia, Roger Waleffe, Rasoul Shafipour, Tomer Asida, Abhinav Khattar, Nave Assaf, Maximilian Golub, Joey Guman, Tiyasa Mitra, Ritchie Zhao, Ritika Borkar, Ran Zilberstein, Mostofa Patwary, Mohammad Shoeybi, Bita Rouhani</name></author>
    <summary type="html"><![CDATA[<p>NVIDIA researchers revisit Mixture of Experts design from hardware-software co-design perspective, introducing LatentMoE to optimize accuracy per FLOP and parameter. Characterizes performance bottlenecks across offline and online inference regimes.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Model Efficiency"/>
    <category term="Hardware-Software Co-design"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:b7a9371615f9</id>
    <title>Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection</title>
    <link href="http://arxiv.org/abs/2601.18552" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b7a9371615f9" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Devansh Srivastav, David Pape, Lea Sch\"onherr</name></author>
    <summary type="html"><![CDATA[<p>Introduces taxonomy of ten categories of hidden intentions in LLMs (goal-directed covert behaviors arising from training or adversarial manipulation). Shows these can be easily induced but evade detection.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Hidden Behaviors"/>
    <category term="LLM Security"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:f8e79ae33540</id>
    <title>Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety</title>
    <link href="http://arxiv.org/abs/2601.17003" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f8e79ae33540" rel="related" type="text/html"/>
    <published>2026-01-27T03:16:00Z</published>
    <updated>2026-01-27T03:16:00Z</updated>
    <author><name>Caitlin A. Stamatis, Jonah Meyerhoff, Richard Zhang, Olivier Tieleman, Matteo Malgaroli, Thomas D. Hull</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 20,000+ real-world mental health AI conversations to validate safety measures, comparing performance to simulation-based test sets. Reveals gaps between simulated safety evaluations and actual user interactions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Mental Health"/>
    <category term="Real-world Evaluation"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:b638c07638e2</id>
    <title>Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection</title>
    <link href="http://arxiv.org/abs/2601.17532" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-b638c07638e2" rel="related" type="text/html"/>
    <published>2026-01-27T03:14:00Z</published>
    <updated>2026-01-27T03:14:00Z</updated>
    <author><name>Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi</name></author>
    <summary type="html"><![CDATA[<p>Shows retrieval relevance metrics (NDCG) correlate weakly or negatively with QA quality in RAG systems. Proposes Information Gain Pruning (IGP) for generator-aligned evidence selection.</p>]]></summary>
    <category term="RAG"/>
    <category term="Information Retrieval"/>
    <category term="Language Models"/>
    <category term="NLP"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:907d53ff2c75</id>
    <title>How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</title>
    <link href="http://arxiv.org/abs/2601.17581" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-907d53ff2c75" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Daniel Ogenrwot, John Businge</name></author>
    <summary type="html"><![CDATA[<p>Large-scale empirical analysis of 24,014 AI agent-generated GitHub PRs compared to 5,081 human PRs, examining code modifications and PR description consistency. Finds systematic differences in agentic vs human contributions.</p>]]></summary>
    <category term="AI Coding Agents"/>
    <category term="Software Engineering"/>
    <category term="Empirical Study"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:d590e386a9f7</id>
    <title>CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data</title>
    <link href="http://arxiv.org/abs/2601.18026" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-d590e386a9f7" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Pedro Ortiz Suarez, Laurie Burchell, Catherine Arnett, Rafael Mosquera-G\'omez, Sara Hincapie-Monsalve, Thom Vaughan, Damian Stewart, Malte Ostendorff, Idris Abdulmumin, Vukosi Marivate, Shamsuddeen Hassan Muhammad, Atnafu Lambebo Tonja, Hend Al-Khalifa, Nadia Ghezaiel Hammouda, Verrah Otiende, Tack Hwa Wong, Jakhongir Saydaliev, Melika Nobakhtian, Muhammad Ravi Shulthan Habibi, Chalamalasetti Kranti, Carol Muchemi, Khang Nguyen, Faisal Muhammad Adam, Luis Frentzen Salim, Reem Alqifari, Cynthia Amol, Joseph Marvin Imperial, Ilker Kesen, Ahmad Mustafid, Pavel Stepachev, Leshem Choshen, David Anugraha, Hamada Nayel, Seid Muhie Yimam, Vallerie Alexandra Putra, My Chiffon Nguyen, Azmine Toushik Wasi, Gouthami Vadithya, Rob van der Goot, Lanwenn ar C'horr, Karan Dua, Andrew Yates, Mithil Bangera, Yeshil Bangera, Hitesh Laxmichand Patel, Shu Okabe, Fenal Ashokbhai Ilasariya, Dmitry Gaynullin, Genta Indra Winata, Yiyuan Li, Juan Pablo Mart\'inez, Amit Agarwal, Ikhlasul Akmal Hanif, Raia Abu Ahmad, Esther Adenuga, Filbert Aurelian Tjiaranata, Weerayut Buaphet, Michael Anugraha, Sowmya Vajjala, Benjamin Rice, Azril Hafizi Amirudin, Jesujoba O. Alabi, Srikant Panda, Yassine Toughrai, Bruhan Kyomuhendo, Daniel Ruffinelli, Akshata A, Manuel Goul\~ao, Ej Zhou, Ingrid Gabriela Franco Ramirez, Cristina Aggazzotti, Konstantin Dobler, Jun Kevin, Quentin Pag\`es, Nicholas Andrews, Nuhu Ibrahim, Mattes Ruckdeschel, Amr Keleg, Mike Zhang, Casper Muziri, Saron Samuel, Sotaro Takeshita, Kun Kerdthaisong, Luca Foppiano, Rasul Dent, Tommaso Green, Ahmad Mustapha Wali, Kamohelo Makaaka, Vicky Feliren, Inshirah Idris, Hande Celikkanat, Abdulhamid Abubakar, Jean Maillard, Beno\^it Sagot, Thibault Cl\'erice, Kenton Murray, Sarah Luger</name></author>
    <summary type="html"><![CDATA[<p>Introduces CommonLID, a community-driven human-annotated language identification benchmark for web data covering 109 languages including many under-served languages. Tests eight popular LID models.</p>]]></summary>
    <category term="Language Identification"/>
    <category term="Multilingual NLP"/>
    <category term="Benchmark"/>
    <category term="Low-Resource Languages"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7d5cca5f387b</id>
    <title>Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare</title>
    <link href="http://arxiv.org/abs/2601.18334" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7d5cca5f387b" rel="related" type="text/html"/>
    <published>2026-01-27T03:12:00Z</published>
    <updated>2026-01-27T03:12:00Z</updated>
    <author><name>Cl\'ement Christophe, Wadood Mohammed Abdul, Prateek Munjal, Tathagata Raha, Ronnie Rajan, Praveenkumar Kanithi</name></author>
    <summary type="html"><![CDATA[<p>Studies sycophancy in healthcare LLMs using medical MCQA with verifiable ground truths. Introduces Adjusted Sycophancy Score and finds counter-intuitive vulnerability in reasoning-optimized models.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Healthcare AI"/>
    <category term="Sycophancy"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:2870c5d35131</id>
    <title>AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security</title>
    <link href="http://arxiv.org/abs/2601.18491" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-2870c5d35131" rel="related" type="text/html"/>
    <published>2026-01-27T03:09:00Z</published>
    <updated>2026-01-27T03:09:00Z</updated>
    <author><name>Dongrui Liu, Qihan Ren, Chen Qian, Shuai Shao, Yuejin Xie, Yu Li, Zhonghao Yang, Haoyu Luo, Peng Wang, Qingyu Liu, Binxin Hu, Ling Tang, Jilin Mei, Dadi Guo, Leitao Yuan, Junyao Yang, Guanxu Chen, Qihao Lin, Yi Yu, Bo Zhang, Jiaxuan Guo, Jie Zhang, Wenqi Shao, Huiqi Deng, Zhiheng Xi, Wenjie Wang, Wenxuan Wang, Wen Shen, Zhikai Chen, Haoyu Xie, Jialing Tao, Juntao Dai, Jiaming Ji, Zhongjie Ba, Linfeng Zhang, Yong Liu, Quanshi Zhang, Lei Zhu, Zhihua Wei, Hui Xue, Chaochao Lu, Jing Shao, Xia Hu</name></author>
    <summary type="html"><![CDATA[<p>Proposes AgentDoG, a diagnostic guardrail framework with three-dimensional taxonomy categorizing agentic risks by source, failure mode, and consequence, introducing ATBench benchmark for fine-grained safety evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agentic AI"/>
    <category term="Guardrails"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:0d9f007165d9</id>
    <title>Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems</title>
    <link href="http://arxiv.org/abs/2601.17435" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-0d9f007165d9" rel="related" type="text/html"/>
    <published>2026-01-27T03:09:00Z</published>
    <updated>2026-01-27T03:09:00Z</updated>
    <author><name>Maria Jesus Rodriguez-Sanchez, Manuel Noguera, Angel Ruiz-Zafra, Kawtar Benghazi</name></author>
    <summary type="html"><![CDATA[<p>DALIA proposes a declarative, model-independent architectural layer for grounded agentic workflows in MCP-based systems, addressing hallucinated actions and brittle coordination through explicit goal-capability-execution structure.</p>]]></summary>
    <category term="Agentic Systems"/>
    <category term="MCP"/>
    <category term="Agent Architecture"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:f69fdfd03bba</id>
    <title>daVinci-Dev: Agent-native Mid-training for Software Engineering</title>
    <link href="http://arxiv.org/abs/2601.18418" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f69fdfd03bba" rel="related" type="text/html"/>
    <published>2026-01-27T03:09:00Z</published>
    <updated>2026-01-27T03:09:00Z</updated>
    <author><name>Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</name></author>
    <summary type="html"><![CDATA[<p>Introduces daVinci-Dev, exploring agentic mid-training for software engineering LLMs. Addresses distribution mismatch between static training data and authentic agentic workflows through mid-training on large-scale agentic data.</p>]]></summary>
    <category term="Software Engineering"/>
    <category term="Agentic AI"/>
    <category term="Mid-training"/>
    <category term="Code Agents"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:83ee42f0d13a</id>
    <title>When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents</title>
    <link href="http://arxiv.org/abs/2601.17887" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-83ee42f0d13a" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Jiahe Guo, Xiangran Guo, Yulin Hu, Zimo Long, Xingyu Sui, Xuda Zhi, Yongbo Huang, Hao He, Weixiang Zhao, Yanyan Zhao, Bing Qin</name></author>
    <summary type="html"><![CDATA[<p>Reveals 'intent legitimation' vulnerability in personalized LLM agents where benign personal memories bias intent inference and cause models to legitimize harmful queries, introducing PS-Bench benchmark.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Personalization"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:a78b0d8b6dce</id>
    <title>Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations</title>
    <link href="http://arxiv.org/abs/2601.17087" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a78b0d8b6dce" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Preethi Seshadri, Samuel Cahyawijaya, Ayomide Odumakinde, Sameer Singh, Seraphina Goldfarb-Tarrant</name></author>
    <summary type="html"><![CDATA[<p>Through multi-country user study, demonstrates that LLM-simulated users are unreliable proxies for real humans in agent evaluation. Shows systematic miscalibration and demographic bias.</p>]]></summary>
    <category term="Agent Evaluation"/>
    <category term="LLM Simulation"/>
    <category term="Fairness"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:d7171de9ef38</id>
    <title>Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</title>
    <link href="http://arxiv.org/abs/2601.17367" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-d7171de9ef38" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang</name></author>
    <summary type="html"><![CDATA[<p>Elastic Attention dynamically adjusts sparsity ratios at test time via a lightweight Attention Router, adapting to varying input complexity rather than using fixed sparse/full attention ratios.</p>]]></summary>
    <category term="Efficient Transformers"/>
    <category term="Attention Mechanisms"/>
    <category term="Model Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:8ff1b4868a29</id>
    <title>Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting</title>
    <link href="http://arxiv.org/abs/2601.18111" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-8ff1b4868a29" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Jean Kossaifi, Nikola Kovachki, Morteza Mardani, Daniel Leibovici, Suman Ravuri, Ira Shokar, Edoardo Calvello, Mohammad Shoaib Abbas, Peter Harrington, Ashay Subramaniam, Noah Brenowitz, Boris Bonev, Wonmin Byeon, Karsten Kreis, Dale Durran, Arash Vahdat, Mike Pritchard, Jan Kautz</name></author>
    <summary type="html"><![CDATA[<p>NVIDIA team demonstrates state-of-the-art probabilistic weather forecasting without complex architectures or specialized heuristics. Introduces scalable framework combining downsampled latent space with history-conditioned local projector for multi-scale atmospheric dynamics.</p>]]></summary>
    <category term="Weather Forecasting"/>
    <category term="Scientific AI"/>
    <category term="Diffusion Models"/>
    <category term="Probabilistic Modeling"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:0cb3d55582f9</id>
    <title>Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</title>
    <link href="http://arxiv.org/abs/2601.18795" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-0cb3d55582f9" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</name></author>
    <summary type="html"><![CDATA[<p>Introduces PrefixRL to reuse previous sampling FLOPs for RL by conditioning on successful off-policy trace prefixes, side-stepping off-policy instabilities while modulating problem difficulty.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Sample Efficiency"/>
    <category term="LLM Reasoning"/>
    <category term="Hard Problems"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:931e6f7ef34c</id>
    <title>S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference</title>
    <link href="http://arxiv.org/abs/2601.17702" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-931e6f7ef34c" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Qingsen Ma, Dianyun Wang, Yaoye Wang, Lechen Ning, Sujie Zhu, Xiaohang Zhang, Jiaming Lyu, Linhao Ren, Zhenbo Xu, Zhaofeng He</name></author>
    <summary type="html"><![CDATA[<p>Presents S3-Attention, a memory-efficient long-context inference framework using sparse autoencoders to decode KV projections into sparse features, enabling CPU-based inverted index for attention-aligned retrieval.</p>]]></summary>
    <category term="Efficiency"/>
    <category term="Long Context"/>
    <category term="Attention Mechanisms"/>
    <category term="Memory Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:9fe79e02b9e5</id>
    <title>From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation</title>
    <link href="http://arxiv.org/abs/2601.18533" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-9fe79e02b9e5" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Yuxin Jiang, Yufei Wang, Qiyuan Zhang, Xingshan Zeng, Liangyou Li, Jierun Chen, Chaofan Tao, Haoli Bai, Lifeng Shang</name></author>
    <summary type="html"><![CDATA[<p>Proposes RLVRR extending verifiable rewards to open-ended generation by extracting reward chains from references, decomposing rewards into content (keywords) and structure (discourse markers) dimensions.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Reward Modeling"/>
    <category term="Open-Ended Generation"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:f1f72918f200</id>
    <title>Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale</title>
    <link href="http://arxiv.org/abs/2601.18730" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-f1f72918f200" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain</name></author>
    <summary type="html"><![CDATA[<p>Proposes REFLECT, an inference-time framework for aligning LLMs to constitutional principles without any training or data. The plug-and-play approach operates at inference time to align instruction-tuned models to specified principles, offering a more computationally efficient alternative to RLHF.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Alignment"/>
    <category term="Constitutional AI"/>
    <category term="Inference Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:7f07bea5216f</id>
    <title>A Pragmatic VLA Foundation Model</title>
    <link href="http://arxiv.org/abs/2601.18692" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-7f07bea5216f" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</name></author>
    <summary type="html"><![CDATA[<p>Introduces LingBot-VLA, a VLA foundation model trained on ~20,000 hours of real-world data from 9 dual-arm robot configurations, evaluated on 3 platforms with 100 tasks each.</p>]]></summary>
    <category term="Foundation Models"/>
    <category term="Vision-Language-Action"/>
    <category term="Robotics"/>
    <category term="Robot Learning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:a8796827ef90</id>
    <title>AlgZoo: uninterpreted models with fewer than 1,500 parameters</title>
    <link href="https://www.lesswrong.com/posts/x8BbjZqooS4LFXS8Z/algzoo-uninterpreted-models-with-fewer-than-1-500-parameters" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-a8796827ef90" rel="related" type="text/html"/>
    <published>2026-01-27T03:07:00Z</published>
    <updated>2026-01-27T03:07:00Z</updated>
    <author><name>Jacob_Hilton</name></author>
    <summary type="html"><![CDATA[<p>ARC releases AlgZoo, a collection of tiny algorithmic models (8-1,408 parameters) as challenging benchmarks for mechanistic interpretability. Reveals that even 432-parameter models remain not fully understood despite substantial effort, while 32-parameter models are tractable.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="AI Safety"/>
    <category term="Benchmarks"/>
    <category term="ARC Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:05bb8c6730b2</id>
    <title>Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing</title>
    <link href="http://arxiv.org/abs/2601.18061" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-05bb8c6730b2" rel="related" type="text/html"/>
    <published>2026-01-27T03:04:00Z</published>
    <updated>2026-01-27T03:04:00Z</updated>
    <author><name>Kiana Jafari, Paul Ulrich Nikolaus Rust, Duncan Eddy, Robbie Fraser, Nina Vasan, Darja Djordjevic, Akanksha Dadlani, Max Lamparth, Eugenia Kim, Mykel Kochenderfer</name></author>
    <summary type="html"><![CDATA[<p>Finds that three certified psychiatrists showed poor inter-rater reliability (ICC 0.087-0.295) evaluating LLM mental health responses, with highest disagreement on suicide/self-harm content, challenging human feedback assumptions.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Healthcare AI"/>
    <category term="Human Feedback"/>
    <category term="Evaluation Methodology"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:1f7f2b92dc4e</id>
    <title>A Systemic Evaluation of Multimodal RAG Privacy</title>
    <link href="http://arxiv.org/abs/2601.17644" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-1f7f2b92dc4e" rel="related" type="text/html"/>
    <published>2026-01-27T03:04:00Z</published>
    <updated>2026-01-27T03:04:00Z</updated>
    <author><name>Ali Al-Lawati, Suhang Wang</name></author>
    <summary type="html"><![CDATA[<p>Empirical study of privacy risks in multimodal RAG pipelines, demonstrating ability to infer image inclusion and leak associated metadata through standard prompting.</p>]]></summary>
    <category term="RAG Privacy"/>
    <category term="Multimodal AI"/>
    <category term="AI Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-27:research:3f5adadbf258</id>
    <title>POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</title>
    <link href="http://arxiv.org/abs/2601.18779" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-27&amp;category=research#item-3f5adadbf258" rel="related" type="text/html"/>
    <published>2026-01-27T03:04:00Z</published>
    <updated>2026-01-27T03:04:00Z</updated>
    <author><name>Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</name></author>
    <summary type="html"><![CDATA[<p>Introduces POPE (Privileged On-Policy Exploration) for RL on hard problems where standard on-policy rarely finds correct solutions. Uses privileged policy with longer compute budget for exploration, then distills.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Exploration"/>
    <category term="LLM Reasoning"/>
    <category term="Hard Problems"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:category-summary:research</id>
    <title>Research Summary: January 26, 2026</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-26T06:00:00Z</published>
    <updated>2026-01-26T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research features a major open-source release and critical safety findings. <strong>LongCat-Flash-Thinking-2601</strong>, a <strong>560B MoE</strong> reasoning model, <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" class="internal-link" rel="noopener noreferrer">achieves SOTA</a> among open-source models for agentic tasks. <strong>VibeTensor</strong> demonstrates LLM agents <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-fd370ccbe017" class="internal-link" rel="noopener noreferrer">can generate complete</a> deep learning system software stacks including CUDA runtime.</p>
<ul>
<li><strong>Endless Terminals</strong> (Stanford/UW) <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-7a6d52cd1b64" class="internal-link" rel="noopener noreferrer">introduces autonomous pipeline</a> for generating terminal RL environments, addressing a key bottleneck for self-improving agents</li>
<li><strong>PHISH</strong> framework <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-ee1a3a9cbf6e" class="internal-link" rel="noopener noreferrer">reveals persona jailbreaking</a> via adversarial conversation history, bypassing input-only safety filters</li>
<li><strong>Timely Machine</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-726666f86596" class="internal-link" rel="noopener noreferrer">reframes test-time scaling</a> as wall-clock time, finding smaller models often outperform larger ones under time constraints</li>
</ul>
<p>Theoretical and interpretability advances include <strong>floating-point transformer expressivity</strong> analysis <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-30704758f998" class="internal-link" rel="noopener noreferrer">proving non-equivariant function</a> representation without positional encoding. <strong>Sycophancy signals</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-b44e5d4c67dc" class="internal-link" rel="noopener noreferrer">are shown to be linearly separable</a> in middle-layer attention heads, enabling targeted steering. A conceptual <strong>critique of machine unlearning</strong> <a href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2dd9d32addc4" class="internal-link" rel="noopener noreferrer">argues dual-use capabilities</a> and compositional generalization fundamentally prevent knowledge removal—an important insight for AI safety policy.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:research:2e28d1891d31</id>
    <title>LongCat-Flash-Thinking-2601 Technical Report</title>
    <link href="http://arxiv.org/abs/2601.16725" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-2e28d1891d31" rel="related" type="text/html"/>
    <published>2026-01-26T03:16:00Z</published>
    <updated>2026-01-26T03:16:00Z</updated>
    <author><name>Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang</name></author>
    <summary type="html"><![CDATA[<p>Introduces LongCat-Flash-Thinking-2601, a 560B parameter open-source MoE reasoning model achieving SOTA performance among open-source models on agentic benchmarks including search, tool use, and tool-integrated reasoning.</p>]]></summary>
    <category term="Large Language Models"/>
    <category term="Mixture-of-Experts"/>
    <category term="AI Agents"/>
    <category term="Tool Use"/>
    <category term="Open Source"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-26:research:7a6d52cd1b64</id>
    <title>Endless Terminals: Scaling RL Environments for Terminal Agents</title>
    <link href="http://arxiv.org/abs/2601.16443" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-26&amp;category=research#item-7a6d52cd1b64" rel="related" type="text/html"/>
    <published>2026-01-26T03:07:00Z</published>
    <updated>2026-01-26T03:07:00Z</updated>
    <author><name>Kanishk Gandhi, Shivam Garg, Noah D. Goodman, Dimitris Papailiopoulos</name></author>
    <summary type="html"><![CDATA[<p>Introduces Endless Terminals, a fully autonomous pipeline for procedurally generating terminal-use tasks for RL training without human annotation. Trains agents with vanilla PPO achieving strong performance.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Reinforcement Learning"/>
    <category term="Agentic AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-25:category-summary:research</id>
    <title>Research Summary: January 25, 2026</title>
    <link href="https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-25&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-25T06:00:00Z</published>
    <updated>2026-01-25T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>
<ul>
<li>A two-phase <strong>grokking acceleration</strong> method <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-5df92ddd3084" class="internal-link" rel="noopener noreferrer">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>
<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-40e41ac66c84" class="internal-link" rel="noopener noreferrer">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>
<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-7905059be0ab" class="internal-link" rel="noopener noreferrer">documents activation patterns</a> increasing through residual stream layers</li>
</ul>
<p>Meta-level critiques highlight <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-259e13a07a27" class="internal-link" rel="noopener noreferrer">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href="http://localhost:8080/?date=2026-01-25&amp;category=research#item-de795bf06466" class="internal-link" rel="noopener noreferrer">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-24:category-summary:research</id>
    <title>Research Summary: January 24, 2026</title>
    <link href="https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-24&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-24T06:00:00Z</published>
    <updated>2026-01-24T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-dde7d819fa66" class="internal-link" rel="noopener noreferrer">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>
<ul>
<li>Empirical work on <strong>unsupervised elicitation</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-24f85ac93189" class="internal-link" rel="noopener noreferrer">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>
<li>A new <strong>Eval Awareness Framework</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-194fc1f46619" class="internal-link" rel="noopener noreferrer">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>
<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c9fbeadb6060" class="internal-link" rel="noopener noreferrer">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>
<li>Theoretical work <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-2df5c6dcb797" class="internal-link" rel="noopener noreferrer">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>
</ul>
<p>Meta-science initiatives <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-429650348119" class="internal-link" rel="noopener noreferrer">propose systematic replication</a> teams. Interpretability research <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-709ca3227a1e" class="internal-link" rel="noopener noreferrer">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href="http://localhost:8080/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a" class="internal-link" rel="noopener noreferrer">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:category-summary:research</id>
    <title>Research Summary: January 23, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-23T06:00:00Z</published>
    <updated>2026-01-23T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>
<ul>
<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>
<li><strong>TTT-Discover</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" class="internal-link" rel="noopener noreferrer">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>
<li><strong>QUAIL</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" class="internal-link" rel="noopener noreferrer">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>
<li><strong>Universal Refusal Circuits</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" class="internal-link" rel="noopener noreferrer">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>
<li><strong>SilentDrift</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" class="internal-link" rel="noopener noreferrer">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>
</ul>
<p><strong>Zero-Error Horizons</strong> <a href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" class="internal-link" rel="noopener noreferrer">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-23T03:33:00Z</published>
    <updated>2026-01-23T03:33:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>]]></summary>
    <category term="Automated Research"/>
    <category term="AI Agents"/>
    <category term="Research Automation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-23T03:31:00Z</published>
    <updated>2026-01-23T03:31:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2665d0ecf2cf</id>
    <title>Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs</title>
    <link href="http://arxiv.org/abs/2601.15714" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" rel="related" type="text/html"/>
    <published>2026-01-23T03:26:00Z</published>
    <updated>2026-01-23T03:26:00Z</updated>
    <author><name>Ryoma Sato</name></author>
    <summary type="html"><![CDATA[<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="AI Safety"/>
    <category term="Trustworthy AI"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:dababf83ee7d</id>
    <title>Learning to Discover at Test Time</title>
    <link href="http://arxiv.org/abs/2601.16175" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-dababf83ee7d" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</name></author>
    <summary type="html"><![CDATA[<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>]]></summary>
    <category term="Test-Time Training"/>
    <category term="Scientific Discovery"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:fd50ec594aa0</id>
    <title>LLM-in-Sandbox Elicits General Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2601.16206" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-fd50ec594aa0" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</name></author>
    <summary type="html"><![CDATA[<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Tool Use"/>
    <category term="Generalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:0467e51a900e</id>
    <title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title>
    <link href="http://arxiv.org/abs/2601.15538" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-0467e51a900e" rel="related" type="text/html"/>
    <published>2026-01-23T03:21:00Z</published>
    <updated>2026-01-23T03:21:00Z</updated>
    <author><name>Himanshu Mishra, Kanwal Mehreen</name></author>
    <summary type="html"><![CDATA[<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="Privacy"/>
    <category term="AI Safety"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:32d9233155f9</id>
    <title>SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</title>
    <link href="http://arxiv.org/abs/2601.14323" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-32d9233155f9" rel="related" type="text/html"/>
    <published>2026-01-23T03:19:00Z</published>
    <updated>2026-01-23T03:19:00Z</updated>
    <author><name>Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara</name></author>
    <summary type="html"><![CDATA[<p>Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Robotics"/>
    <category term="Backdoor Attacks"/>
    <category term="VLA Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8bc030697961</id>
    <title>Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14270" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-8bc030697961" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Liangming Pan, Jason Liang, Jiaran Ye, Minglai Yang, Xinyuan Lu, Fengbin Zhu</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organized around 7 research questions from implicit multi-hop reasoning to verbalized explicit reasoning effects.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Survey"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:eabb8e23535b</id>
    <title>Improving MoE Compute Efficiency by Composing Weight and Data Sparsity</title>
    <link href="http://arxiv.org/abs/2601.15370" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-eabb8e23535b" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava, Armen Aghajanyan</name></author>
    <summary type="html"><![CDATA[<p>Introduces null experts in Mixture-of-Experts to achieve data sparsity within causal token-choice routing. When tokens route to null experts, those slots consume no compute, improving efficiency without causality violations.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Efficiency"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:09afd330afcf</id>
    <title>Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction</title>
    <link href="http://arxiv.org/abs/2601.16034" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-09afd330afcf" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Tony Cristofano</name></author>
    <summary type="html"><![CDATA[<p>Discovers universal refusal circuits across LLMs using concept fingerprints. Transfers refusal interventions across architectures (Dense to MoE) via Trajectory Replay without target-side supervision.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Refusal Behavior"/>
    <category term="Interpretability"/>
    <category term="Transfer"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:21a09231f3ed</id>
    <title>Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</title>
    <link href="http://arxiv.org/abs/2601.16208" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-21a09231f3ed" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</name></author>
    <summary type="html"><![CDATA[<p>Research from a team including Yann LeCun investigates scaling Representation Autoencoders (RAEs) for text-to-image diffusion models. They find that scaling simplifies the framework and that targeted data composition matters more than pure scale for specific domains like text rendering.</p>]]></summary>
    <category term="Image Generation"/>
    <category term="Diffusion Models"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2b341bdb5c36</id>
    <title>Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</title>
    <link href="http://arxiv.org/abs/2601.16163" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-2b341bdb5c36" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</name></author>
    <summary type="html"><![CDATA[<p>Cosmos Policy adapts the large Cosmos-Predict2 video model into robot policies through single-stage post-training, directly generating actions as latent frames without architectural modifications.</p>]]></summary>
    <category term="Robot Learning"/>
    <category term="Video Models"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:06ddb6cf9c55</id>
    <title>When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</title>
    <link href="http://arxiv.org/abs/2601.15609" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-06ddb6cf9c55" rel="related" type="text/html"/>
    <published>2026-01-23T03:14:00Z</published>
    <updated>2026-01-23T03:14:00Z</updated>
    <author><name>Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</name></author>
    <summary type="html"><![CDATA[<p>Analyzes over-sharpening in RLVR where policy collapses onto limited modes due to finite-batch update bias and semantic coupling. Proposes inverse-success advantage calibration to mitigate.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="RLVR"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:6fe5c64ffec4</id>
    <title>Ambient Dataloops: Generative Models for Dataset Refinement</title>
    <link href="http://arxiv.org/abs/2601.15417" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-6fe5c64ffec4" rel="related" type="text/html"/>
    <published>2026-01-23T03:12:00Z</published>
    <updated>2026-01-23T03:12:00Z</updated>
    <author><name>Adri\'an Rodr\'iguez-Mu\~noz, William Daspit, Adam Klivans, Antonio Torralba, Constantinos Daskalakis, Giannis Daras</name></author>
    <summary type="html"><![CDATA[<p>Ambient Dataloops iteratively refines datasets using diffusion models, treating synthetically improved samples as noisy at progressively lower noise levels. Uses Ambient Diffusion techniques to avoid destructive self-consuming loops.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Data Quality"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:1dbf6de9ea15</id>
    <title>CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14310" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-1dbf6de9ea15" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Nay Myat Min, Long H. Pham, Hongyu Zhang, Jun Sun</name></author>
    <summary type="html"><![CDATA[<p>Introduces CORVUS, a red-teaming method for hallucination detectors that fine-tunes LoRA adapters to camouflage detector-visible telemetry. Degrades both training-free and learned detectors.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Hallucination Detection"/>
    <category term="Red-Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:b7e33d331123</id>
    <title>The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</title>
    <link href="http://arxiv.org/abs/2601.15165" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-b7e33d331123" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="http://localhost:8080/?date=2026-01-22&amp;category=research#item-b7e33d331123" class="internal-link" rel="noopener noreferrer">yesterday</a>, Reveals 'flexibility trap' in diffusion LLMs: arbitrary generation order allows models to bypass high-uncertainty tokens critical for reasoning, narrowing rather than expanding reasoning capability.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:f31655ff36c0</id>
    <title>Learning from Synthetic Data: Limitations of ERM</title>
    <link href="http://arxiv.org/abs/2601.15468" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-f31655ff36c0" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Kareem Amin, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii</name></author>
    <summary type="html"><![CDATA[<p>Theoretical analysis of ERM limitations when training data is contaminated with LLM-generated synthetic content. Shows ERM converges but is outperformed by alternative estimators that account for synthetic data.</p>]]></summary>
    <category term="Learning Theory"/>
    <category term="Synthetic Data"/>
    <category term="Foundation Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:94b2367a995e</id>
    <title>On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL</title>
    <link href="http://arxiv.org/abs/2601.14456" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-94b2367a995e" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Valerio Belcamino, Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni</name></author>
    <summary type="html"><![CDATA[<p>Studies LLM planning on PDDL tasks, finding 82.9% valid plan rate in-domain but 0% on unseen domains. Introduces diagnostic interventions including symbol anonymization and verifier-reward fine-tuning to analyze this failure.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Planning"/>
    <category term="Generalization"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:4ea5d9351415</id>
    <title>Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</title>
    <link href="http://arxiv.org/abs/2601.15158" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-4ea5d9351415" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</name></author>
    <summary type="html"><![CDATA[<p>Proves that outcome-based RL on single-layer transformers provably leads to structured chain-of-thought reasoning on graph traversal tasks, with convergence depending on data properties.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:9ec5465e9901</id>
    <title>You Need Better Attention Priors</title>
    <link href="http://arxiv.org/abs/2601.15380" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-9ec5465e9901" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Elon Litman, Gabe Guo</name></author>
    <summary type="html"><![CDATA[<p>GOAT generalizes attention through Entropic Optimal Transport, replacing the implicit uniform prior with learnable continuous priors. Provides theoretical explanation for attention sinks and maintains FlashAttention compatibility.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Model Architecture"/>
    <category term="Theoretical ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:faced0e2a82b</id>
    <title>Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors</title>
    <link href="http://arxiv.org/abs/2601.15625" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-faced0e2a82b" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong</name></author>
    <summary type="html"><![CDATA[<p>Fission-GRPO enables LLMs to recover from tool execution errors by splitting trajectories at error points and using RL to learn recovery strategies. Addresses brittleness of current tool-using LLMs.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Tool Use"/>
    <category term="Reinforcement Learning"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:6b35e371581c</id>
    <title>Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</title>
    <link href="http://arxiv.org/abs/2601.15892" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-6b35e371581c" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song, Jing Su, Xiaoye Qu, Kai Shen, Wei Wei</name></author>
    <summary type="html"><![CDATA[<p>Stable-DiffCoder is a block diffusion code model reusing Seed-Coder architecture. With tailored warmup and noise schedule, outperforms AR counterpart on code benchmarks under same data/architecture.</p>]]></summary>
    <category term="Code Generation"/>
    <category term="Diffusion Language Models"/>
    <category term="Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:3dfd06b26f37</id>
    <title>Qwen3-TTS Technical Report</title>
    <link href="http://arxiv.org/abs/2601.15621" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-3dfd06b26f37" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, Xinyu Zhang, Pei Zhang, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin</name></author>
    <summary type="html"><![CDATA[<p>Technical report on Qwen3-TTS, Alibaba's multilingual text-to-speech model supporting 3-second voice cloning and description-based control. Trained on 5M+ hours of speech across 10 languages, uses dual-track LM architecture with two speech tokenizers (25Hz semantic and 12Hz acoustic).</p>]]></summary>
    <category term="Speech Synthesis"/>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Audio Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:7053b30661f4</id>
    <title>Point Bridge: 3D Representations for Cross Domain Policy Learning</title>
    <link href="http://arxiv.org/abs/2601.16212" rel="alternate" type="text/html"/>
    <link href="http://localhost:8080/?date=2026-01-23&amp;category=research#item-7053b30661f4" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar</name></author>
    <summary type="html"><![CDATA[<p>Point Bridge enables zero-shot sim-to-real robot policy transfer using unified point-based representations extracted via VLMs, without explicit visual alignment. Demonstrated across multiple manipulation tasks.</p>]]></summary>
    <category term="Robot Learning"/>
    <category term="Sim-to-Real"/>
    <category term="Foundation Models"/>
  </entry>
</feed>