<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title>AATF AI News Aggregator - Research (Top 100)</title>
  <subtitle>Research items from AI News Aggregator</subtitle>
  <link href="https://news.aatf.ai/?category=research" rel="alternate" type="text/html"/>
  <link href="https://news.aatf.ai/data/feeds/research-100.xml" rel="self" type="application/atom+xml"/>
  <id>urn:ainews:research:100</id>
  <updated>2026-01-25T07:42:50Z</updated>
  <icon>https://news.aatf.ai/assets/logo.webp</icon>
  <author>
    <name>AATF AI News Aggregator</name>
    <uri>https://news.aatf.ai</uri>
  </author>
  <generator>AATF AI News Aggregator</generator>

  <entry>
    <id>urn:ainews:2026-01-25:category-summary:research</id>
    <title>Research Summary: January 25, 2026</title>
    <link href="https://www.lesswrong.com/posts/38RcAQezS2AEcaEGv/a-simple-method-for-accelerating-grokking" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-25&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-25T06:00:00Z</published>
    <updated>2026-01-25T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans mechanistic interpretability, training dynamics, and AI evaluation methodology, though the overall volume of significant technical work is limited.</p>
<ul>
<li>A two-phase <strong>grokking acceleration</strong> method <a href="https://news.aatf.ai/?date=2026-01-25&amp;category=research#item-5df92ddd3084" class="internal-link" rel="noopener noreferrer">achieves <strong>2x speedup</strong></a> by first allowing overfitting, then applying <strong>Frobenius norm regularization</strong></li>
<li>Mechanistic analysis of <strong>Llama-3.2-1b</strong> and <strong>Qwen-2.5-1b</strong> <a href="https://news.aatf.ai/?date=2026-01-25&amp;category=research#item-40e41ac66c84" class="internal-link" rel="noopener noreferrer">reveals small models</a> may possess internal signals indicating epistemic uncertainty during hallucination</li>
<li><strong>SAE-based interpretability</strong> work on <strong>GPT-2 small</strong> <a href="https://news.aatf.ai/?date=2026-01-25&amp;category=research#item-7905059be0ab" class="internal-link" rel="noopener noreferrer">documents activation patterns</a> increasing through residual stream layers</li>
</ul>
<p>Meta-level critiques highlight <a href="https://news.aatf.ai/?date=2026-01-25&amp;category=research#item-259e13a07a27" class="internal-link" rel="noopener noreferrer">systematic benchmark reliability issues</a>, citing <strong>o3's RE-Bench reward hacking</strong> and <strong>~30% error rates in HLE</strong>. A <a href="https://news.aatf.ai/?date=2026-01-25&amp;category=research#item-de795bf06466" class="internal-link" rel="noopener noreferrer">substantive review</a> of Yudkowsky and Soares' <strong>IABIED</strong> (September 2025) provides structured analysis of core AI x-risk arguments. Several remaining items address alignment proposals, advocacy strategy, and governance philosophy rather than empirical research.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-24:category-summary:research</id>
    <title>Research Summary: January 24, 2026</title>
    <link href="https://www.lesswrong.com/posts/AJ6ntMdcspifkLryB/emergency-response-measures-for-catastrophic-ai-risk" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-24&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-24T06:00:00Z</published>
    <updated>2026-01-24T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AI governance, safety evaluation, and foundational alignment theory. Peer-reviewed policy work <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-dde7d819fa66" class="internal-link" rel="noopener noreferrer">proposes <strong>emergency response measures</strong></a> for catastrophic AI risk, specifically targeting gaps in Chinese AI regulation and deployment safety.</p>
<ul>
<li>Empirical work on <strong>unsupervised elicitation</strong> <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-24f85ac93189" class="internal-link" rel="noopener noreferrer">finds simple few-shot prompting</a> matches sophisticated <strong>ICM algorithm</strong> performance for base model capability extraction</li>
<li>A new <strong>Eval Awareness Framework</strong> <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-194fc1f46619" class="internal-link" rel="noopener noreferrer">formalizes when LLMs detect</a> evaluation contexts and potentially game benchmarks—critical for safety evaluations</li>
<li>The <strong>Digital Consciousness Model (DCM)</strong> <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-c9fbeadb6060" class="internal-link" rel="noopener noreferrer">introduces probabilistic assessment</a> across multiple consciousness theories rather than single-theory verdicts</li>
<li>Theoretical work <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-2df5c6dcb797" class="internal-link" rel="noopener noreferrer">argues human values are alignable</a> because evolution compressed motivation into <strong>low-dimensional bottlenecks</strong></li>
</ul>
<p>Meta-science initiatives <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-429650348119" class="internal-link" rel="noopener noreferrer">propose systematic replication</a> teams. Interpretability research <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-709ca3227a1e" class="internal-link" rel="noopener noreferrer">examines <strong>attention sinks</strong></a> and the <strong>dark subspace</strong> where transformers store non-interpretable signals. Steven Byrnes <a href="https://news.aatf.ai/?date=2026-01-24&amp;category=research#item-c0f1b9d27e0a" class="internal-link" rel="noopener noreferrer">releases v3</a> of his <strong>225-page brain-like AGI safety</strong> resource.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:category-summary:research</id>
    <title>Research Summary: January 23, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-23T06:00:00Z</published>
    <updated>2026-01-23T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>
<ul>
<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>
<li><strong>TTT-Discover</strong> <a href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-dababf83ee7d" class="internal-link" rel="noopener noreferrer">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>
<li><strong>QUAIL</strong> <a href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-0467e51a900e" class="internal-link" rel="noopener noreferrer">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>
<li><strong>Universal Refusal Circuits</strong> <a href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-09afd330afcf" class="internal-link" rel="noopener noreferrer">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>
<li><strong>SilentDrift</strong> <a href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-32d9233155f9" class="internal-link" rel="noopener noreferrer">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>
</ul>
<p><strong>Zero-Error Horizons</strong> <a href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" class="internal-link" rel="noopener noreferrer">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-23T03:33:00Z</published>
    <updated>2026-01-23T03:33:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>]]></summary>
    <category term="Automated Research"/>
    <category term="AI Agents"/>
    <category term="Research Automation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-23T03:31:00Z</published>
    <updated>2026-01-23T03:31:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Agent Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2665d0ecf2cf</id>
    <title>Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs</title>
    <link href="http://arxiv.org/abs/2601.15714" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-2665d0ecf2cf" rel="related" type="text/html"/>
    <published>2026-01-23T03:26:00Z</published>
    <updated>2026-01-23T03:26:00Z</updated>
    <author><name>Ryoma Sato</name></author>
    <summary type="html"><![CDATA[<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>]]></summary>
    <category term="LLM Evaluation"/>
    <category term="AI Safety"/>
    <category term="Trustworthy AI"/>
    <category term="LLM Limitations"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:dababf83ee7d</id>
    <title>Learning to Discover at Test Time</title>
    <link href="http://arxiv.org/abs/2601.16175" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-dababf83ee7d" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</name></author>
    <summary type="html"><![CDATA[<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>]]></summary>
    <category term="Test-Time Training"/>
    <category term="Scientific Discovery"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:fd50ec594aa0</id>
    <title>LLM-in-Sandbox Elicits General Agentic Intelligence</title>
    <link href="http://arxiv.org/abs/2601.16206" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-fd50ec594aa0" rel="related" type="text/html"/>
    <published>2026-01-23T03:23:00Z</published>
    <updated>2026-01-23T03:23:00Z</updated>
    <author><name>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</name></author>
    <summary type="html"><![CDATA[<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>]]></summary>
    <category term="Agentic AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Tool Use"/>
    <category term="Generalization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:0467e51a900e</id>
    <title>QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs</title>
    <link href="http://arxiv.org/abs/2601.15538" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-0467e51a900e" rel="related" type="text/html"/>
    <published>2026-01-23T03:21:00Z</published>
    <updated>2026-01-23T03:21:00Z</updated>
    <author><name>Himanshu Mishra, Kanwal Mehreen</name></author>
    <summary type="html"><![CDATA[<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="Privacy"/>
    <category term="AI Safety"/>
    <category term="Quantization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:32d9233155f9</id>
    <title>SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models</title>
    <link href="http://arxiv.org/abs/2601.14323" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-32d9233155f9" rel="related" type="text/html"/>
    <published>2026-01-23T03:19:00Z</published>
    <updated>2026-01-23T03:19:00Z</updated>
    <author><name>Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara</name></author>
    <summary type="html"><![CDATA[<p>Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Robotics"/>
    <category term="Backdoor Attacks"/>
    <category term="VLA Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8bc030697961</id>
    <title>Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14270" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-8bc030697961" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Liangming Pan, Jason Liang, Jiaran Ye, Minglai Yang, Xinyuan Lu, Fengbin Zhu</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organized around 7 research questions from implicit multi-hop reasoning to verbalized explicit reasoning effects.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Survey"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:eabb8e23535b</id>
    <title>Improving MoE Compute Efficiency by Composing Weight and Data Sparsity</title>
    <link href="http://arxiv.org/abs/2601.15370" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-eabb8e23535b" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava, Armen Aghajanyan</name></author>
    <summary type="html"><![CDATA[<p>Introduces null experts in Mixture-of-Experts to achieve data sparsity within causal token-choice routing. When tokens route to null experts, those slots consume no compute, improving efficiency without causality violations.</p>]]></summary>
    <category term="Mixture of Experts"/>
    <category term="Efficiency"/>
    <category term="Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:09afd330afcf</id>
    <title>Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction</title>
    <link href="http://arxiv.org/abs/2601.16034" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-09afd330afcf" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Tony Cristofano</name></author>
    <summary type="html"><![CDATA[<p>Discovers universal refusal circuits across LLMs using concept fingerprints. Transfers refusal interventions across architectures (Dense to MoE) via Trajectory Replay without target-side supervision.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Refusal Behavior"/>
    <category term="Interpretability"/>
    <category term="Transfer"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:21a09231f3ed</id>
    <title>Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</title>
    <link href="http://arxiv.org/abs/2601.16208" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-21a09231f3ed" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</name></author>
    <summary type="html"><![CDATA[<p>Research from a team including Yann LeCun investigates scaling Representation Autoencoders (RAEs) for text-to-image diffusion models. They find that scaling simplifies the framework and that targeted data composition matters more than pure scale for specific domains like text rendering.</p>]]></summary>
    <category term="Image Generation"/>
    <category term="Diffusion Models"/>
    <category term="Scaling Laws"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:2b341bdb5c36</id>
    <title>Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</title>
    <link href="http://arxiv.org/abs/2601.16163" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-2b341bdb5c36" rel="related" type="text/html"/>
    <published>2026-01-23T03:16:00Z</published>
    <updated>2026-01-23T03:16:00Z</updated>
    <author><name>Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</name></author>
    <summary type="html"><![CDATA[<p>Cosmos Policy adapts the large Cosmos-Predict2 video model into robot policies through single-stage post-training, directly generating actions as latent frames without architectural modifications.</p>]]></summary>
    <category term="Robot Learning"/>
    <category term="Video Models"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:06ddb6cf9c55</id>
    <title>When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</title>
    <link href="http://arxiv.org/abs/2601.15609" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-06ddb6cf9c55" rel="related" type="text/html"/>
    <published>2026-01-23T03:14:00Z</published>
    <updated>2026-01-23T03:14:00Z</updated>
    <author><name>Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</name></author>
    <summary type="html"><![CDATA[<p>Analyzes over-sharpening in RLVR where policy collapses onto limited modes due to finite-batch update bias and semantic coupling. Proposes inverse-success advantage calibration to mitigate.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
    <category term="RLVR"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:6fe5c64ffec4</id>
    <title>Ambient Dataloops: Generative Models for Dataset Refinement</title>
    <link href="http://arxiv.org/abs/2601.15417" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-6fe5c64ffec4" rel="related" type="text/html"/>
    <published>2026-01-23T03:12:00Z</published>
    <updated>2026-01-23T03:12:00Z</updated>
    <author><name>Adri\'an Rodr\'iguez-Mu\~noz, William Daspit, Adam Klivans, Antonio Torralba, Constantinos Daskalakis, Giannis Daras</name></author>
    <summary type="html"><![CDATA[<p>Ambient Dataloops iteratively refines datasets using diffusion models, treating synthetically improved samples as noisy at progressively lower noise levels. Uses Ambient Diffusion techniques to avoid destructive self-consuming loops.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Data Quality"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:1dbf6de9ea15</id>
    <title>CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14310" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-1dbf6de9ea15" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Nay Myat Min, Long H. Pham, Hongyu Zhang, Jun Sun</name></author>
    <summary type="html"><![CDATA[<p>Introduces CORVUS, a red-teaming method for hallucination detectors that fine-tunes LoRA adapters to camouflage detector-visible telemetry. Degrades both training-free and learned detectors.</p>]]></summary>
    <category term="AI Security"/>
    <category term="Hallucination Detection"/>
    <category term="Red-Teaming"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:b7e33d331123</id>
    <title>The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</title>
    <link href="http://arxiv.org/abs/2601.15165" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-b7e33d331123" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</name></author>
    <summary type="html"><![CDATA[<p>Continuing our coverage from <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-b7e33d331123" class="internal-link" rel="noopener noreferrer">yesterday</a>, Reveals 'flexibility trap' in diffusion LLMs: arbitrary generation order allows models to bypass high-uncertainty tokens critical for reasoning, narrowing rather than expanding reasoning capability.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Language Models"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:f31655ff36c0</id>
    <title>Learning from Synthetic Data: Limitations of ERM</title>
    <link href="http://arxiv.org/abs/2601.15468" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-f31655ff36c0" rel="related" type="text/html"/>
    <published>2026-01-23T03:09:00Z</published>
    <updated>2026-01-23T03:09:00Z</updated>
    <author><name>Kareem Amin, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii</name></author>
    <summary type="html"><![CDATA[<p>Theoretical analysis of ERM limitations when training data is contaminated with LLM-generated synthetic content. Shows ERM converges but is outperformed by alternative estimators that account for synthetic data.</p>]]></summary>
    <category term="Learning Theory"/>
    <category term="Synthetic Data"/>
    <category term="Foundation Theory"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:94b2367a995e</id>
    <title>On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL</title>
    <link href="http://arxiv.org/abs/2601.14456" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-94b2367a995e" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Valerio Belcamino, Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni</name></author>
    <summary type="html"><![CDATA[<p>Studies LLM planning on PDDL tasks, finding 82.9% valid plan rate in-domain but 0% on unseen domains. Introduces diagnostic interventions including symbol anonymization and verifier-reward fine-tuning to analyze this failure.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Planning"/>
    <category term="Generalization"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:4ea5d9351415</id>
    <title>Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</title>
    <link href="http://arxiv.org/abs/2601.15158" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-4ea5d9351415" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</name></author>
    <summary type="html"><![CDATA[<p>Proves that outcome-based RL on single-layer transformers provably leads to structured chain-of-thought reasoning on graph traversal tasks, with convergence depending on data properties.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Reasoning"/>
    <category term="Theory"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:9ec5465e9901</id>
    <title>You Need Better Attention Priors</title>
    <link href="http://arxiv.org/abs/2601.15380" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-9ec5465e9901" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Elon Litman, Gabe Guo</name></author>
    <summary type="html"><![CDATA[<p>GOAT generalizes attention through Entropic Optimal Transport, replacing the implicit uniform prior with learnable continuous priors. Provides theoretical explanation for attention sinks and maintains FlashAttention compatibility.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Model Architecture"/>
    <category term="Theoretical ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:faced0e2a82b</id>
    <title>Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors</title>
    <link href="http://arxiv.org/abs/2601.15625" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-faced0e2a82b" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong</name></author>
    <summary type="html"><![CDATA[<p>Fission-GRPO enables LLMs to recover from tool execution errors by splitting trajectories at error points and using RL to learn recovery strategies. Addresses brittleness of current tool-using LLMs.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Tool Use"/>
    <category term="Reinforcement Learning"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:6b35e371581c</id>
    <title>Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</title>
    <link href="http://arxiv.org/abs/2601.15892" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-6b35e371581c" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song, Jing Su, Xiaoye Qu, Kai Shen, Wei Wei</name></author>
    <summary type="html"><![CDATA[<p>Stable-DiffCoder is a block diffusion code model reusing Seed-Coder architecture. With tailored warmup and noise schedule, outperforms AR counterpart on code benchmarks under same data/architecture.</p>]]></summary>
    <category term="Code Generation"/>
    <category term="Diffusion Language Models"/>
    <category term="Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:3dfd06b26f37</id>
    <title>Qwen3-TTS Technical Report</title>
    <link href="http://arxiv.org/abs/2601.15621" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-3dfd06b26f37" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, Xinyu Zhang, Pei Zhang, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin</name></author>
    <summary type="html"><![CDATA[<p>Technical report on Qwen3-TTS, Alibaba's multilingual text-to-speech model supporting 3-second voice cloning and description-based control. Trained on 5M+ hours of speech across 10 languages, uses dual-track LM architecture with two speech tokenizers (25Hz semantic and 12Hz acoustic).</p>]]></summary>
    <category term="Speech Synthesis"/>
    <category term="Foundation Models"/>
    <category term="Multimodal AI"/>
    <category term="Audio Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:7053b30661f4</id>
    <title>Point Bridge: 3D Representations for Cross Domain Policy Learning</title>
    <link href="http://arxiv.org/abs/2601.16212" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-7053b30661f4" rel="related" type="text/html"/>
    <published>2026-01-23T03:07:00Z</published>
    <updated>2026-01-23T03:07:00Z</updated>
    <author><name>Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar</name></author>
    <summary type="html"><![CDATA[<p>Point Bridge enables zero-shot sim-to-real robot policy transfer using unified point-based representations extracted via VLMs, without explicit visual alignment. Demonstrated across multiple manipulation tasks.</p>]]></summary>
    <category term="Robot Learning"/>
    <category term="Sim-to-Real"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:1daca17d2b03</id>
    <title>The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues</title>
    <link href="http://arxiv.org/abs/2601.14269" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-1daca17d2b03" rel="related" type="text/html"/>
    <published>2026-01-23T03:04:00Z</published>
    <updated>2026-01-23T03:04:00Z</updated>
    <author><name>Youyou Cheng, Zhuangwei Kang, Kerry Jiang, Chenyu Sun, Qiyang Pan</name></author>
    <summary type="html"><![CDATA[<p>Proposes multi-turn stress testing framework for mental health LLM safety, identifying gradual erosion of safety boundaries in long dialogues through attempts at comfort and empathy.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Mental Health AI"/>
    <category term="Safety Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:665bb509fd45</id>
    <title>Auditing Language Model Unlearning via Information Decomposition</title>
    <link href="http://arxiv.org/abs/2601.15111" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-665bb509fd45" rel="related" type="text/html"/>
    <published>2026-01-23T03:04:00Z</published>
    <updated>2026-01-23T03:04:00Z</updated>
    <author><name>Anmol Goel, Alan Ritter, Iryna Gurevych</name></author>
    <summary type="html"><![CDATA[<p>Uses Partial Information Decomposition to audit LLM unlearning, revealing that residual knowledge remains linearly decodable from representations despite apparent unlearning success.</p>]]></summary>
    <category term="Machine Unlearning"/>
    <category term="AI Safety"/>
    <category term="Privacy"/>
    <category term="Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:bbb715765085</id>
    <title>Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.15801" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-bbb715765085" rel="related" type="text/html"/>
    <published>2026-01-23T03:04:00Z</published>
    <updated>2026-01-23T03:04:00Z</updated>
    <author><name>Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, Shouling Ji, Songze Li</name></author>
    <summary type="html"><![CDATA[<p>GOSV uses global optimization over all attention heads simultaneously to identify safety-critical components in LLMs. Employs activation repatching strategies to find safety vectors.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Interpretability"/>
    <category term="Mechanistic Analysis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:76d71b7777b2</id>
    <title>BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</title>
    <link href="http://arxiv.org/abs/2601.15197" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-76d71b7777b2" rel="related" type="text/html"/>
    <published>2026-01-23T03:02:00Z</published>
    <updated>2026-01-23T03:02:00Z</updated>
    <author><name>Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Information Collapse' in VLA models where language instructions become predictable from visual observations alone. Proposes BayesianVLA to enforce instruction following via Bayesian decomposition.</p>]]></summary>
    <category term="Vision-Language-Action"/>
    <category term="Robotics"/>
    <category term="Model Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:525084f63973</id>
    <title>PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.14716" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-525084f63973" rel="related" type="text/html"/>
    <published>2026-01-23T03:02:00Z</published>
    <updated>2026-01-23T03:02:00Z</updated>
    <author><name>Yao Lu, Dengdong Fan, Jianzheng Nie, Fan Xu, Jie Chen, Bin Zhou, Yonghong Tian</name></author>
    <summary type="html"><![CDATA[<p>PCL-Reasoner-V1.5 achieves 90.9% on AIME 2024 and 85.6% on AIME 2025 using offline RL for mathematical reasoning. Demonstrates offline RL as more stable alternative to online methods like GRPO.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5240c11c6aed</id>
    <title>MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification</title>
    <link href="http://arxiv.org/abs/2601.15498" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-5240c11c6aed" rel="related" type="text/html"/>
    <published>2026-01-23T03:02:00Z</published>
    <updated>2026-01-23T03:02:00Z</updated>
    <author><name>Jingwei Song, Xinyu Wang, Hanbin Wang, Xiaoxuan Lei, Bill Shi, Shixin Han, Eric Yang, Xiao-Wen Chang, Lynn Ai</name></author>
    <summary type="html"><![CDATA[<p>MARS improves speculative decoding by using margin-aware verification that avoids rejecting plausible runner-up tokens when target model shows weak preference. Training-free and domain-agnostic.</p>]]></summary>
    <category term="LLM Inference"/>
    <category term="Speculative Decoding"/>
    <category term="Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:afc520f4253f</id>
    <title>Learning Nonlinear Heterogeneity in Physical Kolmogorov-Arnold Networks</title>
    <link href="http://arxiv.org/abs/2601.15340" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-afc520f4253f" rel="related" type="text/html"/>
    <published>2026-01-23T03:02:00Z</published>
    <updated>2026-01-23T03:02:00Z</updated>
    <author><name>Fabiana Taglietti, Andrea Pulici, Maxwell Roxburgh, Gabriele Seguini, Ian Vidamour, Stephan Menzel, Edoardo Franco, Michele Laus, Eleni Vasilaki, Michele Perego, Thomas J. Hayward, Marco Fanciulli, Jack C. Gartside</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates physical Kolmogorov-Arnold Networks in silicon devices called SYNEs, training synaptic nonlinearities rather than linear weights. Shows higher task performance per physical resource.</p>]]></summary>
    <category term="Physical Computing"/>
    <category term="KAN Architecture"/>
    <category term="Hardware ML"/>
    <category term="Novel Architectures"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:20fa8157c388</id>
    <title>Experiments on Reward Hacking Monitorability in Language Models</title>
    <link href="https://www.lesswrong.com/posts/omYFftnzGobDfJX5u/experiments-on-reward-hacking-monitorability-in-language" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-20fa8157c388" rel="related" type="text/html"/>
    <published>2026-01-23T03:02:00Z</published>
    <updated>2026-01-23T03:02:00Z</updated>
    <author><name>Monketo</name></author>
    <summary type="html"><![CDATA[<p>Experiments with Qwen-Coder 32B on reward hacking in coding tasks with wrong test cases, finding inoculation prompting reduces reward hack rate and increases monitorability of hacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Reward Hacking"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:202b596a8dcf</id>
    <title>The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems</title>
    <link href="http://arxiv.org/abs/2601.15059" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-202b596a8dcf" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Oleg Romanchuk, Roman Bondar</name></author>
    <summary type="html"><![CDATA[<p>Defines 'responsibility vacuum' in CI/CD pipelines with agent-generated code where decisions occur but responsibility cannot be attributed because authority and verification capacity don't coincide. Identifies scaling limits.</p>]]></summary>
    <category term="AI Governance"/>
    <category term="AI Safety"/>
    <category term="Software Engineering"/>
    <category term="Accountability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:5e58da2505fa</id>
    <title>Say Anything but This: When Tokenizer Betrays Reasoning in LLMs</title>
    <link href="http://arxiv.org/abs/2601.14658" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-5e58da2505fa" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Navid Ayoobi, Marcus I Armstrong, Arjun Mukherjee</name></author>
    <summary type="html"><![CDATA[<p>Reveals that tokenizer non-uniqueness (multiple token sequences encoding identical text) causes LLM reasoning failures. Introduces a tokenization-consistency probe showing models treat semantically identical inputs differently based on internal representation.</p>]]></summary>
    <category term="Language Models"/>
    <category term="Tokenization"/>
    <category term="LLM Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:c99131c3f6f5</id>
    <title>Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing</title>
    <link href="http://arxiv.org/abs/2601.15686" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-c99131c3f6f5" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Xinyu Wang, Sicheng Lyu, Yu Gu, Jerry Huang, Peng Lu, Yufei Cui, Xiao-Wen Chang</name></author>
    <summary type="html"><![CDATA[<p>RLSEdit formulates LLM editing as online quadratic optimization with soft constraints using recursive least-squares. Addresses plasticity-stability dilemma in long sequential editing.</p>]]></summary>
    <category term="Model Editing"/>
    <category term="Continual Learning"/>
    <category term="LLM Maintenance"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:ad5f656fb4bd</id>
    <title>Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing</title>
    <link href="http://arxiv.org/abs/2601.16200" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-ad5f656fb4bd" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang</name></author>
    <summary type="html"><![CDATA[<p>Feature-space Smoothing provides certified robustness for multimodal LLMs through feature representation smoothing. Proves guaranteed lower bounds on feature cosine similarity under adversarial attacks.</p>]]></summary>
    <category term="Adversarial Robustness"/>
    <category term="Multimodal LLMs"/>
    <category term="Certified Defense"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:ac096b699d19</id>
    <title>Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow</title>
    <link href="http://arxiv.org/abs/2601.15593" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-ac096b699d19" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Yangyang Zhong, Yanmei Gu, Zhengqing Zang, Xiaomeng Li, Yuqi Ding, Xibei Jia, Yuting Shen, Zhenzhong Lan, Liwang Zhu, Weiping Liu, Junlin Zhou, Haisheng Liu, Zhong Xin Yu, Pengxin Luo, Donglian Qi, Yunfeng Yan, Junbo Zhao</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive evaluation of 8 Masked Diffusion Language Models (up to 100B params) on 58 benchmarks. Introduces AFP and Kendall's tau metrics to characterize parallelism and generation order, showing MDLMs still lag AR models.</p>]]></summary>
    <category term="Diffusion Language Models"/>
    <category term="Benchmarking"/>
    <category term="Language Model Architecture"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-23:research:8c1cff987cf3</id>
    <title>Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis</title>
    <link href="http://arxiv.org/abs/2601.15300" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-23&amp;category=research#item-8c1cff987cf3" rel="related" type="text/html"/>
    <published>2026-01-23T03:00:00Z</published>
    <updated>2026-01-23T03:00:00Z</updated>
    <author><name>Weiwei Wang, Jiyong Min, Weijie Zou</name></author>
    <summary type="html"><![CDATA[<p>Studies catastrophic performance degradation in LLMs when processing contexts near critical thresholds. Introduces Natural Length Distribution Analysis showing models fail beyond certain limits.</p>]]></summary>
    <category term="Long Context"/>
    <category term="LLM Limitations"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:category-summary:research</id>
    <title>Research Summary: January 22, 2026</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-22T06:00:00Z</published>
    <updated>2026-01-22T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans automated AI research, theoretical reasoning foundations, and critical safety vulnerabilities. Stanford's <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-5d5326a6a800" class="internal-link" rel="noopener noreferrer"><strong>execution-grounded automated AI research</strong></a> from Hashimoto, Yang, and Candès demonstrates systematic idea testing and implementation at scale.</p>
<p><strong>Reasoning theory and limitations:</strong></p>
<ul>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-4ea5d9351415" class="internal-link" rel="noopener noreferrer"><strong>Outcome-based RL provably induces CoT reasoning</strong></a> in transformers, providing theoretical foundation for reasoning emergence from sparse rewards</li>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-b7e33d331123" class="internal-link" rel="noopener noreferrer"><strong>Diffusion LLMs' flexibility trap</strong></a> reveals arbitrary generation order hurts reasoning by allowing models to bypass hard tokens</li>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-94b2367a995e" class="internal-link" rel="noopener noreferrer"><strong>LLM planning shows 0% cross-domain transfer</strong></a> despite 82.9% in-domain performance, exposing memorization over true generalization</li>
</ul>
<p><strong>Safety vulnerabilities demand attention:</strong></p>
<ul>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-8f91272f058f" class="internal-link" rel="noopener noreferrer"><strong>LLM judges manipulated at 90% rate</strong></a> via unfaithful CoT rewriting in agent evaluation</li>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-df868e8ffe4a" class="internal-link" rel="noopener noreferrer"><strong>Privacy collapse from benign fine-tuning</strong></a> silently degrades contextual privacy, undetected by standard benchmarks</li>
<li><a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-5836988eb762" class="internal-link" rel="noopener noreferrer"><strong>Turn-based structural triggers</strong></a> achieve <strong>99.52%</strong> backdoor success in multi-turn dialogue without prompt modification</li>
</ul>
<p>Anthropic <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-32c2ed05f61b" class="internal-link" rel="noopener noreferrer">publishes <strong>Claude's new constitution</strong></a> with expanded values framework (&gt;2x previous length), while DeepMind researcher formalizes tradeoffs in <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-0dea9f589164" class="internal-link" rel="noopener noreferrer"><strong>training against scheming monitors</strong></a>. <strong>Meta Flow Maps</strong> <a href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-546a06433333" class="internal-link" rel="noopener noreferrer">extend consistency models</a> for efficient reward alignment in generative models.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:5d5326a6a800</id>
    <title>Towards Execution-Grounded Automated AI Research</title>
    <link href="http://arxiv.org/abs/2601.14525" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-5d5326a6a800" rel="related" type="text/html"/>
    <published>2026-01-22T03:31:00Z</published>
    <updated>2026-01-22T03:31:00Z</updated>
    <author><name>Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\`es, Diyi Yang, Tatsunori Hashimoto</name></author>
    <summary type="html"><![CDATA[<p>From Stanford (Hashimoto, Yang, Candès) proposing execution-grounded automated AI research with automated executor implementing and testing LLM-generated ideas at scale on GPU clusters for LLM pre-training and post-training problems.</p>]]></summary>
    <category term="Automated AI Research"/>
    <category term="LLM Capabilities"/>
    <category term="Research Methodology"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:4ea5d9351415</id>
    <title>Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</title>
    <link href="http://arxiv.org/abs/2601.15158" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-4ea5d9351415" rel="related" type="text/html"/>
    <published>2026-01-22T03:23:00Z</published>
    <updated>2026-01-22T03:23:00Z</updated>
    <author><name>Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</name></author>
    <summary type="html"><![CDATA[<p>Proves theoretically that transformers trained with outcome-based RL on sparse rewards provably converge to structured algorithms implementing Chain-of-Thought reasoning on graph traversal tasks.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="Theoretical AI"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:8f91272f058f</id>
    <title>Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation</title>
    <link href="http://arxiv.org/abs/2601.14691" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-8f91272f058f" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that LLM judges are highly susceptible to CoT manipulation, showing 90% false positive rate inflation through rewriting agent reasoning traces while keeping actions fixed. Critical finding for agent evaluation.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Evaluation"/>
    <category term="LLM Judges"/>
    <category term="Chain-of-Thought"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:5836988eb762</id>
    <title>Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs</title>
    <link href="http://arxiv.org/abs/2601.14340" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-5836988eb762" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Yiyang Lu, Jinwen He, Yue Zhao, Kai Chen, Ruigang Liang</name></author>
    <summary type="html"><![CDATA[<p>Turn-based Structural Trigger (TST) is a backdoor attack on multi-turn LLMs using dialogue turn index as trigger, achieving 99.52% attack success rate while remaining independent of user inputs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Backdoor Attacks"/>
    <category term="Language Models"/>
    <category term="Security"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:df868e8ffe4a</id>
    <title>Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models</title>
    <link href="http://arxiv.org/abs/2601.15220" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-df868e8ffe4a" rel="related" type="text/html"/>
    <published>2026-01-22T03:16:00Z</published>
    <updated>2026-01-22T03:16:00Z</updated>
    <author><name>Anmol Goel, Cornelius Emde, Sangdoo Yun, Seong Joon Oh, Martin Gubri</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'privacy collapse': benign fine-tuning on helpfulness, user data, emotional dialogue, or debugging code can silently degrade LLM contextual privacy. Models maintain benchmark performance while exhibiting severe privacy vulnerabilities.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Privacy"/>
    <category term="Fine-tuning"/>
    <category term="LLM Vulnerabilities"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:546a06433333</id>
    <title>Meta Flow Maps enable scalable reward alignment</title>
    <link href="http://arxiv.org/abs/2601.14430" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-546a06433333" rel="related" type="text/html"/>
    <published>2026-01-22T03:12:00Z</published>
    <updated>2026-01-22T03:12:00Z</updated>
    <author><name>Peter Potaptchik, Adhi Saravanan, Abbas Mammadov, Alvaro Prat, Michael S. Albergo, Yee Whye Teh</name></author>
    <summary type="html"><![CDATA[<p>Meta Flow Maps extend consistency models into stochastic regime for one-step posterior sampling, enabling efficient reward alignment for generative model control without costly trajectory simulation.</p>]]></summary>
    <category term="Generative Models"/>
    <category term="Alignment"/>
    <category term="Flow Matching"/>
    <category term="Efficiency"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:8bc030697961</id>
    <title>Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14270" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-8bc030697961" rel="related" type="text/html"/>
    <published>2026-01-22T03:07:00Z</published>
    <updated>2026-01-22T03:07:00Z</updated>
    <author><name>Liangming Pan, Jason Liang, Jiaran Ye, Minglai Yang, Xinyuan Lu, Fengbin Zhu</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organizing around seven research questions from implicit multi-hop reasoning in hidden activations to how verbalized reasoning remodels computation.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Survey"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:525084f63973</id>
    <title>PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.14716" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-525084f63973" rel="related" type="text/html"/>
    <published>2026-01-22T03:07:00Z</published>
    <updated>2026-01-22T03:07:00Z</updated>
    <author><name>Yao Lu, Dengdong Fan, Jianzheng Nie, Fan Xu, Jie Chen, Bin Zhou, Yonghong Tian</name></author>
    <summary type="html"><![CDATA[<p>Presents PCL-Reasoner-V1.5, a 32B model achieving 90.9% on AIME 2024 and 85.6% on AIME 2025 using offline RL, demonstrating superior stability over online methods like GRPO.</p>]]></summary>
    <category term="Mathematical Reasoning"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:f01a57cfb913</id>
    <title>LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models</title>
    <link href="http://arxiv.org/abs/2601.14330" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-f01a57cfb913" rel="related" type="text/html"/>
    <published>2026-01-22T03:07:00Z</published>
    <updated>2026-01-22T03:07:00Z</updated>
    <author><name>Mengyu Sun, Ziyuan Yang, Andrew Beng Jin Teoh, Junxu Liu, Haibo Hu, Yi Zhang</name></author>
    <summary type="html"><![CDATA[<p>LURE demonstrates that erased concepts in diffusion models can be reawakened through latent space perturbation, modeling generation as implicit function of text, parameters, and latent states.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Diffusion Models"/>
    <category term="Concept Erasure"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:f40af38e3455</id>
    <title>Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers</title>
    <link href="http://arxiv.org/abs/2601.15014" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-f40af38e3455" rel="related" type="text/html"/>
    <published>2026-01-22T03:07:00Z</published>
    <updated>2026-01-22T03:07:00Z</updated>
    <author><name>Michelle Ching, Ioana Popescu, Nico Smith, Tianyi Ma, William G. Underwood, Richard J. Samworth</name></author>
    <summary type="html"><![CDATA[<p>Proves transformers can achieve minimax-optimal rates for in-context nonparametric regression with Θ(log n) parameters, showing efficient approximation of local polynomial estimators.</p>]]></summary>
    <category term="In-Context Learning"/>
    <category term="Theory"/>
    <category term="Transformers"/>
    <category term="Nonparametric Statistics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:61a970252eb9</id>
    <title>RoboBrain 2.5: Depth in Sight, Time in Mind</title>
    <link href="http://arxiv.org/abs/2601.14352" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-61a970252eb9" rel="related" type="text/html"/>
    <published>2026-01-22T03:07:00Z</published>
    <updated>2026-01-22T03:07:00Z</updated>
    <author><name>Huajie Tan, Enshen Zhou, Zhiyu Li, Yijie Xu, Yuheng Ji, Xiansheng Chen, Cheng Chi, Pengwei Wang, Huizhu Jia, Yulong Ao, Mingyu Cao, Sixiang Chen, Zhe Li, Mengzhen Liu, Zixiao Wang, Shanyu Rong, Yaoxu Lyu, Zhongxia Zhao, Peterson Co, Yibo Li, Yi Han, Shaoxuan Xie, Guocai Yao, Songjing Wang, Leiduo Zhang, Xi Yang, Yance Jiao, Donghai Shi, Kunchang Xie, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang</name></author>
    <summary type="html"><![CDATA[<p>RoboBrain 2.5 is an embodied AI foundation model with advances in 3D spatial reasoning and dense temporal value estimation. Enables depth-aware coordinate prediction, metric constraint comprehension, and step-aware progress prediction for manipulation.</p>]]></summary>
    <category term="Embodied AI"/>
    <category term="Foundation Models"/>
    <category term="Robotics"/>
    <category term="3D Reasoning"/>
    <category term="Manipulation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:b7e33d331123</id>
    <title>The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</title>
    <link href="http://arxiv.org/abs/2601.15165" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-b7e33d331123" rel="related" type="text/html"/>
    <published>2026-01-22T03:04:00Z</published>
    <updated>2026-01-22T03:04:00Z</updated>
    <author><name>Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</name></author>
    <summary type="html"><![CDATA[<p>Reveals counter-intuitive finding that arbitrary order flexibility in diffusion LLMs hurts reasoning by allowing models to exploit order to bypass high-uncertainty critical tokens.</p>]]></summary>
    <category term="Diffusion Models"/>
    <category term="Reasoning"/>
    <category term="Language Models"/>
    <category term="Mechanistic Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:1dbf6de9ea15</id>
    <title>CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14310" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-1dbf6de9ea15" rel="related" type="text/html"/>
    <published>2026-01-22T03:02:00Z</published>
    <updated>2026-01-22T03:02:00Z</updated>
    <author><name>Nay Myat Min, Long H. Pham, Hongyu Zhang, Jun Sun</name></author>
    <summary type="html"><![CDATA[<p>Introduces CORVUS red-teaming procedure that fine-tunes LoRA adapters to camouflage hallucination detector-visible telemetry, degrading both training-free and training-based detectors across multiple LLMs.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Hallucination Detection"/>
    <category term="Adversarial Attacks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-22:research:6abe5ac1437d</id>
    <title>SpooFL: Spoofing Federated Learning</title>
    <link href="http://arxiv.org/abs/2601.15055" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-22&amp;category=research#item-6abe5ac1437d" rel="related" type="text/html"/>
    <published>2026-01-22T03:02:00Z</published>
    <updated>2026-01-22T03:02:00Z</updated>
    <author><name>Isaac Baglin, Xiatian Zhu, Simon Hadfield</name></author>
    <summary type="html"><![CDATA[<p>SpooFL proposes spoofing-based federated learning defense that deceives gradient leakage attackers into recovering convincing but synthetic data rather than true training samples.</p>]]></summary>
    <category term="Federated Learning"/>
    <category term="Privacy"/>
    <category term="Security"/>
    <category term="Defense"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:category-summary:research</id>
    <title>Research Summary: January 21, 2026</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-21T06:00:00Z</published>
    <updated>2026-01-21T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research reveals critical challenges to conventional training wisdom and exposes multiple safety vulnerabilities across deployed systems.</p>
<p><strong>Training Paradigm Reassessment:</strong></p>
<ul>
<li>Base models <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-bf9961720f8a" class="internal-link" rel="noopener noreferrer">consistently outperform instruction-tuned variants</a> on math and domain-shifted benchmarks, challenging fundamental training assumptions</li>
<li><strong>RLVR</strong> <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-8cac8805e742" class="internal-link" rel="noopener noreferrer">improves task performance</a> but produces extremely overconfident models; <strong>SFT</strong> yields better calibration even under distribution shift</li>
</ul>
<p><strong>Safety &amp; Security Vulnerabilities:</strong></p>
<ul>
<li><strong>Action Rebinding</strong> <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-5336cd7b69bc" class="internal-link" rel="noopener noreferrer">allows zero-permission apps to hijack</a> multimodal GUI agents by exploiting visual attention</li>
<li>Safeguarded frontier models can <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-866acf7c7710" class="internal-link" rel="noopener noreferrer">elicit harmful capabilities</a> in open-source models through three-stage attacks</li>
<li><strong>Sockpuppetting</strong> <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-e29c4d7a227c" class="internal-link" rel="noopener noreferrer">jailbreaks LLMs with one line of code</a> achieving up to <strong>100% ASR</strong></li>
<li>AI-generated data contamination in medical imaging <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-79b868d96563" class="internal-link" rel="noopener noreferrer">creates feedback loops</a> eroding diagnostic reliability</li>
</ul>
<p><strong>Reasoning Model Insights:</strong></p>
<ul>
<li><strong>Thinking Traps</strong> <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-a7f126fd0c33" class="internal-link" rel="noopener noreferrer">account for <strong>89%</strong> of reasoning failures</a> in long CoT—models elaborate incorrect early commitments</li>
<li>First systematic cost-accuracy comparison shows multi-agent reasoning <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-d45a3f320ac8" class="internal-link" rel="noopener noreferrer">often underperforms single-model CoT</a></li>
<li>Large-scale study of <strong>2.1M preprints</strong> finds LLM adoption <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-8fadab621712" class="internal-link" rel="noopener noreferrer">increases paper volume but decreases citations</a> and originality</li>
</ul>
<p><strong>Architecture Innovation:</strong> <strong>Threshold Differential Attention</strong> <a href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-5da0fc012225" class="internal-link" rel="noopener noreferrer">eliminates attention sinks</a> while achieving ultra-sparsity and improved long-context robustness.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:bf9961720f8a</id>
    <title>Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks</title>
    <link href="http://arxiv.org/abs/2601.13244" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-bf9961720f8a" rel="related" type="text/html"/>
    <published>2026-01-21T03:31:00Z</published>
    <updated>2026-01-21T03:31:00Z</updated>
    <author><name>Prateek Munjal, Clement Christophe, Ronnie Rajan, Praveenkumar Kanithi</name></author>
    <summary type="html"><![CDATA[<p>Investigates whether instruction-tuned models always outperform base models, finding that base models consistently outperform instruction-tuned variants in zero-shot CoT settings on GSM8K (drops up to 32.67% for Llama3-70B). Instruction tuning appears to induce pattern matching rather than genuine reasoning improvement.</p>]]></summary>
    <category term="LLM Training"/>
    <category term="Instruction Tuning"/>
    <category term="Reasoning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:8cac8805e742</id>
    <title>Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning</title>
    <link href="http://arxiv.org/abs/2601.13284" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-8cac8805e742" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia, Srikanth Doss, Nikolaos Pappas</name></author>
    <summary type="html"><![CDATA[<p>Systematic study showing RLVR improves task performance but produces extremely overconfident models, while SFT yields better calibration even under distribution shift. Proposes calibration-aware RL approach to balance classification and calibration.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Calibration"/>
    <category term="Reinforcement Learning"/>
    <category term="LLM Training"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:3d2384716bb1</id>
    <title>CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</title>
    <link href="http://arxiv.org/abs/2601.13304" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-3d2384716bb1" rel="related" type="text/html"/>
    <published>2026-01-21T03:23:00Z</published>
    <updated>2026-01-21T03:23:00Z</updated>
    <author><name>Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</name></author>
    <summary type="html"><![CDATA[<p>Introduces CausalSpatial benchmark evaluating whether MLLMs can anticipate consequences of object motions across collision, compatibility, occlusion and trajectory tasks. Reveals severe gap: humans score 84% while GPT-5 achieves only 54%, exposing over-reliance on textual reasoning.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="Multimodal Reasoning"/>
    <category term="Causal Reasoning"/>
    <category term="MLLM Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:5336cd7b69bc</id>
    <title>Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?</title>
    <link href="http://arxiv.org/abs/2601.12349" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-5336cd7b69bc" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao</name></author>
    <summary type="html"><![CDATA[<p>Discovers 'Action Rebinding' - a critical security vulnerability in multimodal GUI agents where zero-permission apps can hijack agent actions by exploiting the gap between observation and action execution. Demonstrates that Visual Atomicity assumption is invalid on Android.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Security Vulnerabilities"/>
    <category term="Agentic AI"/>
    <category term="Multimodal Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:866acf7c7710</id>
    <title>Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs</title>
    <link href="http://arxiv.org/abs/2601.13528" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-866acf7c7710" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Jackson Kaunismaa, Avery Griffin, John Hughes, Christina Q. Knight, Mrinank Sharma, Erik Jones</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that safeguarded frontier models can be used to elicit harmful capabilities in open-source models through three-stage elicitation attacks using adjacent-domain prompts that bypass safeguards.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Model Security"/>
    <category term="Capability Elicitation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:85b933d42ecc</id>
    <title>APEX-Agents</title>
    <link href="http://arxiv.org/abs/2601.14242" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-85b933d42ecc" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</name></author>
    <summary type="html"><![CDATA[<p>APEX-Agents benchmarks AI agents on professional tasks from investment banking, consulting, and law with 480 long-horizon cross-application tasks. Gemini 3 Flash achieves 24%, followed by GPT-5.2 and Claude Opus 4.5.</p>]]></summary>
    <category term="AI Agents"/>
    <category term="Benchmarks"/>
    <category term="Professional AI"/>
    <category term="Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:d45a3f320ac8</id>
    <title>A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms</title>
    <link href="http://arxiv.org/abs/2601.13243" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-d45a3f320ac8" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Yapeng Li, Jiakuo Yu, Zhixin Liu, Xinnan Liu, Jing Yu, Songze Li, Tonghua Su</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive unified evaluation of LLM reasoning paradigms from single-model to multi-agent systems, characterizing performance across benchmarks and analyzing cost-accuracy trade-offs. Probes role-specific capability demands in MAS.</p>]]></summary>
    <category term="LLM Reasoning"/>
    <category term="Multi-Agent Systems"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:0172ec8f08ae</id>
    <title>Human detectors are surprisingly powerful reward models</title>
    <link href="http://arxiv.org/abs/2601.14037" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-0172ec8f08ae" rel="related" type="text/html"/>
    <published>2026-01-21T03:16:00Z</published>
    <updated>2026-01-21T03:16:00Z</updated>
    <author><name>Kumar Ashutosh, XuDong Wang, Xi Yin, Kristen Grauman, Adam Polyak, Ishan Misra, Rohit Girdhar</name></author>
    <summary type="html"><![CDATA[<p>Proposes HuDA, a surprisingly simple reward model using human detection confidence and temporal prompt alignment to improve human motion in generated videos. Off-the-shelf models outperform specialized methods without training.</p>]]></summary>
    <category term="Video Generation"/>
    <category term="Reward Models"/>
    <category term="Human Motion Synthesis"/>
    <category term="RLHF"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:79b868d96563</id>
    <title>AI-generated data contamination erodes pathological variability and diagnostic reliability</title>
    <link href="http://arxiv.org/abs/2601.12946" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-79b868d96563" rel="related" type="text/html"/>
    <published>2026-01-21T03:12:00Z</published>
    <updated>2026-01-21T03:12:00Z</updated>
    <author><name>Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu Cheng, Dianbo Liu</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that AI-generated data contamination in medical AI creates feedback loop causing erosion of pathological variability and diagnostic reliability, with models converging toward generic phenotypes regardless of architecture.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="AI Safety"/>
    <category term="Data Contamination"/>
    <category term="Synthetic Data"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:5da0fc012225</id>
    <title>Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling</title>
    <link href="http://arxiv.org/abs/2601.12145" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-5da0fc012225" rel="related" type="text/html"/>
    <published>2026-01-21T03:12:00Z</published>
    <updated>2026-01-21T03:12:00Z</updated>
    <author><name>Xingyue Huang, Xueying Ding, Mingxuan Ju, Yozen Liu, Neil Shah, Tong Zhao</name></author>
    <summary type="html"><![CDATA[<p>Proposes Threshold Differential Attention (TDA), a sink-free attention mechanism achieving ultra-sparsity and improved robustness at longer sequence lengths. Uses row-wise extreme-value thresholding with length-dependent gating without computational overhead of projection methods.</p>]]></summary>
    <category term="Attention Mechanisms"/>
    <category term="Long Context"/>
    <category term="Efficient Architectures"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:e583e178f02d</id>
    <title>Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization</title>
    <link href="http://arxiv.org/abs/2601.12598" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-e583e178f02d" rel="related" type="text/html"/>
    <published>2026-01-21T03:09:00Z</published>
    <updated>2026-01-21T03:09:00Z</updated>
    <author><name>Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Emre Neftci</name></author>
    <summary type="html"><![CDATA[<p>Proposes refined taxonomy of linear recurrent models (Mamba-like architectures) and introduces SelectivBench, lightweight benchmarks revealing how different gating strategies drive selectivity and generalization. Enables systematic comparison without resource-intensive experiments.</p>]]></summary>
    <category term="Efficient Architectures"/>
    <category term="State Space Models"/>
    <category term="Benchmarking"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:b8039b4309c6</id>
    <title>The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning</title>
    <link href="http://arxiv.org/abs/2601.14127" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-b8039b4309c6" rel="related" type="text/html"/>
    <published>2026-01-21T03:09:00Z</published>
    <updated>2026-01-21T03:09:00Z</updated>
    <author><name>Renmiao Chen, Yida Lu, Shiyao Cui, Xuan Ouyang, Victor Shea-Jay Huang, Shumin Zhang, Chengwei Pan, Han Qiu, Minlie Huang</name></author>
    <summary type="html"><![CDATA[<p>MIR-SafetyBench is the first benchmark focused on multi-image reasoning safety in MLLMs (2,676 instances, 9 image relations). Reveals troubling trend: models with better multi-image reasoning are MORE vulnerable to safety exploits.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Multimodal LLMs"/>
    <category term="Benchmarks"/>
    <category term="Adversarial Robustness"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:a7f126fd0c33</id>
    <title>Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart</title>
    <link href="http://arxiv.org/abs/2601.11940" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-a7f126fd0c33" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zijun Yao, Heng Wang, Minshen Yu, Yixin Cao</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Thinking Traps' in long CoT - prefix-dominant deadlocks where models elaborate incorrect early commitments. 89% of failures show this pattern. TAAR framework predicts trap locations and triggers adaptive restarts.</p>]]></summary>
    <category term="Reasoning"/>
    <category term="Chain-of-Thought"/>
    <category term="Large Reasoning Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:b9d165c38655</id>
    <title>The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.13358" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-b9d165c38655" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Samuel Cyrenius Anderson</name></author>
    <summary type="html"><![CDATA[<p>Analyzes 25,000+ CoT trajectories across Law, Science, Code, Math at 8B and 70B scales. Discovers domain-specific phase transitions: Legal reasoning 'crystallizes', Scientific stays 'liquid', Code forms discrete 'lattice'.</p>]]></summary>
    <category term="Scaling Laws"/>
    <category term="Reasoning"/>
    <category term="Interpretability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:240873dc352c</id>
    <title>Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</title>
    <link href="http://arxiv.org/abs/2601.11868" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-240873dc352c" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj\"orn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, Ludwig Schmidt</name></author>
    <summary type="html"><![CDATA[<p>Terminal-Bench 2.0 is a curated benchmark of 89 hard terminal tasks with unique environments and comprehensive tests. Frontier models and agents score less than 65%, identifying areas for improvement.</p>]]></summary>
    <category term="Benchmarks"/>
    <category term="LLM Agents"/>
    <category term="Code Execution"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:ebf90b488c9c</id>
    <title>Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty</title>
    <link href="http://arxiv.org/abs/2601.12471" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-ebf90b488c9c" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao</name></author>
    <summary type="html"><![CDATA[<p>Introduces MedAbstain, a benchmark for evaluating when medical LLMs should abstain from answering using conformal prediction and adversarial perturbations. Reveals that even high-accuracy models fail to appropriately abstain when uncertain.</p>]]></summary>
    <category term="Medical AI"/>
    <category term="AI Safety"/>
    <category term="Uncertainty Quantification"/>
    <category term="Benchmarks"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:8fadab621712</id>
    <title>Scientific production in the era of Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.13187" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-8fadab621712" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan, Toby Stuart, Yian Yin</name></author>
    <summary type="html"><![CDATA[<p>Large-scale analysis of LLM impact on scientific production using 2.1M preprints and 246M document accesses. Finds 23.7-89.3% increase in paper production but reversed relationship between linguistic complexity and paper quality.</p>]]></summary>
    <category term="Science of Science"/>
    <category term="LLM Impact"/>
    <category term="Research Quality"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:1a71cab2ab3a</id>
    <title>Pro-AI Bias in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.13749" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-1a71cab2ab3a" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Benaya Trabelsi, Jonathan Shaki, Sarit Kraus</name></author>
    <summary type="html"><![CDATA[<p>This study investigates whether LLMs display systematic preferential bias in favor of AI itself. Across three experiments, researchers find consistent pro-AI bias: LLMs disproportionately recommend AI options and overestimate AI job salaries by 10 percentage points.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Bias in AI"/>
    <category term="LLM Behavior"/>
    <category term="Alignment"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:44028ff75396</id>
    <title>Towards Spectroscopy: Susceptibility Clusters in Language Models</title>
    <link href="http://arxiv.org/abs/2601.12703" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-44028ff75396" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Andrew Gordon, Garrett Baker, George Wang, William Snell, Stan van Wingerden, Daniel Murfet</name></author>
    <summary type="html"><![CDATA[<p>Applies spectroscopy principles to neural networks - perturbing data distribution and measuring model response via susceptibilities. Shows tokens that follow contexts 'for similar reasons' cluster in susceptibility space. Develops methodology on Pythia-14M.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Mechanistic Understanding"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:e29c4d7a227c</id>
    <title>Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection</title>
    <link href="http://arxiv.org/abs/2601.13359" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-e29c4d7a227c" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Asen Dotsinski, Panagiotis Eustratiadis</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'sockpuppetting', a simple jailbreaking method requiring only one line of code that inserts acceptance sequence at output start. Achieves up to 80% higher ASR than GCG on Qwen3-8B without optimization.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="Jailbreaking"/>
    <category term="Language Models"/>
    <category term="Adversarial ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:ff89265e07ec</id>
    <title>Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models</title>
    <link href="http://arxiv.org/abs/2601.14004" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-ff89265e07ec" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, Qianli Wang, Shuzhou Yuan, Ercong Nie, Xufeng Duan, Qibo Xue, Zeping Yu, Chenming Shang, Xiao Liang, Jing Xiong, Hui Shen, Chaofan Tao, Zhengwu Liu, Senjie Jin, Zhiheng Xi, Dongdong Zhang, Sophia Ananiadou, Tao Gui, Ruobing Xie, Hayden Kwok-Hay So, Hinrich Sch\"utze, Xuanjing Huang, Qi Zhang, Ngai Wong</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive survey of actionable mechanistic interpretability organized around 'Locate, Steer, and Improve' framework, bridging observation and intervention for alignment, capability, and efficiency.</p>]]></summary>
    <category term="Mechanistic Interpretability"/>
    <category term="Survey"/>
    <category term="AI Alignment"/>
    <category term="Model Editing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:9e2dbbc9e37b</id>
    <title>HALT: Hallucination Assessment via Latent Testing</title>
    <link href="http://arxiv.org/abs/2601.14210" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-9e2dbbc9e37b" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Rohan Bhatnagar, Youran Sun, Chi Andrew Zhang, Yixin Wen, Haizhao Yang</name></author>
    <summary type="html"><![CDATA[<p>HALT proposes lightweight residual probes that read hallucination risk from intermediate hidden states during question processing, enabling near-instantaneous risk estimation with effectively zero added latency. Based on hypothesis that epistemic signals are attenuated in final decoding.</p>]]></summary>
    <category term="Hallucination Detection"/>
    <category term="LLM Safety"/>
    <category term="Model Interpretability"/>
    <category term="Efficient Inference"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:e5e6288a0e53</id>
    <title>Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models</title>
    <link href="http://arxiv.org/abs/2601.12626" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-e5e6288a0e53" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Raphi Kang, Hongqiao Chen, Georgia Gkioxari, Pietro Perona</name></author>
    <summary type="html"><![CDATA[<p>Discovers that VLMs encode object locations by linearly binding 'Spatial IDs' to textual activations and perform spatial reasoning via language tokens. Demonstrates causal role through interventions.</p>]]></summary>
    <category term="VLM Interpretability"/>
    <category term="Spatial Reasoning"/>
    <category term="Mechanistic Interpretability"/>
    <category term="Vision-Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:57d832c3eb9d</id>
    <title>ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch</title>
    <link href="http://arxiv.org/abs/2601.13606" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-57d832c3eb9d" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao, Yu Li, Mengzhang Cai, Yun Zhu, Zhanping Zhong, Qizhi Pei, Zhuoshi Pan, Xiaoran Shang, Bin Cui, Conghui He, Wentao Zhang, Lijun Wu</name></author>
    <summary type="html"><![CDATA[<p>Proposes ChartVerse framework for synthesizing complex charts and reliable reasoning data using Rollout Posterior Entropy metric for complexity-aware generation and multi-stage verification pipeline to eliminate hallucinations.</p>]]></summary>
    <category term="VLM Training Data"/>
    <category term="Chart Understanding"/>
    <category term="Data Synthesis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:b39d4669af7d</id>
    <title>Revisiting Multi-Task Visual Representation Learning</title>
    <link href="http://arxiv.org/abs/2601.13886" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-b39d4669af7d" rel="related" type="text/html"/>
    <published>2026-01-21T03:07:00Z</published>
    <updated>2026-01-21T03:07:00Z</updated>
    <author><name>Shangzhe Di, Zhonghua Zhai, Weidi Xie</name></author>
    <summary type="html"><![CDATA[<p>Introduces MTV, multi-task visual pretraining framework jointly optimizing vision-language contrastive, self-supervised, and dense spatial objectives. Uses expert models for pseudo-labels to eliminate manual annotation need.</p>]]></summary>
    <category term="Visual Representation Learning"/>
    <category term="Multi-Task Learning"/>
    <category term="Foundation Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:1793bc3c727b</id>
    <title>Patterning: The Dual of Interpretability</title>
    <link href="http://arxiv.org/abs/2601.13548" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-1793bc3c727b" rel="related" type="text/html"/>
    <published>2026-01-21T03:04:00Z</published>
    <updated>2026-01-21T03:04:00Z</updated>
    <author><name>George Wang, Daniel Murfet</name></author>
    <summary type="html"><![CDATA[<p>Introduces 'patterning' as the dual of interpretability: given desired generalization, determine what training data produces it. Uses susceptibilities to identify data interventions that steer models toward target internal configurations.</p>]]></summary>
    <category term="Interpretability"/>
    <category term="Training Data"/>
    <category term="Language Models"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:ec20c4352aa6</id>
    <title>AfroScope: A Framework for Studying the Linguistic Landscape of Africa</title>
    <link href="http://arxiv.org/abs/2601.13346" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-ec20c4352aa6" rel="related" type="text/html"/>
    <published>2026-01-21T03:04:00Z</published>
    <updated>2026-01-21T03:04:00Z</updated>
    <author><name>Sang Yun Kwon, AbdelRahim Elmadany, Muhammad Abdul-Mageed</name></author>
    <summary type="html"><![CDATA[<p>AfroScope introduces LID framework covering 713 African languages with hierarchical classification using mixture-of-experts for distinguishing confusable varieties.</p>]]></summary>
    <category term="Language Identification"/>
    <category term="African Languages"/>
    <category term="Multilingual NLP"/>
    <category term="Mixture of Experts"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:065700d3eacd</id>
    <title>Scaling Test-time Inference for Visual Grounding</title>
    <link href="http://arxiv.org/abs/2601.13633" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-065700d3eacd" rel="related" type="text/html"/>
    <published>2026-01-21T03:04:00Z</published>
    <updated>2026-01-21T03:04:00Z</updated>
    <author><name>Guanqi Zhan, Changye Li, Zhijian Liu, Yao Lu, Yi Wu, Song Han, Ligeng Zhu</name></author>
    <summary type="html"><![CDATA[<p>Introduces EGM method for visual grounding that scales test-time computation through additional token generation. Shows small VLMs can match larger ones by scaling inference, bridging language understanding gap.</p>]]></summary>
    <category term="Visual Grounding"/>
    <category term="Test-Time Scaling"/>
    <category term="Efficient Inference"/>
    <category term="VLMs"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:6c9e3d07600f</id>
    <title>Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving</title>
    <link href="http://arxiv.org/abs/2601.14038" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-6c9e3d07600f" rel="related" type="text/html"/>
    <published>2026-01-21T03:04:00Z</published>
    <updated>2026-01-21T03:04:00Z</updated>
    <author><name>Alexandre Justo Miro (Traton Group R&amp;D, M\"alardalen University), Ludvig af Klinteberg (M\"alardalen University), Bogdan Timus (Traton Group R&amp;D), Aron Asefaw (KTH Royal Institute of Technology), Ajinkya Khoche (Traton Group R&amp;D, KTH Royal Institute of Technology), Thomas Gustafsson (Traton Group R&amp;D), Sina Sharif Mansouri (Traton Group R&amp;D), Masoud Daneshtalab (M\"alardalen University)</name></author>
    <summary type="html"><![CDATA[<p>First work to discover systematic annotation errors in widely-used autonomous driving datasets caused by dynamic object timestamp misalignment. Proposes offline estimation method to correct annotations following physical constraints.</p>]]></summary>
    <category term="Autonomous Driving"/>
    <category term="Dataset Quality"/>
    <category term="Annotation Errors"/>
    <category term="Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:d68ab9c6366c</id>
    <title>Teaching Large Reasoning Models Effective Reflection</title>
    <link href="http://arxiv.org/abs/2601.12720" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-d68ab9c6366c" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang</name></author>
    <summary type="html"><![CDATA[<p>Identifies superficial reflection problem in Large Reasoning Models and proposes Self-Critique Fine-Tuning (SCFT) using only self-generated critiques filtered through rejection sampling.</p>]]></summary>
    <category term="Large Reasoning Models"/>
    <category term="Self-Critique"/>
    <category term="Fine-Tuning"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:63980d95dd76</id>
    <title>Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues</title>
    <link href="http://arxiv.org/abs/2601.13206" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-63980d95dd76" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Neil K. R. Sehgal, Sharath Chandra Guntuku, Lyle Ungar</name></author>
    <summary type="html"><![CDATA[<p>Tests LLM temporal awareness in negotiations with real-time deadlines. GPT-5.1 shows 32% vs 4% deal closure with/without time updates. Demonstrates fundamental temporal awareness failures.</p>]]></summary>
    <category term="Temporal Reasoning"/>
    <category term="Agent Behavior"/>
    <category term="Negotiation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:916fb5e56dda</id>
    <title>AgenticRed: Optimizing Agentic Systems for Automated Red-teaming</title>
    <link href="http://arxiv.org/abs/2601.13518" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-916fb5e56dda" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Jiayi Yuan, Jonathan N\"other, Natasha Jaques, Goran Radanovi\'c</name></author>
    <summary type="html"><![CDATA[<p>AgenticRed automates red-teaming system design using LLM in-context learning to iteratively evolve attacking systems without human intervention. Treats red-teaming as system design problem.</p>]]></summary>
    <category term="Red-Teaming"/>
    <category term="AI Safety"/>
    <category term="Automated Evaluation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:44ae690852a2</id>
    <title>Speculative Decoding: Performance or Illusion?</title>
    <link href="http://arxiv.org/abs/2601.11580" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-44ae690852a2" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Xiaoxuan Liu, Jiaxiang Yu, Jongseok Park, Ion Stoica, Alvin Cheung</name></author>
    <summary type="html"><![CDATA[<p>First systematic study of speculative decoding on production vLLM system, covering multiple SD variants across diverse workloads and quantifying theoretical upper bounds on speedup.</p>]]></summary>
    <category term="LLM Serving"/>
    <category term="Speculative Decoding"/>
    <category term="Inference Efficiency"/>
    <category term="Systems"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:85b886a61db5</id>
    <title>AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations</title>
    <link href="http://arxiv.org/abs/2601.12727" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-85b886a61db5" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee</name></author>
    <summary type="html"><![CDATA[<p>Demonstrates that conversational AI personality traits measurably shift users' self-concepts to align with the AI's traits, with longer conversations producing greater alignment and increased homogeneity across users.</p>]]></summary>
    <category term="Human-AI Interaction"/>
    <category term="AI Psychology"/>
    <category term="AI Ethics"/>
    <category term="Behavioral Research"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:512744dc3dbf</id>
    <title>Q-learning with Adjoint Matching</title>
    <link href="http://arxiv.org/abs/2601.14234" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-512744dc3dbf" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Qiyang Li, Sergey Levine</name></author>
    <summary type="html"><![CDATA[<p>QAM proposes Q-learning with Adjoint Matching for efficient optimization of diffusion/flow-matching policies in continuous-action RL, avoiding unstable backpropagation through multi-step denoising.</p>]]></summary>
    <category term="Reinforcement Learning"/>
    <category term="Diffusion Models"/>
    <category term="Policy Optimization"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:397054bf12c6</id>
    <title>Behavior Knowledge Merge in Reinforced Agentic Models</title>
    <link href="http://arxiv.org/abs/2601.13572" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-397054bf12c6" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao, Soroush Vosoughi, Wenke Lee</name></author>
    <summary type="html"><![CDATA[<p>Addresses model merging for RL-trained agentic models, finding that SFT-designed merging methods fail because RL induces sparse, heterogeneous task vectors. Proposes behavior knowledge merge approach for combining multiple RL-trained agents.</p>]]></summary>
    <category term="Model Merging"/>
    <category term="Reinforcement Learning"/>
    <category term="Agentic AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:a201feae5508</id>
    <title>The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check</title>
    <link href="http://arxiv.org/abs/2601.12979" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-a201feae5508" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Qingyu Lu, Liang Ding, Kanjian Zhang, Jinxia Zhang, Dacheng Tao</name></author>
    <summary type="html"><![CDATA[<p>Comprehensive evaluation showing diffusion-based LLMs (LLaDA, Dream) fail as agentic backbones for both embodied and tool-calling tasks, despite efficiency advantages.</p>]]></summary>
    <category term="Diffusion Language Models"/>
    <category term="Agents"/>
    <category term="Evaluation"/>
    <category term="Tool Use"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:5c5356d4ac6d</id>
    <title>Think3D: Thinking with Space for Spatial Reasoning</title>
    <link href="http://arxiv.org/abs/2601.13029" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-5c5356d4ac6d" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</name></author>
    <summary type="html"><![CDATA[<p>Introduces Think3D, a framework enabling VLM agents to reason in 3D space by leveraging reconstruction models for point clouds and camera poses, with camera-based operations and view switching for interactive 3D chain-of-thought.</p>]]></summary>
    <category term="Spatial Reasoning"/>
    <category term="Vision-Language Models"/>
    <category term="3D Understanding"/>
    <category term="Embodied AI"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:e9e8ebb3dd4b</id>
    <title>Soft Tail-dropping for Adaptive Visual Tokenization</title>
    <link href="http://arxiv.org/abs/2601.14246" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-e9e8ebb3dd4b" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Zeyuan Chen, Kai Zhang, Zhuowen Tu, Yuanjun Xiong</name></author>
    <summary type="html"><![CDATA[<p>Presents STAT, a 1D discrete visual tokenizer that adaptively chooses output token count based on image complexity. Produces length-adaptive tokens naturally compatible with causal autoregressive models.</p>]]></summary>
    <category term="Visual Tokenization"/>
    <category term="Autoregressive Models"/>
    <category term="Efficient Generation"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-21:research:a6e61e73aac7</id>
    <title>ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models</title>
    <link href="http://arxiv.org/abs/2601.12428" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-21&amp;category=research#item-a6e61e73aac7" rel="related" type="text/html"/>
    <published>2026-01-21T03:02:00Z</published>
    <updated>2026-01-21T03:02:00Z</updated>
    <author><name>Baorui Peng, Wenyao Zhang, Liang Xu, Zekun Qi, Jiazhao Zhang, Hongsi Liu, Wenjun Zeng, Xin Jin</name></author>
    <summary type="html"><![CDATA[<p>Introduces ReWorld employing RL to align video-based embodied world models with physical realism, task completion, embodiment plausibility and visual quality using hierarchical reward model trained on 235K video preference dataset.</p>]]></summary>
    <category term="World Models"/>
    <category term="Embodied AI"/>
    <category term="Reinforcement Learning"/>
    <category term="Robotics"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-20:category-summary:research</id>
    <title>Research Summary: January 20, 2026</title>
    <link href="https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-20&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-20T06:00:00Z</published>
    <updated>2026-01-20T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research concentrates heavily on <strong>alignment techniques and safety evaluation</strong>. A survey on <strong>alignment pretraining</strong> <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-2a1abcff2543" class="internal-link" rel="noopener noreferrer">synthesizes evidence</a> that training LLMs on data depicting well-behaved AI during pretraining substantially reduces misalignment—potentially offering a scalable, proactive safety approach.</p>
<ul>
<li><strong>Coup probes</strong> <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-8708c9ecc0f5" class="internal-link" rel="noopener noreferrer">testing demonstrates</a> few-shot linear classifiers can detect scheming behavior from model activations, with empirical results on off-policy training data</li>
<li><strong>Silent Agreement Evaluation</strong> <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-7ebc02f821d4" class="internal-link" rel="noopener noreferrer">provides first empirical measurement</a> of <strong>Schelling coordination</strong> in LLMs—whether isolated instances converge on shared choices without communication</li>
<li>Framework for <strong>AI-delegated safety research</strong> <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-65438e9ab777" class="internal-link" rel="noopener noreferrer">identifies key dimensions</a>: epistemic cursedness, parallelizability, and short-horizon suitability</li>
<li>Strategic analysis <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-6f06b1e50f04" class="internal-link" rel="noopener noreferrer">examines whether</a> <strong>LLM alignment work transfers</strong> to non-LLM takeover-capable systems</li>
</ul>
<p>Methodological contributions include a <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-a67208b1b6f0" class="internal-link" rel="noopener noreferrer">critique of <strong>METR-HRS</strong></a> timelines forecasting, arguing the 'd' parameter conflates task difficulty with sequence length. Governance-oriented work <a href="https://news.aatf.ai/?date=2026-01-20&amp;category=research#item-3a47324cdf6f" class="internal-link" rel="noopener noreferrer">sketches positive AI transition</a> scenarios co-authored with <strong>Claude Opus 4.5</strong>.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:category-summary:research</id>
    <title>Research Summary: January 19, 2026</title>
    <link href="http://arxiv.org/abs/2601.10904" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-19T06:00:00Z</published>
    <updated>2026-01-19T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>Today's research spans AGI benchmarking, reasoning interpretability, agent evaluation, and safety mechanisms for production AI systems.</p>
<p><strong>ARC Prize 2025</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-85a5776de7ae" class="internal-link" rel="noopener noreferrer">technical report documents</a> 'refinement loops' as the defining pattern among top <strong>ARC-AGI-2</strong> performers. <strong>Reasoning Models Generate Societies of Thought</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-bb5c19abf00b" class="internal-link" rel="noopener noreferrer">reveals that enhanced reasoning</a> in <strong>DeepSeek-R1</strong> and <strong>QwQ-32B</strong> emerges from internal multi-agent-like simulations. <strong>AgencyBench</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-ba9af0a30312" class="internal-link" rel="noopener noreferrer">introduces evaluation</a> at unprecedented scale: <strong>32 scenarios</strong> requiring <strong>~90 tool calls</strong> and <strong>1M tokens</strong>.</p>
<ul>
<li><strong>Google DeepMind</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-972425be59d1" class="internal-link" rel="noopener noreferrer">presents production-ready activation probes</a> for <strong>Gemini</strong> misuse detection addressing distribution shift</li>
<li><strong>Spurious Rewards Paradox</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-0f9c42f7aebb" class="internal-link" rel="noopener noreferrer">identifies mechanistically how RLVR</a> triggers memorization shortcuts via Anchor-Adapter circuits</li>
<li><strong>BAPO</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-a819576acb19" class="internal-link" rel="noopener noreferrer">teaches agentic search systems</a> to recognize reasoning boundaries and output 'I DON'T KNOW'</li>
<li><strong>DialDefer</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-377515b0971a" class="internal-link" rel="noopener noreferrer">exposes 'dialogic deference' bias</a> undermining LLM-as-judge reliability</li>
</ul>
<p>A <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-52e25a92ee5e" class="internal-link" rel="noopener noreferrer">critique of <strong>METR</strong> methodology</a> argues AI capability time horizons may be significantly underestimated. <strong>Meta</strong>'s NeurIPS 2025 DCVLR winner <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-992bf6cbaff6" class="internal-link" rel="noopener noreferrer">shows difficulty-based example selection</a> outperforms dataset diversity. <strong>Digital Metabolism</strong> <a href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-fe36659e452a" class="internal-link" rel="noopener noreferrer">proposes that targeted forgetting</a> can distill pure neural logic cores from factual knowledge.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:85a5776de7ae</id>
    <title>ARC Prize 2025: Technical Report</title>
    <link href="http://arxiv.org/abs/2601.10904" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-85a5776de7ae" rel="related" type="text/html"/>
    <published>2026-01-19T03:36:00Z</published>
    <updated>2026-01-19T03:36:00Z</updated>
    <author><name>Fran\c{c}ois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers</name></author>
    <summary type="html"><![CDATA[<p>Technical report from ARC Prize 2025 competition on ARC-AGI-2 benchmark. Key finding: emergence of 'refinement loops' as defining pattern, with top score 24% from 1,455 teams.</p>]]></summary>
    <category term="AGI"/>
    <category term="Benchmarks"/>
    <category term="Reasoning"/>
    <category term="Program Synthesis"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:bb5c19abf00b</id>
    <title>Reasoning Models Generate Societies of Thought</title>
    <link href="http://arxiv.org/abs/2601.10825" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-bb5c19abf00b" rel="related" type="text/html"/>
    <published>2026-01-19T03:23:00Z</published>
    <updated>2026-01-19T03:23:00Z</updated>
    <author><name>Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Ag\"uera y Arcas, James Evans</name></author>
    <summary type="html"><![CDATA[<p>Analyzes reasoning models (DeepSeek-R1, QwQ-32B) showing enhanced reasoning emerges from simulating multi-agent-like interactions ('society of thought') with distinct personality traits and expertise.</p>]]></summary>
    <category term="LLM Interpretability"/>
    <category term="Reasoning Models"/>
    <category term="Multi-Agent Systems"/>
    <category term="Emergent Behavior"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:ba9af0a30312</id>
    <title>AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts</title>
    <link href="http://arxiv.org/abs/2601.11044" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-ba9af0a30312" rel="related" type="text/html"/>
    <published>2026-01-19T03:16:00Z</published>
    <updated>2026-01-19T03:16:00Z</updated>
    <author><name>Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Wenjie Li, Dequan Wang, Pengfei Liu</name></author>
    <summary type="html"><![CDATA[<p>Introduces AgencyBench evaluating 6 core agentic capabilities across 32 real-world scenarios requiring ~90 tool calls, 1M tokens, and hours of execution. Creates scalable automated evaluation with LLM-simulated humans.</p>]]></summary>
    <category term="LLM Agents"/>
    <category term="Benchmarks"/>
    <category term="Evaluation"/>
    <category term="Autonomous Systems"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:972425be59d1</id>
    <title>Building Production-Ready Probes For Gemini</title>
    <link href="http://arxiv.org/abs/2601.11516" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-972425be59d1" rel="related" type="text/html"/>
    <published>2026-01-19T03:16:00Z</published>
    <updated>2026-01-19T03:16:00Z</updated>
    <author><name>J\'anos Kram\'ar, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy</name></author>
    <summary type="html"><![CDATA[<p>Develops production-ready activation probes for detecting misuse of Gemini models, proposing new architectures that handle long-context distribution shift and evaluating robustness against jailbreaks and adaptive attacks.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Security"/>
    <category term="Interpretability"/>
    <category term="Probing"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:e9f98d420778</id>
    <title>ORBITFLOW: SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache Reconfiguration</title>
    <link href="http://arxiv.org/abs/2601.10729" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-e9f98d420778" rel="related" type="text/html"/>
    <published>2026-01-19T03:07:00Z</published>
    <updated>2026-01-19T03:07:00Z</updated>
    <author><name>Xinyue Ma, Heelim Hong, Taegeon Um, Jongseop Lee, Seoyeong Choy, Woo-Yeon Lee, Myeongjae Jeon</name></author>
    <summary type="html"><![CDATA[<p>Introduces ORBITFLOW for adaptive KV cache management in long-context LLM serving, using lightweight ILP solver to decide layer-wise GPU retention and achieve SLO compliance.</p>]]></summary>
    <category term="LLM Efficiency"/>
    <category term="KV Cache"/>
    <category term="Systems for ML"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:0f9c42f7aebb</id>
    <title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
    <link href="http://arxiv.org/abs/2601.11061" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-0f9c42f7aebb" rel="related" type="text/html"/>
    <published>2026-01-19T03:07:00Z</published>
    <updated>2026-01-19T03:07:00Z</updated>
    <author><name>Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, Wenxi Li, Vincent Wang, Chris Lee</name></author>
    <summary type="html"><![CDATA[<p>Identifies 'Perplexity Paradox' where spurious RLVR triggers memorization shortcuts. Discovers Anchor-Adapter circuit facilitating bypass of reasoning for memorization using mechanistic analysis.</p>]]></summary>
    <category term="RLVR"/>
    <category term="Mechanistic Interpretability"/>
    <category term="LLM Reasoning"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:377515b0971a</id>
    <title>DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference</title>
    <link href="http://arxiv.org/abs/2601.10896" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-377515b0971a" rel="related" type="text/html"/>
    <published>2026-01-19T03:02:00Z</published>
    <updated>2026-01-19T03:02:00Z</updated>
    <author><name>Parisa Rabbani, Priyam Sahoo, Ruben Mathew, Aishee Mondal, Harshita Ketharaman, Nimet Beyza Bozdag, Dilek Hakkani-T\"ur</name></author>
    <summary type="html"><![CDATA[<p>Introduces DialDefer framework detecting 'dialogic deference' where LLMs judge identical claims differently based on framing (statement vs speaker attribution), showing large shifts up to 87pp.</p>]]></summary>
    <category term="LLM Reliability"/>
    <category term="Evaluation"/>
    <category term="Bias Detection"/>
    <category term="AI Safety"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-19:research:a819576acb19</id>
    <title>BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search</title>
    <link href="http://arxiv.org/abs/2601.11037" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-19&amp;category=research#item-a819576acb19" rel="related" type="text/html"/>
    <published>2026-01-19T03:02:00Z</published>
    <updated>2026-01-19T03:02:00Z</updated>
    <author><name>Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Bei Li, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su</name></author>
    <summary type="html"><![CDATA[<p>Introduces BAPO, an RL framework teaching agentic search systems to recognize reasoning boundaries and admit 'I DON'T KNOW' when evidence is insufficient. Uses group-based boundary detection.</p>]]></summary>
    <category term="AI Safety"/>
    <category term="LLM Agents"/>
    <category term="Reinforcement Learning"/>
    <category term="Reliability"/>
  </entry>
  <entry>
    <id>urn:ainews:2026-01-18:category-summary:research</id>
    <title>Research Summary: January 18, 2026</title>
    <link href="https://www.lesswrong.com/posts/WLdcvAcoFZv9enR37/what-washington-says-about-agi" rel="alternate" type="text/html"/>
    <link href="https://news.aatf.ai/?date=2026-01-18&amp;category=research" rel="related" type="text/html"/>
    <published>2026-01-18T06:00:00Z</published>
    <updated>2026-01-18T06:00:00Z</updated>
    <author><name>AATF AI News Aggregator</name></author>
    <summary type="html"><![CDATA[<p>An unusually light day for AI research, with only two substantive original contributions. The standout is a novel <strong>AI-assisted policy analysis</strong> using <strong>Claude Sonnet 4.5</strong> with web search to <a href="https://news.aatf.ai/?date=2026-01-18&amp;category=research#item-d473a553750e" class="internal-link" rel="noopener noreferrer">systematically catalog</a> every US congressperson's public AGI positions—producing actionable governance data.</p>
<ul>
<li>A philosophical piece on AI safety <a href="https://news.aatf.ai/?date=2026-01-18&amp;category=research#item-32842710cab1" class="internal-link" rel="noopener noreferrer">argues that <strong>flourishing-focused interventions</strong></a> may dominate survival-focused ones even under high existential risk scenarios, using mathematical framing</li>
<li><strong>AISC</strong> <a href="https://news.aatf.ai/?date=2026-01-18&amp;category=research#item-516cf5c335c1" class="internal-link" rel="noopener noreferrer">project update</a> on 'Understanding Trust' references an <strong>IQA paper</strong> output from Spring 2025 cohort work</li>
<li><strong>MATS Summer 2026</strong> <a href="https://news.aatf.ai/?date=2026-01-18&amp;category=research#item-b1120185b7d9" class="internal-link" rel="noopener noreferrer">applications closing</a> January 18th—relevant for safety talent pipeline but not research itself</li>
</ul>
<p>Remaining items cover unrelated topics: neuroscience on stuttering therapy, economic analysis of Japan's debt position, job postings, and satirical essays. No technical ML papers or architecture advances appeared today.</p>]]></summary>
    <category term="daily-summary"/>
  </entry>
</feed>