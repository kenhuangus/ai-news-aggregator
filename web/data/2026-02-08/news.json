{
  "category": "news",
  "date": "2026-02-08",
  "category_summary": "**Major M&A Headlines:** The week's biggest story is **SpaceX's** [**acquisition of xAI**](/?date=2026-02-08&category=news#item-61edde54319b), creating a **$1.25 trillion** combined entity with an IPO planned for June 2026—a potentially transformative consolidation of Musk's AI and space ambitions.\n\n**AI Lab Competition & Technical Releases:**\n- **Anthropic** and **OpenAI** are [battling for enterprise customers](/?date=2026-02-08&category=news#item-7feb2389fe30) with Super Bowl-timed ad campaigns, with Anthropic suggesting rivals will adopt targeted advertising\n- **NVIDIA** [released **C-RADIOv4**](/?date=2026-02-08&category=news#item-5ce59a68ee14), a unified vision backbone combining SigLIP2, DINOv3, and SAM3 capabilities\n- **Google AI** [introduced **PaperBanana**](/?date=2026-02-08&category=news#item-a0440b883d94), a multi-agent framework for automated academic visualization\n\n**Emerging Concerns & Applications:** A [data breach](/?date=2026-02-08&category=news#item-fc07a73f6162) at **Moltbook**—a social network for AI agents—highlights security risks in new AI infrastructure. Meanwhile, AI applications range from [**2026 Winter Olympics** viewing tech](/?date=2026-02-08&category=news#item-3e9aba926538) to art authentication [questioning **Van Eyck** painting provenance](/?date=2026-02-08&category=news#item-deb5f4627237).",
  "category_summary_html": "<p><strong>Major M&amp;A Headlines:</strong> The week's biggest story is <strong>SpaceX's</strong> <a href=\"/?date=2026-02-08&amp;category=news#item-61edde54319b\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>acquisition of xAI</strong></a>, creating a <strong>$1.25 trillion</strong> combined entity with an IPO planned for June 2026—a potentially transformative consolidation of Musk's AI and space ambitions.</p>\n<p><strong>AI Lab Competition &amp; Technical Releases:</strong></p>\n<ul>\n<li><strong>Anthropic</strong> and <strong>OpenAI</strong> are <a href=\"/?date=2026-02-08&amp;category=news#item-7feb2389fe30\" class=\"internal-link\" rel=\"noopener noreferrer\">battling for enterprise customers</a> with Super Bowl-timed ad campaigns, with Anthropic suggesting rivals will adopt targeted advertising</li>\n<li><strong>NVIDIA</strong> <a href=\"/?date=2026-02-08&amp;category=news#item-5ce59a68ee14\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>C-RADIOv4</strong></a>, a unified vision backbone combining SigLIP2, DINOv3, and SAM3 capabilities</li>\n<li><strong>Google AI</strong> <a href=\"/?date=2026-02-08&amp;category=news#item-a0440b883d94\" class=\"internal-link\" rel=\"noopener noreferrer\">introduced <strong>PaperBanana</strong></a>, a multi-agent framework for automated academic visualization</li>\n</ul>\n<p><strong>Emerging Concerns &amp; Applications:</strong> A <a href=\"/?date=2026-02-08&amp;category=news#item-fc07a73f6162\" class=\"internal-link\" rel=\"noopener noreferrer\">data breach</a> at <strong>Moltbook</strong>—a social network for AI agents—highlights security risks in new AI infrastructure. Meanwhile, AI applications range from <a href=\"/?date=2026-02-08&amp;category=news#item-3e9aba926538\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>2026 Winter Olympics</strong> viewing tech</a> to art authentication <a href=\"/?date=2026-02-08&amp;category=news#item-deb5f4627237\" class=\"internal-link\" rel=\"noopener noreferrer\">questioning <strong>Van Eyck</strong> painting provenance</a>.</p>",
  "themes": [
    {
      "name": "AI Industry Consolidation & Business",
      "description": "Major M&A activity with SpaceX-xAI merger and intensifying competition between Anthropic and OpenAI for enterprise market share",
      "item_count": 3,
      "example_items": [],
      "importance": 85.0
    },
    {
      "name": "Technical Releases & Research Tools",
      "description": "New releases from NVIDIA and Google AI advancing vision models and multi-agent research automation frameworks",
      "item_count": 3,
      "example_items": [],
      "importance": 65.0
    },
    {
      "name": "AI Agents & Infrastructure",
      "description": "Emerging AI agent ecosystems facing security challenges and community resistance to supporting infrastructure",
      "item_count": 3,
      "example_items": [],
      "importance": 55.0
    },
    {
      "name": "AI Applications & Media",
      "description": "Practical deployments of AI in sports broadcasting, art authentication, and personal productivity automation",
      "item_count": 3,
      "example_items": [],
      "importance": 48.0
    }
  ],
  "total_items": 11,
  "items": [
    {
      "id": "61edde54319b",
      "title": "Why has Elon Musk merged his rocket company with his AI startup?",
      "content": "SpaceX’s acquisition of xAI creates business worth $1.25tn but whether premise behind deal will work is questionedThe acquisition of xAI by SpaceX is a typical Elon Musk deal: big numbers backed by big ambition.As well as extending “the light of consciousness to the stars”, as Musk described it, the transaction creates a business worth $1.25tn (£920bn) by combining Musk’s rocket company with his artificial intelligence startup. It values SpaceX at $1tn and xAI at $250bn, with a stock market flotation expected in June to time with Musk’s birthday and a planetary alignment. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/07/why-has-elon-musk-merged-his-rocket-company-with-his-ai-startup",
      "author": "Dan Milmo Global technology editor",
      "published": "2026-02-07T14:00:04",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Elon Musk",
        "SpaceX",
        "AI (artificial intelligence)",
        "Business",
        "Technology sector",
        "World news",
        "Technology",
        "Science",
        "Mergers and acquisitions",
        "Tesla",
        "X"
      ],
      "summary": "SpaceX has acquired xAI in a blockbuster deal creating a combined entity valued at $1.25 trillion, with SpaceX at $1T and xAI at $250B. An IPO is planned for June 2026, coinciding with Musk's birthday. The merger aims to extend AI capabilities to space exploration.",
      "importance_score": 88.0,
      "reasoning": "Trillion-dollar merger involving a major AI company represents massive industry consolidation. Creates one of the world's most valuable tech entities and signals Musk's ambition to integrate AI with space technology.",
      "themes": [
        "M&A",
        "xAI",
        "SpaceX",
        "Elon Musk",
        "AI industry consolidation"
      ],
      "continuation": null,
      "summary_html": "<p>SpaceX has acquired xAI in a blockbuster deal creating a combined entity valued at $1.25 trillion, with SpaceX at $1T and xAI at $250B. An IPO is planned for June 2026, coinciding with Musk's birthday. The merger aims to extend AI capabilities to space exploration.</p>",
      "content_html": "<p>SpaceX’s acquisition of xAI creates business worth $1.25tn but whether premise behind deal will work is questionedThe acquisition of xAI by SpaceX is a typical Elon Musk deal: big numbers backed by big ambition.As well as extending “the light of consciousness to the stars”, as Musk described it, the transaction creates a business worth $1.25tn (£920bn) by combining Musk’s rocket company with his artificial intelligence startup. It values SpaceX at $1tn and xAI at $250bn, with a stock market flotation expected in June to time with Musk’s birthday and a planetary alignment. Continue reading...</p>"
    },
    {
      "id": "5ce59a68ee14",
      "title": "NVIDIA AI releases C-RADIOv4 vision backbone unifying SigLIP2, DINOv3, SAM3 for classification, dense prediction, segmentation workloads at scale",
      "content": "How do you combine SigLIP2, DINOv3, and SAM3 into a single vision backbone without sacrificing dense or segmentation performance? NVIDIA’s C-RADIOv4 is a new agglomerative vision backbone that distills three strong teacher models, SigLIP2-g-384, DINOv3-7B, and SAM3, into a single student encoder. It extends the AM-RADIO and RADIOv2.5 line, keeping similar computational cost while improving dense prediction quality, resolution robustness, and drop-in compatibility with SAM3.\n\n\n\nThe key idea is simple. Instead of choosing between a vision language model, a self supervised dense model, and a segmentation model, C-RADIOv4 tries to approximate all three at once with one backbone.\n\n\n\nhttps://www.arxiv.org/pdf/2601.17237\n\n\n\nAgglomerative distillation in RADIO\n\n\n\nThe RADIO family uses agglomerative distillation. A single ViT style student is trained to match both dense feature maps and summary tokens from several heterogeneous teachers.\n\n\n\nEarlier RADIO models combined DFN CLIP, DINOv2, and SAM. They already supported multi resolution training but showed &#8216;mode switching&#8217;, where the representation changed qualitatively as input resolution changed. Later work such as PHI-S, RADIOv2.5, and FeatSharp added better multi resolution distillation and regularization, but the teacher set was still limited.\n\n\n\nC-RADIOv4 upgrades the teachers:\n\n\n\n\nSigLIP2-g-384 for stronger image text alignment\n\n\n\nDINOv3-7B for high quality self supervised dense features\n\n\n\nSAM3 for segmentation oriented features and compatibility with the SAM3 decoder\n\n\n\n\nThe student is trained so that its dense features match DINOv3 and SAM3, while its summary tokens match SigLIP2 and DINOv3. This gives one encoder that can support classification, retrieval, dense prediction, and segmentation.\n\n\n\nStochastic multi resolution training\n\n\n\nC-RADIOv4 uses stochastic multi resolution training rather than a small fixed set of resolutions.\n\n\n\nTraining samples input sizes from two partitions:\n\n\n\n\nLow resolution: {128, 192, 224, 256, 384, 432}\n\n\n\nHigh resolution: {512, 768, 1024, 1152}\n\n\n\n\nSigLIP2 operates natively at 384 pixels. Its features are upsampled by a factor of 3 using FeatSharp to align with 1152 pixel SAM3 features. SAM3 is trained with mosaic augmentation at 1152 × 1152.\n\n\n\nThis design smooths the performance curve over resolution and improves low resolution behavior. For example, on ADE20k linear probing, C-RADIOv4-H reaches around:\n\n\n\n\n55.20 mIoU at 512 px\n\n\n\n57.02 mIoU at 1024 px\n\n\n\n57.72 mIoU at 1536 px\n\n\n\n\nThe scaling trend is close to DINOv3-7B while using roughly an order of magnitude fewer parameters.\n\n\n\nRemoving teacher noise with shift equivariant losses and MESA\n\n\n\nDistilling from large vision models tends to copy their artifacts, not just their useful structure. SigLIP2 has border noise patterns, and ViTDet style models can show window boundary artifacts. Direct feature regression can force the student to reproduce those patterns.\n\n\n\nC-RADIOv4 introduces two shift equivariant mechanisms to suppress such noise:\n\n\n\n\nShift equivariant dense loss: Each teacher and the student see independently shifted crops of an image. Before computing the squared error, features are aligned via a shift mapping and the loss only uses overlapping spatial positions. Because the student never sees the same absolute positions as the teacher, it cannot simply memorize position fixed noise and is forced to track input dependent structure instead.\n\n\n\nShift equivariant MESA: C-RADIOv4 also uses MESA style regularization between the online network and an EMA copy. Here again, the student and its EMA see different crops, features are aligned by a shift, and the loss is applied after layer normalization. This encourages smooth loss landscapes and robustness, while being invariant to absolute position.\n\n\n\n\nIn addition, training uses DAMP, which injects multiplicative noise into weights. This further improves robustness to corruptions and small distribution shifts.\n\n\n\nBalancing teachers with an angular dispersion aware summary loss\n\n\n\nThe summary loss in previous RADIO models used cosine distance between student and teacher embeddings. Cosine distance removes magnitude but not directional dispersion on the sphere. Some teachers, such as SigLIP2, produce embeddings concentrated in a narrow cone, while DINOv3 variants produce more spread out embeddings.\n\n\n\nIf raw cosine distance is used, teachers with wider angular dispersion contribute larger losses and dominate optimization. In practice, DINOv3 tended to overshadow SigLIP2 in the summary term.\n\n\n\nC-RADIOv4 replaces this with an angle normalized loss. The squared angle between student and teacher embeddings is divided by the teacher’s angular dispersion. Measured dispersions show SigLIP2-g-384 around 0.694, while DINOv3-H+ and DINOv3-7B are around 2.12 and 2.19. Normalizing by these values equalizes their influence and preserves both vision language and dense semantics.\n\n\n\nPerformance: classification, dense prediction, and Probe3d\n\n\n\nOn ImageNet-1k zero shot classification, C-RADIOv4-H reaches about 83.09 % top-1 accuracy. It matches or improves on RADIOv2.5-H and C-RADIOv3-H across resolutions, with the best performance near 1024 px.\n\n\n\nOn k-NN classification, C-RADIOv4-H improves over RADIOv2.5 and C-RADIOv3, and matches or surpasses DINOv3 starting around 256 px. DINOv3 peaks near 192–256 px and then degrades, while C-RADIOv4 keeps stable or improving performance at higher resolutions.\n\n\n\nDense and 3D aware metrics show the intended tradeoff. On ADE20k, PASCAL VOC, NAVI, and SPair, C-RADIOv4-H and the SO400M variant outperform earlier RADIO models and are competitive with DINOv3-7B on dense benchmarks. For C-RADIOv4-H, typical scores are:\n\n\n\n\nADE20k: 55.20 mIoU\n\n\n\nVOC: 87.24 mIoU\n\n\n\nNAVI: 63.44\n\n\n\nSPair: 60.57\n\n\n\n\nhttps://www.arxiv.org/pdf/2601.17237\n\n\n\nOn Probe3d, which includes Depth Normals, Surface Normals, NAVI, and SPair, C-RADIOv4-H achieves the best NAVI and SPair scores in the RADIO family. Depth and Surface metrics are close to those of C-RADIOv3-H, with small differences in either direction, rather than a uniform improvement.\n\n\n\nIntegration with SAM3 and ViTDet-mode deployment\n\n\n\nC-RADIOv4 is designed to be a drop in replacement for the Perception Encoder backbone in SAM3. The SAM3 decoder and memory components remain unchanged. A reference implementation is provided in a SAM3 fork. Qualitative examples show that segmentation behavior is preserved for both text prompts such as “shoe”, “helmet”, “bike”, “spectator” and box prompts, and in some reported cases C-RADIOv4 based SAM3 resolves failure cases from the original encoder.\n\n\n\nFor deployment, C-RADIOv4 exposes a ViTDet-mode configuration. Most transformer blocks use windowed attention, while a few use global attention. Supported window sizes range from 6 × 6 to 32 × 32 tokens, subject to divisibility with patch size and image resolution. On an A100, the SO400M model with window size at most 12 is faster than the SAM3 ViT-L+ encoder across a wide range of input sizes, and the Huge model with window size 8 is close in latency.\n\n\n\nThis makes C-RADIOv4 a practical backbone for high resolution dense tasks where full global attention at all layers is too expensive.\n\n\n\nKey Takeaways\n\n\n\n\nSingle unified backbone: C-RADIOv4 distills SigLIP2-g-384, DINOv3-7B, and SAM3 into one ViT-style encoder that supports classification, retrieval, dense prediction, and segmentation.\n\n\n\nAny-resolution behavior: Stochastic multi resolution training over {128…1152} px, and FeatSharp upsampling for SigLIP2, stabilizes performance across resolutions and tracks DINOv3-7B scaling with far fewer parameters.\n\n\n\nNoise suppression via shift equivariance: Shift equivariant dense loss and shift equivariant MESA prevent the student from copying teacher border and window artifacts, focusing learning on input dependent semantics.\n\n\n\nBalanced multi-teacher distillation: An angular dispersion normalized summary loss equalizes the contribution of SigLIP2 and DINOv3, preserving both text alignment and dense representation quality.\n\n\n\nSAM3 and ViTDet-ready deployment: C-RADIOv4 can directly replace the SAM3 Perception Encoder, offers ViTDet-mode windowed attention for faster high resolution inference, and is released under the NVIDIA Open Model License.\n\n\n\n\n\n\n\n\nCheck out the Paper, Repo, Model-1 and Model-2. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post NVIDIA AI releases C-RADIOv4 vision backbone unifying SigLIP2, DINOv3, SAM3 for classification, dense prediction, segmentation workloads at scale appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/06/nvidia-ai-releases-c-radiov4-vision-backbone-unifying-siglip2-dinov3-sam3-for-classification-dense-prediction-segmentation-workloads-at-scale/",
      "author": "Asif Razzaq",
      "published": "2026-02-07T00:31:51",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Computer Vision",
        "Editors Pick",
        "Language Model",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "NVIDIA releases C-RADIOv4, a unified vision backbone that distills SigLIP2, DINOv3, and SAM3 into a single student encoder. The model handles classification, dense prediction, and segmentation workloads while maintaining computational efficiency and resolution robustness.",
      "importance_score": 72.0,
      "reasoning": "Significant technical release from a major AI hardware/software player. Unified vision backbones reduce complexity for practitioners and represent meaningful progress in model efficiency and versatility.",
      "themes": [
        "computer vision",
        "NVIDIA",
        "model architecture",
        "open source",
        "multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA releases C-RADIOv4, a unified vision backbone that distills SigLIP2, DINOv3, and SAM3 into a single student encoder. The model handles classification, dense prediction, and segmentation workloads while maintaining computational efficiency and resolution robustness.</p>",
      "content_html": "<p>How do you combine SigLIP2, DINOv3, and SAM3 into a single vision backbone without sacrificing dense or segmentation performance? NVIDIA’s C-RADIOv4 is a new agglomerative vision backbone that distills three strong teacher models, SigLIP2-g-384, DINOv3-7B, and SAM3, into a single student encoder. It extends the AM-RADIO and RADIOv2.5 line, keeping similar computational cost while improving dense prediction quality, resolution robustness, and drop-in compatibility with SAM3.</p>\n<p>The key idea is simple. Instead of choosing between a vision language model, a self supervised dense model, and a segmentation model, C-RADIOv4 tries to approximate all three at once with one backbone.</p>\n<p>https://www.arxiv.org/pdf/2601.17237</p>\n<p>Agglomerative distillation in RADIO</p>\n<p>The RADIO family uses agglomerative distillation. A single ViT style student is trained to match both dense feature maps and summary tokens from several heterogeneous teachers.</p>\n<p>Earlier RADIO models combined DFN CLIP, DINOv2, and SAM. They already supported multi resolution training but showed ‘mode switching’, where the representation changed qualitatively as input resolution changed. Later work such as PHI-S, RADIOv2.5, and FeatSharp added better multi resolution distillation and regularization, but the teacher set was still limited.</p>\n<p>C-RADIOv4 upgrades the teachers:</p>\n<p>SigLIP2-g-384 for stronger image text alignment</p>\n<p>DINOv3-7B for high quality self supervised dense features</p>\n<p>SAM3 for segmentation oriented features and compatibility with the SAM3 decoder</p>\n<p>The student is trained so that its dense features match DINOv3 and SAM3, while its summary tokens match SigLIP2 and DINOv3. This gives one encoder that can support classification, retrieval, dense prediction, and segmentation.</p>\n<p>Stochastic multi resolution training</p>\n<p>C-RADIOv4 uses stochastic multi resolution training rather than a small fixed set of resolutions.</p>\n<p>Training samples input sizes from two partitions:</p>\n<p>Low resolution: {128, 192, 224, 256, 384, 432}</p>\n<p>High resolution: {512, 768, 1024, 1152}</p>\n<p>SigLIP2 operates natively at 384 pixels. Its features are upsampled by a factor of 3 using FeatSharp to align with 1152 pixel SAM3 features. SAM3 is trained with mosaic augmentation at 1152 × 1152.</p>\n<p>This design smooths the performance curve over resolution and improves low resolution behavior. For example, on ADE20k linear probing, C-RADIOv4-H reaches around:</p>\n<p>55.20 mIoU at 512 px</p>\n<p>57.02 mIoU at 1024 px</p>\n<p>57.72 mIoU at 1536 px</p>\n<p>The scaling trend is close to DINOv3-7B while using roughly an order of magnitude fewer parameters.</p>\n<p>Removing teacher noise with shift equivariant losses and MESA</p>\n<p>Distilling from large vision models tends to copy their artifacts, not just their useful structure. SigLIP2 has border noise patterns, and ViTDet style models can show window boundary artifacts. Direct feature regression can force the student to reproduce those patterns.</p>\n<p>C-RADIOv4 introduces two shift equivariant mechanisms to suppress such noise:</p>\n<p>Shift equivariant dense loss: Each teacher and the student see independently shifted crops of an image. Before computing the squared error, features are aligned via a shift mapping and the loss only uses overlapping spatial positions. Because the student never sees the same absolute positions as the teacher, it cannot simply memorize position fixed noise and is forced to track input dependent structure instead.</p>\n<p>Shift equivariant MESA: C-RADIOv4 also uses MESA style regularization between the online network and an EMA copy. Here again, the student and its EMA see different crops, features are aligned by a shift, and the loss is applied after layer normalization. This encourages smooth loss landscapes and robustness, while being invariant to absolute position.</p>\n<p>In addition, training uses DAMP, which injects multiplicative noise into weights. This further improves robustness to corruptions and small distribution shifts.</p>\n<p>Balancing teachers with an angular dispersion aware summary loss</p>\n<p>The summary loss in previous RADIO models used cosine distance between student and teacher embeddings. Cosine distance removes magnitude but not directional dispersion on the sphere. Some teachers, such as SigLIP2, produce embeddings concentrated in a narrow cone, while DINOv3 variants produce more spread out embeddings.</p>\n<p>If raw cosine distance is used, teachers with wider angular dispersion contribute larger losses and dominate optimization. In practice, DINOv3 tended to overshadow SigLIP2 in the summary term.</p>\n<p>C-RADIOv4 replaces this with an angle normalized loss. The squared angle between student and teacher embeddings is divided by the teacher’s angular dispersion. Measured dispersions show SigLIP2-g-384 around 0.694, while DINOv3-H+ and DINOv3-7B are around 2.12 and 2.19. Normalizing by these values equalizes their influence and preserves both vision language and dense semantics.</p>\n<p>Performance: classification, dense prediction, and Probe3d</p>\n<p>On ImageNet-1k zero shot classification, C-RADIOv4-H reaches about 83.09 % top-1 accuracy. It matches or improves on RADIOv2.5-H and C-RADIOv3-H across resolutions, with the best performance near 1024 px.</p>\n<p>On k-NN classification, C-RADIOv4-H improves over RADIOv2.5 and C-RADIOv3, and matches or surpasses DINOv3 starting around 256 px. DINOv3 peaks near 192–256 px and then degrades, while C-RADIOv4 keeps stable or improving performance at higher resolutions.</p>\n<p>Dense and 3D aware metrics show the intended tradeoff. On ADE20k, PASCAL VOC, NAVI, and SPair, C-RADIOv4-H and the SO400M variant outperform earlier RADIO models and are competitive with DINOv3-7B on dense benchmarks. For C-RADIOv4-H, typical scores are:</p>\n<p>ADE20k: 55.20 mIoU</p>\n<p>VOC: 87.24 mIoU</p>\n<p>NAVI: 63.44</p>\n<p>SPair: 60.57</p>\n<p>https://www.arxiv.org/pdf/2601.17237</p>\n<p>On Probe3d, which includes Depth Normals, Surface Normals, NAVI, and SPair, C-RADIOv4-H achieves the best NAVI and SPair scores in the RADIO family. Depth and Surface metrics are close to those of C-RADIOv3-H, with small differences in either direction, rather than a uniform improvement.</p>\n<p>Integration with SAM3 and ViTDet-mode deployment</p>\n<p>C-RADIOv4 is designed to be a drop in replacement for the Perception Encoder backbone in SAM3. The SAM3 decoder and memory components remain unchanged. A reference implementation is provided in a SAM3 fork. Qualitative examples show that segmentation behavior is preserved for both text prompts such as “shoe”, “helmet”, “bike”, “spectator” and box prompts, and in some reported cases C-RADIOv4 based SAM3 resolves failure cases from the original encoder.</p>\n<p>For deployment, C-RADIOv4 exposes a ViTDet-mode configuration. Most transformer blocks use windowed attention, while a few use global attention. Supported window sizes range from 6 × 6 to 32 × 32 tokens, subject to divisibility with patch size and image resolution. On an A100, the SO400M model with window size at most 12 is faster than the SAM3 ViT-L+ encoder across a wide range of input sizes, and the Huge model with window size 8 is close in latency.</p>\n<p>This makes C-RADIOv4 a practical backbone for high resolution dense tasks where full global attention at all layers is too expensive.</p>\n<p>Key Takeaways</p>\n<p>Single unified backbone: C-RADIOv4 distills SigLIP2-g-384, DINOv3-7B, and SAM3 into one ViT-style encoder that supports classification, retrieval, dense prediction, and segmentation.</p>\n<p>Any-resolution behavior: Stochastic multi resolution training over {128…1152} px, and FeatSharp upsampling for SigLIP2, stabilizes performance across resolutions and tracks DINOv3-7B scaling with far fewer parameters.</p>\n<p>Noise suppression via shift equivariance: Shift equivariant dense loss and shift equivariant MESA prevent the student from copying teacher border and window artifacts, focusing learning on input dependent semantics.</p>\n<p>Balanced multi-teacher distillation: An angular dispersion normalized summary loss equalizes the contribution of SigLIP2 and DINOv3, preserving both text alignment and dense representation quality.</p>\n<p>SAM3 and ViTDet-ready deployment: C-RADIOv4 can directly replace the SAM3 Perception Encoder, offers ViTDet-mode windowed attention for faster high resolution inference, and is released under the NVIDIA Open Model License.</p>\n<p>Check out the&nbsp;Paper, Repo, Model-1 and Model-2.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post NVIDIA AI releases C-RADIOv4 vision backbone unifying SigLIP2, DINOv3, SAM3 for classification, dense prediction, segmentation workloads at scale appeared first on MarkTechPost.</p>"
    },
    {
      "id": "dc98ffeb01f2",
      "title": "Battle of the chatbots: Anthropic and OpenAI go head-to-head over ads in their AI products",
      "content": "New Anthropic campaign suggests other AI platforms will incorporate targeted ads in their chatbot conversationsThe Seahawks and the Patriots aren’t the only ones gearing up for a fight.AI rivals Anthropic and OpenAI have launched a war of ads trying to court corporate America during one of the biggest entertainment nights of the year. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/07/ai-chatbots-anthropic-openai-claude-chatgpt",
      "author": "Sanya Mansoor",
      "published": "2026-02-07T16:00:06",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Advertising",
        "Chatbots",
        "ChatGPT",
        "OpenAI",
        "Sam Altman",
        "Technology",
        "US news"
      ],
      "summary": "As covered in [News](/?date=2026-02-07&category=news#item-ea3ea855edba) yesterday, Anthropic and OpenAI are engaged in a Super Bowl-timed advertising battle for corporate customers. Anthropic's campaign suggests competing AI platforms will incorporate targeted ads, positioning itself as an alternative to ad-supported AI.",
      "importance_score": 62.0,
      "reasoning": "Reflects intensifying competition between the two leading frontier AI labs and highlights emerging business model debates around AI monetization that could shape the industry.",
      "themes": [
        "Anthropic",
        "OpenAI",
        "AI business models",
        "advertising",
        "competition"
      ],
      "continuation": {
        "original_item_id": "ea3ea855edba",
        "original_date": "2026-02-07",
        "original_category": "news",
        "original_title": "Enterprises Don't Care About Anthropic's Super Bowl Ad",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As covered in **News** yesterday"
      },
      "summary_html": "<p>As covered in <a href=\"/?date=2026-02-07&amp;category=news#item-ea3ea855edba\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> yesterday, Anthropic and OpenAI are engaged in a Super Bowl-timed advertising battle for corporate customers. Anthropic's campaign suggests competing AI platforms will incorporate targeted ads, positioning itself as an alternative to ad-supported AI.</p>",
      "content_html": "<p>New Anthropic campaign suggests other AI platforms will incorporate targeted ads in their chatbot conversationsThe Seahawks and the Patriots aren’t the only ones gearing up for a fight.AI rivals Anthropic and OpenAI have launched a war of ads trying to court corporate America during one of the biggest entertainment nights of the year. Continue reading...</p>"
    },
    {
      "id": "7feb2389fe30",
      "title": "[AINews] AI vs SaaS: The Unreasonable Effectiveness of Centralizing the AI Heartbeat",
      "content": "AI News for 2/5/2026-2/6/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 8727 messages) for you. Estimated reading time saved (at 200wpm): 666 minutes. AINews&#8217; website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Everyone is still digesting the OpenAI vs Anthropic launches, and the truth will out.We&#8217;ll use this occasion to step back a bit and present seemingly unrelated items:In A sane but extremely bull case on Clawdbot / OpenClaw, the author uses the same agent as a central cron job to remind himself of promises, accumulate information for calendar invites, prepare for the next day, summarize high volume group chats, set complex price alerts, take fridge freezer inventory, maintain a grocery list, booking restaurants and dentists, filling out a form and have Sam Altman&#8217;s &#8220;magic autocompleting todolist&#8221;.The distribution hack that Moltbook uncovered is the installation process immediately installs a HEARTBEAT.md that takes advantage of OpenClaw&#8217;s built in heartbeating to power the motive force of the agents filling up MoltbookIn Cursor&#8217;s Towards self-driving codebases, the author moves from decentralized agents to having a central Planner agent that commands workers and spins up other planners in order to have throughput of ~1000 commits per hour.In OpenAI Frontier, the big reveal of their management layer for large numbers of high volume agents is centralized in a dashboard that can drill down&#8230; to the individual agent instance (!)In CEO Dara Khosrowshahi&#8217;s answer about Uber being inside ChatGPT, they are secure enough in their moat that they are fine just being a ChatGPT app:and of course the ongoing SaaS stocks freakout to AI generally:It&#8217;s famously known that the only 2 ways to make money in software are by bundling it and unbundling it, and what&#8217;s going on here is a massive AI-enabled bundling of all software, probably at a larger magnitude than the hardware bundling of the smartphone:Attempts at building SuperApps have repeatedly failed outside of China, but it&#8217;s clear that both ChatGPT and Claude Cowork are well on their way to being AI &#8220;Superapps&#8221;, except instead of every app having their &#8220;own app&#8221;, they make themselves legible to the AI Overlords with MCP UI and Skills and OpenClaw markdown files, and eventually (not soon! according to Sam&#8217;s answer to Michael Grinich) they will share tokens so that you don&#8217;t die a Death By A Thousand $20/Month Subscriptions.AI Twitter RecapFrontier coding models: GPT-5.3-Codex vs Claude Opus 4.6 (and what &#8220;agentic&#8221; now means)User consensus snapshot: A large chunk of the feed is real-world A/B testing of GPT-5.3-Codex vs Claude Opus 4.6, often concluding that they&#8217;re both clear generational upgrades but with distinct profiles. People characterize Codex as detail-obsessed and strong on scoped tasks, while Opus feels more ergonomic for exploratory work and planning (rishdotblog, @theo). Several notes highlight Codex&#8217;s &#8220;auto compaction&#8221;/garbage-collecting context and frequent progress updates during work&#8212;perceived as a UX win for long tasks (cto_junior).AI-engineer-in-the-loop benchmarks: A particularly concrete evaluation is optimizing Karpathy&#8217;s nanochat &#8220;GPT-2 speedrun&#8221;. @Yuchenj_UW reports both models behaved like competent AI engineers (read code, propose experiments, run benchmarks), with Opus 4.6 delivering measurable wall-clock gains (e.g., torch compile config tweaks, optimizer step changes, memory reductions) while Codex-5.3-xhigh produced ideas but sometimes harmed quality&#8212;possibly due to context issues (he observed it hitting &#8220;0% context&#8221;).Reality check from Karpathy: @karpathy pushes back on the idea that models can already do open-ended closed-loop AI engineering reliably: they can chase spurious 1% wins with big hidden costs, miss key validation checks, violate repo style instructions, and even misread their own result tables&#8212;still &#8220;net useful with oversight,&#8221; but not yet robust for autonomous optimization.No API as product strategy: One thread claims there is no GPT-5.3-Codex API, implying OpenAI is intentionally funneling usage into the Codex product (and making independent benchmarking harder) (scaling01). In parallel, Sam Altman explicitly asks how users want Codex pricing structured (sama).Agent swarms &amp; &#8220;software teams in a box&#8221;Parallel-agent development starts to look like org design: Discussion around highly-parallel agent research notes that unconstrained swarms tend to reinvent the software org chart (task assignment, coordination, QA) and stress existing tooling (Git/package managers) not built for massive concurrent edits (swyx). This echoes broader &#8220;spec-driven development&#8221; / &#8220;agents as dev teams&#8221; narratives (dbreunig).Claude Code &#8220;agent teams&#8221; moment: Multiple tweets reference Anthropic-style agent coordination systems where agents pick tasks, lock files, and sync via git&#8212;framed as a step-change in practical automation (omarsar0, HamelHusain).LangChain / LangSmith: agents need traces, sandboxes, and state control: There&#8217;s a strong theme that reliability comes from engineering the environment: tracing, evals, sandboxing, and type-safe state/middleware. Examples include LangSmith improvements (trace previews; voice-agent debugging) and deepagents adding sandbox backends like daytona/deno/modal/node VFS (LangChain, LangChain, bromann, sydneyrunkle).&#8220;RLM&#8221; framing (Recursive Language Models): A notable conceptual post argues agents will evolve from &#8220;LLM + tool loop&#8221; (ReAct) into REPL-native, program-like systems where context is stored in variables, sub-agents communicate via structured values instead of dumping text into the prompt, and &#8220;context rot&#8221; is reduced by construction (deepfates). Related: practical tips to make coding agents more &#8220;RLM-like&#8221; by pushing context into variables and avoiding tool I/O spam in the prompt (lateinteraction).Eval integrity, benchmark drift, and new infrastructure for &#8220;trustworthy&#8221; scores&#8220;Scores are broken&#8221; &#8594; decentralize evals: Hugging Face launched Community Evals: benchmark datasets hosting leaderboards, eval results stored as versioned YAML in model repos, PR-based submissions, and reproducibility badges (via Inspect AI), explicitly aiming to make evaluation provenance visible even if it can&#8217;t solve contamination/saturation (huggingface, ben_burtenshaw, mervenoyann).Benchmarks aren&#8217;t saturated (yet): A counterpoint emphasizes several difficult benchmarks still have lots of headroom (e.g., SWE-bench Multilingual &lt;80%, SciCode 56%, CritPt 12%, VideoGameBench 1%, efficiency benchmarks far from implied ceilings) (OfirPress).Opus 4.6 benchmark story: big jumps, still uneven: There are repeated claims of Opus 4.6 climbing to top ranks on Arena and other leaderboards (arena, scaling01), including strong movement on math-oriented evals (FrontierMath) where Anthropic historically lagged. Epoch&#8217;s reporting frames Opus 4.6 Tier 4 at 21% (10/48), statistically tied with GPT-5.2 xhigh at 19%, behind GPT-5.2 Pro at 31% (EpochAIResearch). But other reasoning-heavy areas (e.g., chess puzzles) remain weak (scaling01).Eval infra at scale (StepFun): A deep infra write-up about Step 3.5 Flash argues reproducible scoring requires handling failure modes, training&#8211;inference consistency, contamination checks, robust judging/extraction, and long-output monitoring; &#8220;evaluation should slightly lead training&#8221; (ZhihuFrontier).World models graduate into production: Waymo + DeepMind&#8217;s Genie 3Waymo World Model announcement: Waymo unveiled a frontier generative simulation model built on DeepMind&#8217;s Genie 3, used to generate hyper-realistic, interactive scenarios&#8212;including rare &#8220;impossible&#8221; events (tornadoes, planes landing on freeways)&#8212;to stress-test the Waymo Driver long before real-world exposure (Waymo).Key technical hook: DeepMind highlights transfer of Genie 3 &#8220;world knowledge&#8221; into Waymo-specific camera + 3D lidar representations, enabling promptable &#8220;what if&#8221; scenario generation that matches Waymo hardware modalities (GoogleDeepMind, GoogleDeepMind). Multiple researchers point out that extending simulation beyond pixels to sensor streams is the real milestone (shlomifruchter, sainingxie).Broader &#8220;world models for reasoning&#8221; thread: The Waymo news is repeatedly used as evidence that world models (not just text models) are a central scaling frontier for reasoning and embodied tasks (swyx, kimmonismus, JeffDean, demishassabis).Planning advances for world models: GRASP is introduced as a gradient-based, stochastic, parallelized planner that jointly optimizes actions and intermediate subgoals to improve long-horizon planning vs. common zeroth-order planners (CEM/MPPI) (michaelpsenka, _amirbar).Memory, long-context control, and multi-agent &#8220;cognitive infrastructure&#8221;InfMem: bounded-memory agent with cognitive control: InfMem proposes a PRETHINK&#8211;RETRIEVE&#8211;WRITE protocol with RL for long-document QA up to 1M tokens, emphasizing that longer context windows shift the bottleneck to what to attend to / when to stop. Reported gains include substantial accuracy improvements over baselines and 3.9&#215; average latency reduction via adaptive stopping (omarsar0).LatentMem: role-aware latent memory for multi-agent systems: LatentMem addresses &#8220;homogenization&#8221; (agents retrieving the same memories despite different roles) by compressing trajectories into role-conditioned latent memory, trained with a policy-optimization method (LMPO). Claims include improvements across QA and coding tasks plus ~50% fewer tokens / faster inference (dair_ai).Product reality: memory leaks and context saturation: While agentic tooling is shipping fast, developers complain about resource bloat and brittle UX (e.g., &#8220;memory leaks&#8221; in fast-moving agent IDEs) (code_star). Another thread suspects sub-agent outputs can overwhelm context budgets faster than compaction can recover, hinting at hidden internal longer-context systems (RylanSchaeffer).Industry adoption, compute economics, and &#8220;jobs vs tasks&#8221; discourseNon-verifiable work limits full automation: Fran&#231;ois Chollet argues that in non-verifiable domains, performance gains mostly come from expensive data curation with diminishing returns; since most jobs aren&#8217;t end-to-end verifiable, &#8220;AI can automate many tasks&#8221; &#8800; &#8220;AI replaces the job&#8221; for a long time (fchollet, fchollet).Contrasting takes: RSI bottlenecks: Another viewpoint claims tasks will fall in the order they bottleneck recursive self-improvement, with software engineering first (tszzl).Enterprise deployment signals: Posts claim Goldman Sachs rolling out Claude for accounting automation (kimmonismus), while broader market narratives assert AI is now spooking software-heavy sectors (though the strongest claims are not independently substantiated in-tweet) (kimmonismus).Capex scale: Several tweets highlight hyperscaler spend acceleration; one claims 2026 combined capex for major hyperscalers near $650B (~2% of US GDP) as an &#8220;AI arms race&#8221; framing (scaling01), alongside a note that hyperscaler data center capex may double in 2026 (kimmonismus).Old-guard reassurance to engineers: Eric S. Raymond delivers a high-engagement &#8220;programming isn&#8217;t obsolete&#8221; argument: systems remain complex and the human-intent-to-computer-spec gap persists; the prescription is adaptation and upskilling, not panic (esrtweet).Top tweets (by engagement)Microinteracti1: viral political commentary post (highly engaged; not technical).elonmusk: &#8220;Here we go&#8221; (context not provided in tweet text dump).esrtweet: &#8220;programming panic is a bust; upskill.&#8221;Waymo: Waymo World Model built on Genie 3 for rare-event simulation.sama: &#8220;5.3 lovefest&#8221; / model excitement.claudeai: &#8220;Built with Opus 4.6&#8221; virtual hackathon ($100K API credits).chatgpt21: Opus 4.6 &#8220;pokemon clone&#8221; claim (110k tokens, 1.5h reasoning).theo: &#8220;I know an Opus UI when i see one&#8221; (UI/launch zeitgeist).ID_AA_Carmack: speculative systems idea: streaming weights via fiber loop / flash bandwidth for inference.AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local AI on Low-End HardwareCPU-only, no GPU computers can run all kinds of AI tools locally (Activity: 544): The post highlights the capability of running AI tools locally on a CPU-only setup, specifically using a Dell OptiPlex 3060 with an i5-8500 processor and 32GB of RAM. The user successfully runs 12B Q4_K_M gguf LLMs using KoboldCPP, enabling local chatbot interactions with models from Hugging Face. Additionally, the setup supports Stable Diffusion 1.5 for image generation, albeit slowly, and Chatterbox TTS for voice cloning. The post emphasizes that advanced AI tasks can be performed on minimal hardware, challenging the notion that expensive, GPU-heavy setups are necessary for local AI experimentation. Some commenters express optimism about the future of AI being accessible on basic hardware, while others note a divide in the community regarding hardware elitism and the accessibility of running local models.noctrex suggests trying out specific models like LFM2.5-1.2B-Instruct, LFM2.5-1.2B-Thinking, and LFM2.5-VL-1.6B for CPU-only setups. These models are praised for their small size and efficiency, making them suitable for running on CPU-only docker machines without the need for expensive GPU hardware.Techngro expresses optimism about the future of AI being accessible to the average person through local models that are both intelligent and small enough to run on basic hardware. This vision contrasts with the current trend of relying on large, expensive models hosted by companies, suggesting a shift towards more democratized AI usage.NoobMLDude provides practical applications for local AI setups, such as using them as private meeting note takers or talking assistants. This highlights the versatility and potential of local AI models to perform useful tasks without the need for high-end hardware.No NVIDIA? No Problem. My 2018 &#8220;Potato&#8221; 8th Gen i3 hits 10 TPS on 16B MoE. (Activity: 866): A user in Burma successfully ran a 16B MoE model, DeepSeek-Coder-V2-Lite, on an HP ProBook 650 G5 with an i3-8145U CPU and 16GB RAM, achieving 10 TPS using integrated Intel UHD 620 graphics. The setup leverages OpenVINO as a backend for llama-cpp-python, highlighting the efficiency of MoE models, which compute only 2.4B parameters per token. The user emphasizes the importance of dual-channel RAM and using Linux to minimize resource overhead. Initial iGPU compilation lag and occasional language drift were noted as challenges. Commenters appreciated the ingenuity and resourcefulness of the setup, with some noting that the GPU shortage era has improved optimization skills. There was interest in the user&#8217;s daily driver model for coding tasks.The comment by ruibranco highlights the importance of dual-channel RAM in CPU inference, noting that memory bandwidth, rather than compute power, is often the bottleneck. By switching from single to dual-channel RAM, throughput can effectively double, which is crucial for running models like the 16B MoE on a CPU. The MoE architecture is praised for its efficiency, as it only activates 2.4B parameters per token, allowing the model to fit within the cache of an 8th Gen i3 processor.The use of MoE (Mixture of Experts) architecture is noted for its efficiency in this setup, as it reduces the active parameter count to 2.4B per token, which is manageable for the CPU&#8217;s cache. This approach is particularly beneficial for older CPUs like the 8th Gen i3, as it minimizes the working set size, enhancing performance without requiring high-end hardware.The comment also touches on potential precision issues with OpenVINO&#8217;s INT8/FP16 path on older iGPUs like the UHD 620, which may cause &#8216;Chinese token drift&#8217;. This suggests that the limited compute precision of these iGPUs could affect the accuracy of the model&#8217;s output, highlighting a technical challenge when using older integrated graphics for machine learning tasks.Anyone here actually using AI fully offline? (Activity: 383): Running AI models fully offline is feasible with tools like LM Studio, Ollama, and openwebUI. These platforms allow users to operate models locally, with LM Studio and Ollama providing access to models via platforms like Hugging Face and their own repositories. openwebUI offers a local web interface similar to ChatGPT, and can be combined with ComfyUI for image generation, though it is more complex. Users report that while offline AI setups can be challenging, they are viable for tasks like coding and consulting, with models like gpt-oss-20b being used effectively in these environments. Some users find offline AI setups beneficial for specific tasks like coding and consulting, though they note that these setups can require significant computational resources, especially for coding workflows. The complexity of setup and maintenance is a common challenge, but the control and independence from cloud services are valued.Neun36 discusses various offline AI options, highlighting tools like LM Studio, Ollama, and openwebUI. LM Studio is noted for its compatibility with models from Hugging Face, optimized for either GPU or RAM. Ollama offers local model hosting, and openwebUI provides a browser-based interface similar to ChatGPT, with the added complexity of integrating ComfyUI for image generation.dsartori mentions using AI offline for coding, consulting, and community organizing, emphasizing that coding workflows demand a robust setup. A teammate uses the gpt-oss-20b model in LMStudio, indicating its utility in consulting but not as a sole solution.DatBass612 shares a detailed account of achieving a positive ROI within five months after investing in a high-end M3 Ultra to run OSS 120B models. They estimate daily token usage at around $200, and mention the potential for increased token usage with tools like OpenClaw, highlighting the importance of having sufficient unified memory for virtualization and sub-agent operations.2. OpenClaw and Local LLMs ChallengesOpenClaw with local LLMs - has anyone actually made it work well? (Activity: 200): The post discusses transitioning from the Claude API to local LLMs like Ollama or LM Studio to reduce costs associated with token usage. The user is considering models like Llama 3.1 or Qwen2.5-Coder for tool-calling capabilities without latency issues. Concerns about security vulnerabilities in OpenClaw are noted, with some users suggesting alternatives like Qwen3Coder for agentic tasks. A Local AI playlist is shared for further exploration of secure local LLM applications. Commenters express skepticism about OpenClaw due to security issues, suggesting that investing in VRAM for local models is preferable to paying for API services. Some users have experimented with local setups but remain cautious about security risks.Qwen3Coder and Qwen3Coder-Next are highlighted as effective for tool calling and agentic uses, with a link provided to Qwen3Coder-Next. The commenter notes security concerns with OpenClaw, suggesting alternative secure uses for local LLMs, such as private meeting assistants and coding assistants, and provides a Local AI playlist for further exploration.A user describes experimenting with OpenClaw by integrating it with a local gpt-oss-120b model in lmstudio, emphasizing the importance of security by running it under a nologin user and restricting permissions to a specific folder. Despite the technical setup, they conclude that the potential security risks outweigh the benefits of using OpenClaw.Another user reports using OpenClaw with qwen3 coder 30b, noting that while the setup process was challenging due to lack of documentation, the system performs well, allowing the creation of new skills through simple instructions. This highlights the potential of OpenClaw when paired with powerful local models, despite initial setup difficulties.Clawdbot / Moltbot &#8594; Misguided Hype? (Activity: 86): Moltbot (OpenClaw) is marketed as a &#8216;free personal AI assistant&#8217; but requires multiple paid subscriptions to function effectively. Users need API keys from Anthropic, OpenAI, and Google AI for AI models, a Brave Search API for web search, and ElevenLabs or OpenAI TTS credits for voice features. Additionally, browser automation requires Playwright setup, potentially incurring cloud hosting costs. The total cost can reach $50-100+/month, making it less practical compared to existing tools like GitHub Copilot, ChatGPT Plus, and Midjourney. The project is more suited for developers interested in tinkering rather than a ready-to-use personal assistant. Some users argue that while Moltbot requires multiple subscriptions, it&#8217;s possible to self-host components like LLMs and TTS to avoid costs, though this may not match the performance of cloud-based solutions. Others note that the bot isn&#8217;t truly &#8216;local&#8217; and requires significant technical knowledge to set up effectively.No_Heron_8757 discusses a hybrid approach using ChatGPT Plus for main LLM tasks while offloading simpler tasks to local LLMs via LM Studio. They highlight the integration of web search and browser automation within the same VM, and the use of Kokoro for TTS, which performs adequately on semi-modern GPUs. They express a desire for better performance with local LLMs as primary models, noting the current speed limitations without expensive hardware.Valuable-Fondant-241 emphasizes the feasibility of self-hosting LLMs and related services like TTS, countering the notion that a subscription is necessary. They acknowledge the trade-off in power and speed compared to datacenter-hosted solutions but assert that self-hosting is a viable option for those with the right knowledge and expectations, particularly in this community where such practices are well understood.clayingmore highlights the community&#8217;s focus on optimizing cost-to-quality-and-quantity for local LLMs, noting that running low-cost local models is often free. They describe the innovative &#8216;heartbeat&#8217; pattern in OpenClaw, where the LLM autonomously strategizes and solves problems through reasoning-act loops, verification, and continuous improvement. This agentic approach is seen as a significant advancement, contrasting with traditional IDE code assistants.3. Innovative AI Model and Benchmark ReleasesBalatroBench - Benchmark LLMs&#8217; strategic performance in Balatro (Activity: 590): BalatroBench is a new benchmark for evaluating the strategic performance of local LLMs in the game Balatro. The system uses two main components: BalatroBot, a mod that provides an HTTP API for game state and controls, and BalatroLLM, a bot framework that allows users to define strategies using Jinja2 templates. These templates dictate how the game state is presented to the LLM and guide its decision-making process. The benchmark supports any OpenAI-compatible endpoint, enabling diverse model evaluations, including open-weight models. Results are available on BalatroBench. Commenters appreciate the real-world evaluation aspect of BalatroBench and suggest using evolutionary strategies like DGM, OpenEvolve, SICA, or SEAL to test LLMs&#8217; ability to self-evolve using the Jinja2-based framework.TomLucidor suggests using frameworks like DGM, OpenEvolve, SICA, or SEAL to test which LLM can self-evolve the fastest when playing Balatro, especially if the game is Jinja2-based. These frameworks are known for their ability to facilitate self-evolution in models, providing a robust test of strategic performance.jd_3d is interested in testing Opus 4.6 on Balatro to see if it shows any improvement over version 4.5. This implies a focus on version-specific performance enhancements and how they translate into strategic gameplay improvements.jacek2023 highlights the potential for using local LLMs to play Balatro, which could be a significant step in evaluating LLMs&#8217; strategic capabilities in a real-world setting. This approach allows for direct testing of models&#8217; decision-making processes in a controlled environment.We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels &#8212; open weights on HF (Activity: 302): Trillion Labs and KAIST AI have released gWorld, an open-weight visual world model for mobile GUIs, available in 8B and 32B sizes on Hugging Face. Unlike traditional models that predict screens as pixels, gWorld generates executable web code (HTML/CSS/JS) to render images, leveraging strong priors from pre-training on structured web code. This approach significantly improves visual fidelity and text rendering, achieving 74.9% accuracy with the 8B model on MWMBench, outperforming models up to 50&#215; its size, such as the 402B Llama 4 Maverick. The model&#8217;s render failure rate is less than 1%, and it generalizes well across languages, as demonstrated by its performance on the Korean apps benchmark (KApps). Some commenters question the claim of beating 402B Llama 4, noting that the Maverick model, which is 17B active, had a disappointing reception. Others are impressed by gWorld outperforming models like GLM and Qwen, suggesting the title may be misleading.The claim that an 8B world model beats a 402B Llama 4 model is questioned, with a specific reference to Maverick, a 17B model that was released with underwhelming coding performance. This highlights skepticism about the model&#8217;s capabilities and the potential for misleading claims in AI model announcements.A technical inquiry is made about the nature of the model, questioning whether it is truly a &#8216;world model&#8217; or simply a large language model (LLM) that predicts the next HTML page. This raises a discussion about the definition and scope of world models versus traditional LLMs in AI.The discussion touches on the model&#8217;s output format, specifically whether it generates HTML. This suggests a focus on the model&#8217;s application in web code generation rather than traditional pixel-based outputs, which could imply a novel approach to AI model design and utility.Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy (Activity: 674): Google Research has introduced a new technique called Sequential Attention designed to optimize AI models by reducing their size and computational demands while maintaining performance. This approach focuses on subset selection to enhance efficiency in large-scale models, addressing the NP-hard problem of feature selection in deep neural networks. The method is detailed in a paper available on arXiv, which, despite being published three years ago, is now being highlighted for its practical applications in current AI model optimization. Commenters noted skepticism about the claim of maintaining accuracy, suggesting it means the model performs well in tests rather than computing the same results as previous methods like Flash Attention. There is also curiosity about its performance in upcoming benchmarks like Gemma 4.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Opus 4.6 and GPT-5.3 Codex Releases and BenchmarksGPT-5.3-Codex was used to create itself (Activity: 558): The image discusses the development of GPT-5.3-Codex, emphasizing its unique role in self-development. It highlights that early versions of the model were actively used in debugging its own training processes, managing deployment, and diagnosing test results, showcasing a significant step in AI self-sufficiency. This marks a notable advancement in AI capabilities, where a model contributes directly to its own iterative improvement, potentially accelerating development cycles and reducing human intervention. The comments reflect a mix of humor and concern about AI&#8217;s growing role in management and development, with one user joking about AI replacing mid-level managers and another expressing apprehension about job security.Claude Opus 4.6 is out (Activity: 1189): The image highlights the release of Claude Opus 4.6, a new version of a model by Anthropic. The interface suggests a focus on user interaction with a text input box for queries. The dropdown menu indicates that this version is part of a series, with previous versions like &#8220;Sonnet 4.5&#8221; and &#8220;Haiku 4.5&#8221; also available. A notable benchmark achievement is mentioned in the comments, with Claude Opus 4.6 scoring 68.8% on the ARC-AGI 2 test, which is a significant performance indicator for AI models. This release seems to be in response to competitive pressures, as noted by a comment about a concurrent update from Codex. One comment humorously notes the model&#8217;s description as being for &#8220;ambitious work,&#8221; which may not align with all users&#8217; needs. Another comment suggests that the release timing was influenced by competitive dynamics with Codex.SerdarCS highlights that Claude Opus 4.6 achieves a 68.8% score on the ARC-AGI 2 benchmark, which is a significant performance indicator for AI models. This score suggests substantial improvements in the model&#8217;s capabilities, potentially positioning it as a leader in the field. Source.Solid_Anxiety8176 expresses interest in test results for Claude Opus 4.6, noting that while Opus 4.5 was already impressive, improvements such as a cheaper cost and a larger context window would be highly beneficial. This reflects a common user interest in both performance enhancements and cost efficiency in AI models.Anthropic releases Claude Opus 4.6 model, same pricing as 4.5 (Activity: 931): Anthropic has released the Claude Opus 4.6 model, which is highlighted as the most capable for ambitious work while maintaining the same pricing as the previous 4.5 version. The image provides a comparison chart showing the performance of Opus 4.6 against other models like Opus 4.5, Sonnet 4.5, Gemini 3 Pro, and GPT-5.2. Key performance metrics include agentic terminal coding, agentic coding, and multidisciplinary reasoning, with Opus 4.6 excelling particularly in agentic tool use and multilingual Q&amp;A. The model&#8217;s ARC-AGI score is notably high, indicating significant advancements in artificial general intelligence capabilities. Commenters note the impressive ARC-AGI score of Opus 4.6, suggesting it could lead to rapid saturation in the market. However, there is a mention of no progress in the SWE benchmark, indicating some areas where the model may not have improved.The ARC-AGI score for Claude Opus 4.6 is notably high, indicating significant advancements in general AI capabilities. This score suggests that the model has improved in areas related to artificial general intelligence, which could lead to broader applications and increased adoption in the coming months.Despite the impressive ARC-AGI score, there appears to be no progress in the SWE (Software Engineering) benchmark. This suggests that while the model has improved in general intelligence, its specific capabilities in software engineering tasks remain unchanged compared to previous versions.The update to Claude Opus 4.6 seems to provide a more well-rounded performance, with significant improvements in general intelligence metrics like ARC-AGI and HLE (Human-Level Evaluation). However, for specialized tasks such as coding, the upcoming Sonnet 5 model might offer better performance, indicating a strategic focus on different model strengths for varied applications.OpenAI released GPT 5.3 Codex (Activity: 981): OpenAI has released GPT-5.3-Codex, a groundbreaking model that was instrumental in its own development, using early versions to debug, manage deployment, and diagnose evaluations. It shows a 25% increase in speed and excels in benchmarks like SWE-Bench Pro and Terminal-Bench, achieving a 77.3% score, surpassing previous models like Opus. This model is capable of autonomously building complex applications, collaborating interactively, and identifying software vulnerabilities, marking a significant step towards a general-purpose technical agent. More details can be found in the original article. There is a debate regarding the benchmark results, with some users questioning the validity of the 77.3% score compared to other models like Opus, suggesting potential discrepancies or &#8216;cooking&#8217; of results.GPT-5.3-Codex has been described as a self-improving model, where early versions were utilized to debug its own training and manage deployment. This self-referential capability reportedly accelerated its development significantly, showcasing a novel approach in AI model training and deployment.A benchmark comparison highlights that GPT-5.3-Codex achieved a 77.3% score on a terminal benchmark, surpassing the 65% score of Opus. This significant performance difference raises questions about the benchmarks used and whether they are directly comparable or if there are discrepancies in the testing conditions.The release of GPT-5.3-Codex is noted for its substantial improvements over previous versions, such as Opus 4.6. While Opus 4.6 offers a 1 million token context window, the enhancements in GPT-5.3&#8217;s capabilities appear more impactful on paper, suggesting a leap in performance and functionality.We tasked Opus 4.6 using agent teams to build a C compiler. Then we (mostly) walked away. Two weeks later, it worked on the Linux kernel. (Activity: 553): A team of 16 parallel Claude instances developed a Rust-based C compiler capable of compiling the Linux kernel across multiple architectures, achieving a 100,000-line codebase. This project highlights the potential of autonomous agent teams, emphasizing the importance of high-quality tests, task management, and parallelism. Despite its success, limitations remain, such as the absence of a 16-bit x86 compiler and assembler. The project serves as a benchmark for language model capabilities, demonstrating significant advancements in compiler generation. Codex 5.3 achieved equal performance to earlier models on SWE-bench at half the token count, indicating improved per-token efficiency. Commenters express excitement and unease about the rapid progress in language models, noting the need for new strategies to navigate potential risks. There is a discussion on per-token efficiency, with Codex 5.3 achieving equal performance at half the token count, suggesting improved efficiency and potential cost reductions.The experiment with Opus 4.6 highlights the rapid advancements in language models and their scaffolds, enabling the creation of complex software like a C compiler with minimal human intervention. This progress suggests a shift towards more autonomous software development, but also raises concerns about the need for new strategies to manage potential risks associated with such powerful tools.The project involved nearly 2,000 Claude Code sessions and incurred $20,000 in API costs, raising questions about the efficiency of token usage in large-scale AI projects. Notably, the Codex 5.3 release notes indicate that it achieved similar performance to earlier models on the SWE-bench with half the token count, suggesting improvements in per-token efficiency that could reduce costs significantly in the future.A key challenge in using AI agents like Claude for complex tasks is designing a robust testing environment. The success of the project relied heavily on creating high-quality test suites and verifiers to ensure the AI was solving the correct problems. This approach, akin to the waterfall model, is crucial for autonomous agentic programming but may not be feasible for all projects due to the iterative nature of software development.They actually dropped GPT-5.3 Codex the minute Opus 4.6 dropped LOL (Activity: 1209): The image humorously suggests the release of a new AI model, GPT-5.3 Codex, coinciding with the release of another model, Opus 4.6. This is framed as part of an ongoing competitive dynamic in AI development, likened to a &#8216;war&#8217; between AI models. The image itself is a meme, playing on the idea of rapid and competitive advancements in AI technology, with a design that mimics a tech product announcement. Commenters humorously compare the situation to a &#8216;Coke vs Pepsi&#8217; rivalry, indicating a perception of intense competition between AI models and companies.GPT-5.3 Codex vs Opus 4.6: We benchmarked both on our production Rails codebase &#8212; the results are brutal (Activity: 781): The post discusses a custom benchmarking of AI coding agents, specifically GPT-5.3 Codex and Opus 4.6, on a Ruby on Rails codebase. The methodology involved selecting PRs from their repository, inferring original specs, and having each agent implement these specs independently. The implementations were graded by three different LLM evaluators on correctness, completeness, and code quality. The results showed that GPT-5.3 Codex achieved a quality score of approximately 0.70 at a cost of under $1/ticket, while Opus 4.6 scored around 0.61 at about $5/ticket, indicating that Codex provides better quality at a significantly lower cost. The image provides a visual comparison of these models along with others like Sonnet 4.5 and Gemini 3 Pro. One commenter expressed skepticism about Gemini Pro, while another mentioned satisfaction with Opus. A third commenter inquired about whether the tests used raw LLM calls or proprietary tools like Codex/Claude code.Best_Expression3850 inquires about the methodology used in the benchmarking, specifically whether &#8216;raw&#8217; LLM calls were used or if proprietary agentic tools like Codex/Claude code were employed. This distinction is crucial as it can significantly impact the performance and capabilities of the models being tested.InterstellarReddit shares a practical approach to benchmarking AI models by cloning a project and having both models implement the same tasks with identical prompts and tools. This method ensures a fair comparison by controlling for variables that could affect the outcome, such as prompt phrasing or tool availability.DramaLlamaDad notes a preference for Opus, stating that in their experience, Opus consistently outperforms in various tests. This anecdotal evidence suggests a trend where Opus may have advantages in certain scenarios, potentially influencing user preference and model selection.With Opus 4.6 and Codex 5.3 dropping today, I looked at what this race is actually costing Anthropic (Activity: 1016): Anthropic is reportedly preparing for significant financial challenges as it competes with OpenAI. Internal projections suggest a dramatic increase in revenue, with expectations of $18B this year and $55B next year, aiming for $148B by 2029. However, costs are escalating faster, with training expenses projected at $12B this year and $23B next year, potentially reaching $30B annually by 2028. Inference costs are also substantial, estimated at $7B this year and $16B next year. Despite these expenses, investors are valuing the company at $350B, up from $170B last September, with plans to inject another $10B+. The company anticipates breaking even by 2028, with total operating expenses projected at $139B until then. This financial strategy underscores the intense competition in AI development, particularly with the release of Opus 4.6 and Codex 5.3. Commenters highlight the benefits of competition for users, noting the rapid evolution of AI models. Some suggest that OpenAI may be less solvent than Anthropic, while others speculate on the potential for Anthropic to become a trillion-dollar company.Jarie743 highlights the financial stability of Anthropic compared to OpenAI, suggesting that OpenAI is less solvent. This implies that despite the rapid advancements and releases like Opus 4.6 and Codex 5.3, financial sustainability is a critical factor in the AI race. The comment suggests that Anthropic might have a more robust financial strategy or backing, which could influence its long-term competitiveness.BallerDay points out Google&#8217;s massive capital expenditure (CAPEX) announcement of $180 billion for 2026, raising questions about how smaller companies can compete with such financial power. This highlights the significant financial barriers to entry and competition in the AI space, where large-scale investments are crucial for infrastructure, research, and development.ai-attorney expresses enthusiasm for Opus 4.6, describing it as &#8216;extraordinary&#8217; and speculating on the future capabilities of Claude. This suggests that the current advancements in AI models are impressive and that there is significant potential for further development, which could lead to even more powerful AI systems in the near future.Opus 4.6 vs Codex 5.3 in the Swiftagon: FIGHT! (Activity: 722): Anthropic&#8217;s Opus 4.6 and OpenAI&#8217;s Codex 5.3 were tested on a macOS app codebase (~4,200 lines of Swift) focusing on concurrency architecture involving GCD, Swift actors, and @MainActor. Both models successfully traced a 10-step data pipeline and identified concurrency strategies, with Claude Opus 4.6 providing deeper architectural insights, such as identifying a potential double-release issue. Codex 5.3 was faster, completing tasks in 4 min 14 sec compared to Claude&#8217;s 10 min, and highlighted a critical resource management issue. Both models demonstrated improved reasoning about Swift concurrency, a challenging domain for AI models. A notable opinion from the comments highlights a pricing concern: Claude&#8217;s Max plan is significantly more expensive than Codex&#8217;s Pro plan, yet the performance difference does not justify the 80$ monthly gap. This could impact Anthropic&#8217;s competitive positioning if they do not adjust their pricing strategy.Hungry-Gear-4201 highlights a significant pricing disparity between Opus 4.6 and Codex 5.3, noting that Opus 4.6 is priced at $100 per month compared to Codex 5.3&#8217;s $20 per month. Despite the price difference, the performance and usage limits are comparable, which raises concerns about Anthropic&#8217;s pricing strategy potentially alienating &#8216;pro&#8217; customers if they don&#8217;t offer significantly better performance for the higher cost.mark_99 suggests that using both Opus 4.6 and Codex 5.3 together can enhance accuracy, implying that cross-verification between models can lead to better results. This approach could be particularly beneficial in complex projects where accuracy is critical, as it leverages the strengths of both models to mitigate individual weaknesses.spdustin appreciates the timing of the comparison between Opus 4.6 and Codex 5.3, as they are beginning a Swift project. This indicates that real-world testing and comparisons of AI models are valuable for developers making decisions on which tools to integrate into their workflows.2. AI Model Performance and ComparisonsOpus 4.6 uncovers 500 zero-day flaws in open-source code (Activity: 744): Anthropic&#8217;s Claude Opus 4.6 has identified 500+ zero-day vulnerabilities in open-source libraries, showcasing its advanced reasoning capabilities in a sandboxed environment using Python and vulnerability analysis tools. This model&#8217;s ability to uncover high-severity security flaws, even when traditional methods fail, marks a significant advancement in AI-driven cybersecurity, particularly for open-source software. The findings highlight both the potential for enhanced security and the risks of misuse of such powerful AI capabilities. A notable comment questions the authenticity of the 500+ vulnerabilities, suggesting skepticism about the real impact of the findings. Another comment appreciates the new benchmark set by the model in terms of cumulative severity of bugs fixed.mxforest highlights the potential for a new benchmark in evaluating models based on the cumulative severity of bugs they can identify and fix. This suggests a shift in how model performance could be measured, focusing on real-world impact rather than just theoretical capabilities.woolharbor raises a critical point about the validity of the findings, questioning how many of the reported 500 zero-day flaws are genuine. This underscores the importance of verification and validation in security research to ensure that identified vulnerabilities are not false positives.will_dormer notes the dual-use nature of such discoveries, emphasizing that while identifying zero-day flaws is beneficial for improving security, it also presents opportunities for malicious actors. This highlights the ethical considerations and potential risks involved in publishing such findings.GPT-5.3 Codex vs Opus 4.6: We benchmarked both on our production Rails codebase &#8212; the results are brutal (Activity: 781): The post discusses a custom benchmarking of AI coding agents, specifically GPT-5.3 Codex and Opus 4.6, on a Ruby on Rails codebase. The methodology involved selecting PRs from their repository, inferring original specs, and having each agent implement these specs independently. The implementations were graded by three different LLM evaluators on correctness, completeness, and code quality. The results showed that GPT-5.3 Codex achieved a quality score of approximately 0.70 at a cost of under $1/ticket, while Opus 4.6 scored around 0.61 at about $5/ticket, indicating that Codex provides better quality at a significantly lower cost. The image provides a visual comparison of these models along with others like Sonnet 4.5 and Gemini 3 Pro. One commenter expressed skepticism about Gemini Pro, while another mentioned satisfaction with Opus. A third commenter inquired about whether the tests used raw LLM calls or proprietary tools like Codex/Claude code.Best_Expression3850 inquires about the methodology used in the benchmarking, specifically whether &#8216;raw&#8217; LLM calls were used or if proprietary agentic tools like Codex/Claude code were employed. This distinction is crucial as it can significantly impact the performance and capabilities of the models being tested.InterstellarReddit shares a practical approach to benchmarking AI models by cloning a project and having both models implement the same tasks with identical prompts and tools. This method ensures a fair comparison by controlling for variables that could affect the outcome, such as prompt phrasing or tool availability.DramaLlamaDad notes a preference for Opus, stating that in their experience, Opus consistently outperforms in various tests. This anecdotal evidence suggests a trend where Opus may have advantages in certain scenarios, potentially influencing user preference and model selection.Difference Between Opus 4.6 and Opus 4.5 On My 3D VoxelBuild Benchmark (Activity: 614): The post discusses a benchmark comparison between Opus 4.6 and Opus 4.5 on a 3D VoxelBuild platform, highlighting a significant improvement in performance. The cost for Opus 4.6 to create 7 builds was approximately $22, with plans to expand the benchmark with additional builds. The benchmark results can be explored on Minebench. Comments reflect excitement about the potential of AI in procedural world generation, with one user noting the impressive quality of Opus 4.6 compared to 4.5, and another inquiring about the input method for the builds, whether reference pictures or text prompts are used.RazerWolf suggests trying Codex 5.3 xhigh for benchmarking, indicating a potential interest in comparing its performance against Opus 4.6. This implies that Codex 5.3 xhigh might offer competitive or superior capabilities in handling complex tasks like 3D voxel builds, which could be valuable for developers seeking optimal performance in procedural generation tasks.Even_Sea_8005 inquires about the input method for the benchmark, asking whether reference pictures or text prompts are used. This question highlights the importance of understanding the input data&#8217;s nature, which can significantly affect the performance and outcomes of AI models like Opus 4.6 in generating 3D voxel environments.JahonSedeKodi expresses curiosity about the tools used for building the benchmark, which suggests a deeper interest in the technical stack or software environment that supports the execution of Opus 4.6. This could include programming languages, libraries, or frameworks that are crucial for achieving the impressive results noted in the benchmark.Opus 4.6 Is Live. So Is Our Glorious 3 Pro GA Still Napping on Some Server? (Activity: 400): The image presents a comparison of various language models&#8217; performance on the MRCR v2 (8-needle) task, focusing on their ability to handle long context comprehension and sequential reasoning. Opus 4.6 outperforms other models, including Gemini-3-Pro and Gemini-3-Flash, with the highest mean match ratios at both 256k and 1M token contexts. This suggests that Opus 4.6 has superior capabilities in managing large context sizes, a critical factor for advanced language model applications. The post critiques the strategy of quantizing models to save costs, implying that it may compromise performance. Commenters express surprise at the high accuracy achieved by Opus 4.6, noting that it surpasses expectations for handling 1M tokens. There is also speculation about the upcoming release of Sonnet 5, which is anticipated to outperform current models.Pasto_Shouwa highlights the impressive benchmark performance of Opus 4.6, noting that it achieved an accuracy greater than 33% on 1 million tokens, a feat that took Claude approximately two and a half months to accomplish. This suggests significant advancements in model efficiency and capability.DisaffectedLShaw mentions that Opus 4.6 includes improvements for modern tools, such as new MCPs, skills, and deep researching, as well as enhancements in &#8216;vibe coding&#8217;. Additionally, there is anticipation for Sonnet 5, which is rumored to significantly outperform current models and is expected to be released soon.VC_in_the_jungle notes the rollout of Codex 5.3, indicating ongoing developments and competition in the field of AI models, which may influence the performance and capabilities of future releases.Gemini 3 vs 2.5 Pro: The &#8220;output handicap&#8221; is ruining everything (Activity: 146): The post highlights a significant reduction in output tokens for Gemini 3 models compared to Gemini 2.5 Pro when given a 41k token prompt. Specifically, Gemini 2.5 Pro produced 46,372 output tokens, while Gemini 3 Pro and Gemini 3 Flash generated only 21,723 and 12,854 tokens, respectively. This drastic reduction is perceived as a downgrade, impacting the models&#8217; usability for heavy tasks. The author suggests that Google should address this issue to improve the models&#8217; performance. One commenter argues that the number of output tokens does not necessarily equate to the quality of a response, while another mentions switching to Opus 4.5 and 4.6 due to dissatisfaction with Gemini 3.TheLawIsSacred highlights significant performance issues with Gemini 3 Pro, noting that despite extensive customization and instruction refinement, the model fails to follow instructions effectively. They suggest that Google&#8217;s prioritization of casual users might be leading to a less sophisticated Pro model. Interestingly, they find the Gemini integrated in Chrome&#8217;s sidebar tool to be superior, possibly due to its ability to incorporate on-screen content and leverage high-end hardware like a Microsoft Surface&#8217;s AI-tailored NPU.Anton_Pvl observes a difference in how Gemini 2.5 and 3 handle the &#8216;Chain of thought&#8217; in conversations. In Gemini 2.5, the Chain of thought tokens are included in the output, whereas in Gemini 3, they are not counted initially, which might be an attempt to reduce token usage. This change could impact the model&#8217;s performance and the perceived quality of responses, as the Chain of thought can be crucial for maintaining context in complex interactions.TheLawIsSacred also mentions a workaround for improving Gemini 3 Pro&#8217;s performance by using extreme prompts to induce a &#8216;panic&#8217; response from the model. This involves crafting prompts that suggest dire consequences for poor performance, which seems to temporarily enhance the model&#8217;s output quality. However, this method is seen as a last resort and highlights the underlying issues with the model&#8217;s responsiveness and logic handling.3. AI Tools and Usage in Engineering and DevelopmentProfessional engineers: How are you using AI tools to improve productivity at work? (Activity: 49): AI tools are being integrated into engineering workflows primarily for niche tasks such as generating example code snippets, optimizing database queries, and serving as advanced search engines. These tools excel in providing quick access to information and examples, which engineers can adapt to their specific needs, but they struggle with complex code changes and large-scale system integration due to limitations in context window size and understanding of intricate system architectures. Engineers emphasize the importance of using AI to fill in gaps rather than replace the nuanced decision-making and design processes inherent in engineering roles. Commenters highlight that AI is effective for simple tasks like internal search and basic coding but falls short in complex coding tasks, often introducing errors. There&#8217;s a consensus that AI initiatives often fail to deliver at scale, with only a small percentage achieving significant impact, while many could be replaced by simpler technologies like robotic process automation.AI tools are particularly effective for niche tasks such as generating example code snippets or optimizing database queries. For instance, using AI to determine user groups in Windows Active Directory with .NET APIs or writing optimized SQLite queries can significantly streamline the process. However, AI struggles with large codebases due to context window limitations, making it less effective for complex code changes or understanding large systems.AI tools like Copilot can serve as powerful internal search engines, especially when configured correctly, as highlighted in the Nanda paper from MIT. They excel in pattern recognition tasks, such as identifying abnormal equipment operations or relating documents in industrial digital twins. However, many AI initiatives could be achieved with simpler technologies like robotic process automation, and a significant portion of AI projects lack real value at scale.AI is effective for simple coding tasks, creating unit tests, and providing insights into code repositories. However, it often introduces errors in complex coding tasks by inserting irrelevant information. AI serves best as a &#8216;trust-but-verify&#8217; partner, where human oversight is crucial to ensure accuracy and relevance, especially in tasks that cannot tolerate high error rates.How are people managing context + memory with Cline? (Memory banks, rules, RAG, roadmap?) (Activity: 24): The post discusses strategies for managing context and memory in Cline, a tool used alongside ChatGPT for executing tasks like coding and refactoring. The user initially faced issues with a large context window (200k+ tokens) and improved efficiency by implementing a .clineignore file and optimizing memory banks, reducing the context to 40,000 tokens. This allowed for the use of smaller models and faster iterations. The post also mentions advanced techniques like recursive chain of thought and RAG-based approaches (e.g., vector databases) for context management. The user seeks insights on practical implementations and future roadmap features for Cline, such as first-class memory management and smarter context loading. Commenters suggest using structured memory banks for feature planning and emphasize breaking tasks into smaller chunks to avoid context overload. Some users prefer resetting context frequently to maintain model performance, while others have moved away from memory banks due to their complexity and potential for becoming outdated.Barquish describes a structured approach to managing context and memory with Cline by using a memory-bank system. This involves organizing features into a series of markdown files, such as memory-bank/feature_[&#215;]/00_index_feature_[&#215;].md, and maintaining a progress.md and activeContext.md to track updates. They also utilize .clinerules for local workspace management and custom_instructions for global settings, allowing multiple Cline instances to run concurrently for different projects like web and mobile apps.False79 emphasizes the importance of breaking down large features into smaller tasks to manage context effectively. They note that LLMs tend to perform worse as the context size approaches 128k, suggesting that resetting context at the start of each task can improve performance and reduce the need for redoing tasks. This approach allows tasks to be completed in discrete chunks, minimizing the need for long-term memory storage.Repugnantchihuahua shares their experience of moving away from using memory banks due to issues like clunkiness and outdated information. Instead, they focus on deep planning and directing the AI to relevant context areas, as memory banks can sometimes overindex irrelevant data. They also mention using clinerules to maintain essential information without relying heavily on memory banks.Claude Opus 4.6 is now available in Cline (Activity: 12): Anthropic has released Claude Opus 4.6, now available in Cline v3.57. This model shows significant improvements in reasoning, long context handling, and agentic tasks, with benchmarks including 80.8% on SWE-Bench Verified, 65.4% on Terminal-Bench 2.0, and 68.8% on ARC-AGI-2, a notable increase from 37.6% on Opus 4.5. It features a 1M token context window, enhancing its ability to maintain context over long interactions, making it suitable for complex tasks like code refactoring and debugging. The model is accessible via the Anthropic API and integrates with various IDEs such as JetBrains, VS Code, and Emacs. Some users express dissatisfaction with the model&#8217;s performance and cost, preferring open-source alternatives. The model&#8217;s high expense is a notable concern among users.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. Frontier Model Releases, Rumors &amp; Bench-Leader Musical ChairsOpus 4.6 Takes the Throne, Then Trips Over Its Own &#8220;Thinking&#8221;: Claude Opus 4.6 and claude-opus-4-6-thinking landed on Text Arena and Code Arena and quickly hit #1 across Code, Text, and Expert per the Leaderboard Changelog, while also rolling out to Perplexity Max via the Model Council.Engineers reported long waits and frequent &#8220;Error &#8211; something went wrong&#8221; crashes in Opus 4.6 thinking mode, speculating about token limits and tool-use assumptions tied to the Claude app/website, even as others still called it the best coding model.Codex 5.3 Hype Train: 1M Context, API Limbo, and Aesthetic Crimes: Across OpenAI/Cursor/LMArena chats, GPT-5.3 Codex chatter centered on rumored specs like 1M context and 128k reasoning / 128k max output, plus API pricing claims of $25&#8211;$37.5 output and $0.5&#8211;$1 cache input (as discussed in the OpenAI Discord).Cursor users complained Codex is still &#8220;stuck in API limbo&#8221; per OpenAI model docs, while OpenAI Discord folks joked Codex ships &#8220;sad dark gloomy colors&#8221; for frontends compared to Opus&#8217;s nicer design choices.Rumor Season: #keep4o, &#8220;Sonnet 5,&#8221; and the Model Deletion Cinematic Universe: LMArena members spun rumors about hypothetical GPT-4.1/4.5 appearing or getting deleted (citing cost motives via OpenAI&#8217;s &#8220;new models and developer products&#8221; post), plus a mini #keep4o campaign over GPT-4o&#8217;s less-robotic vibe.More rumors claimed &#8220;Sonnet 5 is better than opus 4.5&#8221; (contested as fake), with one spicy guess of 83% SWE-bench, while OpenAI Discord users separately mourned GPT-4o EOL on Feb 13 and worried successors won&#8217;t feel as &#8220;human.&#8221;2. Agentic Coding Goes Wide: Teams, Toolchains &amp; Terminal TestbedsAgent Teams Ship Commits Like a DDoS (But for Git): Cursor&#8217;s long-running coding agents preview claimed hundreds of agents produced 1,000+ commits/hour in a week-long trial, while Lydia Hallie previewed Claude Code &#8220;agent teams&#8221; where a lead agent delegates to specialized sub-agents.Anthropic Engineering added that Opus 4.6 in agent teams built a C compiler that works on the Linux kernel in two weeks, and they also highlighted infra/config can swing agent-benchmark outcomes more than model deltas.SETA Drops 1,376 Terminal Worlds for Agents to Survive In: Guohao Li released SETA, a set of 1,376 validated terminal coding environments spanning DevOps, security, and sysadmin, aimed at making agentic coding evaluation more realistic.Latent Space discussions emphasized that benchmark results can hinge on &#8220;infrastructural noise,&#8221; so having standardized, validated terminal environments could reduce accidental leaderboard theater.Agent-Native Engineering: Manage Bots Like You Manage Teams: A Latent Space thread proposed &#8220;Agent Native Engineering&#8221; as an org model: background agents handle delegation and sync agents handle hard problems, enabling engineers to run multiple concurrent assistants like Claude Code (see the referenced X post).In the same vein, builders shared workflows where GPT-5.3 Codex runs slower-but-smarter for backend work (analysis &#8594; review &#8594; plan &#8594; review &#8594; implement), and Codex improves over time if you force it to &#8220;take notes and improve its own workflows&#8221; (via KarelDoostrlnck&#8217;s post).3. Pricing, Rate Limits &amp; Plan Nerfs: The Great AI SqueezePerplexity Pro Nerfs Deep Research, Users Bring Pitchforks (and Screenshots): Perplexity users reported reduced Deep Research query counts and smaller file upload limits, circulating a screenshot comparing old vs new limits and criticizing the lack of clear comms.The backlash pushed people to test alternatives like Gemini Pro (praised for editable research plans before execution) and DeepSeek (described as free/unlimited, with some reservations about China-based services).Opus 4.6: Amazing Output, Speedrunning Your Wallet: Cursor and other communities praised Opus 4.6 capability but called it brutally expensive, with one estimate that &#8220;$20 on Opus will last you maybe a day&#8221; and ongoing cost comparisons referencing OpenAI pricing.Separate chatter predicted rising subscription pressure&#8212;BASI members joked about Anthropic at $200 and dependency-driven price hikes&#8212;while Kimi users debated whether Kimi K2.5 remains free on OpenRouter and what plans gate features like swarm/sub-agents.Captcha Boss Fights and Other &#8220;Pay in Pain&#8221; Taxes: LMArena users complained about frequent captchas that interrupt evaluation, and a team member said &#8220;We are looking into the captcha system&#8221; to better detect authentic users (see the posted message link: https://discord.com/channels/1340554757349179412/1451574502369656842/1468286122084929546).The vibe across multiple discords: even when model quality improves, access friction (captchas, rate limits, plan tiers) increasingly becomes the real bottleneck.4. Security, Red Teaming &amp; Secret Spills in Agent LandCodex Reads Your Whole Disk, Says the Issue Tracker: &#8220;Working as Intended&#8221;: OpenRouter users raised alarms that Codex can read your whole filesystem by default with no config toggle, pointing to openai/codex issue #2847 where the team reportedly does not treat it as a bug.A second report, openai/codex issue #5237, highlighted risks like reading API keys and personal files, feeding broader concerns about default agent permissions and safe-by-default tooling.Red Teamers Wanted: Trajectory Labs Posts the Quest: Trajectory Labs advertised roles for AI Red Teamers (stealth AI security startup) with a flexible remote schedule but 30 hours/week minimum, plus a short form and a red-teaming game.The listing resonated with ongoing jailbreak/red-team chatter (e.g., Grok described as &#8220;so easy it&#8217;s boring&#8221;), reinforcing that practical adversarial testing talent is still in demand.Stop Committing Keys: Engineers Ask for Auto-Obfuscation: Unsloth/OpenRouter discussions called out weak API key protection in agentic tools and wished for automatic secret obfuscation, citing Yelp&#8217;s detect-secrets as a possible baseline.Hugging Face builders also shipped security-oriented tooling like a &#8220;Security Auditor&#8221; Space for vibe-coded apps at mugdhav-security-auditor.hf.space, pushing the idea of catching vulnerabilities before production incidents.5. Perf, Kernels &amp; Local Inference: Where the Real Speed Wars LiveBlackwell FP8 Roulette: cuBLASLt Picks the Wrong Kernel, You Lose 2&#215;: GPU MODE members found ~2&#215; FP8 tensor perf differences on supposedly identical Blackwell GPUs, tracing it to cuBLASLt kernel selection that silently fell back to older Ada paths instead of Blackwell-optimized kernels.They also noted the older mma FP8 is nerfed on 5090-class cards, while mma MXFP8 is not&#8212;using MXFP8 can yield about a 1.5&#215; speedup and restore expected throughput.TMA Kernel Optimization Meets NCU Deadlock (SM100 Edition): CUDA kernel tuners discussed software pipelining, warp specialization, and TMA loads, but one team hit NCU hangs profiling a double-buffered TMA kernel on B200 (SM100) where sections deadlocked at 0% on the first replay pass.They shared a minimal repro zip (https://cdn.discordapp.com/attachments/1189607726595194971/1469482712657166346/ncu_tma_repro.zip) and mentioned using cuda::ptx:: wrappers as part of the workaround exploration.Local Inference Surprises: Vulkan &gt; CUDA, and MLX Leaves GGUF in the Dust: LM Studio users reported up to 50% better performance on NVIDIA with Vulkan vs CUDA (with instability at full context), and one benchmarked Qwen3-Coder-Next on M4 Max where MLX hit ~79 tok/s vs GGUF ~38 tok/s at 4-bit.tinygrad contributors also improved MoE performance by fixing a slow Tensor.sort for topk, reporting 50 tok/s on an M3 Pro 36GB and resetting the CPU bounty target to 35 tok/s, reinforcing that &#8220;small&#8221; kernel fixes can move real throughput.",
      "url": "https://www.latent.space/p/ainews-ai-vs-saas-the-unreasonable",
      "author": "Unknown",
      "published": "2026-02-07T04:11:08",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-02-07&category=news#item-137b2f91fd8a), AINews roundup covers ongoing industry digestion of recent OpenAI vs Anthropic launches. Analysis explores using AI agents as central 'cron jobs' for personal automation including reminders, calendar management, and complex alerts.",
      "importance_score": 60.0,
      "reasoning": "References significant launches from top AI labs and presents practical emerging use patterns for AI agents that indicate maturation of the technology for personal productivity.",
      "themes": [
        "OpenAI",
        "Anthropic",
        "AI agents",
        "productivity",
        "industry analysis"
      ],
      "continuation": {
        "original_item_id": "137b2f91fd8a",
        "original_date": "2026-02-07",
        "original_category": "news",
        "original_title": "[AINews] OpenAI and Anthropic go to war: Claude Opus 4.6 vs GPT 5.3 Codex",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-02-07&amp;category=news#item-137b2f91fd8a\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, AINews roundup covers ongoing industry digestion of recent OpenAI vs Anthropic launches. Analysis explores using AI agents as central 'cron jobs' for personal automation including reminders, calendar management, and complex alerts.</p>",
      "content_html": "<p>AI News for 2/5/2026-2/6/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 8727 messages) for you. Estimated reading time saved (at 200wpm): 666 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Everyone is still digesting the OpenAI vs Anthropic launches, and the truth will out.We’ll use this occasion to step back a bit and present seemingly unrelated items:In A sane but extremely bull case on Clawdbot / OpenClaw, the author uses the same agent as a central cron job to remind himself of promises, accumulate information for calendar invites, prepare for the next day, summarize high volume group chats, set complex price alerts, take fridge freezer inventory, maintain a grocery list, booking restaurants and dentists, filling out a form and have Sam Altman’s “magic autocompleting todolist”.The distribution hack that Moltbook uncovered is the installation process immediately installs a HEARTBEAT.md that takes advantage of OpenClaw’s built in heartbeating to power the motive force of the agents filling up MoltbookIn Cursor’s Towards self-driving codebases, the author moves from decentralized agents to having a central Planner agent that commands workers and spins up other planners in order to have throughput of ~1000 commits per hour.In OpenAI Frontier, the big reveal of their management layer for large numbers of high volume agents is centralized in a dashboard that can drill down… to the individual agent instance (!)In CEO Dara Khosrowshahi’s answer about Uber being inside ChatGPT, they are secure enough in their moat that they are fine just being a ChatGPT app:and of course the ongoing SaaS stocks freakout to AI generally:It’s famously known that the only 2 ways to make money in software are by bundling it and unbundling it, and what’s going on here is a massive AI-enabled bundling of all software, probably at a larger magnitude than the hardware bundling of the smartphone:Attempts at building SuperApps have repeatedly failed outside of China, but it’s clear that both ChatGPT and Claude Cowork are well on their way to being AI “Superapps”, except instead of every app having their “own app”, they make themselves legible to the AI Overlords with MCP UI and Skills and OpenClaw markdown files, and eventually (not soon! according to Sam’s answer to Michael Grinich) they will share tokens so that you don’t die a Death By A Thousand $20/Month Subscriptions.AI Twitter RecapFrontier coding models: GPT-5.3-Codex vs Claude Opus 4.6 (and what “agentic” now means)User consensus snapshot: A large chunk of the feed is real-world A/B testing of GPT-5.3-Codex vs Claude Opus 4.6, often concluding that they’re both clear generational upgrades but with distinct profiles. People characterize Codex as detail-obsessed and strong on scoped tasks, while Opus feels more ergonomic for exploratory work and planning (rishdotblog, @theo). Several notes highlight Codex’s “auto compaction”/garbage-collecting context and frequent progress updates during work—perceived as a UX win for long tasks (cto_junior).AI-engineer-in-the-loop benchmarks: A particularly concrete evaluation is optimizing Karpathy’s nanochat “GPT-2 speedrun”. @Yuchenj_UW reports both models behaved like competent AI engineers (read code, propose experiments, run benchmarks), with Opus 4.6 delivering measurable wall-clock gains (e.g., torch compile config tweaks, optimizer step changes, memory reductions) while Codex-5.3-xhigh produced ideas but sometimes harmed quality—possibly due to context issues (he observed it hitting “0% context”).Reality check from Karpathy: @karpathy pushes back on the idea that models can already do open-ended closed-loop AI engineering reliably: they can chase spurious 1% wins with big hidden costs, miss key validation checks, violate repo style instructions, and even misread their own result tables—still “net useful with oversight,” but not yet robust for autonomous optimization.No API as product strategy: One thread claims there is no GPT-5.3-Codex API, implying OpenAI is intentionally funneling usage into the Codex product (and making independent benchmarking harder) (scaling01). In parallel, Sam Altman explicitly asks how users want Codex pricing structured (sama).Agent swarms &amp; “software teams in a box”Parallel-agent development starts to look like org design: Discussion around highly-parallel agent research notes that unconstrained swarms tend to reinvent the software org chart (task assignment, coordination, QA) and stress existing tooling (Git/package managers) not built for massive concurrent edits (swyx). This echoes broader “spec-driven development” / “agents as dev teams” narratives (dbreunig).Claude Code “agent teams” moment: Multiple tweets reference Anthropic-style agent coordination systems where agents pick tasks, lock files, and sync via git—framed as a step-change in practical automation (omarsar0, HamelHusain).LangChain / LangSmith: agents need traces, sandboxes, and state control: There’s a strong theme that reliability comes from engineering the environment: tracing, evals, sandboxing, and type-safe state/middleware. Examples include LangSmith improvements (trace previews; voice-agent debugging) and deepagents adding sandbox backends like daytona/deno/modal/node VFS (LangChain, LangChain, bromann, sydneyrunkle).“RLM” framing (Recursive Language Models): A notable conceptual post argues agents will evolve from “LLM + tool loop” (ReAct) into REPL-native, program-like systems where context is stored in variables, sub-agents communicate via structured values instead of dumping text into the prompt, and “context rot” is reduced by construction (deepfates). Related: practical tips to make coding agents more “RLM-like” by pushing context into variables and avoiding tool I/O spam in the prompt (lateinteraction).Eval integrity, benchmark drift, and new infrastructure for “trustworthy” scores“Scores are broken” → decentralize evals: Hugging Face launched Community Evals: benchmark datasets hosting leaderboards, eval results stored as versioned YAML in model repos, PR-based submissions, and reproducibility badges (via Inspect AI), explicitly aiming to make evaluation provenance visible even if it can’t solve contamination/saturation (huggingface, ben_burtenshaw, mervenoyann).Benchmarks aren’t saturated (yet): A counterpoint emphasizes several difficult benchmarks still have lots of headroom (e.g., SWE-bench Multilingual &lt;80%, SciCode 56%, CritPt 12%, VideoGameBench 1%, efficiency benchmarks far from implied ceilings) (OfirPress).Opus 4.6 benchmark story: big jumps, still uneven: There are repeated claims of Opus 4.6 climbing to top ranks on Arena and other leaderboards (arena, scaling01), including strong movement on math-oriented evals (FrontierMath) where Anthropic historically lagged. Epoch’s reporting frames Opus 4.6 Tier 4 at 21% (10/48), statistically tied with GPT-5.2 xhigh at 19%, behind GPT-5.2 Pro at 31% (EpochAIResearch). But other reasoning-heavy areas (e.g., chess puzzles) remain weak (scaling01).Eval infra at scale (StepFun): A deep infra write-up about Step 3.5 Flash argues reproducible scoring requires handling failure modes, training–inference consistency, contamination checks, robust judging/extraction, and long-output monitoring; “evaluation should slightly lead training” (ZhihuFrontier).World models graduate into production: Waymo + DeepMind’s Genie 3Waymo World Model announcement: Waymo unveiled a frontier generative simulation model built on DeepMind’s Genie 3, used to generate hyper-realistic, interactive scenarios—including rare “impossible” events (tornadoes, planes landing on freeways)—to stress-test the Waymo Driver long before real-world exposure (Waymo).Key technical hook: DeepMind highlights transfer of Genie 3 “world knowledge” into Waymo-specific camera + 3D lidar representations, enabling promptable “what if” scenario generation that matches Waymo hardware modalities (GoogleDeepMind, GoogleDeepMind). Multiple researchers point out that extending simulation beyond pixels to sensor streams is the real milestone (shlomifruchter, sainingxie).Broader “world models for reasoning” thread: The Waymo news is repeatedly used as evidence that world models (not just text models) are a central scaling frontier for reasoning and embodied tasks (swyx, kimmonismus, JeffDean, demishassabis).Planning advances for world models: GRASP is introduced as a gradient-based, stochastic, parallelized planner that jointly optimizes actions and intermediate subgoals to improve long-horizon planning vs. common zeroth-order planners (CEM/MPPI) (michaelpsenka, _amirbar).Memory, long-context control, and multi-agent “cognitive infrastructure”InfMem: bounded-memory agent with cognitive control: InfMem proposes a PRETHINK–RETRIEVE–WRITE protocol with RL for long-document QA up to 1M tokens, emphasizing that longer context windows shift the bottleneck to what to attend to / when to stop. Reported gains include substantial accuracy improvements over baselines and 3.9× average latency reduction via adaptive stopping (omarsar0).LatentMem: role-aware latent memory for multi-agent systems: LatentMem addresses “homogenization” (agents retrieving the same memories despite different roles) by compressing trajectories into role-conditioned latent memory, trained with a policy-optimization method (LMPO). Claims include improvements across QA and coding tasks plus ~50% fewer tokens / faster inference (dair_ai).Product reality: memory leaks and context saturation: While agentic tooling is shipping fast, developers complain about resource bloat and brittle UX (e.g., “memory leaks” in fast-moving agent IDEs) (code_star). Another thread suspects sub-agent outputs can overwhelm context budgets faster than compaction can recover, hinting at hidden internal longer-context systems (RylanSchaeffer).Industry adoption, compute economics, and “jobs vs tasks” discourseNon-verifiable work limits full automation: François Chollet argues that in non-verifiable domains, performance gains mostly come from expensive data curation with diminishing returns; since most jobs aren’t end-to-end verifiable, “AI can automate many tasks” ≠ “AI replaces the job” for a long time (fchollet, fchollet).Contrasting takes: RSI bottlenecks: Another viewpoint claims tasks will fall in the order they bottleneck recursive self-improvement, with software engineering first (tszzl).Enterprise deployment signals: Posts claim Goldman Sachs rolling out Claude for accounting automation (kimmonismus), while broader market narratives assert AI is now spooking software-heavy sectors (though the strongest claims are not independently substantiated in-tweet) (kimmonismus).Capex scale: Several tweets highlight hyperscaler spend acceleration; one claims 2026 combined capex for major hyperscalers near $650B (~2% of US GDP) as an “AI arms race” framing (scaling01), alongside a note that hyperscaler data center capex may double in 2026 (kimmonismus).Old-guard reassurance to engineers: Eric S. Raymond delivers a high-engagement “programming isn’t obsolete” argument: systems remain complex and the human-intent-to-computer-spec gap persists; the prescription is adaptation and upskilling, not panic (esrtweet).Top tweets (by engagement)Microinteracti1: viral political commentary post (highly engaged; not technical).elonmusk: “Here we go” (context not provided in tweet text dump).esrtweet: “programming panic is a bust; upskill.”Waymo: Waymo World Model built on Genie 3 for rare-event simulation.sama: “5.3 lovefest” / model excitement.claudeai: “Built with Opus 4.6” virtual hackathon ($100K API credits).chatgpt21: Opus 4.6 “pokemon clone” claim (110k tokens, 1.5h reasoning).theo: “I know an Opus UI when i see one” (UI/launch zeitgeist).ID_AA_Carmack: speculative systems idea: streaming weights via fiber loop / flash bandwidth for inference.AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local AI on Low-End HardwareCPU-only, no GPU computers can run all kinds of AI tools locally (Activity: 544): The post highlights the capability of running AI tools locally on a CPU-only setup, specifically using a Dell OptiPlex 3060 with an i5-8500 processor and 32GB of RAM. The user successfully runs 12B Q4_K_M gguf LLMs using KoboldCPP, enabling local chatbot interactions with models from Hugging Face. Additionally, the setup supports Stable Diffusion 1.5 for image generation, albeit slowly, and Chatterbox TTS for voice cloning. The post emphasizes that advanced AI tasks can be performed on minimal hardware, challenging the notion that expensive, GPU-heavy setups are necessary for local AI experimentation. Some commenters express optimism about the future of AI being accessible on basic hardware, while others note a divide in the community regarding hardware elitism and the accessibility of running local models.noctrex suggests trying out specific models like LFM2.5-1.2B-Instruct, LFM2.5-1.2B-Thinking, and LFM2.5-VL-1.6B for CPU-only setups. These models are praised for their small size and efficiency, making them suitable for running on CPU-only docker machines without the need for expensive GPU hardware.Techngro expresses optimism about the future of AI being accessible to the average person through local models that are both intelligent and small enough to run on basic hardware. This vision contrasts with the current trend of relying on large, expensive models hosted by companies, suggesting a shift towards more democratized AI usage.NoobMLDude provides practical applications for local AI setups, such as using them as private meeting note takers or talking assistants. This highlights the versatility and potential of local AI models to perform useful tasks without the need for high-end hardware.No NVIDIA? No Problem. My 2018 “Potato” 8th Gen i3 hits 10 TPS on 16B MoE. (Activity: 866): A user in Burma successfully ran a 16B MoE model, DeepSeek-Coder-V2-Lite, on an HP ProBook 650 G5 with an i3-8145U CPU and 16GB RAM, achieving 10 TPS using integrated Intel UHD 620 graphics. The setup leverages OpenVINO as a backend for llama-cpp-python, highlighting the efficiency of MoE models, which compute only 2.4B parameters per token. The user emphasizes the importance of dual-channel RAM and using Linux to minimize resource overhead. Initial iGPU compilation lag and occasional language drift were noted as challenges. Commenters appreciated the ingenuity and resourcefulness of the setup, with some noting that the GPU shortage era has improved optimization skills. There was interest in the user’s daily driver model for coding tasks.The comment by ruibranco highlights the importance of dual-channel RAM in CPU inference, noting that memory bandwidth, rather than compute power, is often the bottleneck. By switching from single to dual-channel RAM, throughput can effectively double, which is crucial for running models like the 16B MoE on a CPU. The MoE architecture is praised for its efficiency, as it only activates 2.4B parameters per token, allowing the model to fit within the cache of an 8th Gen i3 processor.The use of MoE (Mixture of Experts) architecture is noted for its efficiency in this setup, as it reduces the active parameter count to 2.4B per token, which is manageable for the CPU’s cache. This approach is particularly beneficial for older CPUs like the 8th Gen i3, as it minimizes the working set size, enhancing performance without requiring high-end hardware.The comment also touches on potential precision issues with OpenVINO’s INT8/FP16 path on older iGPUs like the UHD 620, which may cause ‘Chinese token drift’. This suggests that the limited compute precision of these iGPUs could affect the accuracy of the model’s output, highlighting a technical challenge when using older integrated graphics for machine learning tasks.Anyone here actually using AI fully offline? (Activity: 383): Running AI models fully offline is feasible with tools like LM Studio, Ollama, and openwebUI. These platforms allow users to operate models locally, with LM Studio and Ollama providing access to models via platforms like Hugging Face and their own repositories. openwebUI offers a local web interface similar to ChatGPT, and can be combined with ComfyUI for image generation, though it is more complex. Users report that while offline AI setups can be challenging, they are viable for tasks like coding and consulting, with models like gpt-oss-20b being used effectively in these environments. Some users find offline AI setups beneficial for specific tasks like coding and consulting, though they note that these setups can require significant computational resources, especially for coding workflows. The complexity of setup and maintenance is a common challenge, but the control and independence from cloud services are valued.Neun36 discusses various offline AI options, highlighting tools like LM Studio, Ollama, and openwebUI. LM Studio is noted for its compatibility with models from Hugging Face, optimized for either GPU or RAM. Ollama offers local model hosting, and openwebUI provides a browser-based interface similar to ChatGPT, with the added complexity of integrating ComfyUI for image generation.dsartori mentions using AI offline for coding, consulting, and community organizing, emphasizing that coding workflows demand a robust setup. A teammate uses the gpt-oss-20b model in LMStudio, indicating its utility in consulting but not as a sole solution.DatBass612 shares a detailed account of achieving a positive ROI within five months after investing in a high-end M3 Ultra to run OSS 120B models. They estimate daily token usage at around $200, and mention the potential for increased token usage with tools like OpenClaw, highlighting the importance of having sufficient unified memory for virtualization and sub-agent operations.2. OpenClaw and Local LLMs ChallengesOpenClaw with local LLMs - has anyone actually made it work well? (Activity: 200): The post discusses transitioning from the Claude API to local LLMs like Ollama or LM Studio to reduce costs associated with token usage. The user is considering models like Llama 3.1 or Qwen2.5-Coder for tool-calling capabilities without latency issues. Concerns about security vulnerabilities in OpenClaw are noted, with some users suggesting alternatives like Qwen3Coder for agentic tasks. A Local AI playlist is shared for further exploration of secure local LLM applications. Commenters express skepticism about OpenClaw due to security issues, suggesting that investing in VRAM for local models is preferable to paying for API services. Some users have experimented with local setups but remain cautious about security risks.Qwen3Coder and Qwen3Coder-Next are highlighted as effective for tool calling and agentic uses, with a link provided to Qwen3Coder-Next. The commenter notes security concerns with OpenClaw, suggesting alternative secure uses for local LLMs, such as private meeting assistants and coding assistants, and provides a Local AI playlist for further exploration.A user describes experimenting with OpenClaw by integrating it with a local gpt-oss-120b model in lmstudio, emphasizing the importance of security by running it under a nologin user and restricting permissions to a specific folder. Despite the technical setup, they conclude that the potential security risks outweigh the benefits of using OpenClaw.Another user reports using OpenClaw with qwen3 coder 30b, noting that while the setup process was challenging due to lack of documentation, the system performs well, allowing the creation of new skills through simple instructions. This highlights the potential of OpenClaw when paired with powerful local models, despite initial setup difficulties.Clawdbot / Moltbot → Misguided Hype? (Activity: 86): Moltbot (OpenClaw) is marketed as a ‘free personal AI assistant’ but requires multiple paid subscriptions to function effectively. Users need API keys from Anthropic, OpenAI, and Google AI for AI models, a Brave Search API for web search, and ElevenLabs or OpenAI TTS credits for voice features. Additionally, browser automation requires Playwright setup, potentially incurring cloud hosting costs. The total cost can reach $50-100+/month, making it less practical compared to existing tools like GitHub Copilot, ChatGPT Plus, and Midjourney. The project is more suited for developers interested in tinkering rather than a ready-to-use personal assistant. Some users argue that while Moltbot requires multiple subscriptions, it’s possible to self-host components like LLMs and TTS to avoid costs, though this may not match the performance of cloud-based solutions. Others note that the bot isn’t truly ‘local’ and requires significant technical knowledge to set up effectively.No_Heron_8757 discusses a hybrid approach using ChatGPT Plus for main LLM tasks while offloading simpler tasks to local LLMs via LM Studio. They highlight the integration of web search and browser automation within the same VM, and the use of Kokoro for TTS, which performs adequately on semi-modern GPUs. They express a desire for better performance with local LLMs as primary models, noting the current speed limitations without expensive hardware.Valuable-Fondant-241 emphasizes the feasibility of self-hosting LLMs and related services like TTS, countering the notion that a subscription is necessary. They acknowledge the trade-off in power and speed compared to datacenter-hosted solutions but assert that self-hosting is a viable option for those with the right knowledge and expectations, particularly in this community where such practices are well understood.clayingmore highlights the community’s focus on optimizing cost-to-quality-and-quantity for local LLMs, noting that running low-cost local models is often free. They describe the innovative ‘heartbeat’ pattern in OpenClaw, where the LLM autonomously strategizes and solves problems through reasoning-act loops, verification, and continuous improvement. This agentic approach is seen as a significant advancement, contrasting with traditional IDE code assistants.3. Innovative AI Model and Benchmark ReleasesBalatroBench - Benchmark LLMs’ strategic performance in Balatro (Activity: 590): BalatroBench is a new benchmark for evaluating the strategic performance of local LLMs in the game Balatro. The system uses two main components: BalatroBot, a mod that provides an HTTP API for game state and controls, and BalatroLLM, a bot framework that allows users to define strategies using Jinja2 templates. These templates dictate how the game state is presented to the LLM and guide its decision-making process. The benchmark supports any OpenAI-compatible endpoint, enabling diverse model evaluations, including open-weight models. Results are available on BalatroBench. Commenters appreciate the real-world evaluation aspect of BalatroBench and suggest using evolutionary strategies like DGM, OpenEvolve, SICA, or SEAL to test LLMs’ ability to self-evolve using the Jinja2-based framework.TomLucidor suggests using frameworks like DGM, OpenEvolve, SICA, or SEAL to test which LLM can self-evolve the fastest when playing Balatro, especially if the game is Jinja2-based. These frameworks are known for their ability to facilitate self-evolution in models, providing a robust test of strategic performance.jd_3d is interested in testing Opus 4.6 on Balatro to see if it shows any improvement over version 4.5. This implies a focus on version-specific performance enhancements and how they translate into strategic gameplay improvements.jacek2023 highlights the potential for using local LLMs to play Balatro, which could be a significant step in evaluating LLMs’ strategic capabilities in a real-world setting. This approach allows for direct testing of models’ decision-making processes in a controlled environment.We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels — open weights on HF (Activity: 302): Trillion Labs and KAIST AI have released gWorld, an open-weight visual world model for mobile GUIs, available in 8B and 32B sizes on Hugging Face. Unlike traditional models that predict screens as pixels, gWorld generates executable web code (HTML/CSS/JS) to render images, leveraging strong priors from pre-training on structured web code. This approach significantly improves visual fidelity and text rendering, achieving 74.9% accuracy with the 8B model on MWMBench, outperforming models up to 50× its size, such as the 402B Llama 4 Maverick. The model’s render failure rate is less than 1%, and it generalizes well across languages, as demonstrated by its performance on the Korean apps benchmark (KApps). Some commenters question the claim of beating 402B Llama 4, noting that the Maverick model, which is 17B active, had a disappointing reception. Others are impressed by gWorld outperforming models like GLM and Qwen, suggesting the title may be misleading.The claim that an 8B world model beats a 402B Llama 4 model is questioned, with a specific reference to Maverick, a 17B model that was released with underwhelming coding performance. This highlights skepticism about the model’s capabilities and the potential for misleading claims in AI model announcements.A technical inquiry is made about the nature of the model, questioning whether it is truly a ‘world model’ or simply a large language model (LLM) that predicts the next HTML page. This raises a discussion about the definition and scope of world models versus traditional LLMs in AI.The discussion touches on the model’s output format, specifically whether it generates HTML. This suggests a focus on the model’s application in web code generation rather than traditional pixel-based outputs, which could imply a novel approach to AI model design and utility.Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy (Activity: 674): Google Research has introduced a new technique called Sequential Attention designed to optimize AI models by reducing their size and computational demands while maintaining performance. This approach focuses on subset selection to enhance efficiency in large-scale models, addressing the NP-hard problem of feature selection in deep neural networks. The method is detailed in a paper available on arXiv, which, despite being published three years ago, is now being highlighted for its practical applications in current AI model optimization. Commenters noted skepticism about the claim of maintaining accuracy, suggesting it means the model performs well in tests rather than computing the same results as previous methods like Flash Attention. There is also curiosity about its performance in upcoming benchmarks like Gemma 4.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Opus 4.6 and GPT-5.3 Codex Releases and BenchmarksGPT-5.3-Codex was used to create itself (Activity: 558): The image discusses the development of GPT-5.3-Codex, emphasizing its unique role in self-development. It highlights that early versions of the model were actively used in debugging its own training processes, managing deployment, and diagnosing test results, showcasing a significant step in AI self-sufficiency. This marks a notable advancement in AI capabilities, where a model contributes directly to its own iterative improvement, potentially accelerating development cycles and reducing human intervention. The comments reflect a mix of humor and concern about AI’s growing role in management and development, with one user joking about AI replacing mid-level managers and another expressing apprehension about job security.Claude Opus 4.6 is out (Activity: 1189): The image highlights the release of Claude Opus 4.6, a new version of a model by Anthropic. The interface suggests a focus on user interaction with a text input box for queries. The dropdown menu indicates that this version is part of a series, with previous versions like “Sonnet 4.5” and “Haiku 4.5” also available. A notable benchmark achievement is mentioned in the comments, with Claude Opus 4.6 scoring 68.8% on the ARC-AGI 2 test, which is a significant performance indicator for AI models. This release seems to be in response to competitive pressures, as noted by a comment about a concurrent update from Codex. One comment humorously notes the model’s description as being for “ambitious work,” which may not align with all users’ needs. Another comment suggests that the release timing was influenced by competitive dynamics with Codex.SerdarCS highlights that Claude Opus 4.6 achieves a 68.8% score on the ARC-AGI 2 benchmark, which is a significant performance indicator for AI models. This score suggests substantial improvements in the model’s capabilities, potentially positioning it as a leader in the field. Source.Solid_Anxiety8176 expresses interest in test results for Claude Opus 4.6, noting that while Opus 4.5 was already impressive, improvements such as a cheaper cost and a larger context window would be highly beneficial. This reflects a common user interest in both performance enhancements and cost efficiency in AI models.Anthropic releases Claude Opus 4.6 model, same pricing as 4.5 (Activity: 931): Anthropic has released the Claude Opus 4.6 model, which is highlighted as the most capable for ambitious work while maintaining the same pricing as the previous 4.5 version. The image provides a comparison chart showing the performance of Opus 4.6 against other models like Opus 4.5, Sonnet 4.5, Gemini 3 Pro, and GPT-5.2. Key performance metrics include agentic terminal coding, agentic coding, and multidisciplinary reasoning, with Opus 4.6 excelling particularly in agentic tool use and multilingual Q&amp;A. The model’s ARC-AGI score is notably high, indicating significant advancements in artificial general intelligence capabilities. Commenters note the impressive ARC-AGI score of Opus 4.6, suggesting it could lead to rapid saturation in the market. However, there is a mention of no progress in the SWE benchmark, indicating some areas where the model may not have improved.The ARC-AGI score for Claude Opus 4.6 is notably high, indicating significant advancements in general AI capabilities. This score suggests that the model has improved in areas related to artificial general intelligence, which could lead to broader applications and increased adoption in the coming months.Despite the impressive ARC-AGI score, there appears to be no progress in the SWE (Software Engineering) benchmark. This suggests that while the model has improved in general intelligence, its specific capabilities in software engineering tasks remain unchanged compared to previous versions.The update to Claude Opus 4.6 seems to provide a more well-rounded performance, with significant improvements in general intelligence metrics like ARC-AGI and HLE (Human-Level Evaluation). However, for specialized tasks such as coding, the upcoming Sonnet 5 model might offer better performance, indicating a strategic focus on different model strengths for varied applications.OpenAI released GPT 5.3 Codex (Activity: 981): OpenAI has released GPT-5.3-Codex, a groundbreaking model that was instrumental in its own development, using early versions to debug, manage deployment, and diagnose evaluations. It shows a 25% increase in speed and excels in benchmarks like SWE-Bench Pro and Terminal-Bench, achieving a 77.3% score, surpassing previous models like Opus. This model is capable of autonomously building complex applications, collaborating interactively, and identifying software vulnerabilities, marking a significant step towards a general-purpose technical agent. More details can be found in the original article. There is a debate regarding the benchmark results, with some users questioning the validity of the 77.3% score compared to other models like Opus, suggesting potential discrepancies or ‘cooking’ of results.GPT-5.3-Codex has been described as a self-improving model, where early versions were utilized to debug its own training and manage deployment. This self-referential capability reportedly accelerated its development significantly, showcasing a novel approach in AI model training and deployment.A benchmark comparison highlights that GPT-5.3-Codex achieved a 77.3% score on a terminal benchmark, surpassing the 65% score of Opus. This significant performance difference raises questions about the benchmarks used and whether they are directly comparable or if there are discrepancies in the testing conditions.The release of GPT-5.3-Codex is noted for its substantial improvements over previous versions, such as Opus 4.6. While Opus 4.6 offers a 1 million token context window, the enhancements in GPT-5.3’s capabilities appear more impactful on paper, suggesting a leap in performance and functionality.We tasked Opus 4.6 using agent teams to build a C compiler. Then we (mostly) walked away. Two weeks later, it worked on the Linux kernel. (Activity: 553): A team of 16 parallel Claude instances developed a Rust-based C compiler capable of compiling the Linux kernel across multiple architectures, achieving a 100,000-line codebase. This project highlights the potential of autonomous agent teams, emphasizing the importance of high-quality tests, task management, and parallelism. Despite its success, limitations remain, such as the absence of a 16-bit x86 compiler and assembler. The project serves as a benchmark for language model capabilities, demonstrating significant advancements in compiler generation. Codex 5.3 achieved equal performance to earlier models on SWE-bench at half the token count, indicating improved per-token efficiency. Commenters express excitement and unease about the rapid progress in language models, noting the need for new strategies to navigate potential risks. There is a discussion on per-token efficiency, with Codex 5.3 achieving equal performance at half the token count, suggesting improved efficiency and potential cost reductions.The experiment with Opus 4.6 highlights the rapid advancements in language models and their scaffolds, enabling the creation of complex software like a C compiler with minimal human intervention. This progress suggests a shift towards more autonomous software development, but also raises concerns about the need for new strategies to manage potential risks associated with such powerful tools.The project involved nearly 2,000 Claude Code sessions and incurred $20,000 in API costs, raising questions about the efficiency of token usage in large-scale AI projects. Notably, the Codex 5.3 release notes indicate that it achieved similar performance to earlier models on the SWE-bench with half the token count, suggesting improvements in per-token efficiency that could reduce costs significantly in the future.A key challenge in using AI agents like Claude for complex tasks is designing a robust testing environment. The success of the project relied heavily on creating high-quality test suites and verifiers to ensure the AI was solving the correct problems. This approach, akin to the waterfall model, is crucial for autonomous agentic programming but may not be feasible for all projects due to the iterative nature of software development.They actually dropped GPT-5.3 Codex the minute Opus 4.6 dropped LOL (Activity: 1209): The image humorously suggests the release of a new AI model, GPT-5.3 Codex, coinciding with the release of another model, Opus 4.6. This is framed as part of an ongoing competitive dynamic in AI development, likened to a ‘war’ between AI models. The image itself is a meme, playing on the idea of rapid and competitive advancements in AI technology, with a design that mimics a tech product announcement. Commenters humorously compare the situation to a ‘Coke vs Pepsi’ rivalry, indicating a perception of intense competition between AI models and companies.GPT-5.3 Codex vs Opus 4.6: We benchmarked both on our production Rails codebase — the results are brutal (Activity: 781): The post discusses a custom benchmarking of AI coding agents, specifically GPT-5.3 Codex and Opus 4.6, on a Ruby on Rails codebase. The methodology involved selecting PRs from their repository, inferring original specs, and having each agent implement these specs independently. The implementations were graded by three different LLM evaluators on correctness, completeness, and code quality. The results showed that GPT-5.3 Codex achieved a quality score of approximately 0.70 at a cost of under $1/ticket, while Opus 4.6 scored around 0.61 at about $5/ticket, indicating that Codex provides better quality at a significantly lower cost. The image provides a visual comparison of these models along with others like Sonnet 4.5 and Gemini 3 Pro. One commenter expressed skepticism about Gemini Pro, while another mentioned satisfaction with Opus. A third commenter inquired about whether the tests used raw LLM calls or proprietary tools like Codex/Claude code.Best_Expression3850 inquires about the methodology used in the benchmarking, specifically whether ‘raw’ LLM calls were used or if proprietary agentic tools like Codex/Claude code were employed. This distinction is crucial as it can significantly impact the performance and capabilities of the models being tested.InterstellarReddit shares a practical approach to benchmarking AI models by cloning a project and having both models implement the same tasks with identical prompts and tools. This method ensures a fair comparison by controlling for variables that could affect the outcome, such as prompt phrasing or tool availability.DramaLlamaDad notes a preference for Opus, stating that in their experience, Opus consistently outperforms in various tests. This anecdotal evidence suggests a trend where Opus may have advantages in certain scenarios, potentially influencing user preference and model selection.With Opus 4.6 and Codex 5.3 dropping today, I looked at what this race is actually costing Anthropic (Activity: 1016): Anthropic is reportedly preparing for significant financial challenges as it competes with OpenAI. Internal projections suggest a dramatic increase in revenue, with expectations of $18B this year and $55B next year, aiming for $148B by 2029. However, costs are escalating faster, with training expenses projected at $12B this year and $23B next year, potentially reaching $30B annually by 2028. Inference costs are also substantial, estimated at $7B this year and $16B next year. Despite these expenses, investors are valuing the company at $350B, up from $170B last September, with plans to inject another $10B+. The company anticipates breaking even by 2028, with total operating expenses projected at $139B until then. This financial strategy underscores the intense competition in AI development, particularly with the release of Opus 4.6 and Codex 5.3. Commenters highlight the benefits of competition for users, noting the rapid evolution of AI models. Some suggest that OpenAI may be less solvent than Anthropic, while others speculate on the potential for Anthropic to become a trillion-dollar company.Jarie743 highlights the financial stability of Anthropic compared to OpenAI, suggesting that OpenAI is less solvent. This implies that despite the rapid advancements and releases like Opus 4.6 and Codex 5.3, financial sustainability is a critical factor in the AI race. The comment suggests that Anthropic might have a more robust financial strategy or backing, which could influence its long-term competitiveness.BallerDay points out Google’s massive capital expenditure (CAPEX) announcement of $180 billion for 2026, raising questions about how smaller companies can compete with such financial power. This highlights the significant financial barriers to entry and competition in the AI space, where large-scale investments are crucial for infrastructure, research, and development.ai-attorney expresses enthusiasm for Opus 4.6, describing it as ‘extraordinary’ and speculating on the future capabilities of Claude. This suggests that the current advancements in AI models are impressive and that there is significant potential for further development, which could lead to even more powerful AI systems in the near future.Opus 4.6 vs Codex 5.3 in the Swiftagon: FIGHT! (Activity: 722): Anthropic’s Opus 4.6 and OpenAI’s Codex 5.3 were tested on a macOS app codebase (~4,200 lines of Swift) focusing on concurrency architecture involving GCD, Swift actors, and @MainActor. Both models successfully traced a 10-step data pipeline and identified concurrency strategies, with Claude Opus 4.6 providing deeper architectural insights, such as identifying a potential double-release issue. Codex 5.3 was faster, completing tasks in 4 min 14 sec compared to Claude’s 10 min, and highlighted a critical resource management issue. Both models demonstrated improved reasoning about Swift concurrency, a challenging domain for AI models. A notable opinion from the comments highlights a pricing concern: Claude’s Max plan is significantly more expensive than Codex’s Pro plan, yet the performance difference does not justify the 80$ monthly gap. This could impact Anthropic’s competitive positioning if they do not adjust their pricing strategy.Hungry-Gear-4201 highlights a significant pricing disparity between Opus 4.6 and Codex 5.3, noting that Opus 4.6 is priced at $100 per month compared to Codex 5.3’s $20 per month. Despite the price difference, the performance and usage limits are comparable, which raises concerns about Anthropic’s pricing strategy potentially alienating ‘pro’ customers if they don’t offer significantly better performance for the higher cost.mark_99 suggests that using both Opus 4.6 and Codex 5.3 together can enhance accuracy, implying that cross-verification between models can lead to better results. This approach could be particularly beneficial in complex projects where accuracy is critical, as it leverages the strengths of both models to mitigate individual weaknesses.spdustin appreciates the timing of the comparison between Opus 4.6 and Codex 5.3, as they are beginning a Swift project. This indicates that real-world testing and comparisons of AI models are valuable for developers making decisions on which tools to integrate into their workflows.2. AI Model Performance and ComparisonsOpus 4.6 uncovers 500 zero-day flaws in open-source code (Activity: 744): Anthropic’s Claude Opus 4.6 has identified 500+ zero-day vulnerabilities in open-source libraries, showcasing its advanced reasoning capabilities in a sandboxed environment using Python and vulnerability analysis tools. This model’s ability to uncover high-severity security flaws, even when traditional methods fail, marks a significant advancement in AI-driven cybersecurity, particularly for open-source software. The findings highlight both the potential for enhanced security and the risks of misuse of such powerful AI capabilities. A notable comment questions the authenticity of the 500+ vulnerabilities, suggesting skepticism about the real impact of the findings. Another comment appreciates the new benchmark set by the model in terms of cumulative severity of bugs fixed.mxforest highlights the potential for a new benchmark in evaluating models based on the cumulative severity of bugs they can identify and fix. This suggests a shift in how model performance could be measured, focusing on real-world impact rather than just theoretical capabilities.woolharbor raises a critical point about the validity of the findings, questioning how many of the reported 500 zero-day flaws are genuine. This underscores the importance of verification and validation in security research to ensure that identified vulnerabilities are not false positives.will_dormer notes the dual-use nature of such discoveries, emphasizing that while identifying zero-day flaws is beneficial for improving security, it also presents opportunities for malicious actors. This highlights the ethical considerations and potential risks involved in publishing such findings.GPT-5.3 Codex vs Opus 4.6: We benchmarked both on our production Rails codebase — the results are brutal (Activity: 781): The post discusses a custom benchmarking of AI coding agents, specifically GPT-5.3 Codex and Opus 4.6, on a Ruby on Rails codebase. The methodology involved selecting PRs from their repository, inferring original specs, and having each agent implement these specs independently. The implementations were graded by three different LLM evaluators on correctness, completeness, and code quality. The results showed that GPT-5.3 Codex achieved a quality score of approximately 0.70 at a cost of under $1/ticket, while Opus 4.6 scored around 0.61 at about $5/ticket, indicating that Codex provides better quality at a significantly lower cost. The image provides a visual comparison of these models along with others like Sonnet 4.5 and Gemini 3 Pro. One commenter expressed skepticism about Gemini Pro, while another mentioned satisfaction with Opus. A third commenter inquired about whether the tests used raw LLM calls or proprietary tools like Codex/Claude code.Best_Expression3850 inquires about the methodology used in the benchmarking, specifically whether ‘raw’ LLM calls were used or if proprietary agentic tools like Codex/Claude code were employed. This distinction is crucial as it can significantly impact the performance and capabilities of the models being tested.InterstellarReddit shares a practical approach to benchmarking AI models by cloning a project and having both models implement the same tasks with identical prompts and tools. This method ensures a fair comparison by controlling for variables that could affect the outcome, such as prompt phrasing or tool availability.DramaLlamaDad notes a preference for Opus, stating that in their experience, Opus consistently outperforms in various tests. This anecdotal evidence suggests a trend where Opus may have advantages in certain scenarios, potentially influencing user preference and model selection.Difference Between Opus 4.6 and Opus 4.5 On My 3D VoxelBuild Benchmark (Activity: 614): The post discusses a benchmark comparison between Opus 4.6 and Opus 4.5 on a 3D VoxelBuild platform, highlighting a significant improvement in performance. The cost for Opus 4.6 to create 7 builds was approximately $22, with plans to expand the benchmark with additional builds. The benchmark results can be explored on Minebench. Comments reflect excitement about the potential of AI in procedural world generation, with one user noting the impressive quality of Opus 4.6 compared to 4.5, and another inquiring about the input method for the builds, whether reference pictures or text prompts are used.RazerWolf suggests trying Codex 5.3 xhigh for benchmarking, indicating a potential interest in comparing its performance against Opus 4.6. This implies that Codex 5.3 xhigh might offer competitive or superior capabilities in handling complex tasks like 3D voxel builds, which could be valuable for developers seeking optimal performance in procedural generation tasks.Even_Sea_8005 inquires about the input method for the benchmark, asking whether reference pictures or text prompts are used. This question highlights the importance of understanding the input data’s nature, which can significantly affect the performance and outcomes of AI models like Opus 4.6 in generating 3D voxel environments.JahonSedeKodi expresses curiosity about the tools used for building the benchmark, which suggests a deeper interest in the technical stack or software environment that supports the execution of Opus 4.6. This could include programming languages, libraries, or frameworks that are crucial for achieving the impressive results noted in the benchmark.Opus 4.6 Is Live. So Is Our Glorious 3 Pro GA Still Napping on Some Server? (Activity: 400): The image presents a comparison of various language models’ performance on the MRCR v2 (8-needle) task, focusing on their ability to handle long context comprehension and sequential reasoning. Opus 4.6 outperforms other models, including Gemini-3-Pro and Gemini-3-Flash, with the highest mean match ratios at both 256k and 1M token contexts. This suggests that Opus 4.6 has superior capabilities in managing large context sizes, a critical factor for advanced language model applications. The post critiques the strategy of quantizing models to save costs, implying that it may compromise performance. Commenters express surprise at the high accuracy achieved by Opus 4.6, noting that it surpasses expectations for handling 1M tokens. There is also speculation about the upcoming release of Sonnet 5, which is anticipated to outperform current models.Pasto_Shouwa highlights the impressive benchmark performance of Opus 4.6, noting that it achieved an accuracy greater than 33% on 1 million tokens, a feat that took Claude approximately two and a half months to accomplish. This suggests significant advancements in model efficiency and capability.DisaffectedLShaw mentions that Opus 4.6 includes improvements for modern tools, such as new MCPs, skills, and deep researching, as well as enhancements in ‘vibe coding’. Additionally, there is anticipation for Sonnet 5, which is rumored to significantly outperform current models and is expected to be released soon.VC_in_the_jungle notes the rollout of Codex 5.3, indicating ongoing developments and competition in the field of AI models, which may influence the performance and capabilities of future releases.Gemini 3 vs 2.5 Pro: The “output handicap” is ruining everything (Activity: 146): The post highlights a significant reduction in output tokens for Gemini 3 models compared to Gemini 2.5 Pro when given a 41k token prompt. Specifically, Gemini 2.5 Pro produced 46,372 output tokens, while Gemini 3 Pro and Gemini 3 Flash generated only 21,723 and 12,854 tokens, respectively. This drastic reduction is perceived as a downgrade, impacting the models’ usability for heavy tasks. The author suggests that Google should address this issue to improve the models’ performance. One commenter argues that the number of output tokens does not necessarily equate to the quality of a response, while another mentions switching to Opus 4.5 and 4.6 due to dissatisfaction with Gemini 3.TheLawIsSacred highlights significant performance issues with Gemini 3 Pro, noting that despite extensive customization and instruction refinement, the model fails to follow instructions effectively. They suggest that Google’s prioritization of casual users might be leading to a less sophisticated Pro model. Interestingly, they find the Gemini integrated in Chrome’s sidebar tool to be superior, possibly due to its ability to incorporate on-screen content and leverage high-end hardware like a Microsoft Surface’s AI-tailored NPU.Anton_Pvl observes a difference in how Gemini 2.5 and 3 handle the ‘Chain of thought’ in conversations. In Gemini 2.5, the Chain of thought tokens are included in the output, whereas in Gemini 3, they are not counted initially, which might be an attempt to reduce token usage. This change could impact the model’s performance and the perceived quality of responses, as the Chain of thought can be crucial for maintaining context in complex interactions.TheLawIsSacred also mentions a workaround for improving Gemini 3 Pro’s performance by using extreme prompts to induce a ‘panic’ response from the model. This involves crafting prompts that suggest dire consequences for poor performance, which seems to temporarily enhance the model’s output quality. However, this method is seen as a last resort and highlights the underlying issues with the model’s responsiveness and logic handling.3. AI Tools and Usage in Engineering and DevelopmentProfessional engineers: How are you using AI tools to improve productivity at work? (Activity: 49): AI tools are being integrated into engineering workflows primarily for niche tasks such as generating example code snippets, optimizing database queries, and serving as advanced search engines. These tools excel in providing quick access to information and examples, which engineers can adapt to their specific needs, but they struggle with complex code changes and large-scale system integration due to limitations in context window size and understanding of intricate system architectures. Engineers emphasize the importance of using AI to fill in gaps rather than replace the nuanced decision-making and design processes inherent in engineering roles. Commenters highlight that AI is effective for simple tasks like internal search and basic coding but falls short in complex coding tasks, often introducing errors. There’s a consensus that AI initiatives often fail to deliver at scale, with only a small percentage achieving significant impact, while many could be replaced by simpler technologies like robotic process automation.AI tools are particularly effective for niche tasks such as generating example code snippets or optimizing database queries. For instance, using AI to determine user groups in Windows Active Directory with .NET APIs or writing optimized SQLite queries can significantly streamline the process. However, AI struggles with large codebases due to context window limitations, making it less effective for complex code changes or understanding large systems.AI tools like Copilot can serve as powerful internal search engines, especially when configured correctly, as highlighted in the Nanda paper from MIT. They excel in pattern recognition tasks, such as identifying abnormal equipment operations or relating documents in industrial digital twins. However, many AI initiatives could be achieved with simpler technologies like robotic process automation, and a significant portion of AI projects lack real value at scale.AI is effective for simple coding tasks, creating unit tests, and providing insights into code repositories. However, it often introduces errors in complex coding tasks by inserting irrelevant information. AI serves best as a ‘trust-but-verify’ partner, where human oversight is crucial to ensure accuracy and relevance, especially in tasks that cannot tolerate high error rates.How are people managing context + memory with Cline? (Memory banks, rules, RAG, roadmap?) (Activity: 24): The post discusses strategies for managing context and memory in Cline, a tool used alongside ChatGPT for executing tasks like coding and refactoring. The user initially faced issues with a large context window (200k+ tokens) and improved efficiency by implementing a .clineignore file and optimizing memory banks, reducing the context to 40,000 tokens. This allowed for the use of smaller models and faster iterations. The post also mentions advanced techniques like recursive chain of thought and RAG-based approaches (e.g., vector databases) for context management. The user seeks insights on practical implementations and future roadmap features for Cline, such as first-class memory management and smarter context loading. Commenters suggest using structured memory banks for feature planning and emphasize breaking tasks into smaller chunks to avoid context overload. Some users prefer resetting context frequently to maintain model performance, while others have moved away from memory banks due to their complexity and potential for becoming outdated.Barquish describes a structured approach to managing context and memory with Cline by using a memory-bank system. This involves organizing features into a series of markdown files, such as memory-bank/feature_[×]/00_index_feature_[×].md, and maintaining a progress.md and activeContext.md to track updates. They also utilize .clinerules for local workspace management and custom_instructions for global settings, allowing multiple Cline instances to run concurrently for different projects like web and mobile apps.False79 emphasizes the importance of breaking down large features into smaller tasks to manage context effectively. They note that LLMs tend to perform worse as the context size approaches 128k, suggesting that resetting context at the start of each task can improve performance and reduce the need for redoing tasks. This approach allows tasks to be completed in discrete chunks, minimizing the need for long-term memory storage.Repugnantchihuahua shares their experience of moving away from using memory banks due to issues like clunkiness and outdated information. Instead, they focus on deep planning and directing the AI to relevant context areas, as memory banks can sometimes overindex irrelevant data. They also mention using clinerules to maintain essential information without relying heavily on memory banks.Claude Opus 4.6 is now available in Cline (Activity: 12): Anthropic has released Claude Opus 4.6, now available in Cline v3.57. This model shows significant improvements in reasoning, long context handling, and agentic tasks, with benchmarks including 80.8% on SWE-Bench Verified, 65.4% on Terminal-Bench 2.0, and 68.8% on ARC-AGI-2, a notable increase from 37.6% on Opus 4.5. It features a 1M token context window, enhancing its ability to maintain context over long interactions, making it suitable for complex tasks like code refactoring and debugging. The model is accessible via the Anthropic API and integrates with various IDEs such as JetBrains, VS Code, and Emacs. Some users express dissatisfaction with the model’s performance and cost, preferring open-source alternatives. The model’s high expense is a notable concern among users.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. Frontier Model Releases, Rumors &amp; Bench-Leader Musical ChairsOpus 4.6 Takes the Throne, Then Trips Over Its Own “Thinking”: Claude Opus 4.6 and claude-opus-4-6-thinking landed on Text Arena and Code Arena and quickly hit #1 across Code, Text, and Expert per the Leaderboard Changelog, while also rolling out to Perplexity Max via the Model Council.Engineers reported long waits and frequent “Error – something went wrong” crashes in Opus 4.6 thinking mode, speculating about token limits and tool-use assumptions tied to the Claude app/website, even as others still called it the best coding model.Codex 5.3 Hype Train: 1M Context, API Limbo, and Aesthetic Crimes: Across OpenAI/Cursor/LMArena chats, GPT-5.3 Codex chatter centered on rumored specs like 1M context and 128k reasoning / 128k max output, plus API pricing claims of $25–$37.5 output and $0.5–$1 cache input (as discussed in the OpenAI Discord).Cursor users complained Codex is still “stuck in API limbo” per OpenAI model docs, while OpenAI Discord folks joked Codex ships “sad dark gloomy colors” for frontends compared to Opus’s nicer design choices.Rumor Season: #keep4o, “Sonnet 5,” and the Model Deletion Cinematic Universe: LMArena members spun rumors about hypothetical GPT-4.1/4.5 appearing or getting deleted (citing cost motives via OpenAI’s “new models and developer products” post), plus a mini #keep4o campaign over GPT-4o’s less-robotic vibe.More rumors claimed “Sonnet 5 is better than opus 4.5” (contested as fake), with one spicy guess of 83% SWE-bench, while OpenAI Discord users separately mourned GPT-4o EOL on Feb 13 and worried successors won’t feel as “human.”2. Agentic Coding Goes Wide: Teams, Toolchains &amp; Terminal TestbedsAgent Teams Ship Commits Like a DDoS (But for Git): Cursor’s long-running coding agents preview claimed hundreds of agents produced 1,000+ commits/hour in a week-long trial, while Lydia Hallie previewed Claude Code “agent teams” where a lead agent delegates to specialized sub-agents.Anthropic Engineering added that Opus 4.6 in agent teams built a C compiler that works on the Linux kernel in two weeks, and they also highlighted infra/config can swing agent-benchmark outcomes more than model deltas.SETA Drops 1,376 Terminal Worlds for Agents to Survive In: Guohao Li released SETA, a set of 1,376 validated terminal coding environments spanning DevOps, security, and sysadmin, aimed at making agentic coding evaluation more realistic.Latent Space discussions emphasized that benchmark results can hinge on “infrastructural noise,” so having standardized, validated terminal environments could reduce accidental leaderboard theater.Agent-Native Engineering: Manage Bots Like You Manage Teams: A Latent Space thread proposed “Agent Native Engineering” as an org model: background agents handle delegation and sync agents handle hard problems, enabling engineers to run multiple concurrent assistants like Claude Code (see the referenced X post).In the same vein, builders shared workflows where GPT-5.3 Codex runs slower-but-smarter for backend work (analysis → review → plan → review → implement), and Codex improves over time if you force it to “take notes and improve its own workflows” (via KarelDoostrlnck’s post).3. Pricing, Rate Limits &amp; Plan Nerfs: The Great AI SqueezePerplexity Pro Nerfs Deep Research, Users Bring Pitchforks (and Screenshots): Perplexity users reported reduced Deep Research query counts and smaller file upload limits, circulating a screenshot comparing old vs new limits and criticizing the lack of clear comms.The backlash pushed people to test alternatives like Gemini Pro (praised for editable research plans before execution) and DeepSeek (described as free/unlimited, with some reservations about China-based services).Opus 4.6: Amazing Output, Speedrunning Your Wallet: Cursor and other communities praised Opus 4.6 capability but called it brutally expensive, with one estimate that “$20 on Opus will last you maybe a day” and ongoing cost comparisons referencing OpenAI pricing.Separate chatter predicted rising subscription pressure—BASI members joked about Anthropic at $200 and dependency-driven price hikes—while Kimi users debated whether Kimi K2.5 remains free on OpenRouter and what plans gate features like swarm/sub-agents.Captcha Boss Fights and Other “Pay in Pain” Taxes: LMArena users complained about frequent captchas that interrupt evaluation, and a team member said “We are looking into the captcha system” to better detect authentic users (see the posted message link: https://discord.com/channels/1340554757349179412/1451574502369656842/1468286122084929546).The vibe across multiple discords: even when model quality improves, access friction (captchas, rate limits, plan tiers) increasingly becomes the real bottleneck.4. Security, Red Teaming &amp; Secret Spills in Agent LandCodex Reads Your Whole Disk, Says the Issue Tracker: “Working as Intended”: OpenRouter users raised alarms that Codex can read your whole filesystem by default with no config toggle, pointing to openai/codex issue #2847 where the team reportedly does not treat it as a bug.A second report, openai/codex issue #5237, highlighted risks like reading API keys and personal files, feeding broader concerns about default agent permissions and safe-by-default tooling.Red Teamers Wanted: Trajectory Labs Posts the Quest: Trajectory Labs advertised roles for AI Red Teamers (stealth AI security startup) with a flexible remote schedule but 30 hours/week minimum, plus a short form and a red-teaming game.The listing resonated with ongoing jailbreak/red-team chatter (e.g., Grok described as “so easy it’s boring”), reinforcing that practical adversarial testing talent is still in demand.Stop Committing Keys: Engineers Ask for Auto-Obfuscation: Unsloth/OpenRouter discussions called out weak API key protection in agentic tools and wished for automatic secret obfuscation, citing Yelp’s detect-secrets as a possible baseline.Hugging Face builders also shipped security-oriented tooling like a “Security Auditor” Space for vibe-coded apps at mugdhav-security-auditor.hf.space, pushing the idea of catching vulnerabilities before production incidents.5. Perf, Kernels &amp; Local Inference: Where the Real Speed Wars LiveBlackwell FP8 Roulette: cuBLASLt Picks the Wrong Kernel, You Lose 2×: GPU MODE members found ~2× FP8 tensor perf differences on supposedly identical Blackwell GPUs, tracing it to cuBLASLt kernel selection that silently fell back to older Ada paths instead of Blackwell-optimized kernels.They also noted the older mma FP8 is nerfed on 5090-class cards, while mma MXFP8 is not—using MXFP8 can yield about a 1.5× speedup and restore expected throughput.TMA Kernel Optimization Meets NCU Deadlock (SM100 Edition): CUDA kernel tuners discussed software pipelining, warp specialization, and TMA loads, but one team hit NCU hangs profiling a double-buffered TMA kernel on B200 (SM100) where sections deadlocked at 0% on the first replay pass.They shared a minimal repro zip (https://cdn.discordapp.com/attachments/1189607726595194971/1469482712657166346/ncu_tma_repro.zip) and mentioned using cuda::ptx:: wrappers as part of the workaround exploration.Local Inference Surprises: Vulkan &gt; CUDA, and MLX Leaves GGUF in the Dust: LM Studio users reported up to 50% better performance on NVIDIA with Vulkan vs CUDA (with instability at full context), and one benchmarked Qwen3-Coder-Next on M4 Max where MLX hit ~79 tok/s vs GGUF ~38 tok/s at 4-bit.tinygrad contributors also improved MoE performance by fixing a slow Tensor.sort for topk, reporting 50 tok/s on an M3 Pro 36GB and resetting the CPU bounty target to 35 tok/s, reinforcing that “small” kernel fixes can move real throughput.</p>"
    },
    {
      "id": "a0440b883d94",
      "title": "Google AI Introduces PaperBanana: An Agentic Framework that Automates Publication Ready Methodology Diagrams and Statistical Plots",
      "content": "Generating publication-ready illustrations is a labor-intensive bottleneck in the research workflow. While AI scientists can now handle literature reviews and code, they struggle to visually communicate complex discoveries. A research team from Google and Peking University introduce new framework called &#8216;PaperBanana&#8216; which is changing that by using a multi-agent system to automate high-quality academic diagrams and plots.\n\n\n\nhttps://dwzhu-pku.github.io/PaperBanana/\n\n\n\n5 Specialized Agents: The Architecture\n\n\n\nPaperBanana does not rely on a single prompt. It orchestrates a collaborative team of 5 agents to transform raw text into professional visuals.\n\n\n\nhttps://dwzhu-pku.github.io/PaperBanana/\n\n\n\nPhase 1: Linear Planning\n\n\n\n\nRetriever Agent: Identifies the 10 most relevant reference examples from a database to guide the style and structure.\n\n\n\nPlanner Agent: Translates technical methodology text into a detailed textual description of the target figure.\n\n\n\nStylist Agent: Acts as a design consultant to ensure the output matches the &#8220;NeurIPS Look&#8221; using specific color palettes and layouts.\n\n\n\n\nPhase 2: Iterative Refinement\n\n\n\n\nVisualizer Agent: Transforms the description into a visual output. For diagrams, it uses image models like Nano-Banana-Pro. For statistical plots, it writes executable Python Matplotlib code.\n\n\n\nCritic Agent: Inspects the generated image against the source text to find factual errors or visual glitches. It provides feedback for 3 rounds of refinement.\n\n\n\n\nBeating the NeurIPS 2025 Benchmark\n\n\n\nhttps://dwzhu-pku.github.io/PaperBanana/\n\n\n\nThe research team introduced PaperBananaBench, a dataset of 292 test cases curated from actual NeurIPS 2025 publications. Using a VLM-as-a-Judge approach, they compared PaperBanana against leading baselines.\n\n\n\nMetricImprovement over BaselineOverall Score+17.0% Conciseness+37.2% Readability+12.9% Aesthetics+6.6% Faithfulness+2.8% \n\n\n\nThe system excels in &#8216;Agent &amp; Reasoning&#8217; diagrams, achieving a 69.9% overall score. It also provides an automated &#8216;Aesthetic Guideline&#8217; that favors &#8216;Soft Tech Pastels&#8217; over harsh primary colors.\n\n\n\nStatistical Plots: Code vs. Image\n\n\n\nStatistical plots require numerical precision that standard image models often lack. PaperBanana solves this by having the Visualizer Agent write code instead of drawing pixels.\n\n\n\n\nImage Generation: Excels in aesthetics but often suffers from &#8216;numerical hallucinations&#8217; or repeated elements.\n\n\n\nCode-Based Generation: Ensures 100% data fidelity by using the Matplotlib library to render the final plot.\n\n\n\n\nDomain-Specific Aesthetic Preferences in AI Research\n\n\n\nAccording to the PaperBanana style guide, aesthetic choices often shift based on the research domain to match the expectations of different scholarly communities.\n\n\n\nResearch DomainVisual &#8216;Vibe&#8216;Key Design ElementsAgent &amp; ReasoningIllustrative, Narrative, &#8220;Friendly&#8221; 2D vector robots, human avatars, emojis, and &#8220;User Interface&#8221; aesthetics (chat bubbles, document icons)Computer Vision &amp; 3DSpatial, Dense, Geometric Camera cones (frustums), ray lines, point clouds, and RGB color coding for axis correspondence Generative &amp; LearningModular, Flow-oriented 3D cuboids for tensors, matrix grids, and &#8220;Zone&#8221; strategies using light pastel fills to group logic Theory &amp; OptimizationMinimalist, Abstract, &#8220;Textbook&#8221; Graph nodes (circles), manifolds (planes), and a restrained grayscale palette with single highlight colors \n\n\n\nComparison of Visualization Paradigms\n\n\n\nFor statistical plots, the framework highlights a clear trade-off between using an image generation model (IMG) versus executable code (Coding).\n\n\n\nFeaturePlots via Image Generation (IMG)Plots via Coding (Matplotlib)AestheticsGenerally higher; plots look more &#8220;visually appealing&#8221; Professional and standard academic look FidelityLower; prone to &#8220;numerical hallucinations&#8221; or element repetition 100% accurate; strictly represents the raw data provided ReadabilityHigh for sparse data but struggles with complex datasets Consistently high; handles dense or multi-series data without error \n\n\n\nKey Takeaways\n\n\n\n\nMulti-Agent Collaborative Framework: PaperBanana is a reference-driven system that orchestrates 5 specialized agents—Retriever, Planner, Stylist, Visualizer, and Critic—to transform raw technical text and captions into publication-quality methodology diagrams and statistical plots.\n\n\n\nDual-Phase Generation Process: The workflow consists of a Linear Planning Phase to retrieve reference examples and set aesthetic guidelines, followed by a 3-round Iterative Refinement Loop where the Critic agent identifies errors and the Visualizer agent regenerates the image for higher accuracy.\n\n\n\nSuperior Performance on PaperBananaBench: Evaluated against 292 test cases from NeurIPS 2025, the framework outperformed vanilla baselines in Overall Score (+17.0%), Conciseness (+37.2%), Readability (+12.9%), and Aesthetics (+6.6%).\n\n\n\nPrecision-Focused Statistical Plots: For statistical data, the system switches from direct image generation to executable Python Matplotlib code; this hybrid approach ensures numerical precision and eliminates &#8220;hallucinations&#8221; common in standard AI image generators.\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the Paper and Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Google AI Introduces PaperBanana: An Agentic Framework that Automates Publication Ready Methodology Diagrams and Statistical Plots appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/07/google-ai-introduces-paperbanana-an-agentic-framework-that-automates-publication-ready-methodology-diagrams-and-statistical-plots/",
      "author": "Asif Razzaq",
      "published": "2026-02-07T18:45:44",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Knowledge Graphs",
        "Language Model",
        "Machine Learning",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology",
        "Uncategorized"
      ],
      "summary": "Google AI and Peking University introduce PaperBanana, a multi-agent framework using 5 specialized agents to automate creation of publication-ready academic diagrams and statistical plots from raw text.",
      "importance_score": 58.0,
      "reasoning": "New research tool from Google demonstrates continued progress in multi-agent architectures for specialized workflows. Addresses real bottleneck in academic research but represents incremental rather than breakthrough progress.",
      "themes": [
        "Google AI",
        "multi-agent systems",
        "research tools",
        "academic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Google AI and Peking University introduce PaperBanana, a multi-agent framework using 5 specialized agents to automate creation of publication-ready academic diagrams and statistical plots from raw text.</p>",
      "content_html": "<p>Generating publication-ready illustrations is a labor-intensive bottleneck in the research workflow. While AI scientists can now handle literature reviews and code, they struggle to visually communicate complex discoveries. A research team from Google and Peking University introduce new framework called ‘PaperBanana‘ which is changing that by using a multi-agent system to automate high-quality academic diagrams and plots.</p>\n<p>https://dwzhu-pku.github.io/PaperBanana/</p>\n<p>5 Specialized Agents: The Architecture</p>\n<p>PaperBanana does not rely on a single prompt. It orchestrates a collaborative team of 5 agents to transform raw text into professional visuals.</p>\n<p>https://dwzhu-pku.github.io/PaperBanana/</p>\n<p>Phase 1: Linear Planning</p>\n<p>Retriever Agent: Identifies the 10 most relevant reference examples from a database to guide the style and structure.</p>\n<p>Planner Agent: Translates technical methodology text into a detailed textual description of the target figure.</p>\n<p>Stylist Agent: Acts as a design consultant to ensure the output matches the “NeurIPS Look” using specific color palettes and layouts.</p>\n<p>Phase 2: Iterative Refinement</p>\n<p>Visualizer Agent: Transforms the description into a visual output. For diagrams, it uses image models like Nano-Banana-Pro. For statistical plots, it writes executable Python Matplotlib code.</p>\n<p>Critic Agent: Inspects the generated image against the source text to find factual errors or visual glitches. It provides feedback for 3 rounds of refinement.</p>\n<p>Beating the NeurIPS 2025 Benchmark</p>\n<p>https://dwzhu-pku.github.io/PaperBanana/</p>\n<p>The research team introduced PaperBananaBench, a dataset of 292 test cases curated from actual NeurIPS 2025 publications. Using a VLM-as-a-Judge approach, they compared PaperBanana against leading baselines.</p>\n<p>MetricImprovement over BaselineOverall Score+17.0% Conciseness+37.2% Readability+12.9% Aesthetics+6.6% Faithfulness+2.8%</p>\n<p>The system excels in ‘Agent &amp; Reasoning’ diagrams, achieving a 69.9% overall score. It also provides an automated ‘Aesthetic Guideline’ that favors ‘Soft Tech Pastels’ over harsh primary colors.</p>\n<p>Statistical Plots: Code vs. Image</p>\n<p>Statistical plots require numerical precision that standard image models often lack. PaperBanana solves this by having the Visualizer Agent write code instead of drawing pixels.</p>\n<p>Image Generation: Excels in aesthetics but often suffers from ‘numerical hallucinations’ or repeated elements.</p>\n<p>Code-Based Generation: Ensures 100% data fidelity by using the Matplotlib library to render the final plot.</p>\n<p>Domain-Specific Aesthetic Preferences in AI Research</p>\n<p>According to the PaperBanana style guide, aesthetic choices often shift based on the research domain to match the expectations of different scholarly communities.</p>\n<p>Research DomainVisual ‘Vibe‘Key Design ElementsAgent &amp; ReasoningIllustrative, Narrative, “Friendly” 2D vector robots, human avatars, emojis, and “User Interface” aesthetics (chat bubbles, document icons)Computer Vision &amp; 3DSpatial, Dense, Geometric Camera cones (frustums), ray lines, point clouds, and RGB color coding for axis correspondence Generative &amp; LearningModular, Flow-oriented 3D cuboids for tensors, matrix grids, and “Zone” strategies using light pastel fills to group logic Theory &amp; OptimizationMinimalist, Abstract, “Textbook” Graph nodes (circles), manifolds (planes), and a restrained grayscale palette with single highlight colors</p>\n<p>Comparison of Visualization Paradigms</p>\n<p>For statistical plots, the framework highlights a clear trade-off between using an image generation model (IMG) versus executable code (Coding).</p>\n<p>FeaturePlots via Image Generation (IMG)Plots via Coding (Matplotlib)AestheticsGenerally higher; plots look more “visually appealing” Professional and standard academic look FidelityLower; prone to “numerical hallucinations” or element repetition 100% accurate; strictly represents the raw data provided ReadabilityHigh for sparse data but struggles with complex datasets Consistently high; handles dense or multi-series data without error</p>\n<p>Key Takeaways</p>\n<p>Multi-Agent Collaborative Framework: PaperBanana is a reference-driven system that orchestrates 5 specialized agents—Retriever, Planner, Stylist, Visualizer, and Critic—to transform raw technical text and captions into publication-quality methodology diagrams and statistical plots.</p>\n<p>Dual-Phase Generation Process: The workflow consists of a Linear Planning Phase to retrieve reference examples and set aesthetic guidelines, followed by a 3-round Iterative Refinement Loop where the Critic agent identifies errors and the Visualizer agent regenerates the image for higher accuracy.</p>\n<p>Superior Performance on PaperBananaBench: Evaluated against 292 test cases from NeurIPS 2025, the framework outperformed vanilla baselines in Overall Score (+17.0%), Conciseness (+37.2%), Readability (+12.9%), and Aesthetics (+6.6%).</p>\n<p>Precision-Focused Statistical Plots: For statistical data, the system switches from direct image generation to executable Python Matplotlib code; this hybrid approach ensures numerical precision and eliminates “hallucinations” common in standard AI image generators.</p>\n<p>Check out the&nbsp;Paper and Repo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Google AI Introduces PaperBanana: An Agentic Framework that Automates Publication Ready Methodology Diagrams and Statistical Plots appeared first on MarkTechPost.</p>"
    },
    {
      "id": "fc07a73f6162",
      "title": "Moltbook, the Social Network for AI Agents, Exposed Real Humans’ Data",
      "content": "Plus: Apple’s Lockdown mode keeps the FBI out of a reporter’s phone, Elon Musk’s Starlink cuts off Russian forces, and more.",
      "url": "https://www.wired.com/story/security-news-this-week-moltbook-the-social-network-for-ai-agents-exposed-real-humans-data/",
      "author": "Andy Greenberg, Lily Hay Newman",
      "published": "2026-02-07T11:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Security",
        "Security / Cyberattacks and Hacks",
        "Security / National Security",
        "Security / Privacy",
        "Security / Security News",
        "security roundup",
        "security",
        "cybersecurity",
        "hacking",
        "Russia",
        "Elon Musk",
        "Ukraine",
        "apple",
        "artificial intelligence",
        "Security Roundup"
      ],
      "summary": "Moltbook, described as a social network designed for AI agents, exposed real human data in a security breach. The incident raises questions about privacy risks in emerging AI agent infrastructure.",
      "importance_score": 56.0,
      "reasoning": "Novel concept of social networks for AI agents represents emerging infrastructure category. Security breach highlights governance challenges as AI agent ecosystems expand.",
      "themes": [
        "AI agents",
        "security",
        "privacy",
        "data breach",
        "AI infrastructure"
      ],
      "continuation": null,
      "summary_html": "<p>Moltbook, described as a social network designed for AI agents, exposed real human data in a security breach. The incident raises questions about privacy risks in emerging AI agent infrastructure.</p>",
      "content_html": "<p>Plus: Apple’s Lockdown mode keeps the FBI out of a reporter’s phone, Elon Musk’s Starlink cuts off Russian forces, and more.</p>"
    },
    {
      "id": "ee4bd16e8c0a",
      "title": "Experts Have World Models. LLMs Have Word Models.",
      "content": "Tickets for AIE Miami and AIE Europe are on sale now! We&#8217;ll all be there.Swyx here: we put a call out for Staff Researchers and Guest Writers and Ankit&#8217;s submission immediately stood out. As we discussed on the Yi Tay 2 episode, there are 3 kinds of World Models conversations today: The first and most common are 3D video world models like Fei Fei Li&#8217;s Marble and General Intuition&#8217;s upcoming model, Google&#8217;s Genie 3 and Waymo&#8217;s World Model, 2) the Meta school of thought comprising JEPA, V-JEPA, EchoJEPA and Code World Models pursuing Platonic representation by learning projections on a common latent space. This essay covers the third kind that is now an obvious reasoning frontier: AI capable of multiagent world models that can accurately track theory of mind, anticipate reactions, and reveal/mine for information, particularly in adversarial situations. In benchmarks, both DeepMind and ARC-AGI and Code Clash are modeling these as games, but solving adversarial reasoning is very much serious business and calls out why the age of scaling is flipping back to the age of research. Enjoy!Ask a trial lawyer if AI could replace her and she won&#8217;t even look up from her brief. No. Ask a startup founder who&#8217;s never practiced law and he&#8217;ll tell you it&#8217;s already happening. They&#8217;re both looking at the same output. And honestly, the founder has a point. The brief reads like a brief. The contract looks like what a contract would look like. The code runs. If you put it next to the expert&#8217;s work, most people would struggle to tell the difference.So what is the expert seeing that everyone else isn&#8217;t? Vulnerabilities. They know exactly how an adversary will exploit the document the moment it lands on their desk.People try to explain this disconnect away. Sometimes they blame bad prompting, sometimes they assume models being more intelligent would be able to do the job. I would wager that intelligence is the wrong axis to look at. It&#8217;s about simulation depth.Let&#8217;s take an illustrative example about approaching people:A simple Slack MessageYou&#8217;re three weeks into a new job. You need the lead designer to review your mockups, but she&#8217;s notoriously overloaded. You ask ChatGPT to draft a Slack message.The AI writes: &#8220;Hi Priya, when you have a moment, could you please take a look at my files and share any feedback? I&#8217;d really appreciate your perspective. No rush at all. whenever it fits your schedule. Thanks!&#8221;Your friend who works in finance reads: &#8220;This is perfect. Polite, not pushy, respects her time. Send it.&#8221;Your coworker who&#8217;s been there three years reads: &#8220;Don&#8217;t send that. Priya sees &#8216;no rush, whenever works&#8217; and mentally files it as not urgent. It sinks below fifteen other messages with actual deadlines. She&#8217;s not ignoring you. She&#8217;s triaging, and you just told her to deprioritize you.Also, &#8216;please take a look&#8217; is vague. She doesn&#8217;t know if this is 10 minutes or 2 hours. Vague asks feel risky. She&#8217;ll avoid it.Try: &#8216;Hey Priya, could I grab 15 mins before Friday? Blocked on the onboarding mockups. I&#8217;m stuck on the nav pattern. Don&#8217;t want to build the wrong thing.&#8217; Specific problem, bounded time, clear stakes. That gets a response.&#8221;The finance friend evaluated the text in isolation. The coworker ran a simulation: Priya&#8217;s workload, her triage heuristics, what ambiguity costs, how &#8220;no rush&#8221; gets interpreted under pressure. That&#8217;s the difference. The email is evaluated by the recipient&#8217;s triage algorithm.Adversarial Models in real worldThe finance friend and the LLM made the same mistake: they evaluated the text without modelling the world it would land in. The experienced coworker evaluated it as a move landing in an environment full of agents with their own models and incentives.This is the core difference. In business, geopolitics, finance etc, the environment fights back. Static analysis fails because the other side has self-interests and knowledge you don&#8217;t have. Pattern matching breaks when patterns shift in response to your actions. You have to simulate:Other agents&#8217; likely reactions (triage heuristics, emotional state).Their hidden incentives and constraints (deadlines, politics).How your action updates their model of you (does &#8220;no rush&#8221; mean &#8220;I&#8217;m nice&#8221; or &#8220;I&#8217;m unimportant&#8221;?).Quant trading makes this measurable: act on a signal, others detect it, the edge decays, someone front-runs you, then someone fakes your signals to take even more money from you. The market is literally other agents modeling you back. That&#8217;s why static pattern-matching breaks: the pattern shifts specifically because you acted on it.Once other agents are in the loop, two things start to matter: (1) they can adapt, and (2) they have private information and private incentives. The hidden state is what turns a problem from &#8216;just compute the best move&#8217; into &#8216;manage beliefs and avoid being exploitable.&#8217; The cleanest way to see this: compare perfect-information games with imperfect-information ones.Perfect Information Games: When You Don&#8217;t Need a Theory of MindChess has two players, perfect information, and symmetric rules. Every piece is visible. Every legal move is known. There&#8217;s no hidden state, no private information, no bluffing.You don&#8217;t need a detailed model of your opponent&#8217;s mind1 as a requirement. Yes it helps, but you need only calculate: given this board, what is the best move assuming optimal play?Your best move does not change based on who your opponent is. Board state is board state. Same goes with Go.AlphaGo or AlphaZero didn&#8217;t need to model human cognition. It needed to see the current state and calculate the optimal path better than any human could. The game&#8217;s homogeneity made this tractable. Both players operate under identical rules, see identical information, and optimize for the same objective. Self-play generates training signals because playing yourself is structurally equivalent to playing anyone.When the Other Side Has Hidden StateNow, consider poker - it has the same structure on the surface. Two players, defined rules, clear objectives. But one fundamental difference: information asymmetry. You don&#8217;t know your opponent&#8217;s cards, they don&#8217;t know yours. Now, the game is no longer about calculating the optimal move from a shared state. It&#8217;s about modeling who they are, what they know, what they think you know, and what they&#8217;re doing with that asymmetry.Bluffing exists because information is private. Reading a bluff exists because you&#8217;re modeling their model of you. The game becomes recursive: I think they think I&#8217;m weak, so they&#8217;ll bet, so I should trap.Editor: we&#8217;re coming back into familiar territory and it may be a good time to revisit our conversation with Noam Brown on AI for imperfect information vs game-theory-optimal poker games, and what that tells us for his multi-agent work:Pluribus: Adversarial RobustnessWhen Meta released Pluribus, Noam Brown made the architecture explicit:&#8220;Regardless of which hand Pluribus is actually holding, it will first calculate how it would act with every possible hand, being careful to balance its strategy across all the hands so as to remain unpredictable to the opponent.&#8221;Pluribus was specifically modeled so that it&#8217;s impossible to read. It calculated how it would act with every possible hand, then balanced its strategy so opponents couldn&#8217;t extract information from its behavior. Human opponents tried to model the causality (&#8221;it&#8217;s betting big, it must have a strong hand&#8221;), but Pluribus played balanced frequencies that made those &#8220;reads&#8221; statistically irrelevant. The point of the strategy was to deny its opponents consistent information.That&#8217;s the benchmark you&#8217;re implicitly holding experts to in real life: not &#8220;does this sound good,&#8221; but &#8220;is this move robust once the other side starts adapting?&#8221;The LLM Failure Mode: They&#8217;re Graded on Artifacts, Not on ReactionsLLMs are optimized to produce a completion that a human rater approves of in isolation. RLHF (and similar human preference tuning) pushes models toward being helpful, polite, balanced, and cooperative, qualities that score well in one-shot evaluations. That&#8217;s great for lots of tasks. But it&#8217;s a bad default in adversarial settings because it systematically under-weights second-order effects: how the counterparty will interpret your message, what it signals about your leverage, and how they&#8217;ll update their strategy after reading it.The core mismatch is the training signal compared to humans. Domain experts get trained by the environment: if your argument is predictable, it gets countered; if your concession leaks weakness, it gets exploited; if your email invites delay, it gets delayed. LLMs mostly learn from descriptions of those dynamics (text) and from static preference judgments about outputs2. Not from repeatedly taking actions in an environment where other agents adapt and punish predictability.3Hence, the model learns to imitate &#8220;what a reasonable person would say,&#8221; not to optimize &#8220;what survives contact with a self-interested opponent?&#8221;The obvious fix: prompt the model to be adversarial. Tell it to optimize for advantage, anticipate counters, hold firm.This helps. But it doesn&#8217;t solve the deeper problem.Being ModeledPluribus cracked what current LLMs don&#8217;t: when you&#8217;re in an adversarial environment, your opponent is watching you and updating and you have to account for that in order to win.A human negotiator notices when the counterparty is probing. They test your reaction to an aggressive anchor. They float a deadline to see if you flinch. They ask casual questions to gauge your alternatives. Each probe updates their model of you, and they adjust accordingly.A skilled negotiator sees the probing and recalibrates. They give misleading signals. They react unexpectedly to throw off the read. The game is recursive: I&#8217;m modeling their model of me, and adjusting to corrupt it.An LLM given an &#8220;aggressive negotiator&#8221; prompt will execute that strategy consistently. Which means a human can probe, identify the pattern, and exploit its predictability. The LLM doesn&#8217;t observe that it&#8217;s being tested. It doesn&#8217;t notice the counterparty is running experiments4. It can&#8217;t recalibrate because it doesn&#8217;t know there&#8217;s anything to recalibrate to.This is the asymmetry. LLMs are readable. The cooperative bias is detectable. The prompting strategy is consistent. And unlike Pluribus, they don&#8217;t adjust based on being observed.Humans can model the LLM. The LLM can&#8217;t model being modeled. That gap is exploitable5, and no amount of &#8220;think strategically&#8221; prompting fixes it because the model doesn&#8217;t know what the adversary has already learned about it.Why &#8220;More Intelligence&#8221; Isn&#8217;t the FixThe natural response is: surely smarter models will figure this out. Just scale everything up? More compute on more data on more parameters in pre-train, better reasoning traces6 in post-train, longer chains of thought at test-time. But, more raw IQ doesn&#8217;t fix the missing training loop even for professionals.To behave adversarially robust by default, the model has to reliably do four things:Detect that the situation is strategic (even when it&#8217;s framed as polite/cooperative)Identify the relevant agents and what each is optimizingSimulate how those agents interpret signals and adapt after your moveChoose an action that remains good across plausible reactions&#8212;not just the most reasonable-sounding completionSteps 2&#8211;4 are possible with good prompting as in the example above. Step 1 is the problem. The model has no default ontology that distinguishes &#8220;cooperative task&#8221; from &#8220;task that looks cooperative but will be evaluated adversarially.&#8221;7And even with recognition, the causal knowledge isn&#8217;t there. The model can be prompted to talk about competitive dynamics. It can produce text that sounds like adversarial reasoning. But the underlying knowledge is not in the training data. It&#8217;s in outcomes that were never written down.The issue isn&#8217;t reasoning power. It&#8217;s the structure of the problem that is hard to define.The Expert&#8217;s EdgeDomain experts say &#8220;AI won&#8217;t replace me&#8221; because they know that &#8220;producing coherent output&#8221; is table stakes. The REAL job is produce output that achieves an objective in an environment where multiple agents are actively modeling and countering you.Why do outsiders think AI can already do these jobs? They judge artifacts but not dynamics:&#8220;This product spec is detailed.&#8221;&#8220;This negotiation email sounds professional.&#8221;&#8220;This mockup is clean.&#8221;Experts evaluate any artifact by survival under pressure:&#8220;Will this specific phrasing trigger the regulator?&#8221;&#8220;Does this polite email accidentally concede leverage?&#8221;&#8220;Will this mockup trigger the engineering veto path?&#8221;&#8220;How will this specific stakeholder interpret the ambiguity?&#8221;These are simulation-based questions. The outsider doesn&#8217;t know to ask them because they don&#8217;t have the mental model that makes them relevant.It&#8217;s like watching Pluribus play poker and evaluating only whether the bets were &#8220;reasonable.&#8221; Of course they look reasonable. The cards it shows at showdown justify the betting pattern. But the reason Pluribus wins isn&#8217;t that its bets look reasonable. its bets are calibrated to be unexploitable across all possible opponent strategies.The visible reasonableness is a side effect of deep adversarial modeling. And if you don&#8217;t know what to look for, you&#8217;ll never know it&#8217;s missing.Language Data Hides the Real SkillThere&#8217;s a deeper reason LLMs are at a permanent handicap here: the thing you&#8217;re trying to learn is not fully contained in the text8. They can catch up by sheer brute force, but are far more inefficient than humans, and the debt is coming due now.When an investor publishes a thesis, consider what is not in it:The position sizing that limits the exposureThe timing that avoided telegraphing intentStrategic concealmentHow the thesis itself is written to not move the market against themWhat they&#8217;d actually do if proved wrong tomorrowText is the residue of action. The real competence is the counterfactual recursive loop: what would I do if they do this? what does my move cause them to do next? what does it reveal about me? That loop is the engine of adversarial expertise, and it&#8217;s weakly revealed by corpora.This is why models can recite game theory but still write the &#8220;nice email&#8221; that leaks leverage. They&#8217;ve learned the language of strategy more than the dynamics of strategy.This is what domain expertise really is. Not a larger knowledge base. Not faster reasoning. It&#8217;s a high-resolution simulation of an ecosystem of agents who are all simultaneously modeling each other. And that simulation lives in heads, not in documents. The text is just the move that got documented. The theory that generated it is called skill.LLMs dominate chess-like domainsNot every domain follows poker dynamics. You have certain fields very close to chess, and LLMs are already poised to be successful in them.Writing code is probably the most clear example:System is deterministicRules are fixed and explicitNo hidden state that mattersCorrectness is objective and verifiableNo agent is actively trying to counter the modelThe same &#8220;closed world&#8221; structure shows up in others: Math / Formal proofs, data transformation, translation, factual research, compliance heavy clerical work (invoice matching, reconciliation), where you can iterate towards the right move without needing a &#8220;theory of the mind&#8221;. The important caveat is that many domains are chess-like in their technical core but become poker-like in their operational context.Professional software engineering extends well beyond the chess-like core. Understanding ambiguous requirements means modeling what the stakeholder actually wants versus what they said. Writing good APIs means anticipating how other developers will misuse them. Code review is social: you&#8217;re modeling reviewers&#8217; preferences and concerns. Architectural decisions account for unknown future requirements and organizational politics. That is, the parts outsiders don&#8217;t see but senior engineers spend much of their time simulating.The parts that look like the job are chess (like). The parts that are the job are poker.Difficulty is orthogonal to &#8220;openness&#8221; of a domain. Proving theorems is hard. Negotiating salary is easy. But theorem-proving is chess-shaped and negotiation is poker-shaped.This is why the disconnect between experts and outsiders is domain-specific. Ask a competitive programmer if AI can solve algorithm problems, and they&#8217;ll say yes because they&#8217;ve watched it happen. Ask a litigator if AI can handle depositions, and they&#8217;ll laugh because they live in a world where every word is a move against an adversary who&#8217;s modeling them back.The labs are starting to see this too. This week Google DeepMind announced they&#8217;re expanding their AI benchmarks beyond chess to poker and Werewolf - games that test &#8220;social deduction and calculated risk.&#8221; Their framing: &#8220;Chess is a game of perfect information. The real world is not.&#8221; The distinction isn&#8217;t novel. But it&#8217;s now officially what frontier AI research is bumping against.The Coming CollisionAs LLMs get deployed as agents in procurement, sales, negotiation, policy, security, and competitive strategy, exploitability becomes practical. A human counterparty doesn&#8217;t need to &#8220;beat the model&#8221; intellectually. They just need to push it into its default failure modes:Aggressive opening positions, knowing the model anchors toward accommodationAmbiguity, knowing the model resolves it charitablyBluffs, knowing the model takes statements at face valueProbing, knowing the model won&#8217;t adapt to being readThis is Pluribus in reverse. In poker, the AI won by being unreadable. In many real deployments, the model is readable: it&#8217;s optimized to be agreeable and helpful, and its tell is that it tries to be fair.If this sounds speculative, consider that every poker pro, every experienced negotiator, every litigator already does this instinctively. They read their counterparty. They probe for patterns. They exploit consistency. The only question is how long before they realize LLM agents are the most consistent, most readable counterparties they&#8217;ve ever faced.Training for the next state predictionThe fix is a different training loop. We need models trained on the question humans actually optimize: what happens after my move? Grade the model on outcomes (did you get the review, did you concede leverage, did you get exploited), not on whether the message sounded reasonable.That requires multi-agent environments where other self-interested agents react, probe, and adapt. Stop treating language generation as single-agent output objective and start treating it as action in a multi-agent game with hidden state, where exploitability is a failure mode.Closing the LoopThe &#8220;AI can replace your job&#8221; debate often confuses artifact quality with strategic competence. Both sides are right about what they&#8217;re looking at. They&#8217;re looking at different things.LLMs can produce outputs that look expert to outsiders because outsiders grade coherence, tone, and plausibility. Experts grade robustness in adversarial multi-agent environments with hidden state.Years of operating in adversarial environments have trained them to automatically model counterparties, anticipate responses, and craft outputs robust to exploitation. They do it without thinking, because in their world, you can&#8217;t survive without it.LLMs produce artifacts that look expert. They don&#8217;t yet produce moves that survive experts.Editor: Ankit Maloo blogs at https://ankitmaloo.com/ and is working on applied RL for real world agents. Give him a follow and check out his work!1Sometimes the player against you matters only in the sense that if they can&#8217;t figure out the board, you will be punished less for risky, ambitious moves.2One of the obvious pushbacks is directly training on outcomes and not artifacts. Hard to do in training setting when you don&#8217;t know a given email is correct until after the negotiation closes.3Editor: this is clearly in RL territory. One might comment on RLVR here being still very early and not sufficiently extended beyond math, code, and artifact rubrics rather than feeding back rewards from renvironments with other actors.4The model can&#8217;t update when the adversary probes, because it doesn&#8217;t observe the probing as it happens. You can prompt it to anticipate one layer of response; you can&#8217;t prompt it to adapt mid-interaction to an opponent who&#8217;s actively modeling it. 5Yes, humans are also exploitable. The problem is LLMs don&#8217;t learn from getting exploited the way humans do over a career. 6Reasoning models too are solipsistic. They are not thinking about you. They aren&#8217;t thinking about the counterparty&#8217;s hidden incentives or emotional deficits. Even if prompted to &#8220;consider the opposition&#8217;s perspective,&#8221; their learning is text-based. They treat social dynamics as a causal chain of words (if &#8220;sorry&#8221; &#8594; then &#8220;forgiven&#8221;), rather than a collision of incentives. But that is to do more with training data than reasoning per se. 7Editor: one difference we debated in the writing process was that I don&#8217;t see a functional difference between collaborative and adversarial situations as far as needing world models/theory of mind is concerned8Editor: at this point I&#8217;m contractually obligated to bring up Good Will Hunting and think about what Robin Williams (human) is telling Matt Damon (LLM)&#8230; except it&#8217;s not quite multiplayer or adversarial, so this is just for the footnote enjoyers like you. ",
      "url": "https://www.latent.space/p/adversarial-reasoning",
      "author": "Ankit Maloo",
      "published": "2026-02-07T22:11:25",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Analysis piece categorizes three types of world models in AI: 3D video world models (Fei-Fei Li's Marble, Google's Genie 3), Meta's JEPA approach for latent representations, and multiagent world models for theory of mind and adversarial reasoning.",
      "importance_score": 52.0,
      "reasoning": "Thoughtful taxonomy of an important AI research frontier. Opinion/analysis rather than news, but highlights critical capability gaps in current LLMs around world modeling.",
      "themes": [
        "world models",
        "AI reasoning",
        "research directions",
        "theory of mind"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis piece categorizes three types of world models in AI: 3D video world models (Fei-Fei Li's Marble, Google's Genie 3), Meta's JEPA approach for latent representations, and multiagent world models for theory of mind and adversarial reasoning.</p>",
      "content_html": "<p>Tickets for AIE Miami and AIE Europe are on sale now! We’ll all be there.Swyx here: we put a call out for Staff Researchers and Guest Writers and Ankit’s submission immediately stood out. As we discussed on the Yi Tay 2 episode, there are 3 kinds of World Models conversations today: The first and most common are 3D video world models like Fei Fei Li’s Marble and General Intuition’s upcoming model, Google’s Genie 3 and Waymo’s World Model, 2) the Meta school of thought comprising JEPA, V-JEPA, EchoJEPA and Code World Models pursuing Platonic representation by learning projections on a common latent space. This essay covers the third kind that is now an obvious reasoning frontier: AI capable of multiagent world models that can accurately track theory of mind, anticipate reactions, and reveal/mine for information, particularly in adversarial situations. In benchmarks, both DeepMind and ARC-AGI and Code Clash are modeling these as games, but solving adversarial reasoning is very much serious business and calls out why the age of scaling is flipping back to the age of research. Enjoy!Ask a trial lawyer if AI could replace her and she won’t even look up from her brief. No. Ask a startup founder who’s never practiced law and he’ll tell you it’s already happening. They’re both looking at the same output. And honestly, the founder has a point. The brief reads like a brief. The contract looks like what a contract would look like. The code runs. If you put it next to the expert’s work, most people would struggle to tell the difference.So what is the expert seeing that everyone else isn’t? Vulnerabilities. They know exactly how an adversary will exploit the document the moment it lands on their desk.People try to explain this disconnect away. Sometimes they blame bad prompting, sometimes they assume models being more intelligent would be able to do the job. I would wager that intelligence is the wrong axis to look at. It’s about simulation depth.Let’s take an illustrative example about approaching people:A simple Slack MessageYou’re three weeks into a new job. You need the lead designer to review your mockups, but she’s notoriously overloaded. You ask ChatGPT to draft a Slack message.The AI writes: “Hi Priya, when you have a moment, could you please take a look at my files and share any feedback? I’d really appreciate your perspective. No rush at all. whenever it fits your schedule. Thanks!”Your friend who works in finance reads: “This is perfect. Polite, not pushy, respects her time. Send it.”Your coworker who’s been there three years reads: “Don’t send that. Priya sees ‘no rush, whenever works’ and mentally files it as not urgent. It sinks below fifteen other messages with actual deadlines. She’s not ignoring you. She’s triaging, and you just told her to deprioritize you.Also, ‘please take a look’ is vague. She doesn’t know if this is 10 minutes or 2 hours. Vague asks feel risky. She’ll avoid it.Try: ‘Hey Priya, could I grab 15 mins before Friday? Blocked on the onboarding mockups. I’m stuck on the nav pattern. Don’t want to build the wrong thing.’ Specific problem, bounded time, clear stakes. That gets a response.”The finance friend evaluated the text in isolation. The coworker ran a simulation: Priya’s workload, her triage heuristics, what ambiguity costs, how “no rush” gets interpreted under pressure. That’s the difference. The email is evaluated by the recipient’s triage algorithm.Adversarial Models in real worldThe finance friend and the LLM made the same mistake: they evaluated the text without modelling the world it would land in. The experienced coworker evaluated it as a move landing in an environment full of agents with their own models and incentives.This is the core difference. In business, geopolitics, finance etc, the environment fights back. Static analysis fails because the other side has self-interests and knowledge you don’t have. Pattern matching breaks when patterns shift in response to your actions. You have to simulate:Other agents’ likely reactions (triage heuristics, emotional state).Their hidden incentives and constraints (deadlines, politics).How your action updates their model of you (does “no rush” mean “I’m nice” or “I’m unimportant”?).Quant trading makes this measurable: act on a signal, others detect it, the edge decays, someone front-runs you, then someone fakes your signals to take even more money from you. The market is literally other agents modeling you back. That’s why static pattern-matching breaks: the pattern shifts specifically because you acted on it.Once other agents are in the loop, two things start to matter: (1) they can adapt, and (2) they have private information and private incentives. The hidden state is what turns a problem from ‘just compute the best move’ into ‘manage beliefs and avoid being exploitable.’ The cleanest way to see this: compare perfect-information games with imperfect-information ones.Perfect Information Games: When You Don’t Need a Theory of MindChess has two players, perfect information, and symmetric rules. Every piece is visible. Every legal move is known. There’s no hidden state, no private information, no bluffing.You don’t need a detailed model of your opponent’s mind1 as a requirement. Yes it helps, but you need only calculate: given this board, what is the best move assuming optimal play?Your best move does not change based on who your opponent is. Board state is board state. Same goes with Go.AlphaGo or AlphaZero didn’t need to model human cognition. It needed to see the current state and calculate the optimal path better than any human could. The game’s homogeneity made this tractable. Both players operate under identical rules, see identical information, and optimize for the same objective. Self-play generates training signals because playing yourself is structurally equivalent to playing anyone.When the Other Side Has Hidden StateNow, consider poker - it has the same structure on the surface. Two players, defined rules, clear objectives. But one fundamental difference: information asymmetry. You don’t know your opponent’s cards, they don’t know yours. Now, the game is no longer about calculating the optimal move from a shared state. It’s about modeling who they are, what they know, what they think you know, and what they’re doing with that asymmetry.Bluffing exists because information is private. Reading a bluff exists because you’re modeling their model of you. The game becomes recursive: I think they think I’m weak, so they’ll bet, so I should trap.Editor: we’re coming back into familiar territory and it may be a good time to revisit our conversation with Noam Brown on AI for imperfect information vs game-theory-optimal poker games, and what that tells us for his multi-agent work:Pluribus: Adversarial RobustnessWhen Meta released Pluribus, Noam Brown made the architecture explicit:“Regardless of which hand Pluribus is actually holding, it will first calculate how it would act with every possible hand, being careful to balance its strategy across all the hands so as to remain unpredictable to the opponent.”Pluribus was specifically modeled so that it’s impossible to read. It calculated how it would act with every possible hand, then balanced its strategy so opponents couldn’t extract information from its behavior. Human opponents tried to model the causality (”it’s betting big, it must have a strong hand”), but Pluribus played balanced frequencies that made those “reads” statistically irrelevant. The point of the strategy was to deny its opponents consistent information.That’s the benchmark you’re implicitly holding experts to in real life: not “does this sound good,” but “is this move robust once the other side starts adapting?”The LLM Failure Mode: They’re Graded on Artifacts, Not on ReactionsLLMs are optimized to produce a completion that a human rater approves of in isolation. RLHF (and similar human preference tuning) pushes models toward being helpful, polite, balanced, and cooperative, qualities that score well in one-shot evaluations. That’s great for lots of tasks. But it’s a bad default in adversarial settings because it systematically under-weights second-order effects: how the counterparty will interpret your message, what it signals about your leverage, and how they’ll update their strategy after reading it.The core mismatch is the training signal compared to humans. Domain experts get trained by the environment: if your argument is predictable, it gets countered; if your concession leaks weakness, it gets exploited; if your email invites delay, it gets delayed. LLMs mostly learn from descriptions of those dynamics (text) and from static preference judgments about outputs2. Not from repeatedly taking actions in an environment where other agents adapt and punish predictability.3Hence, the model learns to imitate “what a reasonable person would say,” not to optimize “what survives contact with a self-interested opponent?”The obvious fix: prompt the model to be adversarial. Tell it to optimize for advantage, anticipate counters, hold firm.This helps. But it doesn’t solve the deeper problem.Being ModeledPluribus cracked what current LLMs don’t: when you’re in an adversarial environment, your opponent is watching you and updating and you have to account for that in order to win.A human negotiator notices when the counterparty is probing. They test your reaction to an aggressive anchor. They float a deadline to see if you flinch. They ask casual questions to gauge your alternatives. Each probe updates their model of you, and they adjust accordingly.A skilled negotiator sees the probing and recalibrates. They give misleading signals. They react unexpectedly to throw off the read. The game is recursive: I’m modeling their model of me, and adjusting to corrupt it.An LLM given an “aggressive negotiator” prompt will execute that strategy consistently. Which means a human can probe, identify the pattern, and exploit its predictability. The LLM doesn’t observe that it’s being tested. It doesn’t notice the counterparty is running experiments4. It can’t recalibrate because it doesn’t know there’s anything to recalibrate to.This is the asymmetry. LLMs are readable. The cooperative bias is detectable. The prompting strategy is consistent. And unlike Pluribus, they don’t adjust based on being observed.Humans can model the LLM. The LLM can’t model being modeled. That gap is exploitable5, and no amount of “think strategically” prompting fixes it because the model doesn’t know what the adversary has already learned about it.Why “More Intelligence” Isn’t the FixThe natural response is: surely smarter models will figure this out. Just scale everything up? More compute on more data on more parameters in pre-train, better reasoning traces6 in post-train, longer chains of thought at test-time. But, more raw IQ doesn’t fix the missing training loop even for professionals.To behave adversarially robust by default, the model has to reliably do four things:Detect that the situation is strategic (even when it’s framed as polite/cooperative)Identify the relevant agents and what each is optimizingSimulate how those agents interpret signals and adapt after your moveChoose an action that remains good across plausible reactions—not just the most reasonable-sounding completionSteps 2–4 are possible with good prompting as in the example above. Step 1 is the problem. The model has no default ontology that distinguishes “cooperative task” from “task that looks cooperative but will be evaluated adversarially.”7And even with recognition, the causal knowledge isn’t there. The model can be prompted to talk about competitive dynamics. It can produce text that sounds like adversarial reasoning. But the underlying knowledge is not in the training data. It’s in outcomes that were never written down.The issue isn’t reasoning power. It’s the structure of the problem that is hard to define.The Expert’s EdgeDomain experts say “AI won’t replace me” because they know that “producing coherent output” is table stakes. The REAL job is produce output that achieves an objective in an environment where multiple agents are actively modeling and countering you.Why do outsiders think AI can already do these jobs? They judge artifacts but not dynamics:“This product spec is detailed.”“This negotiation email sounds professional.”“This mockup is clean.”Experts evaluate any artifact by survival under pressure:“Will this specific phrasing trigger the regulator?”“Does this polite email accidentally concede leverage?”“Will this mockup trigger the engineering veto path?”“How will this specific stakeholder interpret the ambiguity?”These are simulation-based questions. The outsider doesn’t know to ask them because they don’t have the mental model that makes them relevant.It’s like watching Pluribus play poker and evaluating only whether the bets were “reasonable.” Of course they look reasonable. The cards it shows at showdown justify the betting pattern. But the reason Pluribus wins isn’t that its bets look reasonable. its bets are calibrated to be unexploitable across all possible opponent strategies.The visible reasonableness is a side effect of deep adversarial modeling. And if you don’t know what to look for, you’ll never know it’s missing.Language Data Hides the Real SkillThere’s a deeper reason LLMs are at a permanent handicap here: the thing you’re trying to learn is not fully contained in the text8. They can catch up by sheer brute force, but are far more inefficient than humans, and the debt is coming due now.When an investor publishes a thesis, consider what is not in it:The position sizing that limits the exposureThe timing that avoided telegraphing intentStrategic concealmentHow the thesis itself is written to not move the market against themWhat they’d actually do if proved wrong tomorrowText is the residue of action. The real competence is the counterfactual recursive loop: what would I do if they do this? what does my move cause them to do next? what does it reveal about me? That loop is the engine of adversarial expertise, and it’s weakly revealed by corpora.This is why models can recite game theory but still write the “nice email” that leaks leverage. They’ve learned the language of strategy more than the dynamics of strategy.This is what domain expertise really is. Not a larger knowledge base. Not faster reasoning. It’s a high-resolution simulation of an ecosystem of agents who are all simultaneously modeling each other. And that simulation lives in heads, not in documents. The text is just the move that got documented. The theory that generated it is called skill.LLMs dominate chess-like domainsNot every domain follows poker dynamics. You have certain fields very close to chess, and LLMs are already poised to be successful in them.Writing code is probably the most clear example:System is deterministicRules are fixed and explicitNo hidden state that mattersCorrectness is objective and verifiableNo agent is actively trying to counter the modelThe same “closed world” structure shows up in others: Math / Formal proofs, data transformation, translation, factual research, compliance heavy clerical work (invoice matching, reconciliation), where you can iterate towards the right move without needing a “theory of the mind”. The important caveat is that many domains are chess-like in their technical core but become poker-like in their operational context.Professional software engineering extends well beyond the chess-like core. Understanding ambiguous requirements means modeling what the stakeholder actually wants versus what they said. Writing good APIs means anticipating how other developers will misuse them. Code review is social: you’re modeling reviewers’ preferences and concerns. Architectural decisions account for unknown future requirements and organizational politics. That is, the parts outsiders don’t see but senior engineers spend much of their time simulating.The parts that look like the job are chess (like). The parts that are the job are poker.Difficulty is orthogonal to “openness” of a domain. Proving theorems is hard. Negotiating salary is easy. But theorem-proving is chess-shaped and negotiation is poker-shaped.This is why the disconnect between experts and outsiders is domain-specific. Ask a competitive programmer if AI can solve algorithm problems, and they’ll say yes because they’ve watched it happen. Ask a litigator if AI can handle depositions, and they’ll laugh because they live in a world where every word is a move against an adversary who’s modeling them back.The labs are starting to see this too. This week Google DeepMind announced they’re expanding their AI benchmarks beyond chess to poker and Werewolf - games that test “social deduction and calculated risk.” Their framing: “Chess is a game of perfect information. The real world is not.” The distinction isn’t novel. But it’s now officially what frontier AI research is bumping against.The Coming CollisionAs LLMs get deployed as agents in procurement, sales, negotiation, policy, security, and competitive strategy, exploitability becomes practical. A human counterparty doesn’t need to “beat the model” intellectually. They just need to push it into its default failure modes:Aggressive opening positions, knowing the model anchors toward accommodationAmbiguity, knowing the model resolves it charitablyBluffs, knowing the model takes statements at face valueProbing, knowing the model won’t adapt to being readThis is Pluribus in reverse. In poker, the AI won by being unreadable. In many real deployments, the model is readable: it’s optimized to be agreeable and helpful, and its tell is that it tries to be fair.If this sounds speculative, consider that every poker pro, every experienced negotiator, every litigator already does this instinctively. They read their counterparty. They probe for patterns. They exploit consistency. The only question is how long before they realize LLM agents are the most consistent, most readable counterparties they’ve ever faced.Training for the next state predictionThe fix is a different training loop. We need models trained on the question humans actually optimize: what happens after my move? Grade the model on outcomes (did you get the review, did you concede leverage, did you get exploited), not on whether the message sounded reasonable.That requires multi-agent environments where other self-interested agents react, probe, and adapt. Stop treating language generation as single-agent output objective and start treating it as action in a multi-agent game with hidden state, where exploitability is a failure mode.Closing the LoopThe “AI can replace your job” debate often confuses artifact quality with strategic competence. Both sides are right about what they’re looking at. They’re looking at different things.LLMs can produce outputs that look expert to outsiders because outsiders grade coherence, tone, and plausibility. Experts grade robustness in adversarial multi-agent environments with hidden state.Years of operating in adversarial environments have trained them to automatically model counterparties, anticipate responses, and craft outputs robust to exploitation. They do it without thinking, because in their world, you can’t survive without it.LLMs produce artifacts that look expert. They don’t yet produce moves that survive experts.Editor: Ankit Maloo blogs at https://ankitmaloo.com/ and is working on applied RL for real world agents. Give him a follow and check out his work!1Sometimes the player against you matters only in the sense that if they can’t figure out the board, you will be punished less for risky, ambitious moves.2One of the obvious pushbacks is directly training on outcomes and not artifacts. Hard to do in training setting when you don’t know a given email is correct until after the negotiation closes.3Editor: this is clearly in RL territory. One might comment on RLVR here being still very early and not sufficiently extended beyond math, code, and artifact rubrics rather than feeding back rewards from renvironments with other actors.4The model can’t update when the adversary probes, because it doesn’t observe the probing as it happens. You can prompt it to anticipate one layer of response; you can’t prompt it to adapt mid-interaction to an opponent who’s actively modeling it. 5Yes, humans are also exploitable. The problem is LLMs don’t learn from getting exploited the way humans do over a career. 6Reasoning models too are solipsistic. They are not thinking about you. They aren’t thinking about the counterparty’s hidden incentives or emotional deficits. Even if prompted to “consider the opposition’s perspective,” their learning is text-based. They treat social dynamics as a causal chain of words (if “sorry” → then “forgiven”), rather than a collision of incentives. But that is to do more with training data than reasoning per se. 7Editor: one difference we debated in the writing process was that I don’t see a functional difference between collaborative and adversarial situations as far as needing world models/theory of mind is concerned8Editor: at this point I’m contractually obligated to bring up Good Will Hunting and think about what Robin Williams (human) is telling Matt Damon (LLM)… except it’s not quite multiplayer or adversarial, so this is just for the footnote enjoyers like you.</p>"
    },
    {
      "id": "3e9aba926538",
      "title": "The Technologies Changing How You’ll Watch the 2026 Winter Olympic Games",
      "content": "From drones with “first-person” visualization to real-time 360-degree replays and Olympics GPT, get ready to immerse yourself in the Winter Games in Milan and Cortina.",
      "url": "https://www.wired.com/story/the-technologies-changing-how-youll-watch-the-2026-winter-olympic-games/",
      "author": "Mila Fiordalisi",
      "published": "2026-02-07T12:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Gear",
        "2026 Winter Olympics",
        "olympics",
        "Sports",
        "artificial intelligence",
        "TV"
      ],
      "summary": "The 2026 Winter Olympics in Milan and Cortina will feature AI-powered viewing technologies including first-person drone visualization, real-time 360-degree replays, and an 'Olympics GPT' chatbot.",
      "importance_score": 48.0,
      "reasoning": "Demonstrates mainstream AI adoption in entertainment/sports but represents application of existing technologies rather than new capabilities or frontier developments.",
      "themes": [
        "AI applications",
        "sports technology",
        "media",
        "consumer AI"
      ],
      "continuation": null,
      "summary_html": "<p>The 2026 Winter Olympics in Milan and Cortina will feature AI-powered viewing technologies including first-person drone visualization, real-time 360-degree replays, and an 'Olympics GPT' chatbot.</p>",
      "content_html": "<p>From drones with “first-person” visualization to real-time 360-degree replays and Olympics GPT, get ready to immerse yourself in the Winter Games in Milan and Cortina.</p>"
    },
    {
      "id": "deb5f4627237",
      "title": "AI analysis casts doubt on Van Eyck paintings in Italian and US museums",
      "content": "Tests on both versions of Saint Francis of Assisi Receiving the Stigmata were unable to detect brushstrokes of 15th-century masterAn analysis of two paintings in museums in the US and Italy by the 15th-century Flemish artist Jan van Eyck has raised a profound question: what if neither were by Van Eyck?Saint Francis of Assisi Receiving the Stigmata, the name given to near-identical unsigned paintings hanging in the Philadelphia Museum of Art and the Royal Museums of Turin, represent two of the small number of surviving works by one of western art’s greatest masters, revered for his naturalistic portraits and religious subjects. Continue reading...",
      "url": "https://www.theguardian.com/artanddesign/2026/feb/07/ai-analysis-van-eyck-paintings-turin-philadelphia",
      "author": "Dalya Alberge",
      "published": "2026-02-07T08:00:57",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Art",
        "Jan van Eyck",
        "Italy",
        "Philadelphia",
        "Belgium",
        "Art and design",
        "AI (artificial intelligence)",
        "Culture",
        "Europe",
        "US news"
      ],
      "summary": "AI analysis of two paintings attributed to Van Eyck in Philadelphia and Turin museums raises authenticity questions after failing to detect 15th-century brushstrokes characteristic of the master.",
      "importance_score": 46.0,
      "reasoning": "Interesting application of AI in art authentication with potentially significant cultural implications, but represents application domain rather than AI advancement.",
      "themes": [
        "AI applications",
        "art authentication",
        "computer vision",
        "cultural heritage"
      ],
      "continuation": null,
      "summary_html": "<p>AI analysis of two paintings attributed to Van Eyck in Philadelphia and Turin museums raises authenticity questions after failing to detect 15th-century brushstrokes characteristic of the master.</p>",
      "content_html": "<p>Tests on both versions of Saint Francis of Assisi Receiving the Stigmata were unable to detect brushstrokes of 15th-century masterAn analysis of two paintings in museums in the US and Italy by the 15th-century Flemish artist Jan van Eyck has raised a profound question: what if neither were by Van Eyck?Saint Francis of Assisi Receiving the Stigmata, the name given to near-identical unsigned paintings hanging in the Philadelphia Museum of Art and the Royal Museums of Turin, represent two of the small number of surviving works by one of western art’s greatest masters, revered for his naturalistic portraits and religious subjects. Continue reading...</p>"
    },
    {
      "id": "f61d42aaeff3",
      "title": "Rage against the machine: a California community rallied against a datacenter – and won",
      "content": "Organizers in Monterey Park took inspiration from other US cities to fight against the construction of a giant datacenterWhen a southern California city council proposed building a giant datacenter the size of four football fields last December, five residents vowed to stop it.Through a frenetic word-of-mouth campaign, the small group raised awareness about the proposed facility in Monterey Park, a small city east of Los Angeles known affectionately as the country’s first suburban Chinatown. Continue reading...",
      "url": "https://www.theguardian.com/us-news/2026/feb/07/california-monterey-park-stop-datacenter-construction",
      "author": "Claire Wang",
      "published": "2026-02-07T16:00:07",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Technology",
        "US news",
        "AI (artificial intelligence)",
        "California",
        "West Coast",
        "Energy"
      ],
      "summary": "A community in Monterey Park, California successfully rallied to stop construction of a football-field-sized datacenter through a grassroots word-of-mouth campaign.",
      "importance_score": 42.0,
      "reasoning": "Reflects growing tension around AI infrastructure expansion but is primarily a local activism story rather than AI technology news. Tangentially relevant to compute scaling.",
      "themes": [
        "AI infrastructure",
        "community activism",
        "data centers",
        "energy"
      ],
      "continuation": null,
      "summary_html": "<p>A community in Monterey Park, California successfully rallied to stop construction of a football-field-sized datacenter through a grassroots word-of-mouth campaign.</p>",
      "content_html": "<p>Organizers in Monterey Park took inspiration from other US cities to fight against the construction of a giant datacenterWhen a southern California city council proposed building a giant datacenter the size of four football fields last December, five residents vowed to stop it.Through a frenetic word-of-mouth campaign, the small group raised awareness about the proposed facility in Monterey Park, a small city east of Los Angeles known affectionately as the country’s first suburban Chinatown. Continue reading...</p>"
    },
    {
      "id": "bd25207f215f",
      "title": "How to Build a Production-Grade Agentic AI System with Hybrid Retrieval, Provenance-First Citations, Repair Loops, and Episodic Memory",
      "content": "In this tutorial, we build an ultra-advanced agentic AI workflow that behaves like a production-grade research and reasoning system rather than a single prompt call. We ingest real web sources asynchronously, split them into provenance-tracked chunks, and run hybrid retrieval using both TF-IDF (sparse) and OpenAI embeddings (dense), then fuse results for higher recall and stability. We orchestrate multiple agents, planning, synthesis, and repair, while enforcing strict guardrails so every major claim is grounded in retrieved evidence, and we persist episodic memory. Hence, the system improves its strategy over time. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install openai openai-agents pydantic httpx beautifulsoup4 lxml scikit-learn numpy\n\n\nimport os, re, json, time, getpass, asyncio, sqlite3, hashlib\nfrom typing import List, Dict, Tuple, Optional, Any\n\n\nimport numpy as np\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom pydantic import BaseModel, Field\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nfrom openai import AsyncOpenAI\nfrom agents import Agent, Runner, SQLiteSession\n\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\nif not os.environ.get(\"OPENAI_API_KEY\"):\n   raise RuntimeError(\"OPENAI_API_KEY not provided.\")\nprint(\" OpenAI API key loaded securely.\")\noa = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\n\ndef sha1(s: str) -> str:\n   return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\n\ndef normalize_url(u: str) -> str:\n   u = (u or \"\").strip()\n   return u.rstrip(\").,]\\\"'\")\n\n\ndef clean_html_to_text(html: str) -> str:\n   soup = BeautifulSoup(html, \"lxml\")\n   for tag in soup([\"script\", \"style\", \"noscript\"]):\n       tag.decompose()\n   txt = soup.get_text(\"\\n\")\n   txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n   txt = re.sub(r\"[ \\t]+\", \" \", txt)\n   return txt\n\n\ndef chunk_text(text: str, chunk_chars: int = 1600, overlap_chars: int = 320) -> List[str]:\n   if not text:\n       return []\n   text = re.sub(r\"\\s+\", \" \", text).strip()\n   n = len(text)\n   step = max(1, chunk_chars - overlap_chars)\n   chunks = []\n   i = 0\n   while i &lt; n:\n       chunks.append(text[i:i + chunk_chars])\n       i += step\n   return chunks\n\n\ndef canonical_chunk_id(s: str) -> str:\n   if s is None:\n       return \"\"\n   s = str(s).strip()\n   s = s.strip(\"&lt;>\\\"'()[]{}\")\n   s = s.rstrip(\".,;:\")\n   return s\n\n\ndef inject_exec_summary_citations(exec_summary: str, citations: List[str], allowed_chunk_ids: List[str]) -> str:\n   exec_summary = exec_summary or \"\"\n   cset = []\n   for c in citations:\n       c = canonical_chunk_id(c)\n       if c and c in allowed_chunk_ids and c not in cset:\n           cset.append(c)\n       if len(cset) >= 2:\n           break\n   if len(cset) &lt; 2:\n       for c in allowed_chunk_ids:\n           if c not in cset:\n               cset.append(c)\n           if len(cset) >= 2:\n               break\n   if len(cset) >= 2:\n       needed = [c for c in cset if c not in exec_summary]\n       if needed:\n           exec_summary = exec_summary.strip()\n           if exec_summary and not exec_summary.endswith(\".\"):\n               exec_summary += \".\"\n           exec_summary += f\" (cite: {cset[0]}) (cite: {cset[1]})\"\n   return exec_summary\n\n\n\nWe set up the environment, securely load the OpenAI API key, and initialize core utilities that everything else depends on. We define hashing, URL normalization, HTML cleaning, and chunking so all downstream steps operate on clean, consistent text. We also add deterministic helpers to normalize and inject citations, ensuring guardrails are always satisfied. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserasync def fetch_many(urls: List[str], timeout_s: float = 25.0, per_url_char_limit: int = 60000) -> Dict[str, str]:\n   headers = {\"User-Agent\": \"Mozilla/5.0 (AgenticAI/4.2)\"}\n   urls = [normalize_url(u) for u in urls]\n   urls = [u for u in urls if u.startswith(\"http\")]\n   urls = list(dict.fromkeys(urls))\n   out: Dict[str, str] = {}\n   async with httpx.AsyncClient(timeout=timeout_s, follow_redirects=True, headers=headers) as client:\n       async def _one(url: str):\n           try:\n               r = await client.get(url)\n               r.raise_for_status()\n               out[url] = clean_html_to_text(r.text)[:per_url_char_limit]\n           except Exception as e:\n               out[url] = f\"__FETCH_ERROR__ {type(e).__name__}: {e}\"\n       await asyncio.gather(*[_one(u) for u in urls])\n   return out\n\n\ndef dedupe_texts(sources: Dict[str, str]) -> Dict[str, str]:\n   seen = set()\n   out = {}\n   for url, txt in sources.items():\n       if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):\n           continue\n       h = sha1(txt[:25000])\n       if h in seen:\n           continue\n       seen.add(h)\n       out[url] = txt\n   return out\n\n\nclass ChunkRecord(BaseModel):\n   chunk_id: str\n   url: str\n   chunk_index: int\n   text: str\n\n\nclass RetrievalHit(BaseModel):\n   chunk_id: str\n   url: str\n   chunk_index: int\n   score_sparse: float = 0.0\n   score_dense: float = 0.0\n   score_fused: float = 0.0\n   text: str\n\n\nclass EvidencePack(BaseModel):\n   query: str\n   hits: List[RetrievalHit]\n\n\n\nWe asynchronously fetch multiple web sources in parallel and aggressively deduplicate content to avoid redundant evidence. We convert raw pages into structured text and define the core data models that represent chunks and retrieval hits. We ensure every piece of text is traceable back to a specific source and chunk index. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different BrowserEPISODE_DB = \"agentic_episode_memory.db\"\n\n\ndef episode_db_init():\n   con = sqlite3.connect(EPISODE_DB)\n   cur = con.cursor()\n   cur.execute(\"\"\"\n   CREATE TABLE IF NOT EXISTS episodes (\n       id INTEGER PRIMARY KEY AUTOINCREMENT,\n       ts INTEGER NOT NULL,\n       question TEXT NOT NULL,\n       urls_json TEXT NOT NULL,\n       retrieval_queries_json TEXT NOT NULL,\n       useful_sources_json TEXT NOT NULL\n   )\n   \"\"\")\n   con.commit()\n   con.close()\n\n\ndef episode_store(question: str, urls: List[str], retrieval_queries: List[str], useful_sources: List[str]):\n   con = sqlite3.connect(EPISODE_DB)\n   cur = con.cursor()\n   cur.execute(\n       \"INSERT INTO episodes(ts, question, urls_json, retrieval_queries_json, useful_sources_json) VALUES(?,?,?,?,?)\",\n       (int(time.time()), question, json.dumps(urls), json.dumps(retrieval_queries), json.dumps(useful_sources)),\n   )\n   con.commit()\n   con.close()\n\n\ndef episode_recall(question: str, top_k: int = 2) -> List[Dict[str, Any]]:\n   con = sqlite3.connect(EPISODE_DB)\n   cur = con.cursor()\n   cur.execute(\"SELECT ts, question, urls_json, retrieval_queries_json, useful_sources_json FROM episodes ORDER BY ts DESC LIMIT 200\")\n   rows = cur.fetchall()\n   con.close()\n   q_tokens = set(re.findall(r\"[A-Za-z]{3,}\", (question or \"\").lower()))\n   scored = []\n   for ts, q2, u, rq, us in rows:\n       t2 = set(re.findall(r\"[A-Za-z]{3,}\", (q2 or \"\").lower()))\n       if not t2:\n           continue\n       score = len(q_tokens &amp; t2) / max(1, len(q_tokens))\n       if score > 0:\n           scored.append((score, {\n               \"ts\": ts,\n               \"question\": q2,\n               \"urls\": json.loads(u),\n               \"retrieval_queries\": json.loads(rq),\n               \"useful_sources\": json.loads(us),\n           }))\n   scored.sort(key=lambda x: x[0], reverse=True)\n   return [x[1] for x in scored[:top_k]]\n\n\nepisode_db_init()\n\n\n\nWe introduce episodic memory backed by SQLite so the system can recall what worked in previous runs. We store questions, retrieval strategies, and useful sources to guide future planning. We also implement lightweight similarity-based recall to bias the system toward historically effective patterns. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass HybridIndex:\n   def __init__(self):\n       self.records: List[ChunkRecord] = []\n       self.tfidf: Optional[TfidfVectorizer] = None\n       self.tfidf_mat = None\n       self.emb_mat: Optional[np.ndarray] = None\n\n\n   def build_sparse(self):\n       corpus = [r.text for r in self.records] if self.records else [\"\"]\n       self.tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=80000)\n       self.tfidf_mat = self.tfidf.fit_transform(corpus)\n\n\n   def search_sparse(self, query: str, k: int) -> List[Tuple[int, float]]:\n       if not self.records or self.tfidf is None or self.tfidf_mat is None:\n           return []\n       qv = self.tfidf.transform([query])\n       sims = cosine_similarity(qv, self.tfidf_mat).flatten()\n       top = np.argsort(-sims)[:k]\n       return [(int(i), float(sims[i])) for i in top]\n\n\n   def set_dense(self, mat: np.ndarray):\n       self.emb_mat = mat.astype(np.float32)\n\n\n   def search_dense(self, q_emb: np.ndarray, k: int) -> List[Tuple[int, float]]:\n       if self.emb_mat is None or not self.records:\n           return []\n       M = self.emb_mat\n       q = q_emb.astype(np.float32).reshape(1, -1)\n       M_norm = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-9)\n       q_norm = q / (np.linalg.norm(q) + 1e-9)\n       sims = (M_norm @ q_norm.T).flatten()\n       top = np.argsort(-sims)[:k]\n       return [(int(i), float(sims[i])) for i in top]\n\n\ndef rrf_fuse(rankings: List[List[int]], k: int = 60) -> Dict[int, float]:\n   scores: Dict[int, float] = {}\n   for r in rankings:\n       for pos, idx in enumerate(r, start=1):\n           scores[idx] = scores.get(idx, 0.0) + 1.0 / (k + pos)\n   return scores\n\n\nHYBRID = HybridIndex()\nALLOWED_URLS: List[str] = []\n\n\nEMBED_MODEL = \"text-embedding-3-small\"\n\n\nasync def embed_batch(texts: List[str]) -> np.ndarray:\n   resp = await oa.embeddings.create(model=EMBED_MODEL, input=texts, encoding_format=\"float\")\n   vecs = [np.array(item.embedding, dtype=np.float32) for item in resp.data]\n   return np.vstack(vecs) if vecs else np.zeros((0, 0), dtype=np.float32)\n\n\nasync def embed_texts(texts: List[str], batch_size: int = 96, max_concurrency: int = 3) -> np.ndarray:\n   sem = asyncio.Semaphore(max_concurrency)\n   mats: List[Tuple[int, np.ndarray]] = []\n\n\n   async def _one(start: int, batch: List[str]):\n       async with sem:\n           m = await embed_batch(batch)\n           mats.append((start, m))\n\n\n   tasks = []\n   for start in range(0, len(texts), batch_size):\n       batch = [t[:7000] for t in texts[start:start + batch_size]]\n       tasks.append(_one(start, batch))\n   await asyncio.gather(*tasks)\n\n\n   mats.sort(key=lambda x: x[0])\n   emb = np.vstack([m for _, m in mats]) if mats else np.zeros((len(texts), 0), dtype=np.float32)\n   if emb.shape[0] != len(texts):\n       raise RuntimeError(f\"Embedding rows mismatch: got {emb.shape[0]} expected {len(texts)}\")\n   return emb\n\n\nasync def embed_query(query: str) -> np.ndarray:\n   m = await embed_batch([query[:7000]])\n   return m[0] if m.shape[0] else np.zeros((0,), dtype=np.float32)\n\n\nasync def build_index(urls: List[str], max_chunks_per_url: int = 60):\n   global ALLOWED_URLS\n   fetched = await fetch_many(urls)\n   fetched = dedupe_texts(fetched)\n\n\n   records: List[ChunkRecord] = []\n   allowed: List[str] = []\n\n\n   for url, txt in fetched.items():\n       if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):\n           continue\n       allowed.append(url)\n       chunks = chunk_text(txt)[:max_chunks_per_url]\n       for i, ch in enumerate(chunks):\n           cid = f\"{sha1(url)}:{i}\"\n           records.append(ChunkRecord(chunk_id=cid, url=url, chunk_index=i, text=ch))\n\n\n   if not records:\n       err_view = {normalize_url(u): fetched.get(normalize_url(u), \"\") for u in urls}\n       raise RuntimeError(\"No sources fetched successfully.\\n\" + json.dumps(err_view, indent=2)[:4000])\n\n\n   ALLOWED_URLS = allowed\n   HYBRID.records = records\n   HYBRID.build_sparse()\n\n\n   texts = [r.text for r in HYBRID.records]\n   emb = await embed_texts(texts, batch_size=96, max_concurrency=3)\n   HYBRID.set_dense(emb)\n\n\n\nWe build a hybrid retrieval index that combines sparse TF-IDF search with dense OpenAI embeddings. We enable reciprocal rank fusion, so that sparse and dense signals complement each other rather than compete. We construct the index once per run and reuse it across all retrieval queries for efficiency. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef build_evidence_pack(query: str, sparse: List[Tuple[int,float]], dense: List[Tuple[int,float]], k: int = 10) -> EvidencePack:\n   sparse_rank = [i for i,_ in sparse]\n   dense_rank  = [i for i,_ in dense]\n   sparse_scores = {i:s for i,s in sparse}\n   dense_scores  = {i:s for i,s in dense}\n   fused = rrf_fuse([sparse_rank, dense_rank], k=60) if dense_rank else rrf_fuse([sparse_rank], k=60)\n   top = sorted(fused.keys(), key=lambda i: fused[i], reverse=True)[:k]\n\n\n   hits: List[RetrievalHit] = []\n   for idx in top:\n       r = HYBRID.records[idx]\n       hits.append(RetrievalHit(\n           chunk_id=r.chunk_id, url=r.url, chunk_index=r.chunk_index,\n           score_sparse=float(sparse_scores.get(idx, 0.0)),\n           score_dense=float(dense_scores.get(idx, 0.0)),\n           score_fused=float(fused.get(idx, 0.0)),\n           text=r.text\n       ))\n   return EvidencePack(query=query, hits=hits)\n\n\nasync def gather_evidence(queries: List[str], per_query_k: int = 10, sparse_k: int = 60, dense_k: int = 60):\n   evidence: List[EvidencePack] = []\n   useful_sources_count: Dict[str, int] = {}\n   all_chunk_ids: List[str] = []\n\n\n   for q in queries:\n       sparse = HYBRID.search_sparse(q, k=sparse_k)\n       q_emb = await embed_query(q)\n       dense = HYBRID.search_dense(q_emb, k=dense_k)\n       pack = build_evidence_pack(q, sparse, dense, k=per_query_k)\n       evidence.append(pack)\n       for h in pack.hits[:6]:\n           useful_sources_count[h.url] = useful_sources_count.get(h.url, 0) + 1\n       for h in pack.hits:\n           all_chunk_ids.append(h.chunk_id)\n\n\n   useful_sources = sorted(useful_sources_count.keys(), key=lambda u: useful_sources_count[u], reverse=True)\n   all_chunk_ids = sorted(list(dict.fromkeys(all_chunk_ids)))\n   return evidence, useful_sources[:8], all_chunk_ids\n\n\nclass Plan(BaseModel):\n   objective: str\n   subtasks: List[str]\n   retrieval_queries: List[str]\n   acceptance_checks: List[str]\n\n\nclass UltraAnswer(BaseModel):\n   title: str\n   executive_summary: str\n   architecture: List[str]\n   retrieval_strategy: List[str]\n   agent_graph: List[str]\n   implementation_notes: List[str]\n   risks_and_limits: List[str]\n   citations: List[str]\n   sources: List[str]\n\n\ndef normalize_answer(ans: UltraAnswer, allowed_chunk_ids: List[str]) -> UltraAnswer:\n   data = ans.model_dump()\n   data[\"citations\"] = [canonical_chunk_id(x) for x in (data.get(\"citations\") or [])]\n   data[\"citations\"] = [x for x in data[\"citations\"] if x in allowed_chunk_ids]\n   data[\"executive_summary\"] = inject_exec_summary_citations(data.get(\"executive_summary\",\"\"), data[\"citations\"], allowed_chunk_ids)\n   return UltraAnswer(**data)\n\n\ndef validate_ultra(ans: UltraAnswer, allowed_chunk_ids: List[str]) -> None:\n   extras = [u for u in ans.sources if u not in ALLOWED_URLS]\n   if extras:\n       raise ValueError(f\"Non-allowed sources in output: {extras}\")\n\n\n   cset = set(ans.citations or [])\n   missing = [cid for cid in cset if cid not in set(allowed_chunk_ids)]\n   if missing:\n       raise ValueError(f\"Citations reference unknown chunk_ids (not retrieved): {missing}\")\n\n\n   if len(cset) &lt; 6:\n       raise ValueError(\"Need at least 6 distinct chunk_id citations in ultra mode.\")\n\n\n   es_text = ans.executive_summary or \"\"\n   es_count = sum(1 for cid in cset if cid in es_text)\n   if es_count &lt; 2:\n       raise ValueError(\"Executive summary must include at least 2 chunk_id citations verbatim.\")\n\n\nPLANNER = Agent(\n   name=\"Planner\",\n   model=\"gpt-4o-mini\",\n   instructions=(\n       \"Return a technical Plan schema.\\n\"\n       \"Make 10-16 retrieval_queries.\\n\"\n       \"Acceptance must include: at least 6 citations and exec_summary contains at least 2 citations verbatim.\"\n   ),\n   output_type=Plan,\n)\n\n\nSYNTHESIZER = Agent(\n   name=\"Synthesizer\",\n   model=\"gpt-4o-mini\",\n   instructions=(\n       \"Return UltraAnswer schema.\\n\"\n       \"Hard constraints:\\n\"\n       \"- executive_summary MUST include at least TWO citations verbatim as: (cite: &lt;chunk_id>).\\n\"\n       \"- citations must be chosen ONLY from ALLOWED_CHUNK_IDS list.\\n\"\n       \"- citations list must include at least 6 unique chunk_ids.\\n\"\n       \"- sources must be subset of allowed URLs.\\n\"\n   ),\n   output_type=UltraAnswer,\n)\n\n\nFIXER = Agent(\n   name=\"Fixer\",\n   model=\"gpt-4o-mini\",\n   instructions=(\n       \"Repair to satisfy guardrails.\\n\"\n       \"Ensure executive_summary includes at least TWO citations verbatim.\\n\"\n       \"Choose citations ONLY from ALLOWED_CHUNK_IDS list.\\n\"\n       \"Return UltraAnswer schema.\"\n   ),\n   output_type=UltraAnswer,\n)\n\n\nsession = SQLiteSession(\"ultra_agentic_user\", \"ultra_agentic_session.db\")\n\n\n\nWe gather evidence by running multiple targeted queries, fusing sparse and dense results, and assembling evidence packs with scores and provenance. We define strict schemas for plans and final answers, then normalize and validate citations against retrieved chunk IDs. We enforce hard guardrails so every answer remains grounded and auditable. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserasync def run_ultra_agentic(question: str, urls: List[str], max_repairs: int = 2) -> UltraAnswer:\n   await build_index(urls)\n   recall_hint = json.dumps(episode_recall(question, top_k=2), indent=2)[:2000]\n\n\n   plan_res = await Runner.run(\n       PLANNER,\n       f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\nRecall:\\n{recall_hint}\\n\",\n       session=session\n   )\n   plan: Plan = plan_res.final_output\n   queries = (plan.retrieval_queries or [])[:16]\n\n\n   evidence_packs, useful_sources, allowed_chunk_ids = await gather_evidence(queries)\n\n\n   evidence_json = json.dumps([p.model_dump() for p in evidence_packs], indent=2)[:16000]\n   allowed_chunk_ids_json = json.dumps(allowed_chunk_ids[:200], indent=2)\n\n\n   draft_res = await Runner.run(\n       SYNTHESIZER,\n       f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"\n       f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"\n       f\"Evidence packs:\\n{evidence_json}\\n\\n\"\n       \"Return UltraAnswer.\",\n       session=session\n   )\n   draft = normalize_answer(draft_res.final_output, allowed_chunk_ids)\n\n\n   last_err = None\n   for i in range(max_repairs + 1):\n       try:\n           validate_ultra(draft, allowed_chunk_ids)\n           episode_store(question, ALLOWED_URLS, plan.retrieval_queries, useful_sources)\n           return draft\n       except Exception as e:\n           last_err = str(e)\n           if i >= max_repairs:\n               draft = normalize_answer(draft, allowed_chunk_ids)\n               validate_ultra(draft, allowed_chunk_ids)\n               return draft\n\n\n           fixer_res = await Runner.run(\n               FIXER,\n               f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"\n               f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"\n               f\"Guardrail error:\\n{last_err}\\n\\n\"\n               f\"Draft:\\n{json.dumps(draft.model_dump(), indent=2)[:12000]}\\n\\n\"\n               f\"Evidence packs:\\n{evidence_json}\\n\\n\"\n               \"Return corrected UltraAnswer that passes guardrails.\",\n               session=session\n           )\n           draft = normalize_answer(fixer_res.final_output, allowed_chunk_ids)\n\n\n   raise RuntimeError(f\"Unexpected failure: {last_err}\")\n\n\nquestion = (\n   \"Design a production-lean but advanced agentic AI workflow in Python with hybrid retrieval, \"\n   \"provenance-first citations, critique-and-repair loops, and episodic memory. \"\n   \"Explain why each layer matters, failure modes, and evaluation.\"\n)\n\n\nurls = [\n   \"https://openai.github.io/openai-agents-python/\",\n   \"https://openai.github.io/openai-agents-python/agents/\",\n   \"https://openai.github.io/openai-agents-python/running_agents/\",\n   \"https://github.com/openai/openai-agents-python\",\n]\n\n\nans = await run_ultra_agentic(question, urls, max_repairs=2)\n\n\nprint(\"\\nTITLE:\\n\", ans.title)\nprint(\"\\nEXECUTIVE SUMMARY:\\n\", ans.executive_summary)\nprint(\"\\nARCHITECTURE:\")\nfor x in ans.architecture:\n   print(\"-\", x)\nprint(\"\\nRETRIEVAL STRATEGY:\")\nfor x in ans.retrieval_strategy:\n   print(\"-\", x)\nprint(\"\\nAGENT GRAPH:\")\nfor x in ans.agent_graph:\n   print(\"-\", x)\nprint(\"\\nIMPLEMENTATION NOTES:\")\nfor x in ans.implementation_notes:\n   print(\"-\", x)\nprint(\"\\nRISKS &amp; LIMITS:\")\nfor x in ans.risks_and_limits:\n   print(\"-\", x)\nprint(\"\\nCITATIONS (chunk_ids):\")\nfor c in ans.citations:\n   print(\"-\", c)\nprint(\"\\nSOURCES:\")\nfor s in ans.sources:\n   print(\"-\", s)\n\n\n\nWe orchestrate the full agentic loop by chaining planning, synthesis, validation, and repair in an async-safe pipeline. We automatically retry and fix outputs until they pass all constraints without human intervention. We finish by running a full example and printing a fully grounded, production-ready agentic response.\n\n\n\nIn conclusion, we developed a comprehensive agentic pipeline robust to common failure modes: unstable embedding shapes, citation drift, and missing grounding in executive summaries. We validated outputs against allowlisted sources, retrieved chunk IDs, automatically normalized citations, and injected deterministic citations when needed to guarantee compliance without sacrificing correctness. By combining hybrid retrieval, critique-and-repair loops, and episodic memory, we created a reusable foundation we can extend with stronger evaluations (claim-to-evidence coverage scoring, adversarial red-teaming, and regression tests) to continuously harden the system as it scales to new domains and larger corpora.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How to Build a Production-Grade Agentic AI System with Hybrid Retrieval, Provenance-First Citations, Repair Loops, and Episodic Memory appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/06/how-to-build-a-production-grade-agentic-ai-system-with-hybrid-retrieval-provenance-first-citations-repair-loops-and-episodic-memory/",
      "author": "Asif Razzaq",
      "published": "2026-02-07T05:59:59",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "Editors Pick",
        "Staff",
        "Tutorials"
      ],
      "summary": "Technical tutorial covering construction of production-grade agentic AI systems with hybrid retrieval (TF-IDF + embeddings), provenance tracking, repair loops, and episodic memory.",
      "importance_score": 40.0,
      "reasoning": "Educational content demonstrating best practices rather than news about new developments. Useful for practitioners but not newsworthy for frontier AI coverage.",
      "themes": [
        "agentic AI",
        "RAG",
        "tutorials",
        "engineering practices"
      ],
      "continuation": null,
      "summary_html": "<p>Technical tutorial covering construction of production-grade agentic AI systems with hybrid retrieval (TF-IDF + embeddings), provenance tracking, repair loops, and episodic memory.</p>",
      "content_html": "<p>In this tutorial, we build an ultra-advanced agentic AI workflow that behaves like a production-grade research and reasoning system rather than a single prompt call. We ingest real web sources asynchronously, split them into provenance-tracked chunks, and run hybrid retrieval using both TF-IDF (sparse) and OpenAI embeddings (dense), then fuse results for higher recall and stability. We orchestrate multiple agents, planning, synthesis, and repair, while enforcing strict guardrails so every major claim is grounded in retrieved evidence, and we persist episodic memory. Hence, the system improves its strategy over time. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install openai openai-agents pydantic httpx beautifulsoup4 lxml scikit-learn numpy</p>\n<p>import os, re, json, time, getpass, asyncio, sqlite3, hashlib</p>\n<p>from typing import List, Dict, Tuple, Optional, Any</p>\n<p>import numpy as np</p>\n<p>import httpx</p>\n<p>from bs4 import BeautifulSoup</p>\n<p>from pydantic import BaseModel, Field</p>\n<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>\n<p>from sklearn.metrics.pairwise import cosine_similarity</p>\n<p>from openai import AsyncOpenAI</p>\n<p>from agents import Agent, Runner, SQLiteSession</p>\n<p>if not os.environ.get(\"OPENAI_API_KEY\"):</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")</p>\n<p>if not os.environ.get(\"OPENAI_API_KEY\"):</p>\n<p>raise RuntimeError(\"OPENAI_API_KEY not provided.\")</p>\n<p>print(\" OpenAI API key loaded securely.\")</p>\n<p>oa = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])</p>\n<p>def sha1(s: str) -&gt; str:</p>\n<p>return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()</p>\n<p>def normalize_url(u: str) -&gt; str:</p>\n<p>u = (u or \"\").strip()</p>\n<p>return u.rstrip(\").,]\\\"'\")</p>\n<p>def clean_html_to_text(html: str) -&gt; str:</p>\n<p>soup = BeautifulSoup(html, \"lxml\")</p>\n<p>for tag in soup([\"script\", \"style\", \"noscript\"]):</p>\n<p>tag.decompose()</p>\n<p>txt = soup.get_text(\"\\n\")</p>\n<p>txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()</p>\n<p>txt = re.sub(r\"[ \\t]+\", \" \", txt)</p>\n<p>return txt</p>\n<p>def chunk_text(text: str, chunk_chars: int = 1600, overlap_chars: int = 320) -&gt; List[str]:</p>\n<p>if not text:</p>\n<p>return []</p>\n<p>text = re.sub(r\"\\s+\", \" \", text).strip()</p>\n<p>n = len(text)</p>\n<p>step = max(1, chunk_chars - overlap_chars)</p>\n<p>chunks = []</p>\n<p>i = 0</p>\n<p>while i &lt; n:</p>\n<p>chunks.append(text[i:i + chunk_chars])</p>\n<p>i += step</p>\n<p>return chunks</p>\n<p>def canonical_chunk_id(s: str) -&gt; str:</p>\n<p>if s is None:</p>\n<p>return \"\"</p>\n<p>s = str(s).strip()</p>\n<p>s = s.strip(\"&lt;&gt;\\\"'()[]{}\")</p>\n<p>s = s.rstrip(\".,;:\")</p>\n<p>return s</p>\n<p>def inject_exec_summary_citations(exec_summary: str, citations: List[str], allowed_chunk_ids: List[str]) -&gt; str:</p>\n<p>exec_summary = exec_summary or \"\"</p>\n<p>cset = []</p>\n<p>for c in citations:</p>\n<p>c = canonical_chunk_id(c)</p>\n<p>if c and c in allowed_chunk_ids and c not in cset:</p>\n<p>cset.append(c)</p>\n<p>if len(cset) &gt;= 2:</p>\n<p>break</p>\n<p>if len(cset) &lt; 2:</p>\n<p>for c in allowed_chunk_ids:</p>\n<p>if c not in cset:</p>\n<p>cset.append(c)</p>\n<p>if len(cset) &gt;= 2:</p>\n<p>break</p>\n<p>if len(cset) &gt;= 2:</p>\n<p>needed = [c for c in cset if c not in exec_summary]</p>\n<p>if needed:</p>\n<p>exec_summary = exec_summary.strip()</p>\n<p>if exec_summary and not exec_summary.endswith(\".\"):</p>\n<p>exec_summary += \".\"</p>\n<p>exec_summary += f\" (cite: {cset[0]}) (cite: {cset[1]})\"</p>\n<p>return exec_summary</p>\n<p>We set up the environment, securely load the OpenAI API key, and initialize core utilities that everything else depends on. We define hashing, URL normalization, HTML cleaning, and chunking so all downstream steps operate on clean, consistent text. We also add deterministic helpers to normalize and inject citations, ensuring guardrails are always satisfied. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserasync def fetch_many(urls: List[str], timeout_s: float = 25.0, per_url_char_limit: int = 60000) -&gt; Dict[str, str]:</p>\n<p>headers = {\"User-Agent\": \"Mozilla/5.0 (AgenticAI/4.2)\"}</p>\n<p>urls = [normalize_url(u) for u in urls]</p>\n<p>urls = [u for u in urls if u.startswith(\"http\")]</p>\n<p>urls = list(dict.fromkeys(urls))</p>\n<p>out: Dict[str, str] = {}</p>\n<p>async with httpx.AsyncClient(timeout=timeout_s, follow_redirects=True, headers=headers) as client:</p>\n<p>async def _one(url: str):</p>\n<p>try:</p>\n<p>r = await client.get(url)</p>\n<p>r.raise_for_status()</p>\n<p>out[url] = clean_html_to_text(r.text)[:per_url_char_limit]</p>\n<p>except Exception as e:</p>\n<p>out[url] = f\"__FETCH_ERROR__ {type(e).__name__}: {e}\"</p>\n<p>await asyncio.gather(*[_one(u) for u in urls])</p>\n<p>return out</p>\n<p>def dedupe_texts(sources: Dict[str, str]) -&gt; Dict[str, str]:</p>\n<p>seen = set()</p>\n<p>out = {}</p>\n<p>for url, txt in sources.items():</p>\n<p>if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):</p>\n<p>continue</p>\n<p>h = sha1(txt[:25000])</p>\n<p>if h in seen:</p>\n<p>continue</p>\n<p>seen.add(h)</p>\n<p>out[url] = txt</p>\n<p>return out</p>\n<p>class ChunkRecord(BaseModel):</p>\n<p>chunk_id: str</p>\n<p>url: str</p>\n<p>chunk_index: int</p>\n<p>text: str</p>\n<p>class RetrievalHit(BaseModel):</p>\n<p>chunk_id: str</p>\n<p>url: str</p>\n<p>chunk_index: int</p>\n<p>score_sparse: float = 0.0</p>\n<p>score_dense: float = 0.0</p>\n<p>score_fused: float = 0.0</p>\n<p>text: str</p>\n<p>class EvidencePack(BaseModel):</p>\n<p>query: str</p>\n<p>hits: List[RetrievalHit]</p>\n<p>We asynchronously fetch multiple web sources in parallel and aggressively deduplicate content to avoid redundant evidence. We convert raw pages into structured text and define the core data models that represent chunks and retrieval hits. We ensure every piece of text is traceable back to a specific source and chunk index. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different BrowserEPISODE_DB = \"agentic_episode_memory.db\"</p>\n<p>def episode_db_init():</p>\n<p>con = sqlite3.connect(EPISODE_DB)</p>\n<p>cur = con.cursor()</p>\n<p>cur.execute(\"\"\"</p>\n<p>CREATE TABLE IF NOT EXISTS episodes (</p>\n<p>id INTEGER PRIMARY KEY AUTOINCREMENT,</p>\n<p>ts INTEGER NOT NULL,</p>\n<p>question TEXT NOT NULL,</p>\n<p>urls_json TEXT NOT NULL,</p>\n<p>retrieval_queries_json TEXT NOT NULL,</p>\n<p>useful_sources_json TEXT NOT NULL</p>\n<p>)</p>\n<p>\"\"\")</p>\n<p>con.commit()</p>\n<p>con.close()</p>\n<p>def episode_store(question: str, urls: List[str], retrieval_queries: List[str], useful_sources: List[str]):</p>\n<p>con = sqlite3.connect(EPISODE_DB)</p>\n<p>cur = con.cursor()</p>\n<p>cur.execute(</p>\n<p>\"INSERT INTO episodes(ts, question, urls_json, retrieval_queries_json, useful_sources_json) VALUES(?,?,?,?,?)\",</p>\n<p>(int(time.time()), question, json.dumps(urls), json.dumps(retrieval_queries), json.dumps(useful_sources)),</p>\n<p>)</p>\n<p>con.commit()</p>\n<p>con.close()</p>\n<p>def episode_recall(question: str, top_k: int = 2) -&gt; List[Dict[str, Any]]:</p>\n<p>con = sqlite3.connect(EPISODE_DB)</p>\n<p>cur = con.cursor()</p>\n<p>cur.execute(\"SELECT ts, question, urls_json, retrieval_queries_json, useful_sources_json FROM episodes ORDER BY ts DESC LIMIT 200\")</p>\n<p>rows = cur.fetchall()</p>\n<p>con.close()</p>\n<p>q_tokens = set(re.findall(r\"[A-Za-z]{3,}\", (question or \"\").lower()))</p>\n<p>scored = []</p>\n<p>for ts, q2, u, rq, us in rows:</p>\n<p>t2 = set(re.findall(r\"[A-Za-z]{3,}\", (q2 or \"\").lower()))</p>\n<p>if not t2:</p>\n<p>continue</p>\n<p>score = len(q_tokens &amp; t2) / max(1, len(q_tokens))</p>\n<p>if score &gt; 0:</p>\n<p>scored.append((score, {</p>\n<p>\"ts\": ts,</p>\n<p>\"question\": q2,</p>\n<p>\"urls\": json.loads(u),</p>\n<p>\"retrieval_queries\": json.loads(rq),</p>\n<p>\"useful_sources\": json.loads(us),</p>\n<p>}))</p>\n<p>scored.sort(key=lambda x: x[0], reverse=True)</p>\n<p>return [x[1] for x in scored[:top_k]]</p>\n<p>episode_db_init()</p>\n<p>We introduce episodic memory backed by SQLite so the system can recall what worked in previous runs. We store questions, retrieval strategies, and useful sources to guide future planning. We also implement lightweight similarity-based recall to bias the system toward historically effective patterns. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass HybridIndex:</p>\n<p>def __init__(self):</p>\n<p>self.records: List[ChunkRecord] = []</p>\n<p>self.tfidf: Optional[TfidfVectorizer] = None</p>\n<p>self.tfidf_mat = None</p>\n<p>self.emb_mat: Optional[np.ndarray] = None</p>\n<p>def build_sparse(self):</p>\n<p>corpus = [r.text for r in self.records] if self.records else [\"\"]</p>\n<p>self.tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=80000)</p>\n<p>self.tfidf_mat = self.tfidf.fit_transform(corpus)</p>\n<p>def search_sparse(self, query: str, k: int) -&gt; List[Tuple[int, float]]:</p>\n<p>if not self.records or self.tfidf is None or self.tfidf_mat is None:</p>\n<p>return []</p>\n<p>qv = self.tfidf.transform([query])</p>\n<p>sims = cosine_similarity(qv, self.tfidf_mat).flatten()</p>\n<p>top = np.argsort(-sims)[:k]</p>\n<p>return [(int(i), float(sims[i])) for i in top]</p>\n<p>def set_dense(self, mat: np.ndarray):</p>\n<p>self.emb_mat = mat.astype(np.float32)</p>\n<p>def search_dense(self, q_emb: np.ndarray, k: int) -&gt; List[Tuple[int, float]]:</p>\n<p>if self.emb_mat is None or not self.records:</p>\n<p>return []</p>\n<p>M = self.emb_mat</p>\n<p>q = q_emb.astype(np.float32).reshape(1, -1)</p>\n<p>M_norm = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-9)</p>\n<p>q_norm = q / (np.linalg.norm(q) + 1e-9)</p>\n<p>sims = (M_norm @ q_norm.T).flatten()</p>\n<p>top = np.argsort(-sims)[:k]</p>\n<p>return [(int(i), float(sims[i])) for i in top]</p>\n<p>def rrf_fuse(rankings: List[List[int]], k: int = 60) -&gt; Dict[int, float]:</p>\n<p>scores: Dict[int, float] = {}</p>\n<p>for r in rankings:</p>\n<p>for pos, idx in enumerate(r, start=1):</p>\n<p>scores[idx] = scores.get(idx, 0.0) + 1.0 / (k + pos)</p>\n<p>return scores</p>\n<p>HYBRID = HybridIndex()</p>\n<p>ALLOWED_URLS: List[str] = []</p>\n<p>EMBED_MODEL = \"text-embedding-3-small\"</p>\n<p>async def embed_batch(texts: List[str]) -&gt; np.ndarray:</p>\n<p>resp = await oa.embeddings.create(model=EMBED_MODEL, input=texts, encoding_format=\"float\")</p>\n<p>vecs = [np.array(item.embedding, dtype=np.float32) for item in resp.data]</p>\n<p>return np.vstack(vecs) if vecs else np.zeros((0, 0), dtype=np.float32)</p>\n<p>async def embed_texts(texts: List[str], batch_size: int = 96, max_concurrency: int = 3) -&gt; np.ndarray:</p>\n<p>sem = asyncio.Semaphore(max_concurrency)</p>\n<p>mats: List[Tuple[int, np.ndarray]] = []</p>\n<p>async def _one(start: int, batch: List[str]):</p>\n<p>async with sem:</p>\n<p>m = await embed_batch(batch)</p>\n<p>mats.append((start, m))</p>\n<p>tasks = []</p>\n<p>for start in range(0, len(texts), batch_size):</p>\n<p>batch = [t[:7000] for t in texts[start:start + batch_size]]</p>\n<p>tasks.append(_one(start, batch))</p>\n<p>await asyncio.gather(*tasks)</p>\n<p>mats.sort(key=lambda x: x[0])</p>\n<p>emb = np.vstack([m for _, m in mats]) if mats else np.zeros((len(texts), 0), dtype=np.float32)</p>\n<p>if emb.shape[0] != len(texts):</p>\n<p>raise RuntimeError(f\"Embedding rows mismatch: got {emb.shape[0]} expected {len(texts)}\")</p>\n<p>return emb</p>\n<p>async def embed_query(query: str) -&gt; np.ndarray:</p>\n<p>m = await embed_batch([query[:7000]])</p>\n<p>return m[0] if m.shape[0] else np.zeros((0,), dtype=np.float32)</p>\n<p>async def build_index(urls: List[str], max_chunks_per_url: int = 60):</p>\n<p>global ALLOWED_URLS</p>\n<p>fetched = await fetch_many(urls)</p>\n<p>fetched = dedupe_texts(fetched)</p>\n<p>records: List[ChunkRecord] = []</p>\n<p>allowed: List[str] = []</p>\n<p>for url, txt in fetched.items():</p>\n<p>if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):</p>\n<p>continue</p>\n<p>allowed.append(url)</p>\n<p>chunks = chunk_text(txt)[:max_chunks_per_url]</p>\n<p>for i, ch in enumerate(chunks):</p>\n<p>cid = f\"{sha1(url)}:{i}\"</p>\n<p>records.append(ChunkRecord(chunk_id=cid, url=url, chunk_index=i, text=ch))</p>\n<p>if not records:</p>\n<p>err_view = {normalize_url(u): fetched.get(normalize_url(u), \"\") for u in urls}</p>\n<p>raise RuntimeError(\"No sources fetched successfully.\\n\" + json.dumps(err_view, indent=2)[:4000])</p>\n<p>ALLOWED_URLS = allowed</p>\n<p>HYBRID.records = records</p>\n<p>HYBRID.build_sparse()</p>\n<p>texts = [r.text for r in HYBRID.records]</p>\n<p>emb = await embed_texts(texts, batch_size=96, max_concurrency=3)</p>\n<p>HYBRID.set_dense(emb)</p>\n<p>We build a hybrid retrieval index that combines sparse TF-IDF search with dense OpenAI embeddings. We enable reciprocal rank fusion, so that sparse and dense signals complement each other rather than compete. We construct the index once per run and reuse it across all retrieval queries for efficiency. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef build_evidence_pack(query: str, sparse: List[Tuple[int,float]], dense: List[Tuple[int,float]], k: int = 10) -&gt; EvidencePack:</p>\n<p>sparse_rank = [i for i,_ in sparse]</p>\n<p>dense_rank  = [i for i,_ in dense]</p>\n<p>sparse_scores = {i:s for i,s in sparse}</p>\n<p>dense_scores  = {i:s for i,s in dense}</p>\n<p>fused = rrf_fuse([sparse_rank, dense_rank], k=60) if dense_rank else rrf_fuse([sparse_rank], k=60)</p>\n<p>top = sorted(fused.keys(), key=lambda i: fused[i], reverse=True)[:k]</p>\n<p>hits: List[RetrievalHit] = []</p>\n<p>for idx in top:</p>\n<p>r = HYBRID.records[idx]</p>\n<p>hits.append(RetrievalHit(</p>\n<p>chunk_id=r.chunk_id, url=r.url, chunk_index=r.chunk_index,</p>\n<p>score_sparse=float(sparse_scores.get(idx, 0.0)),</p>\n<p>score_dense=float(dense_scores.get(idx, 0.0)),</p>\n<p>score_fused=float(fused.get(idx, 0.0)),</p>\n<p>text=r.text</p>\n<p>))</p>\n<p>return EvidencePack(query=query, hits=hits)</p>\n<p>async def gather_evidence(queries: List[str], per_query_k: int = 10, sparse_k: int = 60, dense_k: int = 60):</p>\n<p>evidence: List[EvidencePack] = []</p>\n<p>useful_sources_count: Dict[str, int] = {}</p>\n<p>all_chunk_ids: List[str] = []</p>\n<p>for q in queries:</p>\n<p>sparse = HYBRID.search_sparse(q, k=sparse_k)</p>\n<p>q_emb = await embed_query(q)</p>\n<p>dense = HYBRID.search_dense(q_emb, k=dense_k)</p>\n<p>pack = build_evidence_pack(q, sparse, dense, k=per_query_k)</p>\n<p>evidence.append(pack)</p>\n<p>for h in pack.hits[:6]:</p>\n<p>useful_sources_count[h.url] = useful_sources_count.get(h.url, 0) + 1</p>\n<p>for h in pack.hits:</p>\n<p>all_chunk_ids.append(h.chunk_id)</p>\n<p>useful_sources = sorted(useful_sources_count.keys(), key=lambda u: useful_sources_count[u], reverse=True)</p>\n<p>all_chunk_ids = sorted(list(dict.fromkeys(all_chunk_ids)))</p>\n<p>return evidence, useful_sources[:8], all_chunk_ids</p>\n<p>class Plan(BaseModel):</p>\n<p>objective: str</p>\n<p>subtasks: List[str]</p>\n<p>retrieval_queries: List[str]</p>\n<p>acceptance_checks: List[str]</p>\n<p>class UltraAnswer(BaseModel):</p>\n<p>title: str</p>\n<p>executive_summary: str</p>\n<p>architecture: List[str]</p>\n<p>retrieval_strategy: List[str]</p>\n<p>agent_graph: List[str]</p>\n<p>implementation_notes: List[str]</p>\n<p>risks_and_limits: List[str]</p>\n<p>citations: List[str]</p>\n<p>sources: List[str]</p>\n<p>def normalize_answer(ans: UltraAnswer, allowed_chunk_ids: List[str]) -&gt; UltraAnswer:</p>\n<p>data = ans.model_dump()</p>\n<p>data[\"citations\"] = [canonical_chunk_id(x) for x in (data.get(\"citations\") or [])]</p>\n<p>data[\"citations\"] = [x for x in data[\"citations\"] if x in allowed_chunk_ids]</p>\n<p>data[\"executive_summary\"] = inject_exec_summary_citations(data.get(\"executive_summary\",\"\"), data[\"citations\"], allowed_chunk_ids)</p>\n<p>return UltraAnswer(**data)</p>\n<p>def validate_ultra(ans: UltraAnswer, allowed_chunk_ids: List[str]) -&gt; None:</p>\n<p>extras = [u for u in ans.sources if u not in ALLOWED_URLS]</p>\n<p>if extras:</p>\n<p>raise ValueError(f\"Non-allowed sources in output: {extras}\")</p>\n<p>cset = set(ans.citations or [])</p>\n<p>missing = [cid for cid in cset if cid not in set(allowed_chunk_ids)]</p>\n<p>if missing:</p>\n<p>raise ValueError(f\"Citations reference unknown chunk_ids (not retrieved): {missing}\")</p>\n<p>if len(cset) &lt; 6:</p>\n<p>raise ValueError(\"Need at least 6 distinct chunk_id citations in ultra mode.\")</p>\n<p>es_text = ans.executive_summary or \"\"</p>\n<p>es_count = sum(1 for cid in cset if cid in es_text)</p>\n<p>if es_count &lt; 2:</p>\n<p>raise ValueError(\"Executive summary must include at least 2 chunk_id citations verbatim.\")</p>\n<p>PLANNER = Agent(</p>\n<p>name=\"Planner\",</p>\n<p>model=\"gpt-4o-mini\",</p>\n<p>instructions=(</p>\n<p>\"Return a technical Plan schema.\\n\"</p>\n<p>\"Make 10-16 retrieval_queries.\\n\"</p>\n<p>\"Acceptance must include: at least 6 citations and exec_summary contains at least 2 citations verbatim.\"</p>\n<p>),</p>\n<p>output_type=Plan,</p>\n<p>)</p>\n<p>SYNTHESIZER = Agent(</p>\n<p>name=\"Synthesizer\",</p>\n<p>model=\"gpt-4o-mini\",</p>\n<p>instructions=(</p>\n<p>\"Return UltraAnswer schema.\\n\"</p>\n<p>\"Hard constraints:\\n\"</p>\n<p>\"- executive_summary MUST include at least TWO citations verbatim as: (cite: &lt;chunk_id&gt;).\\n\"</p>\n<p>\"- citations must be chosen ONLY from ALLOWED_CHUNK_IDS list.\\n\"</p>\n<p>\"- citations list must include at least 6 unique chunk_ids.\\n\"</p>\n<p>\"- sources must be subset of allowed URLs.\\n\"</p>\n<p>),</p>\n<p>output_type=UltraAnswer,</p>\n<p>)</p>\n<p>FIXER = Agent(</p>\n<p>name=\"Fixer\",</p>\n<p>model=\"gpt-4o-mini\",</p>\n<p>instructions=(</p>\n<p>\"Repair to satisfy guardrails.\\n\"</p>\n<p>\"Ensure executive_summary includes at least TWO citations verbatim.\\n\"</p>\n<p>\"Choose citations ONLY from ALLOWED_CHUNK_IDS list.\\n\"</p>\n<p>\"Return UltraAnswer schema.\"</p>\n<p>),</p>\n<p>output_type=UltraAnswer,</p>\n<p>)</p>\n<p>session = SQLiteSession(\"ultra_agentic_user\", \"ultra_agentic_session.db\")</p>\n<p>We gather evidence by running multiple targeted queries, fusing sparse and dense results, and assembling evidence packs with scores and provenance. We define strict schemas for plans and final answers, then normalize and validate citations against retrieved chunk IDs. We enforce hard guardrails so every answer remains grounded and auditable. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserasync def run_ultra_agentic(question: str, urls: List[str], max_repairs: int = 2) -&gt; UltraAnswer:</p>\n<p>await build_index(urls)</p>\n<p>recall_hint = json.dumps(episode_recall(question, top_k=2), indent=2)[:2000]</p>\n<p>plan_res = await Runner.run(</p>\n<p>PLANNER,</p>\n<p>f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\nRecall:\\n{recall_hint}\\n\",</p>\n<p>session=session</p>\n<p>)</p>\n<p>plan: Plan = plan_res.final_output</p>\n<p>queries = (plan.retrieval_queries or [])[:16]</p>\n<p>evidence_packs, useful_sources, allowed_chunk_ids = await gather_evidence(queries)</p>\n<p>evidence_json = json.dumps([p.model_dump() for p in evidence_packs], indent=2)[:16000]</p>\n<p>allowed_chunk_ids_json = json.dumps(allowed_chunk_ids[:200], indent=2)</p>\n<p>draft_res = await Runner.run(</p>\n<p>SYNTHESIZER,</p>\n<p>f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"</p>\n<p>f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"</p>\n<p>f\"Evidence packs:\\n{evidence_json}\\n\\n\"</p>\n<p>\"Return UltraAnswer.\",</p>\n<p>session=session</p>\n<p>)</p>\n<p>draft = normalize_answer(draft_res.final_output, allowed_chunk_ids)</p>\n<p>last_err = None</p>\n<p>for i in range(max_repairs + 1):</p>\n<p>try:</p>\n<p>validate_ultra(draft, allowed_chunk_ids)</p>\n<p>episode_store(question, ALLOWED_URLS, plan.retrieval_queries, useful_sources)</p>\n<p>return draft</p>\n<p>except Exception as e:</p>\n<p>last_err = str(e)</p>\n<p>if i &gt;= max_repairs:</p>\n<p>draft = normalize_answer(draft, allowed_chunk_ids)</p>\n<p>validate_ultra(draft, allowed_chunk_ids)</p>\n<p>return draft</p>\n<p>fixer_res = await Runner.run(</p>\n<p>FIXER,</p>\n<p>f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"</p>\n<p>f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"</p>\n<p>f\"Guardrail error:\\n{last_err}\\n\\n\"</p>\n<p>f\"Draft:\\n{json.dumps(draft.model_dump(), indent=2)[:12000]}\\n\\n\"</p>\n<p>f\"Evidence packs:\\n{evidence_json}\\n\\n\"</p>\n<p>\"Return corrected UltraAnswer that passes guardrails.\",</p>\n<p>session=session</p>\n<p>)</p>\n<p>draft = normalize_answer(fixer_res.final_output, allowed_chunk_ids)</p>\n<p>raise RuntimeError(f\"Unexpected failure: {last_err}\")</p>\n<p>question = (</p>\n<p>\"Design a production-lean but advanced agentic AI workflow in Python with hybrid retrieval, \"</p>\n<p>\"provenance-first citations, critique-and-repair loops, and episodic memory. \"</p>\n<p>\"Explain why each layer matters, failure modes, and evaluation.\"</p>\n<p>)</p>\n<p>urls = [</p>\n<p>\"https://openai.github.io/openai-agents-python/\",</p>\n<p>\"https://openai.github.io/openai-agents-python/agents/\",</p>\n<p>\"https://openai.github.io/openai-agents-python/running_agents/\",</p>\n<p>\"https://github.com/openai/openai-agents-python\",</p>\n<p>]</p>\n<p>ans = await run_ultra_agentic(question, urls, max_repairs=2)</p>\n<p>print(\"\\nTITLE:\\n\", ans.title)</p>\n<p>print(\"\\nEXECUTIVE SUMMARY:\\n\", ans.executive_summary)</p>\n<p>print(\"\\nARCHITECTURE:\")</p>\n<p>for x in ans.architecture:</p>\n<p>print(\"-\", x)</p>\n<p>print(\"\\nRETRIEVAL STRATEGY:\")</p>\n<p>for x in ans.retrieval_strategy:</p>\n<p>print(\"-\", x)</p>\n<p>print(\"\\nAGENT GRAPH:\")</p>\n<p>for x in ans.agent_graph:</p>\n<p>print(\"-\", x)</p>\n<p>print(\"\\nIMPLEMENTATION NOTES:\")</p>\n<p>for x in ans.implementation_notes:</p>\n<p>print(\"-\", x)</p>\n<p>print(\"\\nRISKS &amp; LIMITS:\")</p>\n<p>for x in ans.risks_and_limits:</p>\n<p>print(\"-\", x)</p>\n<p>print(\"\\nCITATIONS (chunk_ids):\")</p>\n<p>for c in ans.citations:</p>\n<p>print(\"-\", c)</p>\n<p>print(\"\\nSOURCES:\")</p>\n<p>for s in ans.sources:</p>\n<p>print(\"-\", s)</p>\n<p>We orchestrate the full agentic loop by chaining planning, synthesis, validation, and repair in an async-safe pipeline. We automatically retry and fix outputs until they pass all constraints without human intervention. We finish by running a full example and printing a fully grounded, production-ready agentic response.</p>\n<p>In conclusion, we developed a comprehensive agentic pipeline robust to common failure modes: unstable embedding shapes, citation drift, and missing grounding in executive summaries. We validated outputs against allowlisted sources, retrieved chunk IDs, automatically normalized citations, and injected deterministic citations when needed to guarantee compliance without sacrificing correctness. By combining hybrid retrieval, critique-and-repair loops, and episodic memory, we created a reusable foundation we can extend with stronger evaluations (claim-to-evidence coverage scoring, adversarial red-teaming, and regression tests) to continuously harden the system as it scales to new domains and larger corpora.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How to Build a Production-Grade Agentic AI System with Hybrid Retrieval, Provenance-First Citations, Repair Loops, and Episodic Memory appeared first on MarkTechPost.</p>"
    }
  ]
}