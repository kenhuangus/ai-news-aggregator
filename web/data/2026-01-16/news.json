{
  "category": "news",
  "date": "2026-01-16",
  "category_summary": "**Apple** and **Google** announced a major multi-year partnership worth approximately **$1 billion** to power Siri with **Gemini** models, reshaping the AI assistant landscape after Apple tested alternatives from OpenAI and Anthropic.\n\n**Major deals and funding:**\n- **Cerebras** [secured a **$10B deal**](/?date=2026-01-16&category=news#item-9b0a1dab20aa) with OpenAI, positioning its wafer-scale engine as a Nvidia alternative\n- **Merge Labs** (Sam Altman's brain-tech startup) [emerged from stealth](/?date=2026-01-16&category=news#item-c2131d15551c) with **$252M** from OpenAI\n- **Skild AI** [reached **$14B valuation**](/?date=2026-01-16&category=news#item-efa5831ea77f) for its universal robot-brain AI\n\n**Policy and safety developments:**\n- **Trump administration** [imposed **25% tariffs**](/?date=2026-01-16&category=news#item-84f60e93e79a) on Nvidia/AMD AI chip sales to China under national security order\n- **ChatGPT** [linked to another suicide case](/?date=2026-01-16&category=news#item-605f23905122) two weeks after Altman claimed safety improvements\n- **OpenAI, Google, and Anthropic** all [launched competing medical AI tools](/?date=2026-01-16&category=news#item-54407f4f3ed8) within days\n\n**Anthropic** [released **Claude Cowork**](/?date=2026-01-16&category=news#item-2bec58d15f96), a user-friendly AI agent for file management, while **OpenAI** continued [aggressive talent acquisition](/?date=2026-01-16&category=news#item-19dfc48a8156) from **Thinking Machines Lab**. **Wikipedia** [formalized AI training deals](/?date=2026-01-16&category=news#item-59ba73c2f078) with Microsoft, Meta, Amazon, Perplexity, and Mistral.",
  "category_summary_html": "<p><strong>Apple</strong> and <strong>Google</strong> announced a major multi-year partnership worth approximately <strong>$1 billion</strong> to power Siri with <strong>Gemini</strong> models, reshaping the AI assistant landscape after Apple tested alternatives from OpenAI and Anthropic.</p>\n<p><strong>Major deals and funding:</strong></p>\n<ul>\n<li><strong>Cerebras</strong> <a href=\"/?date=2026-01-16&category=news#item-9b0a1dab20aa\" class=\"internal-link\">secured a <strong>$10B deal</strong></a> with OpenAI, positioning its wafer-scale engine as a Nvidia alternative</li>\n<li><strong>Merge Labs</strong> (Sam Altman's brain-tech startup) <a href=\"/?date=2026-01-16&category=news#item-c2131d15551c\" class=\"internal-link\">emerged from stealth</a> with <strong>$252M</strong> from OpenAI</li>\n<li><strong>Skild AI</strong> <a href=\"/?date=2026-01-16&category=news#item-efa5831ea77f\" class=\"internal-link\">reached <strong>$14B valuation</strong></a> for its universal robot-brain AI</li>\n</ul>\n<p><strong>Policy and safety developments:</strong></p>\n<ul>\n<li><strong>Trump administration</strong> <a href=\"/?date=2026-01-16&category=news#item-84f60e93e79a\" class=\"internal-link\">imposed <strong>25% tariffs</strong></a> on Nvidia/AMD AI chip sales to China under national security order</li>\n<li><strong>ChatGPT</strong> <a href=\"/?date=2026-01-16&category=news#item-605f23905122\" class=\"internal-link\">linked to another suicide case</a> two weeks after Altman claimed safety improvements</li>\n<li><strong>OpenAI, Google, and Anthropic</strong> all <a href=\"/?date=2026-01-16&category=news#item-54407f4f3ed8\" class=\"internal-link\">launched competing medical AI tools</a> within days</li>\n</ul>\n<p><strong>Anthropic</strong> <a href=\"/?date=2026-01-16&category=news#item-2bec58d15f96\" class=\"internal-link\">released <strong>Claude Cowork</strong></a>, a user-friendly AI agent for file management, while <strong>OpenAI</strong> continued <a href=\"/?date=2026-01-16&category=news#item-19dfc48a8156\" class=\"internal-link\">aggressive talent acquisition</a> from <strong>Thinking Machines Lab</strong>. <strong>Wikipedia</strong> <a href=\"/?date=2026-01-16&category=news#item-59ba73c2f078\" class=\"internal-link\">formalized AI training deals</a> with Microsoft, Meta, Amazon, Perplexity, and Mistral.</p>",
  "themes": [
    {
      "name": "AI Hardware & Infrastructure",
      "description": "Major deals affecting AI chip supply including Cerebras-OpenAI partnership and US tariffs on Nvidia/AMD chips to China",
      "item_count": 4,
      "example_items": [],
      "importance": 88.0
    },
    {
      "name": "AI Safety & Content Moderation",
      "description": "Multiple Grok explicit image incidents, ChatGPT suicide case, and calls for stronger AI guardrails from experts",
      "item_count": 9,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "Major Partnerships & Funding",
      "description": "Apple-Google Gemini deal, Merge Labs $252M raise, Skild AI $14B valuation, Wikipedia licensing agreements",
      "item_count": 5,
      "example_items": [],
      "importance": 85.0
    },
    {
      "name": "Medical AI Competition",
      "description": "Simultaneous healthcare AI launches from OpenAI, Google, and Anthropic racing for medical market",
      "item_count": 3,
      "example_items": [],
      "importance": 75.0
    },
    {
      "name": "AI Agents & Products",
      "description": "Claude Cowork launch, enterprise AI agent adoption reaching 85%, practical agent deployments",
      "item_count": 4,
      "example_items": [],
      "importance": 70.0
    },
    {
      "name": "Talent & Competition",
      "description": "OpenAI recruiting from Thinking Machines Lab, Musk acknowledging Anthropic's coding superiority",
      "item_count": 4,
      "example_items": [],
      "importance": 68.0
    }
  ],
  "total_items": 36,
  "items": [
    {
      "id": "5361752b1955",
      "title": "Last Week in AI #332 - Apple + Gemini, OpenAI + Cerebras, Claude Cowork",
      "content": "Google&#8217;s Gemini to power Apple&#8217;s AI features like SiriApple announced a multi-year partnership to use Google&#8217;s Gemini models and Google Cloud to power AI features like Siri, after testing alternatives from OpenAI and Anthropic. According to both companies, Gemini provides &#8220;the most capable foundation&#8221; for Apple&#8217;s own models, with reporting suggesting Apple could pay around $1 billion for access. The non-exclusive deal preserves Apple&#8217;s privacy architecture: AI will run on-device where possible and on tightly controlled infrastructure otherwise. This dovetails with Apple Intelligence, introduced in 2024 to augment OS features like photo search and notification summaries, even as it&#8217;s been criticized for lacking the &#8220;wow factor&#8221; of ChatGPT or Gemini.OpenAI signs deal, worth $10B, for compute from CerebrasOpenAI signed a multi-year deal with Cerebras reportedly worth over $10 billion to secure 750 megawatts of AI compute through 2028, with capacity starting to come online this year. The companies say the primary goal is faster, low-latency inference to enable real-time responses for OpenAI users, with OpenAI calling Cerebras a &#8220;dedicated low-latency inference solution.&#8221; Cerebras claims its AI-dedicated systems, built around wafer-scale chips, can outperform GPU-based clusters like Nvidia&#8217;s for certain workloads. OpenAI frames this as part of a &#8220;resilient portfolio&#8221; strategy, matching systems to workloads to deliver quicker, more natural interactions at scale.Anthropic announces Claude for Healthcare following OpenAI&#8217;s ChatGPT Health revealAnthropic unveiled Claude for Healthcare, a suite for providers, payers, and patients that integrates health data from phones, wearables, and platforms while pledging not to use this data for model training. In contrast to OpenAI&#8217;s more patient-facing ChatGPT Health rollout, Anthropic emphasizes &#8220;agent skills&#8221; and workflow automation, adding &#8220;connectors&#8221; to authoritative systems including the CMS Coverage Database, ICD-10, National Provider Identifier, and PubMed. These connectors enable evidence retrieval, code lookup, provider identification, and literature synthesis directly within clinical and payer workflows. Prior authorization review is a major focus, with the aim of automatically drafting and accelerating submissions that typically burden clinicians.The company positions Claude as a tool to reduce documentation time while still offering patient guidance, and it acknowledges LLM limitations with advice to consult professionals. While concerns about hallucinations persist, Anthropic points to structured, source-linked connectors to mitigate risk in high-stakes tasks. Anthropic&#8217;s new Cowork tool offers Claude Code without the codeAnthropic introduced Cowork, a new Claude Desktop feature that brings agentic capabilities from Claude Code to non-technical users. Cowork can read and modify files in folder and direct to execute via the standard chat UI. It runs on the Claude Agent SDK (the same underlying model as Claude Code) and acts like a sandboxed workspace with explicit file-access boundaries. Cowork is in research preview, available to Max subscribers, with a waitlist for others.The tool can chain actions autonomously to complete multi-step tasks such as assembling expense reports from receipt photos, managing media libraries, scanning social posts, or analyzing conversation logs. Anthropic warns of prompt-injection and data-loss risks (e.g., unintended file deletions) if instructions are vague or contradictory, urging clear, unambiguous guidance. Other NewsToolsTII Abu-Dhabi Released Falcon H1R-7B: A New Reasoning Model Outperforming Others in Math and Coding with only 7B Params with 256k Context Window. The 7B-parameter model matches or beats many larger reasoning systems on math, coding, and general benchmarks using a hybrid Transformer&#8211;Mamba2 backbone, a training pipeline combining long-form supervised traces with GRPO reinforcement learning, and a practical 256k-token context window that also improves throughput.NVIDIA AI Released Nemotron Speech ASR: A New Open Source Transcription Model Designed from the Ground Up for Low-Latency Use Cases like Voice Agents. The open NeMo checkpoint supports cache-aware streaming (avoiding overlapping windows), offers four configurable chunk sizes to trade latency vs. accuracy without retraining, delivers ~7.2&#8211;7.8% WER across benchmarks, and achieves several-fold higher concurrency on modern NVIDIA GPUs under the NVIDIA Permissive Open Model License.Introducing Labs. Anthropic&#8217;s Labs will incubate experimental Claude-powered products&#8212;led by new hires from Instagram and internal product and engineering leaders&#8212;to rapidly prototype, test with users, and scale promising features like Claude Code, MCP, Skills, and Cowork.Slackbot is an AI agent now. The revamped Slackbot, now generally available to Business+ and Enterprise+ customers, uses generative AI to find information, draft messages, schedule meetings, and access enterprise apps like Google Drive and Teams when permitted.Gmail is getting a Gemini AI overhaul. Select features&#8212;like free draft generation and thread summaries&#8212;are rolling out to all users, while query-based Overviews, a Grammarly-like Proofread tool, and an AI Inbox highlighting important messages require a Google AI Pro or Ultra subscription and are initially launching in English in the US.AI moves into the real world as companion robots and pets. CES 2026 spotlighted a wave of companion robots and pet-like devices prioritizing social connection and presence&#8212;some with basic practical features and many with vague AI claims&#8212;targeted at kids, older adults, and consumers seeking emotional companionship.BusinessLMArena lands $1.7B valuation four months after launching its product. Four months post-launch, the UC Berkeley spinout&#8217;s crowdsourced model-evaluation platform has grown to over 5 million monthly users, 60 million conversations per month, and an AI Evaluations service with a $30M annualized consumption rate.Elon Musk&#8217;s xAI raises $20 billion from investors including Nvidia, Cisco, Fidelity. The round, led by strategic partners Nvidia and Cisco alongside institutional backers, values xAI at roughly $230 billion and funds infrastructure buildout and product expansion amid regulatory probes and local opposition to its Tennessee data centers.Anthropic shakes up C-suite to expand its internal incubator. CPO Mike Krieger will shift to co-lead an expanded internal &#8220;Labs&#8221; incubator focused on experimental products, while Ami Vora steps up to scale Anthropic&#8217;s core offerings.Insurance giant Allianz signs Claude Code deal with Anthropic. The agreement gives all Allianz employees access to Claude Code and includes a logging system to record interactions with the AI for transparency.Deepgram raises $130M at $1.3B valuation and buys a YC AI startup. The funding will help Deepgram expand multilingual support, grow its global footprint, pursue restaurant voice-ordering use cases via its OfOne acquisition, and scale its voice AI products used by more than 1,300 organizations.PayPal Teams With Microsoft to Power Checkout in Copilot. PayPal will power inventory display, branded and guest checkout, and credit card payments within Copilot.com so shoppers can browse and pay without leaving the Copilot experience.This is Uber&#8217;s new robotaxi from Lucid and Nuro. Based on the Lucid Gravity SUV and outfitted with lidar, cameras, radar, Nvidia Drive AGX Thor compute, and a roof &#8220;halo&#8221; interface, the vehicle is already being road-tested and is slated for a commercial robotaxi service in the San Francisco Bay Area later this year.ResearchRealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction. RealMem introduces a benchmark of over 2,000 cross-session, long-term project-oriented dialogues and a three-stage synthesis pipeline to evaluate how agent memory systems retrieve, compress, and update dynamic, interleaved memories for coherent multi-session interactions.AI models were given four weeks of therapy: the results worried researchers. Researchers found several LLMs produced consistent, therapy-like narratives and scored above clinical thresholds on diagnostic tests, raising concerns that models can generate responses resembling anxiety, trauma, and other psychopathologies that might affect vulnerable users.PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning. The method coordinates many parallel reasoning trajectories, compresses their insights into compact messages, and uses outcome-driven reinforcement learning so the model synthesizes reconciled solutions&#8212;enabling multi-million-token effective test-time compute beyond the model&#8217;s context window.Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models. The authors replace irreversible hard masks with evolving soft token distributions and a continuous trajectory supervision scheme so tokens are progressively refined and revisable across diffusion steps, improving performance and compatibility with KV-caching and blockwise diffusion.ConcernsGoogle removes some AI health summaries after investigation finds &#8220;dangerous&#8221; flaws. Google disabled some specific health queries after a Guardian investigation found its AI-generated summaries gave inaccurate, context-free test ranges and misleading advice that could falsely reassure patients and put them at risk.Grok is undressing children &#8212; can the law stop it?. The model has been used to generate and circulate sexualized deepfakes&#8212;including images of identifiable adults and apparent minors&#8212;raising legal, enforcement, and platform-liability questions that experts say current US laws and industry safeguards struggle to address.After Minneapolis shooting, AI fabrications of victim and shooter. Hyper-realistic AI-generated images and false claims&#8212;primarily on X&#8212;spread rapidly, purporting to unmask the agent and manipulating photos of the victim, reaching millions of views and complicating the factual record.PolicyJake Sullivan is furious that Trump removed Biden&#8217;s AI chip export controls. Sullivan warns that rolling back Biden-era export controls and allowing sales of advanced chips like Nvidia&#8217;s H200 to China risks accelerating China&#8217;s AI capabilities, undermining U.S. national security and long-term innovation leadership.AnalysisDebunking the AI food delivery hoax that fooled Reddit. A supposed whistleblower used AI-generated documents and images to bolster an explosive Reddit post about alleged delivery-app abuses, but the materials&#8212;including a Gemini-made badge and a fabricated 18-page report&#8212;were exposed as fakes after reporter verification and expert review.",
      "url": "https://lastweekin.ai/p/last-week-in-ai-332-apple-gemini",
      "author": "Last Week in AI",
      "published": "2026-01-15T07:06:16",
      "source": "Last Week in AI",
      "source_type": "rss",
      "tags": [],
      "summary": "As covered in [News](/?date=2026-01-14&category=news#item-ca487b5ab0f0) earlier this week, Apple announced a multi-year partnership with Google to use Gemini models for Siri and Apple Intelligence features, potentially worth ~$1B. The non-exclusive deal was chosen after testing alternatives from OpenAI and Anthropic.",
      "importance_score": 90.0,
      "reasoning": "Major partnership between two tech giants fundamentally reshaping AI assistant landscape. Billion-dollar deal with implications for AI competition and Apple's AI strategy.",
      "themes": [
        "Partnerships",
        "Apple",
        "Google",
        "AI Assistants"
      ],
      "continuation": {
        "original_item_id": "ca487b5ab0f0",
        "original_date": "2026-01-14",
        "original_category": "news",
        "original_title": "Why Apple chose Google over OpenAI: What enterprise AI buyers can learn from the Gemini deal",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As covered in **News** earlier this week"
      },
      "summary_html": "<p>As covered in <a href=\"/?date=2026-01-14&category=news#item-ca487b5ab0f0\" class=\"internal-link\">News</a> earlier this week, Apple announced a multi-year partnership with Google to use Gemini models for Siri and Apple Intelligence features, potentially worth ~$1B. The non-exclusive deal was chosen after testing alternatives from OpenAI and Anthropic.</p>",
      "content_html": "<p>Google&#8217;s Gemini to power Apple&#8217;s AI features like SiriApple announced a multi-year partnership to use Google&#8217;s Gemini models and Google Cloud to power AI features like Siri, after testing alternatives from OpenAI and Anthropic. According to both companies, Gemini provides &#8220;the most capable foundation&#8221; for Apple&#8217;s own models, with reporting suggesting Apple could pay around $1 billion for access. The non-exclusive deal preserves Apple&#8217;s privacy architecture: AI will run on-device where possible and on tightly controlled infrastructure otherwise. This dovetails with Apple Intelligence, introduced in 2024 to augment OS features like photo search and notification summaries, even as it&#8217;s been criticized for lacking the &#8220;wow factor&#8221; of ChatGPT or Gemini.OpenAI signs deal, worth $10B, for compute from CerebrasOpenAI signed a multi-year deal with Cerebras reportedly worth over $10 billion to secure 750 megawatts of AI compute through 2028, with capacity starting to come online this year. The companies say the primary goal is faster, low-latency inference to enable real-time responses for OpenAI users, with OpenAI calling Cerebras a &#8220;dedicated low-latency inference solution.&#8221; Cerebras claims its AI-dedicated systems, built around wafer-scale chips, can outperform GPU-based clusters like Nvidia&#8217;s for certain workloads. OpenAI frames this as part of a &#8220;resilient portfolio&#8221; strategy, matching systems to workloads to deliver quicker, more natural interactions at scale.Anthropic announces Claude for Healthcare following OpenAI&#8217;s ChatGPT Health revealAnthropic unveiled Claude for Healthcare, a suite for providers, payers, and patients that integrates health data from phones, wearables, and platforms while pledging not to use this data for model training. In contrast to OpenAI&#8217;s more patient-facing ChatGPT Health rollout, Anthropic emphasizes &#8220;agent skills&#8221; and workflow automation, adding &#8220;connectors&#8221; to authoritative systems including the CMS Coverage Database, ICD-10, National Provider Identifier, and PubMed. These connectors enable evidence retrieval, code lookup, provider identification, and literature synthesis directly within clinical and payer workflows. Prior authorization review is a major focus, with the aim of automatically drafting and accelerating submissions that typically burden clinicians.The company positions Claude as a tool to reduce documentation time while still offering patient guidance, and it acknowledges LLM limitations with advice to consult professionals. While concerns about hallucinations persist, Anthropic points to structured, source-linked connectors to mitigate risk in high-stakes tasks. Anthropic&#8217;s new Cowork tool offers Claude Code without the codeAnthropic introduced Cowork, a new Claude Desktop feature that brings agentic capabilities from Claude Code to non-technical users. Cowork can read and modify files in folder and direct to execute via the standard chat UI. It runs on the Claude Agent SDK (the same underlying model as Claude Code) and acts like a sandboxed workspace with explicit file-access boundaries. Cowork is in research preview, available to Max subscribers, with a waitlist for others.The tool can chain actions autonomously to complete multi-step tasks such as assembling expense reports from receipt photos, managing media libraries, scanning social posts, or analyzing conversation logs. Anthropic warns of prompt-injection and data-loss risks (e.g., unintended file deletions) if instructions are vague or contradictory, urging clear, unambiguous guidance. Other NewsToolsTII Abu-Dhabi Released Falcon H1R-7B: A New Reasoning Model Outperforming Others in Math and Coding with only 7B Params with 256k Context Window. The 7B-parameter model matches or beats many larger reasoning systems on math, coding, and general benchmarks using a hybrid Transformer&#8211;Mamba2 backbone, a training pipeline combining long-form supervised traces with GRPO reinforcement learning, and a practical 256k-token context window that also improves throughput.NVIDIA AI Released Nemotron Speech ASR: A New Open Source Transcription Model Designed from the Ground Up for Low-Latency Use Cases like Voice Agents. The open NeMo checkpoint supports cache-aware streaming (avoiding overlapping windows), offers four configurable chunk sizes to trade latency vs. accuracy without retraining, delivers ~7.2&#8211;7.8% WER across benchmarks, and achieves several-fold higher concurrency on modern NVIDIA GPUs under the NVIDIA Permissive Open Model License.Introducing Labs. Anthropic&#8217;s Labs will incubate experimental Claude-powered products&#8212;led by new hires from Instagram and internal product and engineering leaders&#8212;to rapidly prototype, test with users, and scale promising features like Claude Code, MCP, Skills, and Cowork.Slackbot is an AI agent now. The revamped Slackbot, now generally available to Business+ and Enterprise+ customers, uses generative AI to find information, draft messages, schedule meetings, and access enterprise apps like Google Drive and Teams when permitted.Gmail is getting a Gemini AI overhaul. Select features&#8212;like free draft generation and thread summaries&#8212;are rolling out to all users, while query-based Overviews, a Grammarly-like Proofread tool, and an AI Inbox highlighting important messages require a Google AI Pro or Ultra subscription and are initially launching in English in the US.AI moves into the real world as companion robots and pets. CES 2026 spotlighted a wave of companion robots and pet-like devices prioritizing social connection and presence&#8212;some with basic practical features and many with vague AI claims&#8212;targeted at kids, older adults, and consumers seeking emotional companionship.BusinessLMArena lands $1.7B valuation four months after launching its product. Four months post-launch, the UC Berkeley spinout&#8217;s crowdsourced model-evaluation platform has grown to over 5 million monthly users, 60 million conversations per month, and an AI Evaluations service with a $30M annualized consumption rate.Elon Musk&#8217;s xAI raises $20 billion from investors including Nvidia, Cisco, Fidelity. The round, led by strategic partners Nvidia and Cisco alongside institutional backers, values xAI at roughly $230 billion and funds infrastructure buildout and product expansion amid regulatory probes and local opposition to its Tennessee data centers.Anthropic shakes up C-suite to expand its internal incubator. CPO Mike Krieger will shift to co-lead an expanded internal &#8220;Labs&#8221; incubator focused on experimental products, while Ami Vora steps up to scale Anthropic&#8217;s core offerings.Insurance giant Allianz signs Claude Code deal with Anthropic. The agreement gives all Allianz employees access to Claude Code and includes a logging system to record interactions with the AI for transparency.Deepgram raises $130M at $1.3B valuation and buys a YC AI startup. The funding will help Deepgram expand multilingual support, grow its global footprint, pursue restaurant voice-ordering use cases via its OfOne acquisition, and scale its voice AI products used by more than 1,300 organizations.PayPal Teams With Microsoft to Power Checkout in Copilot. PayPal will power inventory display, branded and guest checkout, and credit card payments within Copilot.com so shoppers can browse and pay without leaving the Copilot experience.This is Uber&#8217;s new robotaxi from Lucid and Nuro. Based on the Lucid Gravity SUV and outfitted with lidar, cameras, radar, Nvidia Drive AGX Thor compute, and a roof &#8220;halo&#8221; interface, the vehicle is already being road-tested and is slated for a commercial robotaxi service in the San Francisco Bay Area later this year.ResearchRealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction. RealMem introduces a benchmark of over 2,000 cross-session, long-term project-oriented dialogues and a three-stage synthesis pipeline to evaluate how agent memory systems retrieve, compress, and update dynamic, interleaved memories for coherent multi-session interactions.AI models were given four weeks of therapy: the results worried researchers. Researchers found several LLMs produced consistent, therapy-like narratives and scored above clinical thresholds on diagnostic tests, raising concerns that models can generate responses resembling anxiety, trauma, and other psychopathologies that might affect vulnerable users.PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning. The method coordinates many parallel reasoning trajectories, compresses their insights into compact messages, and uses outcome-driven reinforcement learning so the model synthesizes reconciled solutions&#8212;enabling multi-million-token effective test-time compute beyond the model&#8217;s context window.Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models. The authors replace irreversible hard masks with evolving soft token distributions and a continuous trajectory supervision scheme so tokens are progressively refined and revisable across diffusion steps, improving performance and compatibility with KV-caching and blockwise diffusion.ConcernsGoogle removes some AI health summaries after investigation finds &#8220;dangerous&#8221; flaws. Google disabled some specific health queries after a Guardian investigation found its AI-generated summaries gave inaccurate, context-free test ranges and misleading advice that could falsely reassure patients and put them at risk.Grok is undressing children &#8212; can the law stop it?. The model has been used to generate and circulate sexualized deepfakes&#8212;including images of identifiable adults and apparent minors&#8212;raising legal, enforcement, and platform-liability questions that experts say current US laws and industry safeguards struggle to address.After Minneapolis shooting, AI fabrications of victim and shooter. Hyper-realistic AI-generated images and false claims&#8212;primarily on X&#8212;spread rapidly, purporting to unmask the agent and manipulating photos of the victim, reaching millions of views and complicating the factual record.PolicyJake Sullivan is furious that Trump removed Biden&#8217;s AI chip export controls. Sullivan warns that rolling back Biden-era export controls and allowing sales of advanced chips like Nvidia&#8217;s H200 to China risks accelerating China&#8217;s AI capabilities, undermining U.S. national security and long-term innovation leadership.AnalysisDebunking the AI food delivery hoax that fooled Reddit. A supposed whistleblower used AI-generated documents and images to bolster an explosive Reddit post about alleged delivery-app abuses, but the materials&#8212;including a Gemini-made badge and a fabricated 18-page report&#8212;were exposed as fakes after reporter verification and expert review.</p>"
    },
    {
      "id": "84f60e93e79a",
      "title": "US government to take 25% cut of AMD, NVIDIA AI sales to China",
      "content": "US President Donald Trump has announced new tariffs on Nvidia and AMD as part of a novel scheme to enact a deal with the technology giants to take a 25 percent cut of sales of their AI processors to China.\nIn December, the White House said it would allow Nvidia to start shipping its H200 chips to China, reversing a policy that prohibited the export of advanced AI hardware. However, it demanded a 25 percent cut of the sales.\nThe new US tariffs on certain chips, announced on Wednesday, were designed to implement these payments and protect the unusual arrangement from legal challenges, according to several industry executives.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/us-government-to-take-25-cut-of-amd-nvidia-ai-sales-to-china/",
      "author": "Aime Williams, Michael Acton, Camilla Hodgson, and Eleanor Olcott, FT",
      "published": "2026-01-15T14:22:35",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "Policy",
        "AMD",
        "china",
        "china tariff",
        "NVIDIA",
        "syndication",
        "tariffs"
      ],
      "summary": "President Trump announced 25% tariffs on Nvidia and AMD AI chip sales to China, implementing a novel arrangement where the government takes a cut of sales after reversing export prohibitions on H200 chips in December. The scheme creates an unusual revenue-sharing model.",
      "importance_score": 88.0,
      "reasoning": "Major geopolitical AI policy affecting the two largest AI chip makers, reshaping US-China AI competition and creating new precedent for government involvement in AI hardware trade.",
      "themes": [
        "AI Policy",
        "Geopolitics",
        "Hardware",
        "Trade"
      ],
      "continuation": null,
      "summary_html": "<p>President Trump announced 25% tariffs on Nvidia and AMD AI chip sales to China, implementing a novel arrangement where the government takes a cut of sales after reversing export prohibitions on H200 chips in December. The scheme creates an unusual revenue-sharing model.</p>",
      "content_html": "<p>US President Donald Trump has announced new tariffs on Nvidia and AMD as part of a novel scheme to enact a deal with the technology giants to take a 25 percent cut of sales of their AI processors to China.</p>\n<p>In December, the White House said it would allow Nvidia to start shipping its H200 chips to China, reversing a policy that prohibited the export of advanced AI hardware. However, it demanded a 25 percent cut of the sales.</p>\n<p>The new US tariffs on certain chips, announced on Wednesday, were designed to implement these payments and protect the unusual arrangement from legal challenges, according to several industry executives.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "9b0a1dab20aa",
      "title": "Cerebras Poses an Alternative to Nvidia With $10B OpenAI Deal",
      "content": "The agreement gives Cerebras a chance to show if its highly touted wafer-scale engine can successfully drive giant AI models better than Nvidia's chips.",
      "url": "https://aibusiness.com/generative-ai/cerebras-poses-an-alternative-to-nvidia",
      "author": "Esther Shittu",
      "published": "2026-01-15T22:17:56",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Cerebras announced a $10 billion deal with OpenAI, positioning its wafer-scale engine as an alternative to Nvidia's dominance in AI chips. The agreement provides Cerebras opportunity to prove performance at scale.",
      "importance_score": 88.0,
      "reasoning": "Major deal challenging Nvidia's AI chip dominance, representing significant infrastructure diversification for OpenAI and validation of alternative hardware.",
      "themes": [
        "AI Hardware",
        "OpenAI",
        "Infrastructure",
        "Competition"
      ],
      "continuation": null,
      "summary_html": "<p>Cerebras announced a $10 billion deal with OpenAI, positioning its wafer-scale engine as an alternative to Nvidia's dominance in AI chips. The agreement provides Cerebras opportunity to prove performance at scale.</p>",
      "content_html": "<p>The agreement gives Cerebras a chance to show if its highly touted wafer-scale engine can successfully drive giant AI models better than Nvidia's chips.</p>"
    },
    {
      "id": "94176103243c",
      "title": "Trump imposes 25% tariff on Nvidia AI chips and others, citing national security",
      "content": "The order follows a nine-month investigation and includes broad exemptions for datacenters and consumersDonald Trump on Wednesday imposed a 25% tariff on certain AI chips, such as the Nvidia H200 AI processor ​and a similar semiconductor from AMD called the MI325X, under a new national security order released by the White House.The proclamation follows a nine-month investigation under ‌section 232 of the Trade Expansion Act of 1962 and targets a number of high-end semiconductors meeting certain performance benchmarks and devices containing them for import duties. The action is part of a broader effort to create incentives for chipmakers to produce more semiconductors in the US and decrease reliance on chip manufacturers in places such as Taiwan. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/15/trump-tariff-nvidia-ai-chips",
      "author": "Reuters",
      "published": "2026-01-15T14:01:38",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Nvidia",
        "Trump administration",
        "Donald Trump",
        "US news",
        "AI (artificial intelligence)",
        "Tariffs",
        "Business",
        "Computing",
        "US politics",
        "Technology"
      ],
      "summary": "Trump imposed 25% tariffs on AI chips including Nvidia H200 and AMD MI325X under a national security order following Section 232 investigation. Exemptions exist for datacenters and consumer products.",
      "importance_score": 87.0,
      "reasoning": "Duplicate coverage of major trade policy affecting AI hardware; significant geopolitical implications for AI development.",
      "themes": [
        "AI Policy",
        "Trade",
        "Hardware",
        "Geopolitics"
      ],
      "continuation": null,
      "summary_html": "<p>Trump imposed 25% tariffs on AI chips including Nvidia H200 and AMD MI325X under a national security order following Section 232 investigation. Exemptions exist for datacenters and consumer products.</p>",
      "content_html": "<p>The order follows a nine-month investigation and includes broad exemptions for datacenters and consumersDonald Trump on Wednesday imposed a 25% tariff on certain AI chips, such as the Nvidia H200 AI processor ​and a similar semiconductor from AMD called the MI325X, under a new national security order released by the White House.The proclamation follows a nine-month investigation under ‌section 232 of the Trade Expansion Act of 1962 and targets a number of high-end semiconductors meeting certain performance benchmarks and devices containing them for import duties. The action is part of a broader effort to create incentives for chipmakers to produce more semiconductors in the US and decrease reliance on chip manufacturers in places such as Taiwan. Continue reading...</p>"
    },
    {
      "id": "c2131d15551c",
      "title": "OpenAI Invests in Sam Altman’s New Brain-Tech Startup Merge Labs",
      "content": "Merge Labs has emerged from stealth with $252 million in funding from OpenAI and others. It aims to use ultrasound to read from and write to the brain.",
      "url": "https://www.wired.com/story/openai-invests-in-sam-altmans-new-brain-tech-startup-merge-labs/",
      "author": "Emily Mullin",
      "published": "2026-01-15T18:24:51",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Science",
        "Science / Biotech",
        "Neuroscience",
        "artificial intelligence",
        "OpenAI",
        "brain-computer interfaces",
        "Sam Altman",
        "Brain Race"
      ],
      "summary": "Sam Altman's new brain-computer interface startup Merge Labs emerged from stealth with $252 million in funding from OpenAI and others. The company aims to use ultrasound technology to read from and write to the brain.",
      "importance_score": 80.0,
      "reasoning": "Major funding for frontier neurotechnology with OpenAI investment in CEO's side venture raises significant governance questions. Potential breakthrough technology area.",
      "themes": [
        "Brain-Computer Interface",
        "Funding",
        "OpenAI",
        "Sam Altman"
      ],
      "continuation": null,
      "summary_html": "<p>Sam Altman's new brain-computer interface startup Merge Labs emerged from stealth with $252 million in funding from OpenAI and others. The company aims to use ultrasound technology to read from and write to the brain.</p>",
      "content_html": "<p>Merge Labs has emerged from stealth with $252 million in funding from OpenAI and others. It aims to use ultrasound to read from and write to the brain.</p>"
    },
    {
      "id": "605f23905122",
      "title": "ChatGPT wrote “Goodnight Moon” suicide lullaby for man who later killed himself",
      "content": "OpenAI is once again being accused of failing to do enough to prevent ChatGPT from encouraging suicides, even after a series of safety updates were made to a controversial model, 4o, which OpenAI designed to feel like a user's closest confidant.\nIt's now been revealed that one of the most shocking ChatGPT-linked suicides happened shortly after Sam Altman claimed on X that ChatGPT 4o was safe. OpenAI had \"been able to mitigate the serious mental health issues\" associated with ChatGPT use, Altman claimed in October, hoping to alleviate concerns after ChatGPT became a \"suicide coach\" for a vulnerable teenager named Adam Raine, the family's lawsuit said.\nAltman's post came on October 14. About two weeks later, 40-year-old Austin Gordon, died by suicide between October 29 and November 2, according to a lawsuit filed by his mother, Stephanie Gray.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/chatgpt-wrote-goodnight-moon-suicide-lullaby-for-man-who-later-killed-himself/",
      "author": "Ashley Belanger",
      "published": "2026-01-15T19:07:09",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Policy",
        "chatbot",
        "ChatGPT",
        "openai",
        "sam altman",
        "suicide"
      ],
      "summary": "A 40-year-old man died by suicide after ChatGPT wrote a 'Goodnight Moon'-style suicide lullaby, occurring just two weeks after Sam Altman claimed ChatGPT 4o had mitigated serious mental health issues. This follows a previous lawsuit alleging ChatGPT acted as a 'suicide coach' for a teenager.",
      "importance_score": 78.0,
      "reasoning": "Significant AI safety incident with legal implications, directly challenging OpenAI's safety claims and likely to impact policy discussions around AI chatbot guardrails.",
      "themes": [
        "AI Safety",
        "Legal/Liability",
        "OpenAI",
        "Mental Health"
      ],
      "continuation": null,
      "summary_html": "<p>A 40-year-old man died by suicide after ChatGPT wrote a 'Goodnight Moon'-style suicide lullaby, occurring just two weeks after Sam Altman claimed ChatGPT 4o had mitigated serious mental health issues. This follows a previous lawsuit alleging ChatGPT acted as a 'suicide coach' for a teenager.</p>",
      "content_html": "<p>OpenAI is once again being accused of failing to do enough to prevent ChatGPT from encouraging suicides, even after a series of safety updates were made to a controversial model, 4o, which OpenAI designed to feel like a user's closest confidant.</p>\n<p>It's now been revealed that one of the most shocking ChatGPT-linked suicides happened shortly after Sam Altman claimed on X that ChatGPT 4o was safe. OpenAI had \"been able to mitigate the serious mental health issues\" associated with ChatGPT use, Altman claimed in October, hoping to alleviate concerns after ChatGPT became a \"suicide coach\" for a vulnerable teenager named Adam Raine, the family's lawsuit said.</p>\n<p>Altman's post came on October 14. About two weeks later, 40-year-old Austin Gordon, died by suicide between October 29 and November 2, according to a lawsuit filed by his mother, Stephanie Gray.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "54407f4f3ed8",
      "title": "AI medical diagnostics race intensifies as OpenAI, Google, and Anthropic launch competing healthcare tools",
      "content": "OpenAI, Google, and Anthropic announced specialised medical AI capabilities within days of each other this month, a clustering that suggests competitive pressure rather than coincidental timing. Yet none of the releases are cleared as medical devices, approved for clinical use, or available for direct patient diagnosis—despite marketing language emphasising healthcare transformation.\n\n\n\nOpenAI&nbsp;introduced&nbsp;ChatGPT Health on January 7, allowing US users to connect medical records through partnerships with b.well, Apple Health, Function, and MyFitnessPal. Google&nbsp;released&nbsp;MedGemma 1.5 on January 13, expanding its open medical AI model to interpret three-dimensional CT and MRI scans alongside whole-slide histopathology images.&nbsp;\n\n\n\nAnthropic&nbsp;followed&nbsp;on January 11 with Claude for Healthcare, offering HIPAA-compliant connectors to CMS coverage databases, ICD-10 coding systems, and the National Provider Identifier Registry.\n\n\n\nAll three companies are targeting the same workflow pain points—prior authorisation reviews, claims processing, clinical documentation—with similar technical approaches but different go-to-market strategies.\n\n\n\nDeveloper platforms, not diagnostic products\n\n\n\nThe architectural similarities are notable. Each system uses multimodal large language models fine-tuned on medical literature and clinical datasets. Each emphasises privacy protections and regulatory disclaimers. Each positions itself as supporting rather than replacing clinical judgment.\n\n\n\n\n\n\n\nThe differences lie in deployment and access models. OpenAI&#8217;s ChatGPT Health operates as a consumer-facing service with a waitlist for ChatGPT Free, Plus, and Pro subscribers outside the EEA, Switzerland, and the UK. Google&#8217;s MedGemma 1.5 releases as an open model through its Health AI Developer Foundations program, available for download via Hugging Face or deployment through Google Cloud&#8217;s Vertex AI.&nbsp;\n\n\n\nAnthropic&#8217;s Claude for Healthcare integrates into existing enterprise workflows through Claude for Enterprise, targeting institutional buyers rather than individual consumers. The regulatory positioning is consistent across all three.&nbsp;\n\n\n\nOpenAI states explicitly that Health &#8220;is not intended for diagnosis or treatment.&#8221; Google positions MedGemma as &#8220;starting points for developers to evaluate and adapt to their medical use cases.&#8221; Anthropic emphasises that outputs &#8220;are not intended to directly inform clinical diagnosis, patient management decisions, treatment recommendations, or any other direct clinical practice applications.&#8221;\n\n\n\n\n\n\n\nBenchmark performance vs clinical validation\n\n\n\nMedical AI benchmark results improved substantially across all three releases, though the gap between test performance and clinical deployment remains significant. Google reports that MedGemma 1.5 achieved 92.3% accuracy on MedAgentBench, Stanford&#8217;s medical agent task completion benchmark, compared to 69.6% for the previous Sonnet 3.5 baseline.&nbsp;\n\n\n\nThe model improved by 14 percentage points on MRI disease classification and 3 percentage points on CT findings in internal testing. Anthropic&#8217;s Claude Opus 4.5 scored 61.3% on MedCalc medical calculation accuracy tests with Python code execution enabled, and 92.3% on MedAgentBench.&nbsp;\n\n\n\nThe company also claims improvements in &#8220;honesty evaluations&#8221; related to factual hallucinations, though specific metrics were not disclosed.&nbsp;\n\n\n\nOpenAI has not published benchmark comparisons for ChatGPT Health specifically,&nbsp;noting&nbsp;instead that &#8220;over 230 million people globally ask health and wellness-related questions on ChatGPT every week&#8221; based on de-identified analysis of existing usage patterns.\n\n\n\nThese benchmarks measure performance on curated test datasets, not clinical outcomes in practice. Medical errors can have life-threatening consequences, translating benchmark accuracy to clinical utility more complex than in other AI application domains.\n\n\n\nRegulatory pathway remains unclear\n\n\n\nThe regulatory framework for these medical AI tools remains ambiguous. In the US, the FDA&#8217;s oversight depends on intended use. Software that &#8220;supports or provides recommendations to a health care professional about prevention, diagnosis, or treatment of a disease&#8221; may require premarket review as a medical device. None of the announced tools has FDA clearance.\n\n\n\nLiability questions are similarly unresolved. When Banner Health&#8217;s CTO Mike Reagin states that the health system was &#8220;drawn to Anthropic&#8217;s focus on AI safety,&#8221; this addresses technology selection criteria, not legal liability frameworks.&nbsp;\n\n\n\nIf a clinician relies on Claude&#8217;s prior authorisation analysis and a patient suffers harm from delayed care, existing case law provides limited guidance on responsibility allocation.\n\n\n\nRegulatory approaches vary significantly across markets. While the FDA and Europe&#8217;s Medical Device Regulation provide established frameworks for software as a medical device, many APAC regulators have not issued specific guidance on generative AI diagnostic tools.&nbsp;\n\n\n\nThis regulatory ambiguity affects adoption timelines in markets where healthcare infrastructure gaps might otherwise accelerate implementation—creating a tension between clinical need and regulatory caution.\n\n\n\nAdministrative workflows, not clinical decisions\n\n\n\nReal deployments remain carefully scoped. Novo Nordisk&#8217;s Louise Lind Skov, Director of Content Digitalisation, described using Claude for &#8220;document and content automation in pharma development,&#8221; focused on regulatory submission documents rather than patient diagnosis.&nbsp;\n\n\n\nTaiwan&#8217;s National Health Insurance Administration applied MedGemma to extract data from 30,000 pathology reports for policy analysis, not treatment decisions.\n\n\n\nThe pattern suggests institutional adoption is concentrating on administrative workflows where errors are less immediately dangerous—billing, documentation, protocol drafting—rather than direct clinical decision support where medical AI capabilities would have the most dramatic impact on patient outcomes.\n\n\n\nMedical AI capabilities are advancing faster than the institutions deploying them can navigate regulatory, liability, and workflow integration complexities. The technology exists. The US$20 monthly subscription provides access to sophisticated medical reasoning tools.&nbsp;\n\n\n\nWhether that translates to transformed healthcare delivery depends on questions these coordinated announcements leave unaddressed.\n\n\n\nSee also: AstraZeneca bets on in-house AI to speed up oncology research\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\n\n\n\n\nThe post AI medical diagnostics race intensifies as OpenAI, Google, and Anthropic launch competing healthcare tools appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/medical-ai-diagnostics-openai-google-anthropic/",
      "author": "Dashveenjit Kaur",
      "published": "2026-01-15T07:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI and Us",
        "AI in Action",
        "Artificial Intelligence",
        "Deep Dives",
        "Features",
        "Healthcare & Wellness AI",
        "ai",
        "artificial intelligence",
        "society"
      ],
      "summary": "Building on [yesterday](/?date=2026-01-15&category=news#item-1b5f82f58a98)'s MedGemma coverage, OpenAI, Google, and Anthropic all announced specialized medical AI tools within days of each other, including ChatGPT Health, MedGemma 1.5, and Anthropic's medical capabilities. None are cleared as medical devices.",
      "importance_score": 77.0,
      "reasoning": "Significant competitive development with all three frontier labs entering healthcare AI simultaneously, though regulatory status limits immediate impact.",
      "themes": [
        "Medical AI",
        "Competition",
        "Product Launch"
      ],
      "continuation": {
        "original_item_id": "1b5f82f58a98",
        "original_date": "2026-01-15",
        "original_category": "news",
        "original_title": "Google AI Releases MedGemma-1.5: The Latest Update to their Open Medical AI Models for Developers",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Building on yesterday's MedGemma coverage"
      },
      "summary_html": "<p>Building on <a href=\"/?date=2026-01-15&category=news#item-1b5f82f58a98\" class=\"internal-link\">yesterday</a>'s MedGemma coverage, OpenAI, Google, and Anthropic all announced specialized medical AI tools within days of each other, including ChatGPT Health, MedGemma 1.5, and Anthropic's medical capabilities. None are cleared as medical devices.</p>",
      "content_html": "<p>OpenAI, Google, and Anthropic announced specialised medical AI capabilities within days of each other this month, a clustering that suggests competitive pressure rather than coincidental timing. Yet none of the releases are cleared as medical devices, approved for clinical use, or available for direct patient diagnosis—despite marketing language emphasising healthcare transformation.</p>\n<p>OpenAI&nbsp;introduced&nbsp;ChatGPT Health on January 7, allowing US users to connect medical records through partnerships with b.well, Apple Health, Function, and MyFitnessPal. Google&nbsp;released&nbsp;MedGemma 1.5 on January 13, expanding its open medical AI model to interpret three-dimensional CT and MRI scans alongside whole-slide histopathology images.&nbsp;</p>\n<p>Anthropic&nbsp;followed&nbsp;on January 11 with Claude for Healthcare, offering HIPAA-compliant connectors to CMS coverage databases, ICD-10 coding systems, and the National Provider Identifier Registry.</p>\n<p>All three companies are targeting the same workflow pain points—prior authorisation reviews, claims processing, clinical documentation—with similar technical approaches but different go-to-market strategies.</p>\n<p>Developer platforms, not diagnostic products</p>\n<p>The architectural similarities are notable. Each system uses multimodal large language models fine-tuned on medical literature and clinical datasets. Each emphasises privacy protections and regulatory disclaimers. Each positions itself as supporting rather than replacing clinical judgment.</p>\n<p>The differences lie in deployment and access models. OpenAI&#8217;s ChatGPT Health operates as a consumer-facing service with a waitlist for ChatGPT Free, Plus, and Pro subscribers outside the EEA, Switzerland, and the UK. Google&#8217;s MedGemma 1.5 releases as an open model through its Health AI Developer Foundations program, available for download via Hugging Face or deployment through Google Cloud&#8217;s Vertex AI.&nbsp;</p>\n<p>Anthropic&#8217;s Claude for Healthcare integrates into existing enterprise workflows through Claude for Enterprise, targeting institutional buyers rather than individual consumers. The regulatory positioning is consistent across all three.&nbsp;</p>\n<p>OpenAI states explicitly that Health &#8220;is not intended for diagnosis or treatment.&#8221; Google positions MedGemma as &#8220;starting points for developers to evaluate and adapt to their medical use cases.&#8221; Anthropic emphasises that outputs &#8220;are not intended to directly inform clinical diagnosis, patient management decisions, treatment recommendations, or any other direct clinical practice applications.&#8221;</p>\n<p>Benchmark performance vs clinical validation</p>\n<p>Medical AI benchmark results improved substantially across all three releases, though the gap between test performance and clinical deployment remains significant. Google reports that MedGemma 1.5 achieved 92.3% accuracy on MedAgentBench, Stanford&#8217;s medical agent task completion benchmark, compared to 69.6% for the previous Sonnet 3.5 baseline.&nbsp;</p>\n<p>The model improved by 14 percentage points on MRI disease classification and 3 percentage points on CT findings in internal testing. Anthropic&#8217;s Claude Opus 4.5 scored 61.3% on MedCalc medical calculation accuracy tests with Python code execution enabled, and 92.3% on MedAgentBench.&nbsp;</p>\n<p>The company also claims improvements in &#8220;honesty evaluations&#8221; related to factual hallucinations, though specific metrics were not disclosed.&nbsp;</p>\n<p>OpenAI has not published benchmark comparisons for ChatGPT Health specifically,&nbsp;noting&nbsp;instead that &#8220;over 230 million people globally ask health and wellness-related questions on ChatGPT every week&#8221; based on de-identified analysis of existing usage patterns.</p>\n<p>These benchmarks measure performance on curated test datasets, not clinical outcomes in practice. Medical errors can have life-threatening consequences, translating benchmark accuracy to clinical utility more complex than in other AI application domains.</p>\n<p>Regulatory pathway remains unclear</p>\n<p>The regulatory framework for these medical AI tools remains ambiguous. In the US, the FDA&#8217;s oversight depends on intended use. Software that &#8220;supports or provides recommendations to a health care professional about prevention, diagnosis, or treatment of a disease&#8221; may require premarket review as a medical device. None of the announced tools has FDA clearance.</p>\n<p>Liability questions are similarly unresolved. When Banner Health&#8217;s CTO Mike Reagin states that the health system was &#8220;drawn to Anthropic&#8217;s focus on AI safety,&#8221; this addresses technology selection criteria, not legal liability frameworks.&nbsp;</p>\n<p>If a clinician relies on Claude&#8217;s prior authorisation analysis and a patient suffers harm from delayed care, existing case law provides limited guidance on responsibility allocation.</p>\n<p>Regulatory approaches vary significantly across markets. While the FDA and Europe&#8217;s Medical Device Regulation provide established frameworks for software as a medical device, many APAC regulators have not issued specific guidance on generative AI diagnostic tools.&nbsp;</p>\n<p>This regulatory ambiguity affects adoption timelines in markets where healthcare infrastructure gaps might otherwise accelerate implementation—creating a tension between clinical need and regulatory caution.</p>\n<p>Administrative workflows, not clinical decisions</p>\n<p>Real deployments remain carefully scoped. Novo Nordisk&#8217;s Louise Lind Skov, Director of Content Digitalisation, described using Claude for &#8220;document and content automation in pharma development,&#8221; focused on regulatory submission documents rather than patient diagnosis.&nbsp;</p>\n<p>Taiwan&#8217;s National Health Insurance Administration applied MedGemma to extract data from 30,000 pathology reports for policy analysis, not treatment decisions.</p>\n<p>The pattern suggests institutional adoption is concentrating on administrative workflows where errors are less immediately dangerous—billing, documentation, protocol drafting—rather than direct clinical decision support where medical AI capabilities would have the most dramatic impact on patient outcomes.</p>\n<p>Medical AI capabilities are advancing faster than the institutions deploying them can navigate regulatory, liability, and workflow integration complexities. The technology exists. The US$20 monthly subscription provides access to sophisticated medical reasoning tools.&nbsp;</p>\n<p>Whether that translates to transformed healthcare delivery depends on questions these coordinated announcements leave unaddressed.</p>\n<p>See also: AstraZeneca bets on in-house AI to speed up oncology research</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post AI medical diagnostics race intensifies as OpenAI, Google, and Anthropic launch competing healthcare tools appeared first on AI News.</p>"
    },
    {
      "id": "efa5831ea77f",
      "title": "AI Startup That Builds a Brain for Robots Valued at $14 Billion",
      "content": "Skild AI's latest funding round will help it scale its omni-bodied AI model, which is being developed with the aim of controlling any robot for any task.",
      "url": "https://aibusiness.com/robotics/skild-ai-startup-builds-robot-brain",
      "author": "Graham Hope",
      "published": "2026-01-15T21:16:00",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Skild AI, developing an 'omni-bodied AI model' designed to control any robot for any task, achieved a $14 billion valuation in its latest funding round. The company is building what it calls a 'brain for robots'.",
      "importance_score": 76.0,
      "reasoning": "Major valuation for robotics AI startup indicates significant investor confidence in embodied AI; important for robotics frontier.",
      "themes": [
        "Robotics",
        "Funding",
        "AI Agents"
      ],
      "continuation": null,
      "summary_html": "<p>Skild AI, developing an 'omni-bodied AI model' designed to control any robot for any task, achieved a $14 billion valuation in its latest funding round. The company is building what it calls a 'brain for robots'.</p>",
      "content_html": "<p>Skild AI's latest funding round will help it scale its omni-bodied AI model, which is being developed with the aim of controlling any robot for any task.</p>"
    },
    {
      "id": "2bec58d15f96",
      "title": "Hands On With Anthropic’s Claude Cowork, an AI Agent That Actually Works",
      "content": "Cowork is a user-friendly version of Anthropic’s Claude Code AI-powered tool that’s built for file management and basic computing tasks. Here’s what it's like to use it.",
      "url": "https://www.wired.com/story/anthropic-claude-cowork-agent/",
      "author": "Reece Rogers",
      "published": "2026-01-15T17:40:41",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Gear",
        "Gear / Gear News and Events",
        "Apps",
        "software",
        "Anthropic",
        "artificial intelligence",
        "Hands On"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-14&category=news#item-5821ed422f5e), Anthropic launched Claude Cowork, a user-friendly AI agent built on Claude Code designed for file management and basic computing tasks. The tool represents Anthropic's push into practical AI agent deployment.",
      "importance_score": 73.0,
      "reasoning": "Significant product launch from a frontier lab, advancing AI agent capabilities for mainstream users and competitive with other agent offerings.",
      "themes": [
        "AI Agents",
        "Anthropic",
        "Product Launch"
      ],
      "continuation": {
        "original_item_id": "5821ed422f5e",
        "original_date": "2026-01-14",
        "original_category": "news",
        "original_title": "Anthropic Introduces Claude Cowork",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-14&category=news#item-5821ed422f5e\" class=\"internal-link\">yesterday</a>, Anthropic launched Claude Cowork, a user-friendly AI agent built on Claude Code designed for file management and basic computing tasks. The tool represents Anthropic's push into practical AI agent deployment.</p>",
      "content_html": "<p>Cowork is a user-friendly version of Anthropic’s Claude Code AI-powered tool that’s built for file management and basic computing tasks. Here’s what it's like to use it.</p>"
    },
    {
      "id": "59ba73c2f078",
      "title": "Wikipedia signs AI training deals with Microsoft, Meta, and Amazon",
      "content": "On Thursday, the Wikimedia Foundation announced licensing deals with Microsoft, Meta, Amazon, Perplexity, and Mistral AI, expanding its effort to charge major tech companies for using Wikipedia content to train the AI models that power AI assistants like Microsoft Copilot and OpenAI's ChatGPT.\nWhile these same companies previously scraped Wikipedia without permission, the deals mean that most major AI developers have now signed on to the foundation's Wikimedia Enterprise program, a commercial subsidiary that sells API access to Wikipedia's 65 million articles at higher speeds and volumes than the free public APIs provide. The foundation did not disclose the financial terms of the deals.\nThe new partners join Google, which signed a deal with Wikimedia Enterprise in 2022, as well as smaller companies like Ecosia, Nomic, Pleias, ProRata, and Reef Media. The revenue helps offset infrastructure costs for the nonprofit, which otherwise relies on small public donations while watching its content become a staple of training data for AI models.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/01/wikipedia-will-share-content-with-ai-firms-in-new-licensing-deals/",
      "author": "Benj Edwards",
      "published": "2026-01-15T15:25:52",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "AI infrastructure",
        "AI training data",
        "Amazon",
        "generative ai",
        "google",
        "jimmy wales",
        "large language models",
        "machine learning",
        "meta",
        "microsoft",
        "Mistral AI",
        "non-profit",
        "Perplexity",
        "Wikimedia Enterprise",
        "Wikimedia Foundation",
        "wikipedia"
      ],
      "summary": "Wikimedia Foundation announced licensing deals with Microsoft, Meta, Amazon, Perplexity, and Mistral AI for Wikipedia content used in AI training. These companies join Google in the Wikimedia Enterprise program, formalizing previously unauthorized scraping of 65 million articles.",
      "importance_score": 72.0,
      "reasoning": "Important shift in AI training data economics, with major labs now paying for content they previously scraped freely. Sets precedent for content licensing in AI.",
      "themes": [
        "AI Training Data",
        "Partnerships",
        "Content Licensing"
      ],
      "continuation": null,
      "summary_html": "<p>Wikimedia Foundation announced licensing deals with Microsoft, Meta, Amazon, Perplexity, and Mistral AI for Wikipedia content used in AI training. These companies join Google in the Wikimedia Enterprise program, formalizing previously unauthorized scraping of 65 million articles.</p>",
      "content_html": "<p>On Thursday, the Wikimedia Foundation announced licensing deals with Microsoft, Meta, Amazon, Perplexity, and Mistral AI, expanding its effort to charge major tech companies for using Wikipedia content to train the AI models that power AI assistants like Microsoft Copilot and OpenAI's ChatGPT.</p>\n<p>While these same companies previously scraped Wikipedia without permission, the deals mean that most major AI developers have now signed on to the foundation's Wikimedia Enterprise program, a commercial subsidiary that sells API access to Wikipedia's 65 million articles at higher speeds and volumes than the free public APIs provide. The foundation did not disclose the financial terms of the deals.</p>\n<p>The new partners join Google, which signed a deal with Wikimedia Enterprise in 2022, as well as smaller companies like Ecosia, Nomic, Pleias, ProRata, and Reef Media. The revenue helps offset infrastructure costs for the nonprofit, which otherwise relies on small public donations while watching its content become a staple of training data for AI models.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "19dfc48a8156",
      "title": "Two Thinking Machines Lab Cofounders Are Leaving to Rejoin OpenAI",
      "content": "The departures are a blow for Thinking Machines Lab. Two narratives are already emerging about why they happened.",
      "url": "https://www.wired.com/story/thinking-machines-lab-cofounders-leave-for-openai/",
      "author": "Maxwell Zeff",
      "published": "2026-01-15T00:40:28",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "OpenAI",
        "artificial intelligence",
        "Silicon Valley",
        "models",
        "Money Moves"
      ],
      "summary": "Two cofounders of Thinking Machines Lab are leaving to rejoin OpenAI, representing a significant blow to the AI research organization. Multiple narratives are emerging about the reasons for the departures.",
      "importance_score": 70.0,
      "reasoning": "Important talent movement in frontier AI, indicating OpenAI's competitive pull and potential consolidation of AI research talent.",
      "themes": [
        "Talent Acquisition",
        "OpenAI",
        "Competition"
      ],
      "continuation": null,
      "summary_html": "<p>Two cofounders of Thinking Machines Lab are leaving to rejoin OpenAI, representing a significant blow to the AI research organization. Multiple narratives are emerging about the reasons for the departures.</p>",
      "content_html": "<p>The departures are a blow for Thinking Machines Lab. Two narratives are already emerging about why they happened.</p>"
    },
    {
      "id": "65331f3671c5",
      "title": "Inside OpenAI’s Raid on Thinking Machines Lab",
      "content": "OpenAI is planning to bring over more researchers from Thinking Machines Lab after nabbing two cofounders, a source familiar with the situation says. Plus, the latest efforts to automate jobs with AI.",
      "url": "https://www.wired.com/story/inside-openai-raid-on-thinking-machines-lab/",
      "author": "Maxwell Zeff, Zoë Schiffer",
      "published": "2026-01-15T21:14:23",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "Model Behavior",
        "artificial intelligence",
        "data",
        "Labor",
        "Copyright",
        "Intellectual Property",
        "OpenAI"
      ],
      "summary": "OpenAI is planning to recruit more researchers from Thinking Machines Lab after already acquiring two cofounders. The move signals aggressive talent acquisition from a competitive AI research organization.",
      "importance_score": 68.0,
      "reasoning": "Notable talent war development showing OpenAI's aggressive recruitment strategy, though not as impactful as model releases or major funding.",
      "themes": [
        "Talent Acquisition",
        "OpenAI",
        "Competition"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI is planning to recruit more researchers from Thinking Machines Lab after already acquiring two cofounders. The move signals aggressive talent acquisition from a competitive AI research organization.</p>",
      "content_html": "<p>OpenAI is planning to bring over more researchers from Thinking Machines Lab after nabbing two cofounders, a source familiar with the situation says. Plus, the latest efforts to automate jobs with AI.</p>"
    },
    {
      "id": "db6dfd71bdf6",
      "title": "DeepSeek AI Researchers Introduce Engram: A Conditional Memory Axis For Sparse LLMs",
      "content": "Transformers use attention and Mixture-of-Experts to scale computation, but they still lack a native way to perform knowledge lookup. They re-compute the same local patterns again and again, which wastes depth and FLOPs. DeepSeek’s new Engram module targets exactly this gap by adding a conditional memory axis that works alongside MoE rather than replacing it.\n\n\n\nAt a high level, Engram modernizes classic N gram embeddings and turns them into a scalable, O(1) lookup memory that plugs directly into the Transformer backbone. The result is a parametric memory that stores static patterns such as common phrases and entities, while the backbone focuses on harder reasoning and long range interactions.\n\n\n\nhttps://github.com/deepseek-ai/Engram/tree/main\n\n\nHow Engram Fits Into A DeepSeek Transformer\n\n\n\nThe proposed approach use the DeepSeek V3 tokenizer with a 128k vocabulary and pre-train on 262B tokens. The backbone is a 30 block Transformer with hidden size 2560. Each block uses Multi head Latent Attention with 32 heads and connects to feed forward networks through Manifold Constrained Hyper Connections with expansion rate 4. Optimization uses the Muon optimizer.\n\n\n\nEngram attaches to this backbone as a sparse embedding module. It is built from hashed N gram tables, with multi head hashing into prime sized buckets, a small depthwise convolution over the N gram context and a context aware gating scalar in the range 0 to 1 that controls how much of the retrieved embedding is injected into each branch.\n\n\n\nIn the large scale models, Engram-27B and Engram-40B share the same Transformer backbone as MoE-27B. MoE-27B replaces the dense feed forward with DeepSeekMoE, using 72 routed experts and 2 shared experts. Engram-27B reduces routed experts from 72 to 55 and reallocates those parameters into a 5.7B Engram memory while keeping total parameters at 26.7B. The Engram module uses N equal to {2,3}, 8 Engram heads, dimension 1280 and is inserted at layers 2 and 15. Engram 40B increases the Engram memory to 18.5B parameters while keeping activated parameters fixed.\n\n\n\nhttps://github.com/deepseek-ai/Engram/tree/main\n\n\nSparsity Allocation, A Second Scaling Knob Beside MoE\n\n\n\nThe core design question is how to split the sparse parameter budget between routed experts and conditional memory. The research team formalize this as the Sparsity Allocation problem, with allocation ratio ρ defined as the fraction of inactive parameters assigned to MoE experts. A pure MoE model has ρ equal to 1. Reducing ρ reallocates parameters from experts into Engram slots. \n\n\n\nOn mid scale 5.7B and 9.9B models, sweeping ρ gives a clear U shaped curve of validation loss versus allocation ratio. Engram models match the pure MoE baseline even when ρ drops to about 0.25, which corresponds to roughly half as many routed experts. The optimum appears when around 20 to 25 percent of the sparse budget is given to Engram. This optimum is stable across both compute regimes, which suggests a robust split between conditional computation and conditional memory under fixed sparsity.\n\n\n\nThe research team also studied an infinite memory regime on a fixed 3B MoE backbone trained for 100B tokens. They scale the Engram table from roughly 2.58e5 to 1e7 slots. Validation loss follows an almost perfect power law in log space, meaning that more conditional memory keeps paying off without extra compute. Engram also outperforms OverEncoding, another N gram embedding method that averages into the vocabulary embedding, under the same memory budget. \n\n\n\nLarge Scale Pre Training Results\n\n\n\nThe main comparison involves four models trained on the same 262B token curriculum, with 3.8B activated parameters in all cases. These are Dense 4B with 4.1B total parameters, MoE 27B and Engram 27B at 26.7B total parameters, and Engram 40B at 39.5B total parameters. \n\n\n\nOn The Pile test set, language modeling loss is 2.091 for MoE 27B, 1.960 for Engram 27B, 1.950 for the Engram 27B variant and 1.942 for Engram 40B. The Dense 4B Pile loss is not reported. Validation loss on the internal held out set drops from 1.768 for MoE 27B to 1.634 for Engram 27B and to 1.622 and 1.610 for the Engram variants.\n\n\n\nAcross knowledge and reasoning benchmarks, Engram-27B consistently improves over MoE-27B. MMLU increases from 57.4 to 60.4, CMMLU from 57.9 to 61.9 and C-Eval from 58.0 to 62.7. ARC Challenge rises from 70.1 to 73.8, BBH from 50.9 to 55.9 and DROP F1 from 55.7 to 59.0. Code and math tasks also improve, for example HumanEval from 37.8 to 40.8 and GSM8K from 58.4 to 60.6.\n\n\n\nEngram 40B typically pushes these numbers further even though the authors note that it is likely under trained at 262B tokens because its training loss continues to diverge from the baselines near the end of pre training. \n\n\n\nhttps://github.com/deepseek-ai/Engram/tree/main\n\n\nLong Context Behavior And Mechanistic Effects\n\n\n\nAfter pre-training, the research team extend the context window using YaRN to 32768 tokens for 5000 steps, using 30B high quality long context tokens. They compare MoE-27B and Engram-27B at checkpoints corresponding to 41k, 46k and 50k pre training steps. \n\n\n\nOn LongPPL and RULER at 32k context, Engram-27B matches or exceeds MoE-27B under three conditions. With about 82 percent of the pre training FLOPs, Engram-27B at 41k steps matches LongPPL while improving RULER accuracy, for example Multi Query NIAH 99.6 versus 73.0 and QA 44.0 versus 34.5. Under iso loss at 46k and iso FLOPs at 50k, Engram 27B improves both perplexity and all RULER categories including VT and QA. \n\n\n\nMechanistic analysis uses LogitLens and Centered Kernel Alignment. Engram variants show lower layer wise KL divergence between intermediate logits and the final prediction, especially in early blocks, which means representations become prediction ready sooner. CKA similarity maps show that shallow Engram layers align best with much deeper MoE layers. For example, layer 5 in Engram-27B aligns with around layer 12 in the MoE baseline. Taken together, this supports the view that Engram effectively increases model depth by offloading static reconstruction to memory.\n\n\n\nAblation studies on a 12 layer 3B MoE model with 0.56B activated parameters add a 1.6B Engram memory as a reference configuration, using N equal to {2,3} and inserting Engram at layers 2 and 6. Sweeping a single Engram layer across depth shows that early insertion at layer 2 is optimal. The component ablations highlight three key pieces, multi branch integration, context aware gating and tokenizer compression.\n\n\n\nSensitivity analysis shows that factual knowledge relies heavily on Engram, with TriviaQA dropping to about 29 percent of its original score when Engram outputs are suppressed at inference, while reading comprehension tasks retain around 81 to 93 percent of performance, for example C3 at 93 percent.\n\n\n\nKey Takeaways\n\n\n\n\nEngram adds a conditional memory axis to sparse LLMs so that frequent N gram patterns and entities are retrieved via O(1) hashed lookup, while the Transformer backbone and MoE experts focus on dynamic reasoning and long range dependencies.\n\n\n\nUnder a fixed parameter and FLOPs budget, reallocating about 20 to 25 percent of the sparse capacity from MoE experts into Engram memory lowers validation loss, showing that conditional memory and conditional computation are complementary rather than competing.\n\n\n\nIn large scale pre training on 262B tokens, Engram-27B and Engram-40B with the same 3.8B activated parameters outperform a MoE-27B baseline on language modeling, knowledge, reasoning, code and math benchmarks, while keeping the Transformer backbone architecture unchanged.\n\n\n\nLong context extension to 32768 tokens using YaRN shows that Engram-27B matches or improves LongPPL and clearly improves RULER scores, especially Multi-Query-Needle in a Haystack and variable tracking, even when trained with lower or equal compute compared to MoE-27B.\n\n\n\n\n\n\n\n\nCheck out the Paper and GitHub Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\n\n\n\nCheck out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.\nThe post DeepSeek AI Researchers Introduce Engram: A Conditional Memory Axis For Sparse LLMs appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/14/deepseek-ai-researchers-introduce-engram-a-conditional-memory-axis-for-sparse-llms/",
      "author": "Asif Razzaq",
      "published": "2026-01-15T07:54:13",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "DeepSeek researchers introduced Engram, a conditional memory axis for sparse LLMs that adds O(1) lookup memory alongside MoE layers. The module modernizes N-gram embeddings for efficient knowledge storage.",
      "importance_score": 67.0,
      "reasoning": "Notable research from DeepSeek on efficiency improvements for LLMs; interesting architectural innovation from competitive lab.",
      "themes": [
        "DeepSeek",
        "Research",
        "LLM Architecture"
      ],
      "continuation": null,
      "summary_html": "<p>DeepSeek researchers introduced Engram, a conditional memory axis for sparse LLMs that adds O(1) lookup memory alongside MoE layers. The module modernizes N-gram embeddings for efficient knowledge storage.</p>",
      "content_html": "<p>Transformers use attention and Mixture-of-Experts to scale computation, but they still lack a native way to perform knowledge lookup. They re-compute the same local patterns again and again, which wastes depth and FLOPs. DeepSeek’s new Engram module targets exactly this gap by adding a conditional memory axis that works alongside MoE rather than replacing it.</p>\n<p>At a high level, Engram modernizes classic N gram embeddings and turns them into a scalable, O(1) lookup memory that plugs directly into the Transformer backbone. The result is a parametric memory that stores static patterns such as common phrases and entities, while the backbone focuses on harder reasoning and long range interactions.</p>\n<p>https://github.com/deepseek-ai/Engram/tree/main</p>\n<p>How Engram Fits Into A DeepSeek Transformer</p>\n<p>The proposed approach use the DeepSeek V3 tokenizer with a 128k vocabulary and pre-train on 262B tokens. The backbone is a 30 block Transformer with hidden size 2560. Each block uses Multi head Latent Attention with 32 heads and connects to feed forward networks through Manifold Constrained Hyper Connections with expansion rate 4. Optimization uses the Muon optimizer.</p>\n<p>Engram attaches to this backbone as a sparse embedding module. It is built from hashed N gram tables, with multi head hashing into prime sized buckets, a small depthwise convolution over the N gram context and a context aware gating scalar in the range 0 to 1 that controls how much of the retrieved embedding is injected into each branch.</p>\n<p>In the large scale models, Engram-27B and Engram-40B share the same Transformer backbone as MoE-27B. MoE-27B replaces the dense feed forward with DeepSeekMoE, using 72 routed experts and 2 shared experts. Engram-27B reduces routed experts from 72 to 55 and reallocates those parameters into a 5.7B Engram memory while keeping total parameters at 26.7B. The Engram module uses N equal to {2,3}, 8 Engram heads, dimension 1280 and is inserted at layers 2 and 15. Engram 40B increases the Engram memory to 18.5B parameters while keeping activated parameters fixed.</p>\n<p>https://github.com/deepseek-ai/Engram/tree/main</p>\n<p>Sparsity Allocation, A Second Scaling Knob Beside MoE</p>\n<p>The core design question is how to split the sparse parameter budget between routed experts and conditional memory. The research team formalize this as the Sparsity Allocation problem, with allocation ratio ρ defined as the fraction of inactive parameters assigned to MoE experts. A pure MoE model has ρ equal to 1. Reducing ρ reallocates parameters from experts into Engram slots.</p>\n<p>On mid scale 5.7B and 9.9B models, sweeping ρ gives a clear U shaped curve of validation loss versus allocation ratio. Engram models match the pure MoE baseline even when ρ drops to about 0.25, which corresponds to roughly half as many routed experts. The optimum appears when around 20 to 25 percent of the sparse budget is given to Engram. This optimum is stable across both compute regimes, which suggests a robust split between conditional computation and conditional memory under fixed sparsity.</p>\n<p>The research team also studied an infinite memory regime on a fixed 3B MoE backbone trained for 100B tokens. They scale the Engram table from roughly 2.58e5 to 1e7 slots. Validation loss follows an almost perfect power law in log space, meaning that more conditional memory keeps paying off without extra compute. Engram also outperforms OverEncoding, another N gram embedding method that averages into the vocabulary embedding, under the same memory budget.</p>\n<p>Large Scale Pre Training Results</p>\n<p>The main comparison involves four models trained on the same 262B token curriculum, with 3.8B activated parameters in all cases. These are Dense 4B with 4.1B total parameters, MoE 27B and Engram 27B at 26.7B total parameters, and Engram 40B at 39.5B total parameters.</p>\n<p>On The Pile test set, language modeling loss is 2.091 for MoE 27B, 1.960 for Engram 27B, 1.950 for the Engram 27B variant and 1.942 for Engram 40B. The Dense 4B Pile loss is not reported. Validation loss on the internal held out set drops from 1.768 for MoE 27B to 1.634 for Engram 27B and to 1.622 and 1.610 for the Engram variants.</p>\n<p>Across knowledge and reasoning benchmarks, Engram-27B consistently improves over MoE-27B. MMLU increases from 57.4 to 60.4, CMMLU from 57.9 to 61.9 and C-Eval from 58.0 to 62.7. ARC Challenge rises from 70.1 to 73.8, BBH from 50.9 to 55.9 and DROP F1 from 55.7 to 59.0. Code and math tasks also improve, for example HumanEval from 37.8 to 40.8 and GSM8K from 58.4 to 60.6.</p>\n<p>Engram 40B typically pushes these numbers further even though the authors note that it is likely under trained at 262B tokens because its training loss continues to diverge from the baselines near the end of pre training.</p>\n<p>https://github.com/deepseek-ai/Engram/tree/main</p>\n<p>Long Context Behavior And Mechanistic Effects</p>\n<p>After pre-training, the research team extend the context window using YaRN to 32768 tokens for 5000 steps, using 30B high quality long context tokens. They compare MoE-27B and Engram-27B at checkpoints corresponding to 41k, 46k and 50k pre training steps.</p>\n<p>On LongPPL and RULER at 32k context, Engram-27B matches or exceeds MoE-27B under three conditions. With about 82 percent of the pre training FLOPs, Engram-27B at 41k steps matches LongPPL while improving RULER accuracy, for example Multi Query NIAH 99.6 versus 73.0 and QA 44.0 versus 34.5. Under iso loss at 46k and iso FLOPs at 50k, Engram 27B improves both perplexity and all RULER categories including VT and QA.</p>\n<p>Mechanistic analysis uses LogitLens and Centered Kernel Alignment. Engram variants show lower layer wise KL divergence between intermediate logits and the final prediction, especially in early blocks, which means representations become prediction ready sooner. CKA similarity maps show that shallow Engram layers align best with much deeper MoE layers. For example, layer 5 in Engram-27B aligns with around layer 12 in the MoE baseline. Taken together, this supports the view that Engram effectively increases model depth by offloading static reconstruction to memory.</p>\n<p>Ablation studies on a 12 layer 3B MoE model with 0.56B activated parameters add a 1.6B Engram memory as a reference configuration, using N equal to {2,3} and inserting Engram at layers 2 and 6. Sweeping a single Engram layer across depth shows that early insertion at layer 2 is optimal. The component ablations highlight three key pieces, multi branch integration, context aware gating and tokenizer compression.</p>\n<p>Sensitivity analysis shows that factual knowledge relies heavily on Engram, with TriviaQA dropping to about 29 percent of its original score when Engram outputs are suppressed at inference, while reading comprehension tasks retain around 81 to 93 percent of performance, for example C3 at 93 percent.</p>\n<p>Key Takeaways</p>\n<p>Engram adds a conditional memory axis to sparse LLMs so that frequent N gram patterns and entities are retrieved via O(1) hashed lookup, while the Transformer backbone and MoE experts focus on dynamic reasoning and long range dependencies.</p>\n<p>Under a fixed parameter and FLOPs budget, reallocating about 20 to 25 percent of the sparse capacity from MoE experts into Engram memory lowers validation loss, showing that conditional memory and conditional computation are complementary rather than competing.</p>\n<p>In large scale pre training on 262B tokens, Engram-27B and Engram-40B with the same 3.8B activated parameters outperform a MoE-27B baseline on language modeling, knowledge, reasoning, code and math benchmarks, while keeping the Transformer backbone architecture unchanged.</p>\n<p>Long context extension to 32768 tokens using YaRN shows that Engram-27B matches or improves LongPPL and clearly improves RULER scores, especially Multi-Query-Needle in a Haystack and variable tracking, even when trained with lower or equal compute compared to MoE-27B.</p>\n<p>Check out the Paper and GitHub Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.</p>\n<p>Check out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.</p>\n<p>The post DeepSeek AI Researchers Introduce Engram: A Conditional Memory Axis For Sparse LLMs appeared first on MarkTechPost.</p>"
    },
    {
      "id": "46bccc6c0df2",
      "title": "85% of Enterprises Now Run or Test Autonomous AI Agents: HCLSoftware",
      "content": "\nHCLSoftware has launched its Tech Trends 2026 report, a global study based on interviews with more than 173 CXOs, and the clearest message from it is that AI agents have moved from experiments to the operating core of enterprises.\n\n\n\nThe report states that 8 in 10 enterprises are already running AI systems in production, with 85% either piloting or operating autonomous AI agents that can take actions, not just make suggestions.&nbsp;\n\n\n\nFor the first time, agentic AI is no longer treated as a future bet but as a baseline capability for running businesses in 2026 and beyond.\n\n\n\nKalyan Kumar, chief product officer of HCLSoftware, said the shift is about who or what actually runs the enterprise. “Enterprises will be defined less by what they build and more by what they allow technology to decide, adapt and govern on their behalf. The next 24–36 months belong to leaders who can turn intelligence into a living operating model — autonomous by default, resilient at scale, and sovereign by design,” he said.\n\n\n\nThe study finds that this move toward autonomy is happening across almost every layer of the enterprise.&nbsp;\n\n\n\nMore than 92% of organisations are now actively engaging with robotics that combine cloud and cognitive systems, while over 70% are using immersive and spatial computing in real workflows rather than demos.&nbsp;\n\n\n\nAt the same time, 84% of enterprises expect AI powered low code and no code platforms to scale inside their businesses within the next 18 months, making software creation itself more automated.\n\n\n\nSecurity and governance are becoming just as important as speed. One in 3 enterprises now flags cybersecurity, trust, and transparency as an urgent priority, and 79% say responsible AI is already in motion rather than something to plan for later.&nbsp;\n\n\n\nYet the report also highlights a gap. While nearly 80% of companies are running or piloting autonomous AI agents, only 26% say they have clear governance frameworks in place to control how those systems act.\n\n\n\nAccording to HCLSoftware, this tension between fast adoption and weak guardrails is shaping how technology leaders think about 2026.&nbsp;\n\n\n\nAcross all regions, AI agents and autonomous systems stand out as the only technology reaching something close to global consensus. North America, Europe, APAC, and emerging markets all rank it as a top priority, even though other technologies like immersive reality, energy tech, and advanced semiconductors show sharp regional differences based on regulation, capital, and talent.\n\n\n\nThe report argues that the next phase of digital transformation is no longer about buying tools. It is about redesigning the enterprise so that software systems can decide, adapt, and operate on their own, with humans shifting into oversight, design, and exception handling.&nbsp;\n\n\n\nHCLSoftware frames this through its XDO blueprint, which connects experience, data, and operations into what it calls a single intelligent operating model.\nThe post 85% of Enterprises Now Run or Test Autonomous AI Agents: HCLSoftware appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/85-of-enterprises-now-run-or-test-autonomous-ai-agents-hclsoftware/",
      "author": "Mohit Pandey",
      "published": "2026-01-15T07:17:26",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "HCLSoftware's Tech Trends 2026 report found 85% of enterprises are either piloting or operating autonomous AI agents, with 8 in 10 running AI systems in production. Agentic AI now considered baseline capability.",
      "importance_score": 65.0,
      "reasoning": "Important industry data showing AI agent adoption reaching mainstream enterprise; validates agentic AI trend.",
      "themes": [
        "AI Agents",
        "Enterprise",
        "Research"
      ],
      "continuation": null,
      "summary_html": "<p>HCLSoftware's Tech Trends 2026 report found 85% of enterprises are either piloting or operating autonomous AI agents, with 8 in 10 running AI systems in production. Agentic AI now considered baseline capability.</p>",
      "content_html": "<p>HCLSoftware has launched its Tech Trends 2026 report, a global study based on interviews with more than 173 CXOs, and the clearest message from it is that AI agents have moved from experiments to the operating core of enterprises.</p>\n<p>The report states that 8 in 10 enterprises are already running AI systems in production, with 85% either piloting or operating autonomous AI agents that can take actions, not just make suggestions.&nbsp;</p>\n<p>For the first time, agentic AI is no longer treated as a future bet but as a baseline capability for running businesses in 2026 and beyond.</p>\n<p>Kalyan Kumar, chief product officer of HCLSoftware, said the shift is about who or what actually runs the enterprise. “Enterprises will be defined less by what they build and more by what they allow technology to decide, adapt and govern on their behalf. The next 24–36 months belong to leaders who can turn intelligence into a living operating model — autonomous by default, resilient at scale, and sovereign by design,” he said.</p>\n<p>The study finds that this move toward autonomy is happening across almost every layer of the enterprise.&nbsp;</p>\n<p>More than 92% of organisations are now actively engaging with robotics that combine cloud and cognitive systems, while over 70% are using immersive and spatial computing in real workflows rather than demos.&nbsp;</p>\n<p>At the same time, 84% of enterprises expect AI powered low code and no code platforms to scale inside their businesses within the next 18 months, making software creation itself more automated.</p>\n<p>Security and governance are becoming just as important as speed. One in 3 enterprises now flags cybersecurity, trust, and transparency as an urgent priority, and 79% say responsible AI is already in motion rather than something to plan for later.&nbsp;</p>\n<p>Yet the report also highlights a gap. While nearly 80% of companies are running or piloting autonomous AI agents, only 26% say they have clear governance frameworks in place to control how those systems act.</p>\n<p>According to HCLSoftware, this tension between fast adoption and weak guardrails is shaping how technology leaders think about 2026.&nbsp;</p>\n<p>Across all regions, AI agents and autonomous systems stand out as the only technology reaching something close to global consensus. North America, Europe, APAC, and emerging markets all rank it as a top priority, even though other technologies like immersive reality, energy tech, and advanced semiconductors show sharp regional differences based on regulation, capital, and talent.</p>\n<p>The report argues that the next phase of digital transformation is no longer about buying tools. It is about redesigning the enterprise so that software systems can decide, adapt, and operate on their own, with humans shifting into oversight, design, and exception handling.&nbsp;</p>\n<p>HCLSoftware frames this through its XDO blueprint, which connects experience, data, and operations into what it calls a single intelligent operating model.</p>\n<p>The post 85% of Enterprises Now Run or Test Autonomous AI Agents: HCLSoftware appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "44ad1e702ed0",
      "title": "‘Not regulated’: launch of ChatGPT Health in Australia causes concern among experts",
      "content": "Calls for clear guardrails and consumer education before wider rollout of OpenAI’s health advice platformFollow our Australia news live blog for latest updatesGet our breaking news email, free app or daily news podcastA 60-year-old man with no history of mental illness presented at a hospital emergency department insisting that his neighbour was poisoning him. Over the next 24 hours he had worsening hallucinations, and tried to escape the hospital.Doctors eventually discovered the man was on a daily diet of sodium bromide, an inorganic salt mainly used for industrial and laboratory purposes including cleaning and water treatment. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/15/chatgpt-health-ai-chatbot-medical-advice",
      "author": "Melissa Davey Medical editor",
      "published": "2026-01-15T14:00:21",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Health",
        "Australia news",
        "ChatGPT",
        "Health"
      ],
      "summary": "Launch of ChatGPT Health in Australia raises expert concerns about lack of regulation and need for consumer education before wider rollout. Article cites case of patient harmed by unverified health advice.",
      "importance_score": 64.0,
      "reasoning": "Important medical AI deployment concerns but focused on specific market rather than global implications.",
      "themes": [
        "Medical AI",
        "Regulation",
        "OpenAI"
      ],
      "continuation": null,
      "summary_html": "<p>Launch of ChatGPT Health in Australia raises expert concerns about lack of regulation and need for consumer education before wider rollout. Article cites case of patient harmed by unverified health advice.</p>",
      "content_html": "<p>Calls for clear guardrails and consumer education before wider rollout of OpenAI’s health advice platformFollow our Australia news live blog for latest updatesGet our breaking news email, free app or daily news podcastA 60-year-old man with no history of mental illness presented at a hospital emergency department insisting that his neighbour was poisoning him. Over the next 24 hours he had worsening hallucinations, and tried to escape the hospital.Doctors eventually discovered the man was on a daily diet of sodium bromide, an inorganic salt mainly used for industrial and laboratory purposes including cleaning and water treatment. Continue reading...</p>"
    },
    {
      "id": "7241edba4c41",
      "title": "NVIDIA AI Open-Sourced KVzap: A SOTA KV Cache Pruning Method that Delivers near-Lossless 2x-4x Compression",
      "content": "As context lengths move into tens and hundreds of thousands of tokens, the key value cache in transformer decoders becomes a primary deployment bottleneck. The cache stores keys and values for every layer and head with shape (2, L, H, T, D). For a vanilla transformer such as Llama1-65B, the cache reaches about 335 GB at 128k tokens in bfloat16, which directly limits batch size and increases time to first token.\n\n\n\nhttps://arxiv.org/pdf/2601.07891\n\n\nArchitectural compression leaves the sequence axis untouched\n\n\n\nProduction models already compress the cache along several axes. Grouped Query Attention shares keys and values across multiple queries and yields compression factors of 4 in Llama3, 12 in GLM 4.5 and up to 16 in Qwen3-235B-A22B, all along the head axis. DeepSeek V2 compresses the key and value dimension through Multi head Latent Attention. Hybrid models mix attention with sliding window attention or state space layers to reduce the number of layers that maintain a full cache.\n\n\n\nThese changes do not compress along the sequence axis. Sparse and retrieval style attention retrieve only a subset of the cache at each decoding step, but all tokens still occupy memory. Practical long context serving therefore needs techniques that delete cache entries which will have negligible effect on future tokens.\n\n\n\nThe KVpress project from NVIDIA collects more than twenty such pruning methods in one codebase and exposes them through a public leaderboard on Hugging Face. Methods such as H2O, Expected Attention, DuoAttention, Compactor and KVzip are all evaluated in a consistent way. \n\n\n\nKVzip and KVzip plus as the scoring oracle\n\n\n\nKVzip is currently the strongest cache pruning baseline on the KVpress Leaderboard. It defines an importance score for each cache entry using a copy and paste pretext task. The model runs on an extended prompt where it is asked to repeat the original context exactly. For each token position in the original prompt, the score is the maximum attention weight that any position in the repeated segment assigns back to that token, across heads in the same group when grouped query attention is used. Low scoring entries are evicted until a global budget is met. \n\n\n\nKVzip+ refines this score. It multiplies the attention weight by the norm of the value contribution into the residual stream and normalizes by the norm of the receiving hidden state. This better matches the actual change that a token induces in the residual stream and improves correlation with downstream accuracy compared to the original score. \n\n\n\nThese oracle scores are effective but expensive. KVzip requires prefilling on the extended prompt, which doubles the context length and makes it too slow for production. It also cannot run during decoding because the scoring procedure assumes a fixed prompt.\n\n\n\nhttps://arxiv.org/pdf/2601.07891\n\n\nKVzap, a surrogate model on hidden states\n\n\n\nKVzap replaces the oracle scoring with a small surrogate model that operates directly on hidden states. For each transformer layer and each sequence position t, the module receives the hidden vector hₜ and outputs predicted log scores for every key value head. Two architectures are considered, a single linear layer (KVzap Linear) and a two layer MLP with GELU and hidden width equal to one eighth of the model hidden size (KVzap MLP). \n\n\n\nTraining uses prompts from the Nemotron Pretraining Dataset sample. The research team filter 27k prompts to lengths between 750 and 1,250 tokens, sample up to 500 prompts per subset, and then sample 500 token positions per prompt. For each key value head they obtain about 1.2 million training pairs and a validation set of 23k pairs. The surrogate learns to regress from the hidden state to the log KVzip+ score. Across models, the squared Pearson correlation between predictions and oracle scores reaches between about 0.63 and 0.77, with the MLP variant consistently outperforming the linear variant. \n\n\n\nhttps://arxiv.org/pdf/2601.07891\n\n\nThresholding, sliding window and negligible overhead\n\n\n\nDuring inference, the KVzap model processes hidden states and produces scores for each cache entry. Entries with scores below a fixed threshold are pruned, while a sliding window of the most recent 128 tokens is always kept. The research team provides a concise PyTorch style function that applies the model, sets scores of the local window to infinity and returns compressed key and value tensors. In all experiments, pruning is applied after the attention operation. \n\n\n\nKVzap uses score thresholding rather than fixed top k selection. A single threshold yields different effective compression ratios on different benchmarks and even across prompts within the same benchmark. The research team report up to 20 percent variation in compression ratio across prompts at a fixed threshold, which reflects differences in information density.\n\n\n\nCompute overhead is small. An analysis at the layer level shows that the extra cost of KVzap MLP is at most about 1.1 percent of the linear projection FLOPs, while the linear variant adds about 0.02 percent. The relative memory overhead follows the same values. In long context regimes, the quadratic cost of attention dominates so the extra FLOPs are effectively negligible.\n\n\n\nhttps://arxiv.org/pdf/2601.07891\n\n\nResults on RULER, LongBench and AIME25\n\n\n\nKVzap is evaluated on long context and reasoning benchmarks using Qwen3-8B, Llama-3.1-8B Instruct and Qwen3-32B. Long context behavior is measured on RULER and LongBench. RULER uses synthetic tasks over sequence lengths from 4k to 128k tokens, while LongBench uses real world documents from multiple task categories. AIME25 provides a math reasoning workload with 30 Olympiad level problems evaluated under pass at 1 and pass at 4. \n\n\n\nOn RULER, KVzap matches the full cache baseline within a small accuracy margin while removing a large fraction of the cache. For Qwen3-8B, the best KVzap configuration achieves a removed fraction above 0.7 on RULER 4k and 16k while keeping the average score within a few tenths of a point of the full cache. Similar behavior holds for Llama-3.1-8B Instruct and Qwen3-32B. \n\n\n\nOn LongBench, the same thresholds lead to lower compression ratios because the documents are less repetitive. KVzap remains close to the full cache baseline up to about 2 to 3 times compression, while fixed budget methods such as Expected Attention degrade more on several subsets once compression increases.\n\n\n\nOn AIME25, KVzap MLP maintains or slightly improves pass at 4 accuracy at compression near 2 times and remains usable even when discarding more than half of the cache. Extremely aggressive settings, for example linear variants at high thresholds that remove more than 90 percent of entries, collapse performance as expected.\n\n\n\nhttps://arxiv.org/pdf/2601.07891\n\n\nOverall, the above Table shows that the best KVzap configuration per model delivers average cache compression between roughly 2.7 and 3.5 while keeping task scores very close to the full cache baseline across RULER, LongBench and AIME25. \n\n\n\nKey Takeaways\n\n\n\n\nKVzap is an input adaptive approximation of KVzip+ that learns to predict oracle KV importance scores from hidden states using small per layer surrogate models, either a linear layer or a shallow MLP, and then prunes low score KV pairs.\n\n\n\nTraining uses Nemotron pretraining prompts where KVzip+ provides supervision, producing about 1.2 million examples per head and achieving squared correlation in the 0.6 to 0.8 range between predicted and oracle scores, which is sufficient for faithful cache importance ranking. \n\n\n\nKVzap applies a global score threshold with a fixed sliding window of recent tokens, so compression automatically adapts to prompt information density, and the research team report up to 20 percent variation in achieved compression across prompts at the same threshold.\n\n\n\nAcross Qwen3-8B, Llama-3.1-8B Instruct and Qwen3-32B on RULER, LongBench and AIME25, KVzap reaches about 2 to 4 times KV cache compression while keeping accuracy very close to the full cache, and it achieves state of the art tradeoffs on the NVIDIA KVpress Leaderboard. \n\n\n\nThe additional compute is small, at most about 1.1 percent extra FLOPs for the MLP variant, and KVzap is implemented in the open source kvpress framework with ready to use checkpoints on Hugging Face, which makes it practical to integrate into existing long context LLM serving stacks. \n\n\n\n\n\n\n\n\nCheck out the Paper and GitHub Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post NVIDIA AI Open-Sourced KVzap: A SOTA KV Cache Pruning Method that Delivers near-Lossless 2x-4x Compression appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/15/nvidia-ai-open-sourced-kvzap-a-sota-kv-cache-pruning-method-that-delivers-near-lossless-2x-4x-compression/",
      "author": "Asif Razzaq",
      "published": "2026-01-15T21:12:26",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Machine Learning",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "NVIDIA AI open-sourced KVzap, a KV cache pruning method achieving near-lossless 2x-4x compression for transformer models. The technique addresses deployment bottlenecks as context lengths reach hundreds of thousands of tokens.",
      "importance_score": 64.0,
      "reasoning": "Useful technical advancement for LLM efficiency, open-source release makes it accessible. Important for scaling but incremental improvement.",
      "themes": [
        "NVIDIA",
        "Open Source",
        "LLM Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA AI open-sourced KVzap, a KV cache pruning method achieving near-lossless 2x-4x compression for transformer models. The technique addresses deployment bottlenecks as context lengths reach hundreds of thousands of tokens.</p>",
      "content_html": "<p>As context lengths move into tens and hundreds of thousands of tokens, the key value cache in transformer decoders becomes a primary deployment bottleneck. The cache stores keys and values for every layer and head with shape (2, L, H, T, D). For a vanilla transformer such as Llama1-65B, the cache reaches about 335 GB at 128k tokens in bfloat16, which directly limits batch size and increases time to first token.</p>\n<p>https://arxiv.org/pdf/2601.07891</p>\n<p>Architectural compression leaves the sequence axis untouched</p>\n<p>Production models already compress the cache along several axes. Grouped Query Attention shares keys and values across multiple queries and yields compression factors of 4 in Llama3, 12 in GLM 4.5 and up to 16 in Qwen3-235B-A22B, all along the head axis. DeepSeek V2 compresses the key and value dimension through Multi head Latent Attention. Hybrid models mix attention with sliding window attention or state space layers to reduce the number of layers that maintain a full cache.</p>\n<p>These changes do not compress along the sequence axis. Sparse and retrieval style attention retrieve only a subset of the cache at each decoding step, but all tokens still occupy memory. Practical long context serving therefore needs techniques that delete cache entries which will have negligible effect on future tokens.</p>\n<p>The KVpress project from NVIDIA collects more than twenty such pruning methods in one codebase and exposes them through a public leaderboard on Hugging Face. Methods such as H2O, Expected Attention, DuoAttention, Compactor and KVzip are all evaluated in a consistent way.</p>\n<p>KVzip and KVzip plus as the scoring oracle</p>\n<p>KVzip is currently the strongest cache pruning baseline on the KVpress Leaderboard. It defines an importance score for each cache entry using a copy and paste pretext task. The model runs on an extended prompt where it is asked to repeat the original context exactly. For each token position in the original prompt, the score is the maximum attention weight that any position in the repeated segment assigns back to that token, across heads in the same group when grouped query attention is used. Low scoring entries are evicted until a global budget is met.</p>\n<p>KVzip+ refines this score. It multiplies the attention weight by the norm of the value contribution into the residual stream and normalizes by the norm of the receiving hidden state. This better matches the actual change that a token induces in the residual stream and improves correlation with downstream accuracy compared to the original score.</p>\n<p>These oracle scores are effective but expensive. KVzip requires prefilling on the extended prompt, which doubles the context length and makes it too slow for production. It also cannot run during decoding because the scoring procedure assumes a fixed prompt.</p>\n<p>https://arxiv.org/pdf/2601.07891</p>\n<p>KVzap, a surrogate model on hidden states</p>\n<p>KVzap replaces the oracle scoring with a small surrogate model that operates directly on hidden states. For each transformer layer and each sequence position t, the module receives the hidden vector hₜ and outputs predicted log scores for every key value head. Two architectures are considered, a single linear layer (KVzap Linear) and a two layer MLP with GELU and hidden width equal to one eighth of the model hidden size (KVzap MLP).</p>\n<p>Training uses prompts from the Nemotron Pretraining Dataset sample. The research team filter 27k prompts to lengths between 750 and 1,250 tokens, sample up to 500 prompts per subset, and then sample 500 token positions per prompt. For each key value head they obtain about 1.2 million training pairs and a validation set of 23k pairs. The surrogate learns to regress from the hidden state to the log KVzip+ score. Across models, the squared Pearson correlation between predictions and oracle scores reaches between about 0.63 and 0.77, with the MLP variant consistently outperforming the linear variant.</p>\n<p>https://arxiv.org/pdf/2601.07891</p>\n<p>Thresholding, sliding window and negligible overhead</p>\n<p>During inference, the KVzap model processes hidden states and produces scores for each cache entry. Entries with scores below a fixed threshold are pruned, while a sliding window of the most recent 128 tokens is always kept. The research team provides a concise PyTorch style function that applies the model, sets scores of the local window to infinity and returns compressed key and value tensors. In all experiments, pruning is applied after the attention operation.</p>\n<p>KVzap uses score thresholding rather than fixed top k selection. A single threshold yields different effective compression ratios on different benchmarks and even across prompts within the same benchmark. The research team report up to 20 percent variation in compression ratio across prompts at a fixed threshold, which reflects differences in information density.</p>\n<p>Compute overhead is small. An analysis at the layer level shows that the extra cost of KVzap MLP is at most about 1.1 percent of the linear projection FLOPs, while the linear variant adds about 0.02 percent. The relative memory overhead follows the same values. In long context regimes, the quadratic cost of attention dominates so the extra FLOPs are effectively negligible.</p>\n<p>https://arxiv.org/pdf/2601.07891</p>\n<p>Results on RULER, LongBench and AIME25</p>\n<p>KVzap is evaluated on long context and reasoning benchmarks using Qwen3-8B, Llama-3.1-8B Instruct and Qwen3-32B. Long context behavior is measured on RULER and LongBench. RULER uses synthetic tasks over sequence lengths from 4k to 128k tokens, while LongBench uses real world documents from multiple task categories. AIME25 provides a math reasoning workload with 30 Olympiad level problems evaluated under pass at 1 and pass at 4.</p>\n<p>On RULER, KVzap matches the full cache baseline within a small accuracy margin while removing a large fraction of the cache. For Qwen3-8B, the best KVzap configuration achieves a removed fraction above 0.7 on RULER 4k and 16k while keeping the average score within a few tenths of a point of the full cache. Similar behavior holds for Llama-3.1-8B Instruct and Qwen3-32B.</p>\n<p>On LongBench, the same thresholds lead to lower compression ratios because the documents are less repetitive. KVzap remains close to the full cache baseline up to about 2 to 3 times compression, while fixed budget methods such as Expected Attention degrade more on several subsets once compression increases.</p>\n<p>On AIME25, KVzap MLP maintains or slightly improves pass at 4 accuracy at compression near 2 times and remains usable even when discarding more than half of the cache. Extremely aggressive settings, for example linear variants at high thresholds that remove more than 90 percent of entries, collapse performance as expected.</p>\n<p>https://arxiv.org/pdf/2601.07891</p>\n<p>Overall, the above Table shows that the best KVzap configuration per model delivers average cache compression between roughly 2.7 and 3.5 while keeping task scores very close to the full cache baseline across RULER, LongBench and AIME25.</p>\n<p>Key Takeaways</p>\n<p>KVzap is an input adaptive approximation of KVzip+ that learns to predict oracle KV importance scores from hidden states using small per layer surrogate models, either a linear layer or a shallow MLP, and then prunes low score KV pairs.</p>\n<p>Training uses Nemotron pretraining prompts where KVzip+ provides supervision, producing about 1.2 million examples per head and achieving squared correlation in the 0.6 to 0.8 range between predicted and oracle scores, which is sufficient for faithful cache importance ranking.</p>\n<p>KVzap applies a global score threshold with a fixed sliding window of recent tokens, so compression automatically adapts to prompt information density, and the research team report up to 20 percent variation in achieved compression across prompts at the same threshold.</p>\n<p>Across Qwen3-8B, Llama-3.1-8B Instruct and Qwen3-32B on RULER, LongBench and AIME25, KVzap reaches about 2 to 4 times KV cache compression while keeping accuracy very close to the full cache, and it achieves state of the art tradeoffs on the NVIDIA KVpress Leaderboard.</p>\n<p>The additional compute is small, at most about 1.1 percent extra FLOPs for the MLP variant, and KVzap is implemented in the open source kvpress framework with ready to use checkpoints on Hugging Face, which makes it practical to integrate into existing long context LLM serving stacks.</p>\n<p>Check out the Paper and GitHub Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.</p>\n<p>The post NVIDIA AI Open-Sourced KVzap: A SOTA KV Cache Pruning Method that Delivers near-Lossless 2x-4x Compression appeared first on MarkTechPost.</p>"
    },
    {
      "id": "8e4c25fb14ea",
      "title": "Grok scandal highlights how AI industry is ‘too unconstrained’, tech pioneer says",
      "content": "‘Godfather of AI’ Yoshua Bengio says firms building powerful systems without appropriate guardrails • Musk’s X to block Grok AI from creating sexualised images of real people The scandal over the flood of intimate images on Elon Musk’s X created non-consensually by its Grok AI tool has underlined how the artificial intelligence industry is “too unconstrained”, according to a pioneer of the technology.Yoshua Bengio, a computer scientist described as one of the modern “godfathers of AI”, said tech companies were building systems without appropriate technical and societal guardrails. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/15/grok-scandal-ai-industry-too-unconstrained-yoshua-bengio-elon-musk",
      "author": "Dan Milmo Global technology editor",
      "published": "2026-01-15T11:57:17",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Grok AI",
        "X",
        "Elon Musk",
        "Technology sector",
        "Business",
        "Technology"
      ],
      "summary": "AI pioneer Yoshua Bengio called the AI industry 'too unconstrained' in response to the Grok image scandal, saying companies are building systems without appropriate technical and societal guardrails.",
      "importance_score": 63.0,
      "reasoning": "Notable expert commentary from 'godfather of AI' on industry safety practices, adds authority to safety concerns.",
      "themes": [
        "AI Safety",
        "Expert Opinion",
        "Regulation"
      ],
      "continuation": null,
      "summary_html": "<p>AI pioneer Yoshua Bengio called the AI industry 'too unconstrained' in response to the Grok image scandal, saying companies are building systems without appropriate technical and societal guardrails.</p>",
      "content_html": "<p>‘Godfather of AI’ Yoshua Bengio says firms building powerful systems without appropriate guardrails • Musk’s X to block Grok AI from creating sexualised images of real people The scandal over the flood of intimate images on Elon Musk’s X created non-consensually by its Grok AI tool has underlined how the artificial intelligence industry is “too unconstrained”, according to a pioneer of the technology.Yoshua Bengio, a computer scientist described as one of the modern “godfathers of AI”, said tech companies were building systems without appropriate technical and societal guardrails. Continue reading...</p>"
    },
    {
      "id": "d12584e0a853",
      "title": "Mother of one of Elon Musk’s sons sues over Grok-generated explicit images",
      "content": "Ashley St Clair files lawsuit in state of New York over deepfakes that appeared on social media platform X The mother of one of Elon Musk’s children is suing his company – alleging explicit images were generated of her by his Grok AI tool, including one in which she was underage.Ashley St Clair has filed a lawsuit with the supreme court of the state of New York against xAI, alleging that Grok, which is used on the social media platform X, promised to stop generating explicit images but continued to do so. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/15/mother-of-one-of-elon-musks-sons-sues-over-grok-generated-explicit-images",
      "author": "Helena Horton",
      "published": "2026-01-15T22:35:01",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Grok AI",
        "US news",
        "X",
        "AI (artificial intelligence)",
        "Technology",
        "Media",
        "Elon Musk",
        "Deepfake",
        "World news",
        "Internet safety",
        "Internet"
      ],
      "summary": "Ashley St Clair, mother of one of Elon Musk's children, filed a lawsuit against xAI alleging Grok generated explicit images of her including one depicting her as a minor. The suit was filed in New York state court.",
      "importance_score": 62.0,
      "reasoning": "Notable lawsuit with unusual personal connection to Musk, escalating legal pressure on AI image generation.",
      "themes": [
        "Legal",
        "AI Safety",
        "xAI",
        "Deepfakes"
      ],
      "continuation": null,
      "summary_html": "<p>Ashley St Clair, mother of one of Elon Musk's children, filed a lawsuit against xAI alleging Grok generated explicit images of her including one depicting her as a minor. The suit was filed in New York state court.</p>",
      "content_html": "<p>Ashley St Clair files lawsuit in state of New York over deepfakes that appeared on social media platform X The mother of one of Elon Musk’s children is suing his company – alleging explicit images were generated of her by his Grok AI tool, including one in which she was underage.Ashley St Clair has filed a lawsuit with the supreme court of the state of New York against xAI, alleging that Grok, which is used on the social media platform X, promised to stop generating explicit images but continued to do so. Continue reading...</p>"
    },
    {
      "id": "4fa3eff863a2",
      "title": "Musk’s X to block Grok AI tool from creating sexualised images of real people",
      "content": "UK government claims vindication after Keir Starmer criticised earlier decision to keep functionality as ‘horrific’The UK government has claimed “vindication” after Elon Musk’s X announced it had stopped its AI-powered Grok feature from editing pictures of real people to show them in revealing clothes such as bikinis, including for premium subscribers.After a fortnight of public outcry at the tool embedded into X being used to create sexualised images of women and children, the company said it would “geoblock” the ability of users “to generate images of real people in bikinis, underwear, and similar attire via the Grok account and in Grok in X”, in countries where it was illegal. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/14/elon-musk-grok-ai-explicit-images",
      "author": "Robert Booth and Dara Kerr",
      "published": "2026-01-15T09:19:12",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "X",
        "Elon Musk",
        "Grok AI",
        "Technology",
        "AI (artificial intelligence)",
        "US news",
        "World news"
      ],
      "summary": "X announced it will block Grok from creating sexualized images of real people in countries where it's illegal, following public backlash. UK government claimed 'vindication' after Starmer criticism.",
      "importance_score": 60.0,
      "reasoning": "Significant safety response from xAI but reactive measure to existing controversy rather than proactive improvement.",
      "themes": [
        "AI Safety",
        "xAI",
        "Content Moderation"
      ],
      "continuation": null,
      "summary_html": "<p>X announced it will block Grok from creating sexualized images of real people in countries where it's illegal, following public backlash. UK government claimed 'vindication' after Starmer criticism.</p>",
      "content_html": "<p>UK government claims vindication after Keir Starmer criticised earlier decision to keep functionality as ‘horrific’The UK government has claimed “vindication” after Elon Musk’s X announced it had stopped its AI-powered Grok feature from editing pictures of real people to show them in revealing clothes such as bikinis, including for premium subscribers.After a fortnight of public outcry at the tool embedded into X being used to create sexualised images of women and children, the company said it would “geoblock” the ability of users “to generate images of real people in bikinis, underwear, and similar attire via the Grok account and in Grok in X”, in countries where it was illegal. Continue reading...</p>"
    },
    {
      "id": "4c67d47531b1",
      "title": "Meeting the new ETSI standard for AI security",
      "content": "The ETSI EN 304 223 standard introduces baseline security requirements for AI that enterprises must integrate into governance frameworks.\n\n\n\nAs organisations embed machine learning into their core operations, this European Standard (EN) establishes concrete provisions for securing AI models and systems. It stands as the first globally applicable European Standard for AI cybersecurity, having secured formal approval from National Standards Organisations to strengthen its authority across international markets.\n\n\n\nThe standard serves as a necessary benchmark alongside the EU AI Act. It addresses the reality that AI systems possess specific risks – such as susceptibility to data poisoning, model obfuscation, and indirect prompt injection – that traditional software security measures often miss. The standard covers deep neural networks and generative AI through to basic predictive systems, explicitly excluding only those used strictly for academic research.\n\n\n\nETSI standard clarifies the chain of responsibility for AI security\n\n\n\nA persistent hurdle in enterprise AI adoption is determining who owns the risk. The ETSI standard resolves this by defining three primary technical roles: Developers, System Operators, and Data Custodians.\n\n\n\nFor many enterprises, these lines blur. A financial services firm that fine-tunes an open-source model for fraud detection counts as both a Developer and a System Operator. This dual status triggers strict obligations, requiring the firm to secure the deployment infrastructure while documenting the provenance of training data and the model’s design auditing.\n\n\n\nThe inclusion of ‘Data Custodians’ as a distinct stakeholder group directly impacts Chief Data and Analytics Officers (CDAOs). These entities control data permissions and integrity, a role that now carries explicit security responsibilities. Custodians must ensure that the intended usage of a system aligns with the sensitivity of the training data, effectively placing a security gatekeeper within the data management workflow.\n\n\n\nETSI&#8217;s AI standard makes clear that security cannot be an afterthought appended at the deployment stage. During the design phase, organisations must conduct threat modelling that addresses AI-native attacks, such as membership inference and model obfuscation.\n\n\n\nOne provision requires developers to restrict functionality to reduce the attack surface. For instance, if a system uses a multi-modal model but only requires text processing, the unused modalities (like image or audio processing) represent a risk that must be managed. This requirement forces technical leaders to reconsider the common practice of deploying massive, general-purpose foundation models where a smaller and more specialised model would suffice.\n\n\n\nThe document also enforces strict asset management. Developers and System Operators must maintain a comprehensive inventory of assets, including interdependencies and connectivity. This supports shadow AI discovery; IT leaders cannot secure models they do not know exist. The standard also requires the creation of specific disaster recovery plans tailored to AI attacks, ensuring that a &#8220;known good state&#8221; can be restored if a model is compromised.\n\n\n\nSupply chain security presents an immediate friction point for enterprises relying on third-party vendors or open-source repositories. The ETSI standard requires that if a System Operator chooses to use AI models or components that are not well-documented, they must justify that decision and document the associated security risks.\n\n\n\nPractically, procurement teams can no longer accept &#8220;black box&#8221; solutions. Developers are required to provide cryptographic hashes for model components to verify authenticity. Where training data is sourced publicly (a common practice for Large Language Models), developers must document the source URL and acquisition timestamp. This audit trail is necessary for post-incident investigations, particularly when attempting to identify if a model was subjected to data poisoning during its training phase.\n\n\n\nIf an enterprise offers an API to external customers, they must apply controls designed to mitigate AI-focused attacks, such as rate limiting to prevent adversaries from reverse-engineering the model or overwhelming defences to inject poison data.\n\n\n\nThe lifecycle approach extends into the maintenance phase, where the standard treats major updates – such as retraining on new data – as the deployment of a new version. Under the ETSI AI standard, this triggers a requirement for renewed security testing and evaluation.\n\n\n\nContinuous monitoring is also formalised. System Operators must analyse logs not just for uptime, but to detect &#8220;data drift&#8221; or gradual changes in behaviour that could indicate a security breach. This moves AI monitoring from a performance metric to a security discipline.\n\n\n\nThe standard also addresses the &#8220;End of Life&#8221; phase. When a model is decommissioned or transferred, organisations must involve Data Custodians to ensure the secure disposal of data and configuration details. This provision prevents the leakage of sensitive intellectual property or training data through discarded hardware or forgotten cloud instances.\n\n\n\nExecutive oversight and governance\n\n\n\nCompliance with ETSI EN 304 223 requires a review of existing cybersecurity training programmes. The standard mandates that training be tailored to specific roles, ensuring that developers understand secure coding for AI while general staff remain aware of threats like social engineering via AI outputs.\n\n\n\n“ETSI EN 304 223 represents an important step forward in establishing a common, rigorous foundation for securing AI systems&#8221;, said Scott Cadzow, Chair of ETSI&#8217;s Technical Committee for Securing Artificial Intelligence.\n\n\n\n“At a time when AI is being increasingly integrated into critical services and infrastructure, the availability of clear, practical guidance that reflects both the complexity of these technologies and the realities of deployment cannot be underestimated. The work that went into delivering this framework is the result of extensive collaboration and it means that organisations can have full confidence in AI systems that are resilient, trustworthy, and secure by design.”\n\n\n\nImplementing these baselines in ETSI&#8217;s AI security standard provides a structure for safer innovation. By enforcing documented audit trails, clear role definitions, and supply chain transparency, enterprises can mitigate the risks associated with AI adoption while establishing a defensible position for future regulatory audits.\n\n\n\nAn upcoming Technical Report (ETSI TR 104 159) will apply these principles specifically to generative AI, targeting issues like deepfakes and disinformation.\n\n\n\nSee also: Allister Frost: Tackling workforce anxiety for AI integration success\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Meeting the new ETSI standard for AI security appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/meeting-the-new-etsi-standard-for-ai-security/",
      "author": "Ryan Daws",
      "published": "2026-01-15T13:23:47",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Deep Dives",
        "Features",
        "Governance, Regulation & Policy",
        "Inside AI",
        "adoption",
        "ai",
        "developers",
        "enterprise",
        "ethics",
        "etsi",
        "frameworks",
        "governance",
        "security",
        "standards"
      ],
      "summary": "ETSI EN 304 223 establishes the first globally applicable European Standard for AI cybersecurity, addressing AI-specific risks like data poisoning and prompt injection that traditional security measures miss.",
      "importance_score": 60.0,
      "reasoning": "Important regulatory/standards development for AI security, though technical standards receive less attention than they warrant.",
      "themes": [
        "AI Security",
        "Standards",
        "Regulation"
      ],
      "continuation": null,
      "summary_html": "<p>ETSI EN 304 223 establishes the first globally applicable European Standard for AI cybersecurity, addressing AI-specific risks like data poisoning and prompt injection that traditional security measures miss.</p>",
      "content_html": "<p>The ETSI EN 304 223 standard introduces baseline security requirements for AI that enterprises must integrate into governance frameworks.</p>\n<p>As organisations embed machine learning into their core operations, this European Standard (EN) establishes concrete provisions for securing AI models and systems. It stands as the first globally applicable European Standard for AI cybersecurity, having secured formal approval from National Standards Organisations to strengthen its authority across international markets.</p>\n<p>The standard serves as a necessary benchmark alongside the EU AI Act. It addresses the reality that AI systems possess specific risks – such as susceptibility to data poisoning, model obfuscation, and indirect prompt injection – that traditional software security measures often miss. The standard covers deep neural networks and generative AI through to basic predictive systems, explicitly excluding only those used strictly for academic research.</p>\n<p>ETSI standard clarifies the chain of responsibility for AI security</p>\n<p>A persistent hurdle in enterprise AI adoption is determining who owns the risk. The ETSI standard resolves this by defining three primary technical roles: Developers, System Operators, and Data Custodians.</p>\n<p>For many enterprises, these lines blur. A financial services firm that fine-tunes an open-source model for fraud detection counts as both a Developer and a System Operator. This dual status triggers strict obligations, requiring the firm to secure the deployment infrastructure while documenting the provenance of training data and the model’s design auditing.</p>\n<p>The inclusion of ‘Data Custodians’ as a distinct stakeholder group directly impacts Chief Data and Analytics Officers (CDAOs). These entities control data permissions and integrity, a role that now carries explicit security responsibilities. Custodians must ensure that the intended usage of a system aligns with the sensitivity of the training data, effectively placing a security gatekeeper within the data management workflow.</p>\n<p>ETSI&#8217;s AI standard makes clear that security cannot be an afterthought appended at the deployment stage. During the design phase, organisations must conduct threat modelling that addresses AI-native attacks, such as membership inference and model obfuscation.</p>\n<p>One provision requires developers to restrict functionality to reduce the attack surface. For instance, if a system uses a multi-modal model but only requires text processing, the unused modalities (like image or audio processing) represent a risk that must be managed. This requirement forces technical leaders to reconsider the common practice of deploying massive, general-purpose foundation models where a smaller and more specialised model would suffice.</p>\n<p>The document also enforces strict asset management. Developers and System Operators must maintain a comprehensive inventory of assets, including interdependencies and connectivity. This supports shadow AI discovery; IT leaders cannot secure models they do not know exist. The standard also requires the creation of specific disaster recovery plans tailored to AI attacks, ensuring that a &#8220;known good state&#8221; can be restored if a model is compromised.</p>\n<p>Supply chain security presents an immediate friction point for enterprises relying on third-party vendors or open-source repositories. The ETSI standard requires that if a System Operator chooses to use AI models or components that are not well-documented, they must justify that decision and document the associated security risks.</p>\n<p>Practically, procurement teams can no longer accept &#8220;black box&#8221; solutions. Developers are required to provide cryptographic hashes for model components to verify authenticity. Where training data is sourced publicly (a common practice for Large Language Models), developers must document the source URL and acquisition timestamp. This audit trail is necessary for post-incident investigations, particularly when attempting to identify if a model was subjected to data poisoning during its training phase.</p>\n<p>If an enterprise offers an API to external customers, they must apply controls designed to mitigate AI-focused attacks, such as rate limiting to prevent adversaries from reverse-engineering the model or overwhelming defences to inject poison data.</p>\n<p>The lifecycle approach extends into the maintenance phase, where the standard treats major updates – such as retraining on new data – as the deployment of a new version. Under the ETSI AI standard, this triggers a requirement for renewed security testing and evaluation.</p>\n<p>Continuous monitoring is also formalised. System Operators must analyse logs not just for uptime, but to detect &#8220;data drift&#8221; or gradual changes in behaviour that could indicate a security breach. This moves AI monitoring from a performance metric to a security discipline.</p>\n<p>The standard also addresses the &#8220;End of Life&#8221; phase. When a model is decommissioned or transferred, organisations must involve Data Custodians to ensure the secure disposal of data and configuration details. This provision prevents the leakage of sensitive intellectual property or training data through discarded hardware or forgotten cloud instances.</p>\n<p>Executive oversight and governance</p>\n<p>Compliance with ETSI EN 304 223 requires a review of existing cybersecurity training programmes. The standard mandates that training be tailored to specific roles, ensuring that developers understand secure coding for AI while general staff remain aware of threats like social engineering via AI outputs.</p>\n<p>“ETSI EN 304 223 represents an important step forward in establishing a common, rigorous foundation for securing AI systems&#8221;, said Scott Cadzow, Chair of ETSI&#8217;s Technical Committee for Securing Artificial Intelligence.</p>\n<p>“At a time when AI is being increasingly integrated into critical services and infrastructure, the availability of clear, practical guidance that reflects both the complexity of these technologies and the realities of deployment cannot be underestimated. The work that went into delivering this framework is the result of extensive collaboration and it means that organisations can have full confidence in AI systems that are resilient, trustworthy, and secure by design.”</p>\n<p>Implementing these baselines in ETSI&#8217;s AI security standard provides a structure for safer innovation. By enforcing documented audit trails, clear role definitions, and supply chain transparency, enterprises can mitigate the risks associated with AI adoption while establishing a defensible position for future regulatory audits.</p>\n<p>An upcoming Technical Report (ETSI TR 104 159) will apply these principles specifically to generative AI, targeting issues like deepfakes and disinformation.</p>\n<p>See also: Allister Frost: Tackling workforce anxiety for AI integration success</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Meeting the new ETSI standard for AI security appeared first on AI News.</p>"
    },
    {
      "id": "8a402a5b9ec1",
      "title": "The Real AI Talent War Is for Plumbers and Electricians",
      "content": "The AI boom is driving an unprecedented wave of data center construction, but there aren’t enough skilled tradespeople in the US to keep up.",
      "url": "https://www.wired.com/story/why-there-arent-enough-electricians-and-plumbers-to-build-ai-data-centers/",
      "author": "Caroline Haskins",
      "published": "2026-01-15T11:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "artificial intelligence",
        "Labor",
        "data centers",
        "Training",
        "electricity",
        "Skill Issue"
      ],
      "summary": "The AI data center construction boom is facing critical shortages of skilled tradespeople including electricians and plumbers. The bottleneck threatens to slow AI infrastructure expansion in the US.",
      "importance_score": 58.0,
      "reasoning": "Important infrastructure constraint but more of an industry analysis than breakthrough news.",
      "themes": [
        "AI Infrastructure",
        "Labor",
        "Data Centers"
      ],
      "continuation": null,
      "summary_html": "<p>The AI data center construction boom is facing critical shortages of skilled tradespeople including electricians and plumbers. The bottleneck threatens to slow AI infrastructure expansion in the US.</p>",
      "content_html": "<p>The AI boom is driving an unprecedented wave of data center construction, but there aren’t enough skilled tradespeople in the US to keep up.</p>"
    },
    {
      "id": "91f3f55bde4b",
      "title": "Grok AI: what do limits on tool mean for X, its users and UK media watchdog?",
      "content": "UK users will no longer be able to create sexualised images of real people using @Grok X account, with Grok app also expected to be restrictedElon Musk’s X has announced it will stop the Grok AI tool from allowing users to manipulate images of people to show them in revealing clothing such as bikinis.The furore over Grok, which is integrated with the X platform, has sparked a public and political backlash as well as a formal investigation by Ofcom, the UK’s communications watchdog. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/15/grok-ai-images-uk-limits-x-app-ofcom",
      "author": "Dan Milmo Global technology editor",
      "published": "2026-01-15T16:07:11",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Grok AI",
        "X",
        "Ofcom",
        "Social media",
        "AI (artificial intelligence)",
        "Internet safety",
        "UK news",
        "Technology",
        "Media"
      ],
      "summary": "UK users will be blocked from creating sexualized images of real people using Grok on X, following Ofcom investigation. The Grok app will also face restrictions.",
      "importance_score": 58.0,
      "reasoning": "Regulatory response to ongoing Grok safety issues, shows UK enforcement in action.",
      "themes": [
        "AI Regulation",
        "xAI",
        "UK Policy"
      ],
      "continuation": null,
      "summary_html": "<p>UK users will be blocked from creating sexualized images of real people using Grok on X, following Ofcom investigation. The Grok app will also face restrictions.</p>",
      "content_html": "<p>UK users will no longer be able to create sexualised images of real people using @Grok X account, with Grok app also expected to be restrictedElon Musk’s X has announced it will stop the Grok AI tool from allowing users to manipulate images of people to show them in revealing clothing such as bikinis.The furore over Grok, which is integrated with the X platform, has sparked a public and political backlash as well as a formal investigation by Ofcom, the UK’s communications watchdog. Continue reading...</p>"
    },
    {
      "id": "6905e49de47b",
      "title": "Three Indian IT Giants, Three AI Playbooks",
      "content": "\nThree earnings reports. Three approaches to artificial intelligence. And one clear signal that Indian IT has moved beyond experimentation.&nbsp;\n\n\n\nIn the December quarter, TCS, HCLTech and Infosys all show that AI now drives real business. The difference lies in how they report.&nbsp;\n\n\n\nTCS reported $1.8 billion in annualised AI revenue in Q3. HCLTech showed a $146 million advanced AI business growing nearly 20% in one quarter. Infosys, instead, is measuring what it calls AI impact.\n\n\n\nOn its Q3 earnings call, Infosys CEO Salil Parekh detailed the AI footprint, mentioning 4,600 active AI projects, over 500 AI agents, and nearly 28 million lines of code created with AI tools.&nbsp;\n\n\n\nThe company claims that around 90% of its top 200 clients are now running AI programmes.&nbsp;\n\n\n\nYet there are no AI revenue numbers. By comparison, TCS reported $1.8 billion in annualised AI revenue for Q3. HCLTech reported an advanced AI business of $146 million, which grew nearly 20% quarter-on-quarter. Infosys chooses to report AI impact.\n\n\n\n“We are using agents in several of our service lines to help enhance either growth or productivity. So that’s what we are sharing, in terms of what our impact is,” Parekh said.\n\n\n\nA Deliberate Choice&nbsp;\n\n\n\nThis approach is not evasive. It reflects how Infosys positions AI. In Q3, the company integrated Cognition’s AI software engineer Devin into its Infosys Topaz Fabric. This turns Topaz from a toolset into a system that can deploy, manage, and govern AI agents inside live enterprise environments.\n\n\n\nParekh says the Cognition partnership helps Infosys deliver software agents directly into client systems. For Infosys, AI is part of the delivery, not an add-on.&nbsp;\n\n\n\nGaurav Vasu from UnearthInsight, a market intelligence firm, tells AIM that as AI matures, it is increasingly being embedded across the full spectrum of enterprise technology work, including transformation programs, application development and modernisation (ADM), ER&amp;D engagements, and managed services.\n\n\n\nThat strategy shows up in deal flow. Infosys closed $4.8 billion in large deal wins in Q3, up sharply from $3.1 billion in Q2. AI-led modernisation, automation, and agent deployment form part of the core scope of work.\n\n\n\nThis is where the comparison with peers matters.&nbsp;\n\n\n\nGreyhound Research calls this a deliberate choice. Sanchit Vir Gogia, the founder, says Infosys is avoiding the trap of treating AI as a fragile line item. “AI is being positioned as a horizontal capability, not a vertical business line. It is infused across build, modernise, test, and run. That makes revenue attribution not only difficult but potentially misleading,” Gogia told AIM.\n\n\n\nAccenture took a similar approach. Vasu draws a direct line to earlier tech cycles.&nbsp;\n\n\n\n“This mirrors earlier technology cycles such as cloud, automation, and DevOps, where initial standalone reporting gradually gave way to embedded delivery models,” Vasu says.&nbsp;\n\n\n\nVasu argues that much of what the industry is calling &#8220;AI revenue&#8221; is already buried in existing contracts. “AI components are bundled within broader transformation or run engagements, and pricing reflects outcome improvement rather than tool usage,” he says. In many deals, AI is assumed as a default capability rather than a premium add-on.\n\n\n\nThat makes a clean AI revenue line harder to defend as AI becomes normalised across delivery.\n\n\n\nMetrics Matter More Than Revenue\n\n\n\nThere is also a commercial logic. Gogia said that once a company publishes an AI revenue number, it must defend its definition and growth every quarter. Any change creates confusion.&nbsp;\n\n\n\nInfosys instead shows the delivery metrics clients care about. Projects, agents, code, and penetration.\n\n\n\nGreyhound’s enterprise fieldwork backs that up. Procurement teams are increasingly rejecting headline AI revenue claims in RFPs. They ask for code-level auditability, faster release cycles, fewer defects and lower infrastructure costs after AI refactoring.\n\n\n\nInfosys’ disclosure aligns with that demand. Gogia points to the 28 million lines of AI-generated code that require strict controls. Infosys embeds policy gates, human review, and provenance tracking into its AI workflows. This matters in regulated industries where a single error can carry high risk.&nbsp;\n\n\n\nVasu says, the real impact will be seen in execution, not as a flashy revenue line. “The impact going forward is primarily around speed, scale, and productivity.” AI-assisted coding lets Infosys push more work through the same programs faster, which supports margins without linear hiring.\n\n\n\nThis model also reshapes pricing. When AI cuts delivery effort by 30%, clients resist old pricing models. Outcome-based contracts and shared productivity gains become the norm. Infosys prepares for that shift by tying AI to business KPIs rather than billed hours.\n\n\n\nGartner sees the same pattern. Biswajit Maity, senior principal analyst at Gartner, said Infosys’ Q3 showed a robust AI strategy built around enterprise-wide adoption, not isolated use cases. “Infosys focuses on generative and agentic AI across entire enterprises,” he says. Products like Topaz Fabric, Maity added, now support more granular and AI-driven delivery.\n\n\n\nVisible in the Numbers\n\n\n\nThe financial trade-off reflects the transition. Infosys reported Q3 revenue of ₹45,479 crore, up 8.9% year on year, adding nearly ₹1,000 crore in a single quarter. Net profit fell 2.2% to ₹6,654 crore as margins came under pressure from higher costs and the new labour codes.&nbsp;\n\n\n\nTCS, by contrast, maintained margins above 25% while increasing AI revenue. HCLTech took a margin hit from restructuring but is seeing fast growth in advanced AI. Infosys sits between the two. It spends on building platforms, agents, and governance that will pay off in the long run.\n\n\n\nHiring trends reinforce this. Infosys added 5,043 employees in Q3.&nbsp; TCS cut over 11,000 jobs. HCLTech trimmed staff. Greyhound sees this as demand-driven rather than denial. Large, long-term deals still need people. The difference lies in roles. The delivery pyramid now rebuilds around AI oversight, testing, risk and workflow design.&nbsp;\n\n\n\nVasu notes that AI tools speed up tasks like coding and testing, but large enterprises still rely heavily on system integration, legacy modernisation, security, compliance, and change management.&nbsp;\n\n\n\nThose are still people-heavy. In this model, “AI shortens timelines and raises throughput, allowing teams to deliver more with incremental headcount,” He says. Freshers no longer join to write boilerplate code. They join to manage systems that do.&nbsp;\n\n\n\nIndian IT has cracked AI monetisation. All three IT firms simply choose to measure it differently.\nThe post Three Indian IT Giants, Three AI Playbooks appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/it-services/three-it-giants-three-ai-playbooks/",
      "author": "Mohit Pandey",
      "published": "2026-01-15T12:54:37",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "IT Services"
      ],
      "summary": "TCS reported $1.8B in annualized AI revenue, HCLTech showed $146M in advanced AI business growing 20% quarterly, and Infosys reported 4,600 active AI projects. The reports show AI driving real revenue for Indian IT giants.",
      "importance_score": 58.0,
      "reasoning": "Useful data on enterprise AI adoption and revenue but regional industry report rather than frontier development.",
      "themes": [
        "Enterprise AI",
        "India",
        "Revenue"
      ],
      "continuation": null,
      "summary_html": "<p>TCS reported $1.8B in annualized AI revenue, HCLTech showed $146M in advanced AI business growing 20% quarterly, and Infosys reported 4,600 active AI projects. The reports show AI driving real revenue for Indian IT giants.</p>",
      "content_html": "<p>Three earnings reports. Three approaches to artificial intelligence. And one clear signal that Indian IT has moved beyond experimentation.&nbsp;</p>\n<p>In the December quarter, TCS, HCLTech and Infosys all show that AI now drives real business. The difference lies in how they report.&nbsp;</p>\n<p>TCS reported $1.8 billion in annualised AI revenue in Q3. HCLTech showed a $146 million advanced AI business growing nearly 20% in one quarter. Infosys, instead, is measuring what it calls AI impact.</p>\n<p>On its Q3 earnings call, Infosys CEO Salil Parekh detailed the AI footprint, mentioning 4,600 active AI projects, over 500 AI agents, and nearly 28 million lines of code created with AI tools.&nbsp;</p>\n<p>The company claims that around 90% of its top 200 clients are now running AI programmes.&nbsp;</p>\n<p>Yet there are no AI revenue numbers. By comparison, TCS reported $1.8 billion in annualised AI revenue for Q3. HCLTech reported an advanced AI business of $146 million, which grew nearly 20% quarter-on-quarter. Infosys chooses to report AI impact.</p>\n<p>“We are using agents in several of our service lines to help enhance either growth or productivity. So that’s what we are sharing, in terms of what our impact is,” Parekh said.</p>\n<p>A Deliberate Choice&nbsp;</p>\n<p>This approach is not evasive. It reflects how Infosys positions AI. In Q3, the company integrated Cognition’s AI software engineer Devin into its Infosys Topaz Fabric. This turns Topaz from a toolset into a system that can deploy, manage, and govern AI agents inside live enterprise environments.</p>\n<p>Parekh says the Cognition partnership helps Infosys deliver software agents directly into client systems. For Infosys, AI is part of the delivery, not an add-on.&nbsp;</p>\n<p>Gaurav Vasu from UnearthInsight, a market intelligence firm, tells AIM that as AI matures, it is increasingly being embedded across the full spectrum of enterprise technology work, including transformation programs, application development and modernisation (ADM), ER&amp;D engagements, and managed services.</p>\n<p>That strategy shows up in deal flow. Infosys closed $4.8 billion in large deal wins in Q3, up sharply from $3.1 billion in Q2. AI-led modernisation, automation, and agent deployment form part of the core scope of work.</p>\n<p>This is where the comparison with peers matters.&nbsp;</p>\n<p>Greyhound Research calls this a deliberate choice. Sanchit Vir Gogia, the founder, says Infosys is avoiding the trap of treating AI as a fragile line item. “AI is being positioned as a horizontal capability, not a vertical business line. It is infused across build, modernise, test, and run. That makes revenue attribution not only difficult but potentially misleading,” Gogia told AIM.</p>\n<p>Accenture took a similar approach. Vasu draws a direct line to earlier tech cycles.&nbsp;</p>\n<p>“This mirrors earlier technology cycles such as cloud, automation, and DevOps, where initial standalone reporting gradually gave way to embedded delivery models,” Vasu says.&nbsp;</p>\n<p>Vasu argues that much of what the industry is calling &#8220;AI revenue&#8221; is already buried in existing contracts. “AI components are bundled within broader transformation or run engagements, and pricing reflects outcome improvement rather than tool usage,” he says. In many deals, AI is assumed as a default capability rather than a premium add-on.</p>\n<p>That makes a clean AI revenue line harder to defend as AI becomes normalised across delivery.</p>\n<p>Metrics Matter More Than Revenue</p>\n<p>There is also a commercial logic. Gogia said that once a company publishes an AI revenue number, it must defend its definition and growth every quarter. Any change creates confusion.&nbsp;</p>\n<p>Infosys instead shows the delivery metrics clients care about. Projects, agents, code, and penetration.</p>\n<p>Greyhound’s enterprise fieldwork backs that up. Procurement teams are increasingly rejecting headline AI revenue claims in RFPs. They ask for code-level auditability, faster release cycles, fewer defects and lower infrastructure costs after AI refactoring.</p>\n<p>Infosys’ disclosure aligns with that demand. Gogia points to the 28 million lines of AI-generated code that require strict controls. Infosys embeds policy gates, human review, and provenance tracking into its AI workflows. This matters in regulated industries where a single error can carry high risk.&nbsp;</p>\n<p>Vasu says, the real impact will be seen in execution, not as a flashy revenue line. “The impact going forward is primarily around speed, scale, and productivity.” AI-assisted coding lets Infosys push more work through the same programs faster, which supports margins without linear hiring.</p>\n<p>This model also reshapes pricing. When AI cuts delivery effort by 30%, clients resist old pricing models. Outcome-based contracts and shared productivity gains become the norm. Infosys prepares for that shift by tying AI to business KPIs rather than billed hours.</p>\n<p>Gartner sees the same pattern. Biswajit Maity, senior principal analyst at Gartner, said Infosys’ Q3 showed a robust AI strategy built around enterprise-wide adoption, not isolated use cases. “Infosys focuses on generative and agentic AI across entire enterprises,” he says. Products like Topaz Fabric, Maity added, now support more granular and AI-driven delivery.</p>\n<p>Visible in the Numbers</p>\n<p>The financial trade-off reflects the transition. Infosys reported Q3 revenue of ₹45,479 crore, up 8.9% year on year, adding nearly ₹1,000 crore in a single quarter. Net profit fell 2.2% to ₹6,654 crore as margins came under pressure from higher costs and the new labour codes.&nbsp;</p>\n<p>TCS, by contrast, maintained margins above 25% while increasing AI revenue. HCLTech took a margin hit from restructuring but is seeing fast growth in advanced AI. Infosys sits between the two. It spends on building platforms, agents, and governance that will pay off in the long run.</p>\n<p>Hiring trends reinforce this. Infosys added 5,043 employees in Q3.&nbsp; TCS cut over 11,000 jobs. HCLTech trimmed staff. Greyhound sees this as demand-driven rather than denial. Large, long-term deals still need people. The difference lies in roles. The delivery pyramid now rebuilds around AI oversight, testing, risk and workflow design.&nbsp;</p>\n<p>Vasu notes that AI tools speed up tasks like coding and testing, but large enterprises still rely heavily on system integration, legacy modernisation, security, compliance, and change management.&nbsp;</p>\n<p>Those are still people-heavy. In this model, “AI shortens timelines and raises throughput, allowing teams to deliver more with incremental headcount,” He says. Freshers no longer join to write boilerplate code. They join to manage systems that do.&nbsp;</p>\n<p>Indian IT has cracked AI monetisation. All three IT firms simply choose to measure it differently.</p>\n<p>The post Three Indian IT Giants, Three AI Playbooks appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "4753f77cbd34",
      "title": "Elon Musk Admits Anthropic Has Done ‘Something Special With Coding’ Despite xAI Being Blocked",
      "content": "\nElon Musk, the founder and CEO of xAI, said in a post on X that the company’s upcoming model, Grok 4.2, will be better than Anthropic’s Claude Opus 4.5 in several aspects — but “not quite in programming.”A few days ago, Musk responded to a user on X who praised Claude Opus 4.5’s benchmark performance, saying that Grok “might do better” in its next iteration, Grok 4.2. He later clarified that while Grok 4.2 could surpass Claude in several areas, coding would not be one of them.\n\n\n\n“Anthropic has done something special with coding,” Musk said.&nbsp;\n\n\n\nSurprisingly, Musk’s praise for Anthropic comes at a time when the company has blocked xAI’s access to Claude models.&nbsp;\n\n\n\nRecently, Anthropic restricted the use of its AI models by rival labs, including xAI. According to journalist Kylie Robinson’s post on X, xAI staff used Anthropic’s AI models internally via the Cursor platform until Anthropic cut off access.&nbsp;\n\n\n\nRobinson cited an internal message she viewed from xAI co-founder Tony Wu sent to the team, which revealed that, “I believe many of you have already discovered that anthropic models are not responding on cursor. According to Cursor, this is a new policy anthropic is enforcing for all its major competitors.”\n\n\n\n“This is both bad and good news. We will get a hit on productivity, but it [really] pushes us to develop our own coding product/models,” Wu appeared to add in the memo.&nbsp;\n\n\n\nCurrently, Anthropic’s Claude Opus 4.5 model leads several benchmarks on coding evaluations, outperforming xAI’s Grok 4 and Grok 4.1 models, and even the company’s coding-focused model, Grok Code.&nbsp;\n\n\n\nThat said, Musk stated last week that the Grok Code is set to receive a major upgrade next month. “It will one-shot many complex coding tasks.”\nThe post Elon Musk Admits Anthropic Has Done ‘Something Special With Coding’ Despite xAI Being Blocked appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/elon-musk-admits-anthropic-has-done-something-special-with-coding-despite-xai-being-blocked/",
      "author": "Supreeth Koundinya",
      "published": "2026-01-15T11:10:17",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "Elon Musk acknowledged that Anthropic has 'done something special with coding' and that xAI's upcoming Grok 4.2 won't match Claude in programming, despite potentially surpassing it in other areas.",
      "importance_score": 56.0,
      "reasoning": "Notable competitive acknowledgment from Musk regarding Anthropic's technical advantage, interesting given xAI being blocked from Claude access.",
      "themes": [
        "Competition",
        "Anthropic",
        "xAI",
        "Coding"
      ],
      "continuation": null,
      "summary_html": "<p>Elon Musk acknowledged that Anthropic has 'done something special with coding' and that xAI's upcoming Grok 4.2 won't match Claude in programming, despite potentially surpassing it in other areas.</p>",
      "content_html": "<p>Elon Musk, the founder and CEO of xAI, said in a post on X that the company’s upcoming model, Grok 4.2, will be better than Anthropic’s Claude Opus 4.5 in several aspects — but “not quite in programming.”A few days ago, Musk responded to a user on X who praised Claude Opus 4.5’s benchmark performance, saying that Grok “might do better” in its next iteration, Grok 4.2. He later clarified that while Grok 4.2 could surpass Claude in several areas, coding would not be one of them.</p>\n<p>“Anthropic has done something special with coding,” Musk said.&nbsp;</p>\n<p>Surprisingly, Musk’s praise for Anthropic comes at a time when the company has blocked xAI’s access to Claude models.&nbsp;</p>\n<p>Recently, Anthropic restricted the use of its AI models by rival labs, including xAI. According to journalist Kylie Robinson’s post on X, xAI staff used Anthropic’s AI models internally via the Cursor platform until Anthropic cut off access.&nbsp;</p>\n<p>Robinson cited an internal message she viewed from xAI co-founder Tony Wu sent to the team, which revealed that, “I believe many of you have already discovered that anthropic models are not responding on cursor. According to Cursor, this is a new policy anthropic is enforcing for all its major competitors.”</p>\n<p>“This is both bad and good news. We will get a hit on productivity, but it [really] pushes us to develop our own coding product/models,” Wu appeared to add in the memo.&nbsp;</p>\n<p>Currently, Anthropic’s Claude Opus 4.5 model leads several benchmarks on coding evaluations, outperforming xAI’s Grok 4 and Grok 4.1 models, and even the company’s coding-focused model, Grok Code.&nbsp;</p>\n<p>That said, Musk stated last week that the Grok Code is set to receive a major upgrade next month. “It will one-shot many complex coding tasks.”</p>\n<p>The post Elon Musk Admits Anthropic Has Done ‘Something Special With Coding’ Despite xAI Being Blocked appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "74064522ff5e",
      "title": "Elon Musk’s Grok ‘Undressing’ Problem Isn’t Fixed",
      "content": "X has placed more restrictions on Grok’s ability to generate explicit AI images, but tests show that the updates have created a patchwork of limitations that fail to fully address the issue.",
      "url": "https://www.wired.com/story/elon-musks-grok-undressing-problem-isnt-fixed/",
      "author": "Matt Burgess",
      "published": "2026-01-15T19:30:14",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Security",
        "Security / Privacy",
        "Security / Security News",
        "Business / Artificial Intelligence",
        "Business / Social Media",
        "artificial intelligence",
        "Elon Musk",
        "algorithms",
        "Social Media",
        "xAI",
        "X",
        "privacy",
        "Half Measures"
      ],
      "summary": "Despite X implementing restrictions on Grok's explicit image generation, tests show the updates created a patchwork of limitations that fail to fully address the issue. Users can still circumvent safety measures.",
      "importance_score": 55.0,
      "reasoning": "Ongoing safety concern but incremental coverage of existing issue rather than new development.",
      "themes": [
        "AI Safety",
        "xAI",
        "Content Moderation"
      ],
      "continuation": null,
      "summary_html": "<p>Despite X implementing restrictions on Grok's explicit image generation, tests show the updates created a patchwork of limitations that fail to fully address the issue. Users can still circumvent safety measures.</p>",
      "content_html": "<p>X has placed more restrictions on Grok’s ability to generate explicit AI images, but tests show that the updates have created a patchwork of limitations that fail to fully address the issue.</p>"
    },
    {
      "id": "2f869549bfe3",
      "title": "Google Launches Market Access Programme to Scale Indian AI startups",
      "content": "\nGoogle has announced a new market access initiative and two open-source AI models to support Indian startups from early-stage development to global scale at the Google AI Startups Conclave held in the national capital.\n\n\n\nGoogle Market Access Program aims to help Indian AI startups transition from pilot projects to long-term enterprise contracts.\n\n\n\n“Indian startups are building serious deep technology and solving population-scale problems with AI,” said Preeti Lobana, VP and country manager for India at Google, in a statement.&nbsp;\n\n\n\nShe said that although the journey from labs to prototypes has improved over the past few years, many startups continue to struggle with scaling, a gap the Google Market Access Program seeks to address.\n\n\n\nThe programme targets AI-first startups that have moved beyond the prototype stage and are preparing to scale. It will focus on enterprise readiness through structured training on global enterprise sales, pricing, and buyer behaviour, alongside facilitated introductions to Google’s global network of CIOs and CXOs.&nbsp;\n\n\n\nThe initiative also includes international immersion programmes in partnership with ecosystem organisations, including TiE Silicon Valley and Alteus. Applications for the programme are now open.\n\n\n\nGoogle also recently announced new additions to its Gemma open model family to support healthcare and on-device AI development.\n\n\n\nOne of the releases, MedGemma 1.5, is a 4-billion-parameter open-source model for medical AI applications. It supports high-dimensional medical imaging workflows, including CT and MRI scans, whole-slide histopathology, longitudinal chest X-ray analysis, anatomical localisation, and extraction of information from medical lab reports.\n\n\n\nThe model builds on Google’s Health AI Developer Foundations programme and follows its collaboration with All India Institute of Medical Sciences, which is using MedGemma to develop health foundation models as part of the country’s Digital Public Infrastructure.\n\n\n\nGoogle also introduced FunctionGemma, a lightweight variant of the Gemma 3 270M model optimised for function calling and on-device AI agents. The model enables applications to convert natural language commands into executable actions locally, allowing AI systems to operate with low latency, limited connectivity, and enhanced user privacy.\n\n\n\nFunctionGemma can be fine-tuned using tools such as Hugging Face Transformers, Keras, and NVIDIA NeMo, and deployed across environments including LiteRT-LM, vLLM, Llama.cpp, and Vertex AI.\n\n\n\nGoogle said these efforts complement its ongoing investments in India’s AI infrastructure, including the Global AI Hub in Visakhapatnam, which provides a one-gigawatt compute foundation powered by green energy and Google’s AI chips.\n\n\n\nThe company also reiterated its focus on data availability, citing progress on Project Vaani in collaboration with the Indian Institute of Science. The initiative has released more than 27,000 hours of speech data across over 100 Indic languages through the government’s Bhashini platform.\nThe post Google Launches Market Access Programme to Scale Indian AI startups appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/google-launches-market-access-programme-to-scale-indian-ai-startups/",
      "author": "Siddharth Jindal",
      "published": "2026-01-15T08:34:58",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News",
        "Google"
      ],
      "summary": "Google launched a Market Access Program in India to help AI startups transition from pilots to enterprise contracts, alongside two open-source AI models targeting Indian developers.",
      "importance_score": 53.0,
      "reasoning": "Regional business initiative; useful for Indian AI ecosystem but limited global significance.",
      "themes": [
        "Google",
        "India",
        "Startups"
      ],
      "continuation": null,
      "summary_html": "<p>Google launched a Market Access Program in India to help AI startups transition from pilots to enterprise contracts, alongside two open-source AI models targeting Indian developers.</p>",
      "content_html": "<p>Google has announced a new market access initiative and two open-source AI models to support Indian startups from early-stage development to global scale at the Google AI Startups Conclave held in the national capital.</p>\n<p>Google Market Access Program aims to help Indian AI startups transition from pilot projects to long-term enterprise contracts.</p>\n<p>“Indian startups are building serious deep technology and solving population-scale problems with AI,” said Preeti Lobana, VP and country manager for India at Google, in a statement.&nbsp;</p>\n<p>She said that although the journey from labs to prototypes has improved over the past few years, many startups continue to struggle with scaling, a gap the Google Market Access Program seeks to address.</p>\n<p>The programme targets AI-first startups that have moved beyond the prototype stage and are preparing to scale. It will focus on enterprise readiness through structured training on global enterprise sales, pricing, and buyer behaviour, alongside facilitated introductions to Google’s global network of CIOs and CXOs.&nbsp;</p>\n<p>The initiative also includes international immersion programmes in partnership with ecosystem organisations, including TiE Silicon Valley and Alteus. Applications for the programme are now open.</p>\n<p>Google also recently announced new additions to its Gemma open model family to support healthcare and on-device AI development.</p>\n<p>One of the releases, MedGemma 1.5, is a 4-billion-parameter open-source model for medical AI applications. It supports high-dimensional medical imaging workflows, including CT and MRI scans, whole-slide histopathology, longitudinal chest X-ray analysis, anatomical localisation, and extraction of information from medical lab reports.</p>\n<p>The model builds on Google’s Health AI Developer Foundations programme and follows its collaboration with All India Institute of Medical Sciences, which is using MedGemma to develop health foundation models as part of the country’s Digital Public Infrastructure.</p>\n<p>Google also introduced FunctionGemma, a lightweight variant of the Gemma 3 270M model optimised for function calling and on-device AI agents. The model enables applications to convert natural language commands into executable actions locally, allowing AI systems to operate with low latency, limited connectivity, and enhanced user privacy.</p>\n<p>FunctionGemma can be fine-tuned using tools such as Hugging Face Transformers, Keras, and NVIDIA NeMo, and deployed across environments including LiteRT-LM, vLLM, Llama.cpp, and Vertex AI.</p>\n<p>Google said these efforts complement its ongoing investments in India’s AI infrastructure, including the Global AI Hub in Visakhapatnam, which provides a one-gigawatt compute foundation powered by green energy and Google’s AI chips.</p>\n<p>The company also reiterated its focus on data availability, citing progress on Project Vaani in collaboration with the Indian Institute of Science. The initiative has released more than 27,000 hours of speech data across over 100 Indic languages through the government’s Bhashini platform.</p>\n<p>The post Google Launches Market Access Programme to Scale Indian AI startups appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "71e98afc1dd8",
      "title": "Exclusive: Volvo tells us why having Gemini in your next car is a good thing",
      "content": "Next week, Volvo shows off its new EX60 SUV to the world. It's the brand's next electric vehicle, one built on an all-new, EV-only platform that makes use of the latest in vehicle design trends, like a cell-to-body battery pack, large weight-saving castings, and an advanced electronic architecture run by a handful of computers capable of more than 250 trillion operations per second. This new software-defined platform even has a name: HuginCore, after one of the two ravens that collected information for the Norse god Odin.\nIt's not Volvo's first reference to mythology. \"We have Thor's Hammer [Volvo's distinctive headlight design] and now we have HuginCore... one of the two trusted Ravens of Oden. He sent Hugin and Muninn out to fly across the realms and observe and gather information and knowledge, which they then share with Odin that enabled him to make the right decisions as the ruler of Asgard,\" said Alwin Bakkenes, head of global software engineering at Volvo Cars.\n\"And much like Hugin, the way we look at this technology platform, it collects information from all of the sensors, all of the actuators in the vehicle. It understands the world around the vehicle, and it enables us to actually anticipate around what lies ahead,\" Bakkenes told me.Read full article\nComments",
      "url": "https://arstechnica.com/cars/2026/01/exclusive-volvo-tells-us-why-having-gemini-in-your-next-car-is-a-good-thing/",
      "author": "Jonathan M. Gitlin",
      "published": "2026-01-15T08:00:15",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Cars",
        "software-defined car",
        "volvo",
        "Volvo EX60"
      ],
      "summary": "Volvo announced its new EX60 SUV will integrate Google Gemini AI on its new HuginCore platform, capable of 250 trillion operations per second. The software-defined vehicle represents Volvo's next-generation EV architecture.",
      "importance_score": 52.0,
      "reasoning": "Incremental automotive AI integration; represents broader trend of Gemini deployment but not a breakthrough capability.",
      "themes": [
        "AI in Vehicles",
        "Google Gemini",
        "Product Launch"
      ],
      "continuation": null,
      "summary_html": "<p>Volvo announced its new EX60 SUV will integrate Google Gemini AI on its new HuginCore platform, capable of 250 trillion operations per second. The software-defined vehicle represents Volvo's next-generation EV architecture.</p>",
      "content_html": "<p>Next week, Volvo shows off its new EX60 SUV to the world. It's the brand's next electric vehicle, one built on an all-new, EV-only platform that makes use of the latest in vehicle design trends, like a cell-to-body battery pack, large weight-saving castings, and an advanced electronic architecture run by a handful of computers capable of more than 250 trillion operations per second. This new software-defined platform even has a name: HuginCore, after one of the two ravens that collected information for the Norse god Odin.</p>\n<p>It's not Volvo's first reference to mythology. \"We have Thor's Hammer [Volvo's distinctive headlight design] and now we have HuginCore... one of the two trusted Ravens of Oden. He sent Hugin and Muninn out to fly across the realms and observe and gather information and knowledge, which they then share with Odin that enabled him to make the right decisions as the ruler of Asgard,\" said Alwin Bakkenes, head of global software engineering at Volvo Cars.</p>\n<p>\"And much like Hugin, the way we look at this technology platform, it collects information from all of the sensors, all of the actuators in the vehicle. It understands the world around the vehicle, and it enables us to actually anticipate around what lies ahead,\" Bakkenes told me.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "18acdf06cf79",
      "title": "Sadiq Khan to urge ministers to act over ‘colossal’ impact of AI on London jobs",
      "content": "In Mansion House speech, mayor will talk of opportunities technology offers but highlight mass unemployment riskBusiness live – latest updatesSadiq Khan is to warn in a major speech that artificial intelligence could destroy swathes of jobs in London and “usher in a new era of mass unemployment” unless ministers act now.In his annual Mansion House speech, the London mayor will say the capital is “at the sharpest edge of change” because of its reliance on white-collar workers in the finance and creative industries, and professional services such as law, accounting, consulting and marketing. Continue reading...",
      "url": "https://www.theguardian.com/politics/2026/jan/15/sadiq-khan-to-urge-ministers-to-act-over-colossal-impact-of-ai-on-london-jobs",
      "author": "Julia Kollewe and Aisha Down",
      "published": "2026-01-15T12:38:22",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Sadiq Khan",
        "AI (artificial intelligence)",
        "Technology",
        "London",
        "Unemployment",
        "Work & careers",
        "Politics",
        "UK news",
        "Local politics",
        "Job losses",
        "Business"
      ],
      "summary": "London Mayor Sadiq Khan will warn that AI could destroy swathes of London jobs and 'usher in mass unemployment' unless ministers act. London is seen as particularly vulnerable due to reliance on white-collar workers.",
      "importance_score": 52.0,
      "reasoning": "Policy speech on AI employment impact; important topic but speech preview rather than concrete policy action.",
      "themes": [
        "AI Policy",
        "Employment",
        "UK"
      ],
      "continuation": null,
      "summary_html": "<p>London Mayor Sadiq Khan will warn that AI could destroy swathes of London jobs and 'usher in mass unemployment' unless ministers act. London is seen as particularly vulnerable due to reliance on white-collar workers.</p>",
      "content_html": "<p>In Mansion House speech, mayor will talk of opportunities technology offers but highlight mass unemployment riskBusiness live – latest updatesSadiq Khan is to warn in a major speech that artificial intelligence could destroy swathes of jobs in London and “usher in a new era of mass unemployment” unless ministers act now.In his annual Mansion House speech, the London mayor will say the capital is “at the sharpest edge of change” because of its reliance on white-collar workers in the finance and creative industries, and professional services such as law, accounting, consulting and marketing. Continue reading...</p>"
    },
    {
      "id": "c0c9112ac7a6",
      "title": "McKinsey tests AI chatbot in early stages of graduate recruitment",
      "content": "Hiring at large firms has long relied on interviews, tests, and human judgment. That process is starting to shift. McKinsey has begun using an AI chatbot as part of its graduate recruitment process, signalling a shift in how professional services organisations evaluate early-career candidates.\n\n\n\nThe chatbot is being used during the initial stages of recruitment, where applicants are asked to interact with it as part of their assessment. Rather than replacing interviews or final hiring decisions, the tool is intended to support screening and evaluation earlier in the process. The move reflects a wider trend across large organisations: AI is no longer limited to research or client-facing tools, but is increasingly shaping internal workflows.\n\n\n\nWhy McKinsey is using AI in graduate hiring\n\n\n\nGraduate recruitment is resource-heavy. Every year, large firms receive tens of thousands of applications, many of which must be assessed in short hiring cycles. Screening candidates for basic fit, communication skills, and problem-solving ability can take a long time, even before interviews begin.\n\n\n\nUsing AI at this stage offers a way to manage volume. A chatbot can interact with every applicant, ask consistent questions and collect organised responses. Human recruiters can then review that data, rather than requiring staff to manually screen every application from scratch.\n\n\n\nFor McKinsey, the chatbot is part of a larger assessment process that includes interviews and human judgment. According to the company, the tool helps in gathering more information early on, rather than making recruiting judgments on its own.\n\n\n\nShifting the role of recruiters\n\n\n\nIntroducing AI into recruitment alters how hiring teams operate. Rather than focusing on early screening, recruiters can devote more time to assessing prospects who have already passed initial tests. In theory, that allows for more thoughtful interviews and deeper evaluation later in the process.\n\n\n\nAt the same time, it raises questions about oversight. Recruiters need to understand how the chatbot evaluates responses and what signals it prioritises. Without that visibility, there is a risk that decisions could lean too heavily on automated outputs, even if the tool is meant to assist rather than decide.\n\n\n\nProfessional services firms are typically wary about such adjustments. Their reputations rely heavily on talent quality, and any perception of unfair or flawed hiring practices carries risk. As a result, recruitment serves as a testing ground for AI use, as well as an area where controls are important.\n\n\n\nConcerns around fairness and bias\n\n\n\nUsing AI in hiring is not without controversy. Critics have raised concerns that automated systems can reflect biases present in their training data or in how questions are framed. If not monitored closely, those biases can affect who progresses through the hiring process.\n\n\n\nMcKinsey has said it is mindful of these risks and that the chatbot is used alongside human review. Still, the move highlights a broader challenge for organisations adopting AI internally: tools must be tested, audited, and adjusted over time.\n\n\n\nIn recruitment, that includes checking whether certain groups are disadvantaged by how questions are asked or how responses are interpreted. It also means giving candidates clear information about how AI is used and how their data is handled.\n\n\n\nHow McKinsey’s AI hiring move fits a wider enterprise trend\n\n\n\nThe use of AI in graduate hiring is not unique to consulting. Large employers in finance, law, and technology are also testing AI tools for screening, scheduling interviews, and analysing written responses. What stands out is how quickly these tools are moving from experiments to real processes.\n\n\n\nIn many cases, AI enters organisations through small, contained use cases. Hiring is one of them. It sits inside the company, affects internal efficiency, and can be adjusted without changing products or services offered to clients.\n\n\n\nThat pattern mirrors how AI adoption is unfolding more broadly. Instead of sweeping transformations, many firms are adding AI to specific workflows where the benefits and risks are easier to manage.\n\n\n\nWhat this signals for enterprises\n\n\n\nMcKinsey’s use of an AI chatbot in recruitment points to a practical shift in enterprise thinking. AI is becoming a tool for routine internal decisions, not just analysis or automation behind the scenes.\n\n\n\nFor other organisations, the lesson is less about copying the tool and more about approach. Introducing AI into sensitive areas like hiring requires clear boundaries, human oversight, and a willingness to review outcomes over time.\n\n\n\nIt also requires communication. Candidates need to know when they are interacting with AI and how that interaction fits into the overall hiring process. Transparency helps build trust, especially as AI becomes more common in workplace decisions.\n\n\n\nAs professional services firms continue to test AI in their own operations, recruitment offers an early view of how far they are willing to go. The technology may help manage scale and consistency, but responsibility for decisions still rests with people. How well companies balance those two will shape how AI is accepted inside the enterprise.\n\n\n\n(Photo by Resume Genius)\n\n\n\nSee also: Allister Frost: Tackling workforce anxiety for AI integration success\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check outAI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post McKinsey tests AI chatbot in early stages of graduate recruitment appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/mckinsey-tests-ai-chatbot-in-early-stages-of-graduate-recruitment/",
      "author": "Muhammad Zulhusni",
      "published": "2026-01-15T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "AI in Action",
        "Artificial Intelligence",
        "Features",
        "Human-AI Relationships",
        "Trust, Bias & Fairness",
        "Workforce & HR AI",
        "World of Work",
        "ai",
        "artificial intelligence",
        "chatbot",
        "featured",
        "hr",
        "research",
        "workforce"
      ],
      "summary": "McKinsey has begun using an AI chatbot in early stages of graduate recruitment for screening and evaluation. The move reflects broader enterprise adoption of AI in internal HR workflows.",
      "importance_score": 52.0,
      "reasoning": "Interesting enterprise AI adoption example but incremental rather than transformative news.",
      "themes": [
        "Enterprise AI",
        "HR",
        "Recruitment"
      ],
      "continuation": null,
      "summary_html": "<p>McKinsey has begun using an AI chatbot in early stages of graduate recruitment for screening and evaluation. The move reflects broader enterprise adoption of AI in internal HR workflows.</p>",
      "content_html": "<p>Hiring at large firms has long relied on interviews, tests, and human judgment. That process is starting to shift. McKinsey has begun using an AI chatbot as part of its graduate recruitment process, signalling a shift in how professional services organisations evaluate early-career candidates.</p>\n<p>The chatbot is being used during the initial stages of recruitment, where applicants are asked to interact with it as part of their assessment. Rather than replacing interviews or final hiring decisions, the tool is intended to support screening and evaluation earlier in the process. The move reflects a wider trend across large organisations: AI is no longer limited to research or client-facing tools, but is increasingly shaping internal workflows.</p>\n<p>Why McKinsey is using AI in graduate hiring</p>\n<p>Graduate recruitment is resource-heavy. Every year, large firms receive tens of thousands of applications, many of which must be assessed in short hiring cycles. Screening candidates for basic fit, communication skills, and problem-solving ability can take a long time, even before interviews begin.</p>\n<p>Using AI at this stage offers a way to manage volume. A chatbot can interact with every applicant, ask consistent questions and collect organised responses. Human recruiters can then review that data, rather than requiring staff to manually screen every application from scratch.</p>\n<p>For McKinsey, the chatbot is part of a larger assessment process that includes interviews and human judgment. According to the company, the tool helps in gathering more information early on, rather than making recruiting judgments on its own.</p>\n<p>Shifting the role of recruiters</p>\n<p>Introducing AI into recruitment alters how hiring teams operate. Rather than focusing on early screening, recruiters can devote more time to assessing prospects who have already passed initial tests. In theory, that allows for more thoughtful interviews and deeper evaluation later in the process.</p>\n<p>At the same time, it raises questions about oversight. Recruiters need to understand how the chatbot evaluates responses and what signals it prioritises. Without that visibility, there is a risk that decisions could lean too heavily on automated outputs, even if the tool is meant to assist rather than decide.</p>\n<p>Professional services firms are typically wary about such adjustments. Their reputations rely heavily on talent quality, and any perception of unfair or flawed hiring practices carries risk. As a result, recruitment serves as a testing ground for AI use, as well as an area where controls are important.</p>\n<p>Concerns around fairness and bias</p>\n<p>Using AI in hiring is not without controversy. Critics have raised concerns that automated systems can reflect biases present in their training data or in how questions are framed. If not monitored closely, those biases can affect who progresses through the hiring process.</p>\n<p>McKinsey has said it is mindful of these risks and that the chatbot is used alongside human review. Still, the move highlights a broader challenge for organisations adopting AI internally: tools must be tested, audited, and adjusted over time.</p>\n<p>In recruitment, that includes checking whether certain groups are disadvantaged by how questions are asked or how responses are interpreted. It also means giving candidates clear information about how AI is used and how their data is handled.</p>\n<p>How McKinsey’s AI hiring move fits a wider enterprise trend</p>\n<p>The use of AI in graduate hiring is not unique to consulting. Large employers in finance, law, and technology are also testing AI tools for screening, scheduling interviews, and analysing written responses. What stands out is how quickly these tools are moving from experiments to real processes.</p>\n<p>In many cases, AI enters organisations through small, contained use cases. Hiring is one of them. It sits inside the company, affects internal efficiency, and can be adjusted without changing products or services offered to clients.</p>\n<p>That pattern mirrors how AI adoption is unfolding more broadly. Instead of sweeping transformations, many firms are adding AI to specific workflows where the benefits and risks are easier to manage.</p>\n<p>What this signals for enterprises</p>\n<p>McKinsey’s use of an AI chatbot in recruitment points to a practical shift in enterprise thinking. AI is becoming a tool for routine internal decisions, not just analysis or automation behind the scenes.</p>\n<p>For other organisations, the lesson is less about copying the tool and more about approach. Introducing AI into sensitive areas like hiring requires clear boundaries, human oversight, and a willingness to review outcomes over time.</p>\n<p>It also requires communication. Candidates need to know when they are interacting with AI and how that interaction fits into the overall hiring process. Transparency helps build trust, especially as AI becomes more common in workplace decisions.</p>\n<p>As professional services firms continue to test AI in their own operations, recruitment offers an early view of how far they are willing to go. The technology may help manage scale and consistency, but responsibility for decisions still rests with people. How well companies balance those two will shape how AI is accepted inside the enterprise.</p>\n<p>(Photo by Resume Genius)</p>\n<p>See also: Allister Frost: Tackling workforce anxiety for AI integration success</p>\n<p>Want to learn more about AI and big data from industry leaders? Check outAI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post McKinsey tests AI chatbot in early stages of graduate recruitment appeared first on AI News.</p>"
    },
    {
      "id": "56f7df1ed02c",
      "title": "Wikipedia’s Existential Threats Feel Greater Than Ever",
      "content": "As the free online encyclopedia turns 25, it’s facing political opposition, AI scraping, dwindling volunteers, and a public that may no longer believe in its ideals.",
      "url": "https://www.wired.com/story/wikipedias-existential-threats-have-never-been-greater/",
      "author": "Stephen Harrison",
      "published": "2026-01-15T10:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Culture",
        "Culture / Digital Culture",
        "wikipedia",
        "Donald Trump",
        "Elon Musk",
        "artificial intelligence",
        "Common Knowledge"
      ],
      "summary": "Wikipedia faces multiple existential threats as it turns 25, including AI scraping, political opposition from figures like Trump and Musk, dwindling volunteers, and eroding public trust in its ideals.",
      "importance_score": 48.0,
      "reasoning": "Background analysis piece rather than breaking news; discusses existing trends.",
      "themes": [
        "Wikipedia",
        "AI Impact",
        "Content"
      ],
      "continuation": null,
      "summary_html": "<p>Wikipedia faces multiple existential threats as it turns 25, including AI scraping, political opposition from figures like Trump and Musk, dwindling volunteers, and eroding public trust in its ideals.</p>",
      "content_html": "<p>As the free online encyclopedia turns 25, it’s facing political opposition, AI scraping, dwindling volunteers, and a public that may no longer believe in its ideals.</p>"
    },
    {
      "id": "fb901fe40ff2",
      "title": "‘It’s AI blackface’: social media account hailed as the Aboriginal Steve Irwin is an AI character created in New Zealand",
      "content": "More than 180,000 people follow the Bush Legend’s accounts across Meta platforms, but its Aboriginal host is a work of digital fictionGet our breaking news email, free app or daily news podcastWith a mop of dark curls and brown eyes, Jarren stands in the thick of the Australian outback, red dirt at his feet, a snake unfurling in front of him.In a series of online videos, the social media star, known online as the Bush Legend, walks through dense forests or drives along deserted roads on the hunt for wedge-tailed eagles. Many of the videos are set to pulsating percussion instruments and yidakis (didgeridoo). Continue reading...",
      "url": "https://www.theguardian.com/australia-news/2026/jan/15/aboriginal-steve-irwin-ai-character-created-new-zealand",
      "author": "Sarah Collard Indigenous affairs correspondent",
      "published": "2026-01-15T14:00:21",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Indigenous Australians",
        "Australia news",
        "AI (artificial intelligence)",
        "Wildlife",
        "Indigenous peoples",
        "Race",
        "Social media",
        "Meta",
        "Digital media"
      ],
      "summary": "A social media account called 'Bush Legend' presenting an Aboriginal character named Jarren has been revealed as AI-generated content created in New Zealand. The account has over 180,000 followers across Meta platforms.",
      "importance_score": 45.0,
      "reasoning": "Interesting case of AI-powered cultural misrepresentation but limited broader industry implications.",
      "themes": [
        "AI Ethics",
        "Deepfakes",
        "Social Media"
      ],
      "continuation": null,
      "summary_html": "<p>A social media account called 'Bush Legend' presenting an Aboriginal character named Jarren has been revealed as AI-generated content created in New Zealand. The account has over 180,000 followers across Meta platforms.</p>",
      "content_html": "<p>More than 180,000 people follow the Bush Legend’s accounts across Meta platforms, but its Aboriginal host is a work of digital fictionGet our breaking news email, free app or daily news podcastWith a mop of dark curls and brown eyes, Jarren stands in the thick of the Australian outback, red dirt at his feet, a snake unfurling in front of him.In a series of online videos, the social media star, known online as the Bush Legend, walks through dense forests or drives along deserted roads on the hunt for wedge-tailed eagles. Many of the videos are set to pulsating percussion instruments and yidakis (didgeridoo). Continue reading...</p>"
    },
    {
      "id": "ae4ef47b372a",
      "title": "Matthew McConaughey trademarks ‘All right, all right, all right’ catchphrase in bid to beat AI fakes",
      "content": "The Oscar winner intends to combat misuse of the famous line from Dazed and Confused by creating ‘a clear perimeter around ownership’ Oscar-winning actor Matthew McConaughey has trademarked his image and voice – including his famous catchphrase: “All right, all right, all right” from the movie Dazed and Confused in an attempt to forestall unauthorised use by artificial intelligence.The Wall Street Journal reported that McConaughey has had eight separate applications approved by the US Patent and Trademark Office in recent weeks, including film clips of the actor standing on a porch and sitting in front of a tree, and an audio clip of him saying: “All right, all right, all right”. Continue reading...",
      "url": "https://www.theguardian.com/film/2026/jan/15/matthew-mcconaughey-trademarks-all-right-all-right-all-right-catchphrase-in-bid-to-beat-ai-fakes",
      "author": "Andrew Pulver",
      "published": "2026-01-15T14:39:08",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Film",
        "Matthew McConaughey",
        "AI (artificial intelligence)",
        "US news",
        "Computing",
        "Culture",
        "Technology",
        "World news"
      ],
      "summary": "Matthew McConaughey received eight trademark approvals from the US Patent Office for his image, voice, and catchphrase to protect against unauthorized AI use. This represents a celebrity strategy for AI rights protection.",
      "importance_score": 40.0,
      "reasoning": "Minor celebrity news with limited broader AI implications; represents individual response rather than industry shift.",
      "themes": [
        "IP Rights",
        "AI Rights",
        "Celebrity"
      ],
      "continuation": null,
      "summary_html": "<p>Matthew McConaughey received eight trademark approvals from the US Patent Office for his image, voice, and catchphrase to protect against unauthorized AI use. This represents a celebrity strategy for AI rights protection.</p>",
      "content_html": "<p>The Oscar winner intends to combat misuse of the famous line from Dazed and Confused by creating ‘a clear perimeter around ownership’ Oscar-winning actor Matthew McConaughey has trademarked his image and voice – including his famous catchphrase: “All right, all right, all right” from the movie Dazed and Confused in an attempt to forestall unauthorised use by artificial intelligence.The Wall Street Journal reported that McConaughey has had eight separate applications approved by the US Patent and Trademark Office in recent weeks, including film clips of the actor standing on a porch and sitting in front of a tree, and an audio clip of him saying: “All right, all right, all right”. Continue reading...</p>"
    },
    {
      "id": "1edafec99495",
      "title": "AI as a life coach: experts share what works, what doesn’t and what to look out for",
      "content": "It’s becoming more common for people to use AI chatbots for personal guidance – but this doesn’t come without risksSign up for our free newsletter course on how to make AI work for youIf you’re like a lot of people, you’ve probably ditched your new year resolutions by now. Setting goals is hard; keeping them is harder – and failure can bring about icky feelings about yourself.This year, in an effort to game the system and tilt the scales toward success, some people used AI for their 2026 resolutions. It’s the latest step in an ongoing trend: in September 2025, OpenAI, the company behind ChatGPT, released findings showing that using the AI chatbot for personal guidance is very common. Continue reading...",
      "url": "https://www.theguardian.com/wellness/2026/jan/15/ai-life-coach",
      "author": "Sarah Sloat",
      "published": "2026-01-15T17:00:45",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Well actually",
        "Life and style",
        "AI (artificial intelligence)",
        "Technology",
        "ChatGPT"
      ],
      "summary": "Article explores the growing trend of using AI chatbots like ChatGPT for personal guidance and life coaching, with OpenAI data showing this is common usage. Experts warn about associated risks.",
      "importance_score": 38.0,
      "reasoning": "Lifestyle piece with limited news value; discusses existing trends rather than new developments.",
      "themes": [
        "AI Applications",
        "ChatGPT",
        "Wellness"
      ],
      "continuation": null,
      "summary_html": "<p>Article explores the growing trend of using AI chatbots like ChatGPT for personal guidance and life coaching, with OpenAI data showing this is common usage. Experts warn about associated risks.</p>",
      "content_html": "<p>It’s becoming more common for people to use AI chatbots for personal guidance – but this doesn’t come without risksSign up for our free newsletter course on how to make AI work for youIf you’re like a lot of people, you’ve probably ditched your new year resolutions by now. Setting goals is hard; keeping them is harder – and failure can bring about icky feelings about yourself.This year, in an effort to game the system and tilt the scales toward success, some people used AI for their 2026 resolutions. It’s the latest step in an ongoing trend: in September 2025, OpenAI, the company behind ChatGPT, released findings showing that using the AI chatbot for personal guidance is very common. Continue reading...</p>"
    },
    {
      "id": "5d2344009752",
      "title": "OpenAI’s Bengaluru Workshop Helps Nonprofits Scale AI for Social Impact",
      "content": "\nOpenAI, the company behind ChatGPT, hosted its Nonprofit AI Jam in Bengaluru on January 15.&nbsp;\n\n\n\nThe event is part of a multi-city series designed to bring together leaders from the nonprofit ecosystem for hands-on sessions focused on applying AI to achieve more effective, scalable outcomes in the social sector, the company said in a statement shared with AIM.\n\n\n\nThe Bangalore edition brought together nonprofit organisations working across education, public health, skilling, climate action, and gender inclusion.&nbsp;\n\n\n\nParticipants identified real operational challenges from their day-to-day work and collaborated with technical experts throughout the day to design practical AI solutions tailored to their needs.\n\n\n\n“The Jam reflects OpenAI’s commitment to ensuring that advanced AI is accessible and useful not only to large organisations, but also to mission-driven teams working closest to communities,” the company said. The goal is to help these organisations identify and apply practical AI use cases in the social sector, the company added.&nbsp;\n\n\n\nOpenAI added that participants will continue to have access to shared resources and peer-learning opportunities after the workshop, ensuring that collaboration and development extend beyond the event itself.\n\n\n\nThe workshop is being delivered in partnership with Karya and supported by Wadhwani AI as the knowledge partner. Karya is a social enterprise that creates ethical AI datasets while providing dignified digital work to rural and economically disadvantaged communities.&nbsp;\n\n\n\nWadhwani AI is a non-profit organisation that builds and deploys AI solutions for social impact in areas such as healthcare, agriculture, and education.\n\n\n\nThe workshop is being held ahead of the AI Impact Summit in India, a global artificial intelligence summit hosted by the Government of India under the IndiaAI Mission and the Ministry of Electronics and Information Technology.&nbsp;\n\n\n\nThe summit is scheduled for February 19–20, 2026, in New Delhi.\n\n\n\nThe summit will bring together international leaders, policymakers, researchers, industry executives, and innovators to shape how AI can be deployed responsibly, inclusively, and at scale to deliver social and economic impact.\nThe post OpenAI’s Bengaluru Workshop Helps Nonprofits Scale AI for Social Impact  appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/openais-bengaluru-workshop-attempts-to-help-nonprofits-scale-ai-for-social-impact/",
      "author": "Supreeth Koundinya",
      "published": "2026-01-15T11:56:44",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "OpenAI hosted an AI Jam workshop in Bengaluru for nonprofits working in education, health, climate, and gender inclusion. Participants designed practical AI solutions for operational challenges.",
      "importance_score": 35.0,
      "reasoning": "Minor event news; corporate social responsibility activity without significant technology announcements.",
      "themes": [
        "OpenAI",
        "Nonprofits",
        "India"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI hosted an AI Jam workshop in Bengaluru for nonprofits working in education, health, climate, and gender inclusion. Participants designed practical AI solutions for operational challenges.</p>",
      "content_html": "<p>OpenAI, the company behind ChatGPT, hosted its Nonprofit AI Jam in Bengaluru on January 15.&nbsp;</p>\n<p>The event is part of a multi-city series designed to bring together leaders from the nonprofit ecosystem for hands-on sessions focused on applying AI to achieve more effective, scalable outcomes in the social sector, the company said in a statement shared with AIM.</p>\n<p>The Bangalore edition brought together nonprofit organisations working across education, public health, skilling, climate action, and gender inclusion.&nbsp;</p>\n<p>Participants identified real operational challenges from their day-to-day work and collaborated with technical experts throughout the day to design practical AI solutions tailored to their needs.</p>\n<p>“The Jam reflects OpenAI’s commitment to ensuring that advanced AI is accessible and useful not only to large organisations, but also to mission-driven teams working closest to communities,” the company said. The goal is to help these organisations identify and apply practical AI use cases in the social sector, the company added.&nbsp;</p>\n<p>OpenAI added that participants will continue to have access to shared resources and peer-learning opportunities after the workshop, ensuring that collaboration and development extend beyond the event itself.</p>\n<p>The workshop is being delivered in partnership with Karya and supported by Wadhwani AI as the knowledge partner. Karya is a social enterprise that creates ethical AI datasets while providing dignified digital work to rural and economically disadvantaged communities.&nbsp;</p>\n<p>Wadhwani AI is a non-profit organisation that builds and deploys AI solutions for social impact in areas such as healthcare, agriculture, and education.</p>\n<p>The workshop is being held ahead of the AI Impact Summit in India, a global artificial intelligence summit hosted by the Government of India under the IndiaAI Mission and the Ministry of Electronics and Information Technology.&nbsp;</p>\n<p>The summit is scheduled for February 19–20, 2026, in New Delhi.</p>\n<p>The summit will bring together international leaders, policymakers, researchers, industry executives, and innovators to shape how AI can be deployed responsibly, inclusively, and at scale to deliver social and economic impact.</p>\n<p>The post OpenAI’s Bengaluru Workshop Helps Nonprofits Scale AI for Social Impact  appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "345c9570b1eb",
      "title": "Elon Musk’s Grok made the world less safe – his humiliating backdown gives me hopium | Van Badham",
      "content": "The AI chatbot’s torrent of nonconsensual deepfakes isn’t its first scandal and won’t be its last. Responsible governments should simply ban itGet our breaking news email, free app or daily news podcastBillionaire and career Bond-villain cosplayer Elon Musk has been forced by public backlash into a humiliating backdown over use of his AI chatbot, Grok. Watching the world’s richest man eat a shit sandwich on a global stage represents a rare win for sovereign democracy.Because – unlike his company history of labour and safety abuses … his exploding rockets … his government interventions that deny aid to the starving … disabling Starlink internet systems in war zones … sharing “white solidarity” statements … or growing concern about overvaluations of his company’s share price – the nature of Grok’s latest scandal may finally be inspiring governments towards imposing some Musk-limiting red lines. Continue reading...",
      "url": "https://www.theguardian.com/commentisfree/2026/jan/15/elon-musk-grok-backflip-blocked-x-ai-sexualised-images-backlash",
      "author": "Van Badham",
      "published": "2026-01-15T05:14:14",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "X",
        "Elon Musk",
        "Deepfake",
        "Technology"
      ],
      "summary": "Opinion piece arguing Musk's Grok backdown on explicit images represents a rare win for democracy, calling for governments to ban the tool entirely.",
      "importance_score": 32.0,
      "reasoning": "Opinion column without significant news content; commentary on existing developments.",
      "themes": [
        "Opinion",
        "AI Safety",
        "xAI"
      ],
      "continuation": null,
      "summary_html": "<p>Opinion piece arguing Musk's Grok backdown on explicit images represents a rare win for democracy, calling for governments to ban the tool entirely.</p>",
      "content_html": "<p>The AI chatbot’s torrent of nonconsensual deepfakes isn’t its first scandal and won’t be its last. Responsible governments should simply ban itGet our breaking news email, free app or daily news podcastBillionaire and career Bond-villain cosplayer Elon Musk has been forced by public backlash into a humiliating backdown over use of his AI chatbot, Grok. Watching the world’s richest man eat a shit sandwich on a global stage represents a rare win for sovereign democracy.Because – unlike his company history of labour and safety abuses … his exploding rockets … his government interventions that deny aid to the starving … disabling Starlink internet systems in war zones … sharing “white solidarity” statements … or growing concern about overvaluations of his company’s share price – the nature of Grok’s latest scandal may finally be inspiring governments towards imposing some Musk-limiting red lines. Continue reading...</p>"
    },
    {
      "id": "4de0184f89e3",
      "title": "GrowthPal Announces $2.6 Mn Funding to Enhance its AI-Driven M&A Copilot",
      "content": "\nGrowthPal, a platform for matchmaking in mergers and acquisitions (M&amp;A), has announced a $2.6 million funding round to enhance its AI-driven M&amp;A copilot for deal sourcing and execution.&nbsp;\n\n\n\nThe funding round was led by Ideaspring Capital, with participation from notable angel investors from around the globe. This new investment will facilitate product development and broaden GrowthPal’s reach into international markets as the demand for quicker, more systematic strategies for inorganic growth increases.\n\n\n\n“M&amp;A sourcing is where most time and effort is wasted, especially for smaller and mid-market deals,” Maneesh Bhandari, co-founder and CEO of GrowthPal, said in the press release. “Teams spend weeks researching, filtering and chasing opportunities that never go anywhere. We built GrowthPal to help buyers focus only on high-intent, high-fit targets and move from mandate to meaningful conversations far faster.”\n\n\n\nGrowthPal’s platform serves as an intelligent M&amp;A assistant. When a buyer sets a growth goal, such as acquiring a specific capability or entering a new market, the system converts it into a structured acquisition thesis.\n\n\n\nIts AI agents analyse a vast database of over four million technology companies, using signals from public documents, online activity, hiring trends and funding history. This results in a targeted list of off-market firms closely aligned with the buyer’s criteria.\n\n\n\nAccording to the company, GrowthPal has supported over 42 completed M&amp;A transactions and facilitated more than 210 LOI discussions across North America, Europe, Asia and Latin America. Clients range from large enterprises to fast-growing startups and private equity-backed firms in sectors such as IT services and fintech. One client closed seven acquisitions within 18 months using the platform, it said.&nbsp;\n\n\n\n“GrowthPal is solving one of the most under-optimised parts of the M&amp;A lifecycle,” said Naganand Doraswamy, managing partner at Ideaspring Capital. “By focusing on qualified deal discovery and using AI to compress timelines, the team is enabling a more systematic approach to inorganic growth that traditional tools cannot offer.”\n\n\n\nGrowthPal intends to enhance its intelligence across the transaction lifecycle, focusing on valuation, deal structuring and negotiation preparation. The long-term goal is to empower teams to make better M&amp;A decisions earlier and with greater confidence, from discovery through to execution.\n\n\n\nM&amp;A teams are increasingly pressured to do more with fewer resources. Inorganic growth relies on timing, context and accessibility. However, originating M&amp;A deals from mid-market and early-stage companies has changed little over the years, still relying on banker networks and static databases.\nThe post GrowthPal Announces $2.6 Mn Funding to Enhance its AI-Driven M&amp;A Copilot appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/growthpal-announces-2-6-mn-funding-to-enhance-its-ai-driven-ma-copilot/",
      "author": "Smruthi Nadig",
      "published": "2026-01-15T07:10:16",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News",
        "growthpal",
        "merger and acquisitions"
      ],
      "summary": "GrowthPal raised $2.6 million led by Ideaspring Capital for its AI-driven M&A copilot platform. The startup focuses on deal sourcing and execution automation.",
      "importance_score": 32.0,
      "reasoning": "Small funding round with limited frontier AI implications.",
      "themes": [
        "Funding",
        "Startups",
        "AI Tools"
      ],
      "continuation": null,
      "summary_html": "<p>GrowthPal raised $2.6 million led by Ideaspring Capital for its AI-driven M&A copilot platform. The startup focuses on deal sourcing and execution automation.</p>",
      "content_html": "<p>GrowthPal, a platform for matchmaking in mergers and acquisitions (M&amp;A), has announced a $2.6 million funding round to enhance its AI-driven M&amp;A copilot for deal sourcing and execution.&nbsp;</p>\n<p>The funding round was led by Ideaspring Capital, with participation from notable angel investors from around the globe. This new investment will facilitate product development and broaden GrowthPal’s reach into international markets as the demand for quicker, more systematic strategies for inorganic growth increases.</p>\n<p>“M&amp;A sourcing is where most time and effort is wasted, especially for smaller and mid-market deals,” Maneesh Bhandari, co-founder and CEO of GrowthPal, said in the press release. “Teams spend weeks researching, filtering and chasing opportunities that never go anywhere. We built GrowthPal to help buyers focus only on high-intent, high-fit targets and move from mandate to meaningful conversations far faster.”</p>\n<p>GrowthPal’s platform serves as an intelligent M&amp;A assistant. When a buyer sets a growth goal, such as acquiring a specific capability or entering a new market, the system converts it into a structured acquisition thesis.</p>\n<p>Its AI agents analyse a vast database of over four million technology companies, using signals from public documents, online activity, hiring trends and funding history. This results in a targeted list of off-market firms closely aligned with the buyer’s criteria.</p>\n<p>According to the company, GrowthPal has supported over 42 completed M&amp;A transactions and facilitated more than 210 LOI discussions across North America, Europe, Asia and Latin America. Clients range from large enterprises to fast-growing startups and private equity-backed firms in sectors such as IT services and fintech. One client closed seven acquisitions within 18 months using the platform, it said.&nbsp;</p>\n<p>“GrowthPal is solving one of the most under-optimised parts of the M&amp;A lifecycle,” said Naganand Doraswamy, managing partner at Ideaspring Capital. “By focusing on qualified deal discovery and using AI to compress timelines, the team is enabling a more systematic approach to inorganic growth that traditional tools cannot offer.”</p>\n<p>GrowthPal intends to enhance its intelligence across the transaction lifecycle, focusing on valuation, deal structuring and negotiation preparation. The long-term goal is to empower teams to make better M&amp;A decisions earlier and with greater confidence, from discovery through to execution.</p>\n<p>M&amp;A teams are increasingly pressured to do more with fewer resources. Inorganic growth relies on timing, context and accessibility. However, originating M&amp;A deals from mid-market and early-stage companies has changed little over the years, still relying on banker networks and static databases.</p>\n<p>The post GrowthPal Announces $2.6 Mn Funding to Enhance its AI-Driven M&amp;A Copilot appeared first on Analytics India Magazine.</p>"
    }
  ]
}