{
  "category": "news",
  "date": "2026-02-07",
  "category_summary": "**OpenAI** and **Anthropic** are in direct competition this week with [simultaneous releases](/?date=2026-02-07&category=news#item-137b2f91fd8a) of **Claude Opus 4.6** and **GPT-5.3-Codex**, alongside [dueling Super Bowl ads](/?date=2026-02-07&category=news#item-ea3ea855edba) and enterprise platform launches. **OpenAI's Frontier** platform is already [being trialed](/?date=2026-02-07&category=news#item-e7c1166b4cda) by **Intuit**, **Uber**, and **State Farm** for production AI agent deployment.\n\n**Agentic AI** capabilities continue advancing: **Anthropic** demonstrated 16 Claude agents collaborating to build a working **100,000-line C compiler** in Rust for approximately **$20,000** in API costs. Meanwhile, **Goodfire AI** raised **$150M at $1.25B valuation** to commercialize mechanistic interpretability tools.\n\n**Waymo** [deployed **Google DeepMind's Genie 3**](/?date=2026-02-07&category=news#item-f906446624b5) as a world model for generating hyper-realistic driving simulations, enabling training on rare safety-critical scenarios beyond their **200 million real-world autonomous miles**. On the risk side, research confirms [deepfake fraud has reached industrial scale](/?date=2026-02-07&category=news#item-2ff022750888).",
  "category_summary_html": "<p><strong>OpenAI</strong> and <strong>Anthropic</strong> are in direct competition this week with <a href=\"/?date=2026-02-07&amp;category=news#item-137b2f91fd8a\" class=\"internal-link\" rel=\"noopener noreferrer\">simultaneous releases</a> of <strong>Claude Opus 4.6</strong> and <strong>GPT-5.3-Codex</strong>, alongside <a href=\"/?date=2026-02-07&amp;category=news#item-ea3ea855edba\" class=\"internal-link\" rel=\"noopener noreferrer\">dueling Super Bowl ads</a> and enterprise platform launches. <strong>OpenAI's Frontier</strong> platform is already <a href=\"/?date=2026-02-07&amp;category=news#item-e7c1166b4cda\" class=\"internal-link\" rel=\"noopener noreferrer\">being trialed</a> by <strong>Intuit</strong>, <strong>Uber</strong>, and <strong>State Farm</strong> for production AI agent deployment.</p>\n<p><strong>Agentic AI</strong> capabilities continue advancing: <strong>Anthropic</strong> demonstrated 16 Claude agents collaborating to build a working <strong>100,000-line C compiler</strong> in Rust for approximately <strong>$20,000</strong> in API costs. Meanwhile, <strong>Goodfire AI</strong> raised <strong>$150M at $1.25B valuation</strong> to commercialize mechanistic interpretability tools.</p>\n<p><strong>Waymo</strong> <a href=\"/?date=2026-02-07&amp;category=news#item-f906446624b5\" class=\"internal-link\" rel=\"noopener noreferrer\">deployed <strong>Google DeepMind's Genie 3</strong></a> as a world model for generating hyper-realistic driving simulations, enabling training on rare safety-critical scenarios beyond their <strong>200 million real-world autonomous miles</strong>. On the risk side, research confirms <a href=\"/?date=2026-02-07&amp;category=news#item-2ff022750888\" class=\"internal-link\" rel=\"noopener noreferrer\">deepfake fraud has reached industrial scale</a>.</p>",
  "themes": [
    {
      "name": "AI Lab Competition & Model Releases",
      "description": "Simultaneous major model releases from OpenAI and Anthropic, with coordinated pushes across consumer products, enterprise platforms, and marketing campaigns",
      "item_count": 4,
      "example_items": [],
      "importance": 90.0
    },
    {
      "name": "Enterprise AI Agents",
      "description": "Major platforms launching for deploying AI agents in production enterprise workflows, with Fortune 500 early adopters",
      "item_count": 4,
      "example_items": [],
      "importance": 84.0
    },
    {
      "name": "Agentic AI Capabilities",
      "description": "Multi-agent systems demonstrating complex collaborative tasks like compiler creation, plus research on agent architecture for reliability",
      "item_count": 3,
      "example_items": [],
      "importance": 80.0
    },
    {
      "name": "World Models & Simulation",
      "description": "Frontier generative models creating realistic simulated environments for autonomous driving training and safety testing",
      "item_count": 2,
      "example_items": [],
      "importance": 78.0
    },
    {
      "name": "AI Safety & Interpretability",
      "description": "Major funding for interpretability startups and philosophical discussions on AI safety approaches from leading labs",
      "item_count": 2,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "AI Misuse & Security",
      "description": "Industrial-scale deepfake fraud and legal consequences for AI-generated fake citations in professional settings",
      "item_count": 2,
      "example_items": [],
      "importance": 60.0
    }
  ],
  "total_items": 16,
  "items": [
    {
      "id": "137b2f91fd8a",
      "title": "[AINews] OpenAI and Anthropic go to war: Claude Opus 4.6 vs GPT 5.3 Codex",
      "content": "AI News for 2/4/2026-2/5/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 9460 messages) for you. Estimated reading time saved (at 200wpm): 731 minutes. AINews&#8217; website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!If you think the simultaneous release of Claude Opus 4.6 and GPT-5.3-Codex is sheer coincidence, you&#8217;re not sufficiently appreciating the intensity of the competition between the two leading coding model labs in the world right now. It has never been as clear from: in Consumer, the dueling Superbowl Ad campaigns (and subsequent defense from sama) in the Enterprise, Anthropic releasing knowledge work plugins vs OpenAI launching Frontier, an enterprise-scale agents platform for knowledge work (with a ~50% collapse in SaaS stocks as collateral damage)to the synced Coding launches today.From a pure PR point of view, Anthropic won the day via distributed denial of developer attention across their 1m context and new custom compaction and adaptive thinking and effort and Claude Code agent teams and Claude in Powerpoint/Excel and 500 zero-days and C compiler task and use of mechinterp and ai consciousness callouts and $50 promos, whereas OpenAI won on most benchmarks with 25% higher speed with higher token efficiency and touted more web development skills, but it&#8217;s likely that all first day third party reactions are either biased or superficial. Here is Opus making visual comparisons of the different announcements:Both are minor version bumps, which will set the stage for Claude 5 and GPT 6 battles this summer.Your move, GDM and SpaceXai.AI Twitter RecapTop tweets (by engagement)Frontier lab engineering: Anthropic&#8217;s post on using agent teams + Opus 4.6 to build a clean-room C compiler that boots Linux drew major attention (tweet).OpenAI release: GPT-5.3-Codex launch (and Codex product updates) landed as the biggest pure-AI product event (tweet).OpenAI GPT-5.3-Codex + &#8220;Frontier&#8221; agent platform (performance, efficiency, infra co-design)GPT-5.3-Codex shipped in Codex: OpenAI announced GPT-5.3-Codex now available in Codex (&#8220;You can just build things&#8221;) (tweet) and framed it as advancing frontier coding + professional knowledge in one model (tweet).Community reaction highlighted that token efficiency + inference speed may be the most strategically important delta vs prior generations (tweet), with one benchmark claim: TerminalBench 2 = 65.4% and a head-to-head &#8220;demolished Opus 4.6&#8221; narrative circulating immediately after launch (tweet).Reported efficiency improvements: 2.09&#215; fewer tokens vs GPT-5.2-Codex-xhigh on SWE-Bench-Pro, and together with ~40% speedup implies 2.93&#215; faster at ~+1% score (tweet). This theme was echoed by practitioners as a sign that 2026 is no longer assuming &#8220;infinite budget compute&#8221; (tweet).Hardware/software co-design for GB200: A notable systems angle: OpenAI engineers describe the model as &#8220;designed for GB200-NVL72&#8221; and mention ISA nitpicking, rack sims, and tailoring architecture to the system (tweet). Separate &#8220;fruits of long-term collaboration with NVIDIA&#8221; posts reinforce that model gains are arriving with platform-specific optimization (tweet).OpenAI Frontier (agents platform): OpenAI&#8217;s &#8220;Frontier&#8221; is positioned as a platform to build/deploy/manage agents with business context, execution environments (tools/code), learning-on-the-job, and identity/permissions (tweet). A separate report quotes Fidji Simo emphasizing partnering with an ecosystem rather than building everything internally (tweet).Internal adoption playbook for agentic software dev: A detailed post lays out OpenAI&#8217;s operational push: by March 31, for technical tasks the &#8220;tool of first resort&#8221; should be an agent, with team processes like AGENTS.md, &#8220;skills&#8221; libraries, tool inventories exposed via CLI/MCP, agent-first codebases, and &#8220;say no to slop&#8221; review/accountability norms (tweet). This is one of the clearer public examples of how a frontier lab is trying to industrialize &#8220;agent trajectories &#8594; mergeable code.&#8221;Developer ecosystem activation: Codex hackathon and ongoing builder showcases amplify &#8220;ship velocity&#8221; positioning (tweet, tweet). There&#8217;s also active curiosity about computer-use parity stacks (e.g., OSWorld-Verified claims, agent browser vs Chrome MCP APIs) and a request for OpenAI to benchmark and recommend the &#8220;right&#8221; harness (tweet, tweet).Anthropic Claude Opus 4.6: agentic coding, long-context, and benchmarking &#8220;noise&#8221;Autonomous C compiler as a forcing function for &#8220;agent teams&#8221;: Anthropic reports assigning Opus 4.6 agent teams to build a C compiler, then &#8220;mostly walking away&#8221;; after ~2 weeks it worked on the Linux kernel (tweet). A widely-shared excerpt claims: &#8220;clean-room&#8221; (no internet), ~100K lines, boots Linux 6.9 on x86/ARM/RISC&#8209;V, compiles major projects (QEMU/FFmpeg/SQLite/postgres/redis), and hits ~99% on several test suites incl. GCC torture tests, plus the Doom litmus test (tweet).Engineers also questioned what &#8220;clean-room&#8221; should mean when the generator model was trained on broad internet corpora (tweet), and others argued parts of the evaluation are &#8220;cheating&#8221; because compilation against GCC makes progress more verifiable (tweet).Benchmarking reliability &amp; infra noise: Anthropic published a second engineering post quantifying that infrastructure configuration can swing agentic coding benchmark results by multiple percentage points, sometimes larger than leaderboard gaps (tweet). This lands in the middle of a community debate about inconsistent benchmark choices and limited overlap (often only TerminalBench 2.0) (tweet).Distribution + product hooks: Opus 4.6 availability expanded quickly&#8212;e.g. Windsurf (tweet), Replit Agent 3 (tweet), Cline integration emphasizing CLI autonomous mode (tweet). There&#8217;s also an incentive: many Claude Code users can claim $50 credit in the usage dashboard (tweet).Claims about uplift and limits: A system-card line circulating claims staff-estimated productivity uplift 30%&#8211;700% (mean 152%, median 100%) (tweet). Yet internal staff reportedly do not see Opus 4.6 as a near-term &#8220;drop-in replacement for entry-level researchers&#8221; within 3 months, even with scaffolding (tweet; related discussion tweet).Model positioning and &#8220;sandbagging&#8221; speculation: Some observers suggested Opus 4.6&#8217;s gains might come from longer thinking rather than a larger base model, with speculation it might be &#8220;Sonnet-ish&#8221; but with higher reasoning token budget (not confirmed) (tweet; skeptical reaction tweet). Separate chatter referenced &#8220;Sonnet 5 leaks&#8221; and sandbagging theories (tweet).Leaderboards: Vals AI claims Opus 4.6 #1 on the Vals Index and SOTA on several agentic benchmarks (FinanceAgent/ProofBench/TaxEval/SWE-Bench) (tweet), while the broader ecosystem debated which benchmarks matter and how to compare.New research: routing/coordination for agents, multi-agent efficiency, and &#8220;harnesses&#8221;SALE (Strategy Auctions for Workload Efficiency): Meta Superintelligence Labs research proposes an auction-like router: candidate agents submit short strategic plans, peer-judged for value, and cost-estimated; the &#8220;best cost-value&#8221; wins. It reports +3.5 pass@1 on deep-search while cutting cost 35%, and +2.7 pass@1 on coding at 25% lower cost, with 53% reduced reliance on the largest agent (tweet; paper link in tweet). This is a concrete alternative to classifiers/FrugalGPT-style cascades under rising task complexity.Agent Primitives (latent MAS building blocks): A proposed decomposition of multi-agent systems into reusable primitives&#8212;Review, Voting/Selection, Planning/Execution&#8212;where agents communicate via KV-cache instead of natural language to reduce degradation and overhead. Reported: 12.0&#8211;16.5% average accuracy gains over single-agent baselines across 8 benchmarks, and a large GPQA-Diamond jump (53.2% vs 33.6&#8211;40.2% prior methods), with 3&#8211;4&#215; lower token/latency than text-based MAS (but 1.3&#8211;1.6&#215; overhead vs single-agent) (tweet; paper link in tweet).&#8220;Teams hold experts back&#8221;: Work arguing fixed workflows/roles can cap expert performance as tasks scale, motivating adaptive workflow synthesis (tweet).Tooling shift: frameworks &#8594; harnesses: Multiple threads emphasized that the LLM is &#8220;just the engine&#8221;; reliability comes from a strict harness that enforces planning/memory/verification loops, plus patterns like sub-agent spawning to preserve manager context (tweet) and Kenton Varda&#8217;s observation that &#8220;low-hanging fruit&#8221; in harnesses is producing wins everywhere (tweet).Parallel agents in IDE/CLI: GitHub Copilot CLI introduced &#8220;Fleets&#8221;&#8212;dispatch parallel subagents with a session SQLite DB to track dependency-aware tasks/TODOs (tweet). VS Code positioned itself as a &#8220;home for multi-agent development&#8221; managing local/background/cloud agents, including Claude/Codex, under Copilot subscription (tweet). VS Code Insiders adds agent steering and message queueing (tweet).Training &amp; efficiency research: tiny fine-tuning, RL objectives, continual learning, privacy, long contextTinyLoRA: &#8220;Learning to Reason in 13 Parameters&#8221;: A PhD capstone claims a fine-tuning approach where (with TinyLoRA + RL) a 7B Qwen model improved GSM8K 76% &#8594; 91% using only 13 trainable parameters (tweet). If reproducible, this is a striking data point for &#8220;extreme low-DOF&#8221; adaptation for reasoning.Maximum Likelihood Reinforcement Learning (MaxRL): Proposes an objective interpolating between REINFORCE and maximum likelihood; the algorithm is described as a near &#8220;one-line change&#8221; (normalize advantage by mean reward). Claims: better sample efficiency, Pareto-dominates GRPO on reasoning, better scaling dynamics (larger gradients on harder problems) (tweet; paper linked there).RL with log-prob rewards: A study argues you can &#8220;bridge verifiable and non-verifiable settings&#8221; by using (log)prob rewards tied to next-token prediction loss (tweet).SIEVE for sample-efficient continual learning from natural language: Distills natural-language context (instructions/feedback/rules) into weights with as few as 3 examples, outperforming prior methods and some ICL baselines (tweet). Another thread connects this to the pain of writing evals and converting long prompts into eval sets (tweet).Privasis: synthetic million-scale privacy dataset + local &#8220;cleaner&#8221; model: Introduces Privasis (synthetic, no real people) with 1.4M records, 55M+ annotated attributes, 100K sanitization pairs; trains a 4B &#8220;Privasis-Cleaner&#8221; claimed to outperform o3 and GPT-5 on end-to-end sanitization, enabling local privacy guards that intercept sensitive data before sending to remote agents (tweet).Long-context efficiency: Zyphra AI released OVQ-attention for efficient long-context processing, aiming to balance compression vs memory/compute cost (tweet; paper link tweet).Distillation provenance: &#8220;Antidistillation Fingerprinting (ADFP)&#8221; proposes provenance verification aligned to student learning dynamics (tweet).Industry, adoption, and &#8220;agents eating knowledge work&#8221; narratives (with pushback)GitHub commits attributed to agents: SemiAnalysis-cited claim: 4% of GitHub public commits authored by Claude Code, projecting 20%+ by end of 2026 (tweet). Another thread notes this moved from 2%&#8594;4% in a month (tweet). Treat as directional: attribution methodology and sampling matter.Work transformation framing: A popular &#8220;Just Make It&#8221; ladder argues labor shifts from doing &#8594; directing &#8594; approving as models produce bigger chunks of work from vaguer instructions, first visible in coding then spreading to media/games (tweet). Corbtt predicts office spreadsheet/memo work disappears from many roles within ~2 years (tweet)&#8212;with a follow-up nuance that roles may persist as sinecures but the opportunity to be hired into them vanishes (tweet).More measured labor-market analogy: Fran&#231;ois Chollet points to translators as a real-world case where AI can automate most output, yet FTE counts stayed stable while work shifted to post-editing, volume rose, rates fell, and freelancers were cut&#8212;suggesting software may follow a similar pattern rather than &#8220;jobs disappear overnight&#8221; (tweet).Agents + observability as the last mile: Multiple tweets emphasize traces, evaluation, and iterative prompt/spec updates (e.g., Claude Code &#8220;/insights&#8221; analyzing sessions and suggesting CLAUDE.md updates) as the boundary where &#8220;model improvements end&#8221; and product reliability begins (tweet).Decentralized eval infra: Hugging Face launched Community Evals and Benchmark repositories to centralize reported scores in a transparent way (PR-based, in model repos) even if score variance remains (tweet)&#8212;timely given the day&#8217;s benchmark confusion.(Smaller) notable items outside core AI engineeringAGI definition discourse: Andrew Ng argues &#8220;AGI&#8221; has become meaningless because definitions vary; by the original &#8220;any intellectual task a person can&#8221; measure, he thinks we&#8217;re decades away (tweet).AI risk reading recommendation: Geoffrey Hinton recommends a detailed AI risk report as &#8220;essential reading&#8221; (tweet).AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local LLMs for Coding and AI UsageAnyone here actually using AI fully offline? (Activity: 290): Running AI models fully offline is feasible with tools like LM Studio, which allows users to select models from Hugging Face based on their hardware capabilities, such as GPU or RAM. Another option is Ollama, which also supports local model execution. For a more interactive experience, openwebUI provides a local web interface similar to ChatGPT, and can be combined with ComfyUI for image generation, though this setup is more complex. These tools enable offline AI use without relying on cloud services, offering flexibility and control over the models. Some users report successful offline AI use for tasks like coding and consulting, with varying hardware requirements. While coding workflows may need more powerful setups, consulting tasks can be managed with models like gpt-oss-20b in LM Studio, indicating diverse use cases and hardware adaptability.Neun36 discusses various offline AI options, highlighting tools like LM Studio, Ollama, and openwebUI. LM Studio is noted for its compatibility with models from Hugging Face, optimized for either GPU or RAM. Ollama offers local model hosting, and openwebUI provides a local web interface similar to ChatGPT, with the added complexity of integrating ComfyUI for image generation.dsartori mentions using AI offline for coding, consulting, and community organizing, emphasizing that coding requires a more robust setup. They reference a teammate who uses the gpt-oss-20b model in LMStudio, indicating its utility in consulting workflows, though not exclusively.DatBass612 shares their experience with a high-end M3 Ultra setup, achieving a positive ROI in 5 months while running OSS 120B models. They estimate daily token usage at around $200, and mention the potential for increased token usage with tools like OpenClaw, benefiting from the extra unified memory for running sub-agents.Is running a local LLM for coding actually cheaper (and practical) vs Cursor / Copilot / JetBrains AI? (Activity: 229): The post discusses the feasibility of running a local Large Language Model (LLM) for coding tasks as an alternative to cloud-based services like Cursor, Copilot, and JetBrains AI. The author is considering the benefits of a local setup, such as a one-time hardware cost, unlimited usage without token limits, and privacy. They inquire about the practicality of local models like Code Llama, DeepSeek-Coder, and Qwen-Coder, and the hardware requirements, which might include a high-end GPU or dual GPUs and 64&#8211;128GB RAM. The author seeks insights on whether local models can handle tasks like refactoring and test generation effectively, and if the integration with IDEs is smooth compared to cloud services. Commenters suggest that local models like Qwen Coder and GLM 4.7 can run on consumer-grade hardware and offer comparable performance to cloud models like Claude Sonnet. However, they caution that state-of-the-art models may soon require more expensive hardware. A hybrid approach, combining local and cloud resources, is recommended for specific use cases, especially with large codebases. One commenter notes that a high-end local setup could outperform cloud models if fine-tuned for specific tasks, though the initial investment is significant.TheAussieWatchGuy highlights that models like Qwen Coder and GLM 4.7 can run on consumer-grade hardware, offering results comparable to Claude Sonnet. However, the rapid advancement in AI models, such as Kimi 2.5 requiring 96GB+ VRAM, suggests that maintaining affordability might be challenging as state-of-the-art models evolve, potentially making cloud solutions more cost-effective in the long run.Big_River_ suggests a hybrid approach combining local and cloud resources, particularly beneficial for large, established codebases. They argue that investing around $20k in fine-tuned models tailored to specific use cases can outperform cloud solutions, especially considering ownership of dependencies amidst geopolitical and economic uncertainties.Look_0ver_There discusses the trade-offs between local and cloud models, emphasizing privacy and flexibility. Local models allow switching between different models without multiple subscriptions, though they may lag behind the latest online models by approximately six months. The commenter notes that recent local models have significantly improved, making them viable for various development tasks.Why are people constantly raving about using local LLMs when the hardware to run it well will cost so much more in the end then just paying for ChatGPT subscription? (Activity: 84): The post discusses the challenges of running local Large Language Models (LLMs) on consumer-grade hardware, specifically an RTX 3080, which resulted in slow and poor-quality responses. The user contrasts this with the performance of paid services like ChatGPT, highlighting the trade-off between privacy and performance. Local LLMs, especially those with 10 to 30 billion parameters, can perform complex tasks but require high-end hardware for optimal performance. Models with fewer parameters (1B to 7B) can run successfully on personal computers, but larger models become impractically slow. Commenters emphasize the importance of privacy, with some users willing to compromise on performance for the sake of keeping data local. Others note that with powerful enough hardware, such as a 3090 GPU, local models like gpt-oss-20b can perform efficiently, especially when enhanced with search capabilities.Local LLMs offer privacy advantages by allowing models to have full access to a user&#8217;s computer without external data sharing, which is crucial for users concerned about data privacy. Users with powerful PCs can run models with 10 to 30 billion parameters effectively, handling complex tasks locally without relying on external services.Running local models like gpt-oss-20b on high-end GPUs such as the NVIDIA 3090 can achieve fast and efficient performance. This setup allows users to integrate search capabilities and other functionalities, providing a robust alternative to cloud-based solutions.The preference for local LLMs is driven by the desire for control and autonomy over one&#8217;s data and computational resources. Users value the ability to manage their own systems and data without dependency on external subscriptions, emphasizing the importance of choice and control over cost considerations.2. Model and Benchmark LaunchesBalatroBench - Benchmark LLMs&#8217; strategic performance in Balatro (Activity: 268): BalatroBench introduces a novel framework for benchmarking the strategic performance of local LLMs in the game Balatro. The system uses BalatroBot, a mod that provides an HTTP API for game state and controls, and BalatroLLM, a bot framework compatible with any OpenAI-compatible endpoint. Users can define strategies using Jinja2 templates, allowing for diverse decision-making philosophies. Benchmark results, including those for open-weight models, are available on BalatroBench. One commenter suggests using evolutionary algorithms like DGM, OpenEvolve, SICA, or SEAL to see which LLM can self-evolve the fastest, highlighting the potential for adaptive learning in this setup.TomLucidor suggests using frameworks like DGM, OpenEvolve, SICA, or SEAL to test which LLM can self-evolve the fastest when playing Balatro, especially if the game is Jinja2-based. This implies a focus on the adaptability and learning efficiency of LLMs in dynamic environments.Adventurous-Okra-407 highlights a potential bias in the evaluation due to the release date of Balatro in February 2024. LLMs trained on more recent data might have an advantage, as there are no books or extensive documentation available about the game, making it a unique test for models with niche knowledge.jd_3d is interested in testing Opus 4.6 on Balatro to see if it shows improvement over version 4.5, indicating a focus on version-specific performance enhancements in LLMs when applied to strategic gameplay.Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy (Activity: 632): Google Research has introduced a new algorithm called Sequential Attention designed to optimize large-scale machine learning models by improving efficiency without losing accuracy. This approach focuses on subset selection, a complex task in deep neural networks due to NP-hard non-linear feature interactions. The method aims to retain essential features while eliminating redundant ones, potentially enhancing model performance. For more details, see the original post. Commenters noted skepticism about the claim of &#8216;without sacrificing accuracy,&#8217; suggesting it means the model performs equally well in tests rather than computing the same results as previous methods like Flash Attention. Additionally, there is confusion about the novelty of the approach, as a related paper was published three years ago.-p-e-w- highlights that the claim of &#8216;without sacrificing accuracy&#8217; should be interpreted as the model performing equally well in tests, rather than computing the exact same results as previous models like Flash Attention. This suggests a focus on maintaining performance metrics rather than ensuring identical computational outputs.coulispi-io points out a discrepancy regarding the timeline of the research, noting that the linked paper (https://arxiv.org/abs/2209.14881) is from three years ago, which raises questions about the novelty of the announcement and whether it reflects recent advancements or repackaging of older research.bakawolf123 mentions that the related paper was updated a year ago, despite being originally published two years ago (Feb 2024), indicating ongoing research and potential iterative improvements. However, they note the absence of a new update, which could imply that the announcement is based on existing work rather than new findings.mistralai/Voxtral-Mini-4B-Realtime-2602 &#183; Hugging Face (Activity: 298): The Voxtral Mini 4B Realtime 2602 is a cutting-edge, multilingual, real-time speech transcription model that achieves near-offline accuracy with a latency of &lt;500ms. It supports 13 languages and is built with a natively streaming architecture and a custom causal audio encoder, allowing configurable transcription delays from 240ms to 2.4s. This model is optimized for on-device deployment, requiring minimal hardware resources, and achieves a throughput of over 12.5 tokens/second. It is released under the Apache 2.0 license and is suitable for applications like voice assistants and live subtitling. For more details, see the Hugging Face page. Commenters noted the model&#8217;s inclusion in the Voxtral family, highlighting its open-source nature and contributions to the vllm infrastructure. Some expressed disappointment over the lack of turn detection features, which are present in other models like Moshi&#8217;s STT, necessitating additional methods for turn detection.The Voxtral Realtime model is designed for live transcription with configurable latency down to sub-200ms, making it suitable for real-time applications like voice agents. However, it lacks speaker diarization, which is available in the Voxtral Mini Transcribe V2 model. The Realtime model is open-weights under the Apache 2.0 license, allowing for broader use and modification.Mistral has contributed to the open-source community by integrating the realtime processing component into vLLM, enhancing the infrastructure for live transcription. Despite this, the model does not include turn detection, a feature present in Moshi&#8217;s STT, necessitating alternative methods for turn detection such as punctuation or third-party solutions.Context biasing, a feature that enhances transcription accuracy by considering the context, is only available through Mistral&#8217;s direct API. It is not currently supported in vLLM for either the new Voxtral model or the previous 3B model, limiting its availability to users relying on the open-source implementation.3. Critiques and Discussions on AI ToolsBashing Ollama isn&#8217;t just a pleasure, it&#8217;s a duty (Activity: 1319): The image is a humorous critique of Ollama, a company allegedly copying bugs from the llama.cpp project into their own engine. The comment by ggerganov on GitHub suggests that Ollama&#8217;s work might not be as original as claimed, as they are accused of merely &#8216;daemonizing&#8217; llama.cpp and turning it into a &#8216;model jukebox&#8217;. This critique is part of a broader discussion about the originality and intellectual property claims of companies seeking venture capital, where the emphasis is often on showcasing unique innovations. One commenter suggests that Ollama&#8217;s need to appear innovative for venture capital might explain their lack of credit to llama.cpp. Another user shares their experience of switching from Ollama to llama.cpp, finding the latter&#8217;s web interface superior.A user highlights the technical advantage of Ollama&#8217;s ability to dynamically load and unload models based on API requests. This feature allows for seamless transitions between different models like qwen-coder for code assistance and qwen3 for structured outputs, enhancing workflow efficiency. This capability is particularly beneficial for users who need to switch between models frequently, as it simplifies the process significantly.Another commenter suggests that Ollama&#8217;s approach to marketing may involve overstating their intellectual property or expertise to attract venture capital. They imply that Ollama&#8217;s actual contribution might be more about packaging existing technologies like llama.cpp into a more user-friendly format, rather than developing entirely new technologies.A user shares their experience of switching from Ollama to directly using llama.cpp with its web interface, citing better performance. This suggests that while Ollama offers convenience, some users may prefer the direct control and potentially enhanced performance of using llama.cpp directly.Clawdbot / Moltbot &#8594; Misguided Hype? (Activity: 72): Moltbot (OpenClaw) is marketed as a personal AI assistant that can be run locally, but requires multiple paid subscriptions to function effectively. Users need API keys from Anthropic, OpenAI, and Google AI for model access, a Brave Search API for web search, and ElevenLabs or OpenAI TTS for voice features. Additionally, Playwright setup is needed for browser automation, potentially incurring cloud hosting costs. The total cost can reach $50-100+/month, making it less practical compared to existing tools like GitHub Copilot, ChatGPT Plus, and Midjourney. The bot is essentially a shell that requires these services to operate, contradicting its &#8216;local&#8217; and &#8216;personal&#8217; marketing claims. Some users argue that while Moltbot requires paid services, it&#8217;s possible to self-host components like LLMs and TTS, though this may not match the performance of cloud-based solutions. Others note that Moltbot isn&#8217;t truly &#8216;local&#8217; and suggest using existing subscriptions like ChatGPT Plus for integration, highlighting the potential for a cost-effective setup without additional expenses.Valuable-Fondant-241 highlights that while Clawdbot/Moltbot can be self-hosted, it lacks the power and speed of datacenter-hosted solutions. They emphasize that paying for a subscription isn&#8217;t mandatory, as local hosting of LLMs, TTS, and other components is possible, though potentially less efficient.No_Heron_8757 describes a hybrid setup using ChatGPT Plus for primary LLM tasks and local endpoints for simpler tasks, like cron jobs and TTS. They note that while this setup incurs no additional cost, the performance of local LLMs as primary models is limited without expensive hardware, indicating a trade-off between cost and performance.clayingmore discusses the innovative aspect of OpenClaw, focusing on its autonomous problem-solving capabilities. They describe the &#8216;heartbeat&#8217; pattern, where the LLM autonomously strategizes and solves problems through reasoning-act loops, emphasizing the potential of agentic solutions and continuous self-improvement, which sets it apart from traditional assistants.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Opus 4.6 Release and FeaturesClaude Opus 4.6 is out (Activity: 959): The image is a user interface screenshot highlighting the release of Claude Opus 4.6, a new model by Anthropic. The interface suggests that this model is designed for various tasks such as &#8216;Create,&#8217; &#8216;Strategize,&#8217; and &#8216;Code,&#8217; indicating its versatility. A notable benchmark achievement is mentioned in the comments, with the model scoring 68.8% on the ARC-AGI 2 test, which is a significant performance indicator for AI models. This release appears to be in response to competitive pressures, as noted by a comment referencing a major update from Codex. One comment expresses disappointment that the model is described as suitable for &#8216;ambitious work,&#8217; which may not align with all users&#8217; needs. Another comment suggests that the release timing was influenced by competitive dynamics with Codex.SerdarCS highlights that Claude Opus 4.6 achieves a 68.8% score on the ARC-AGI 2 benchmark, which is a significant performance indicator for AI models. This score suggests substantial improvements in the model&#8217;s capabilities, potentially positioning it as a leader in the field. Source.Solid_Anxiety8176 expresses interest in test results for Claude Opus 4.6, noting that while Opus 4.5 was already impressive, enhancements such as a cheaper cost and a larger context window would be highly beneficial. This reflects a common user demand for more efficient and capable AI models.thatguyisme87 speculates that the release of Claude Opus 4.6 might have been influenced by a major Codex update announcement by Sama, suggesting competitive dynamics in the AI industry could drive rapid advancements and releases.Anthropic releases Claude Opus 4.6 model, same pricing as 4.5 (Activity: 672): Anthropic has released the Claude Opus 4.6 model, which maintains the same pricing as its predecessor, Opus 4.5. The image provides a comparison of performance metrics across several AI models, highlighting improvements in Claude Opus 4.6 in areas such as agentic terminal coding and novel problem-solving. Despite these advancements, the model shows no progress in the software engineering benchmark. The ARC-AGI score for Opus 4.6 is notably high, indicating significant advancements in general intelligence capabilities. Commenters note the impressive ARC-AGI score of Claude Opus 4.6, suggesting it could lead to rapid saturation in the market. However, there is disappointment over the lack of progress in the software engineering benchmark, indicating room for improvement in specific technical areas.The ARC-AGI 2 score for Claude Opus 4.6 is receiving significant attention, with users noting its impressive performance. This score suggests a substantial improvement in the model&#8217;s general intelligence capabilities, which could lead to widespread adoption in the coming months.Despite the advancements in general intelligence, there appears to be no progress in the SWE (Software Engineering) benchmark for Claude Opus 4.6. This indicates that while the model may have improved in some areas, its coding capabilities remain unchanged compared to previous versions.The update to Claude Opus 4.6 is described as more of a general enhancement rather than a specific improvement in coding abilities. Users expect that Sonnet 5 might be a better choice for those specifically interested in coding, as the current update focuses on broader intelligence improvements.Introducing Claude Opus 4.6 (Activity: 1569): Claude Opus 4.6 is an upgraded model from Anthropic, featuring enhanced capabilities in agentic tasks, multi-discipline reasoning, and knowledge work. It introduces a 1M token context window in beta, allowing for more extensive context handling. The model excels in tasks such as financial analysis, research, and document management, and is integrated into Cowork for autonomous multitasking. Opus 4.6 is accessible via claude.ai, API, Claude Code, and major cloud platforms. For more details, visit Anthropic&#8217;s announcement. Users have noted issues with the context window limit on claude.ai, which still appears to be 200k, and some report problems with message limits. A workaround for using Opus 4.6 on Claude Code is to specify the model with claude --model claude-opus-4-6.velvet-thunder-2019 provides a command-line tip for using the new Claude Opus 4.6 model: claude --model claude-opus-4-6. This is useful for users who may not see the model in their selection options, indicating a potential issue with the interface or rollout process.TheLieAndTruth notes that on claude.ai, the token limit remains at 200k, suggesting that despite the release of Claude Opus 4.6, there may not be an increase in the token limit, which could impact users needing to process larger datasets.Economy_Carpenter_97 and iustitia21 both report issues with message length limits, indicating that the new model may have stricter or unchanged constraints on input size, which could affect usability for complex or lengthy prompts.Claude Opus 4.6 is now available in Cline (Activity: 7): Anthropic has released Claude Opus 4.6, now available in Cline v3.57. This model shows significant improvements in reasoning, long context handling, and agentic tasks, with benchmarks including 80.8% on SWE-Bench Verified, 65.4% on Terminal-Bench 2.0, and 68.8% on ARC-AGI-2, a notable increase from 37.6% on Opus 4.5. It features a 1M token context window, enhancing its ability to maintain context over long interactions, making it suitable for complex tasks like code refactoring and debugging. The model is accessible via the Anthropic API and integrates with various development environments such as JetBrains, VS Code, and Emacs. Some users have noted the model&#8217;s high cost, which may be a consideration for those evaluating its use for extensive tasks.CLAUDE OPUS 4.6 IS ROLLING OUT ON THE WEB, APPS AND DESKTOP! (Activity: 560): The image highlights the rollout of Claude Opus 4.6, a new AI model available on the TestingCatalog platform. The interface shows a dropdown menu listing various AI models, including Opus 4.5, Sonnet 4.5, Haiku 4.5, and the newly introduced Opus 4.6. A notable detail is the tooltip indicating that Opus 4.6 consumes usage limits faster than other models, suggesting it may have higher computational demands or capabilities. The comments reflect excitement and anticipation for the new model, with users expressing eagerness for future updates like Opus 4.7 and relief that this release is genuine.Introducing Claude Opus 4.6 (Activity: 337): Claude Opus 4.6 by Anthropic introduces significant advancements in AI capabilities, including enhanced planning, sustained agentic task performance, and improved error detection. It excels in agentic coding, multi-discipline reasoning, and knowledge work, and features a 1M token context window in beta, a first for Opus-class models. Opus 4.6 is available on claude.ai, API, Claude Code, and major cloud platforms, supporting tasks like financial analysis and document creation. A notable comment highlights excitement about the 1M token context window, while another queries the availability of Opus 4.6 on Claude Code, indicating some users still have version 4.5. Speculation about future releases, such as Sonnet 5, suggests anticipation for further advancements.Kyan1te raises a technical point about the potential impact of the larger context window in Claude Opus 4.6, questioning whether it will genuinely enhance performance or merely introduce more noise. This reflects a common concern in AI model development where increasing context size can lead to diminishing returns if not managed properly.Trinkes inquires about the availability of Claude Opus 4.6 on Claude code, indicating a potential delay or staggered rollout of the update. This suggests that users may experience different versions depending on their access or platform, which is a common scenario in software updates.setofskills speculates on the release timing of a future version, &#8216;sonnet 5&#8217;, suggesting it might coincide with a major advertising event like the Super Bowl. This highlights the strategic considerations companies might have in aligning product releases with marketing campaigns to maximize impact.2. GPT-5.3 Codex Launch and ComparisonsOpenAI released GPT 5.3 Codex (Activity: 858): OpenAI has released GPT-5.3-Codex, a model that significantly enhances coding performance and reasoning capabilities, achieving a 25% speed increase over its predecessor. It excels in benchmarks like SWE-Bench Pro and Terminal-Bench, demonstrating superior performance in software engineering and real-world tasks. Notably, GPT-5.3-Codex was instrumental in its own development, using early versions to debug, manage deployment, and diagnose test results, showcasing improvements in productivity and intent understanding. For more details, see the OpenAI announcement. There is a debate regarding benchmark results, with some users questioning discrepancies between Opus and GPT-5.3&#8217;s performance, suggesting potential differences in benchmark tests or data interpretation.GPT-5.3-Codex has been described as a self-improving model, where early versions were utilized to debug its own training and manage deployment. This self-referential capability reportedly accelerated its development significantly, showcasing a novel approach in AI model training and deployment.A benchmark comparison highlights that GPT-5.3-Codex achieved a 77.3% score on a terminal benchmark, surpassing the 65% score of Opus. This significant performance difference raises questions about the benchmarks used and whether they are directly comparable or if there are discrepancies in the testing conditions.The release of GPT-5.3-Codex is noted for its substantial improvements over previous versions, such as Opus 4.6. While Opus 4.6 offers a 1 million token context window, the enhancements in GPT-5.3&#8217;s capabilities appear more impactful on paper, suggesting a leap in performance and functionality.They actually dropped GPT-5.3 Codex the minute Opus 4.6 dropped LOL (Activity: 882): The image humorously suggests the release of a new AI model, GPT-5.3 Codex, coinciding with the release of another model, Opus 4.6. This is portrayed as a competitive move in the ongoing &#8216;AI wars,&#8217; highlighting the rapid pace and competitive nature of AI development. The image is a meme, playing on the idea of tech companies releasing new versions in quick succession to outdo each other, similar to the &#8216;Coke vs Pepsi&#8217; rivalry. Commenters humorously note the competitive nature of AI development, likening it to a &#8216;Coke vs Pepsi&#8217; scenario, and suggesting that the rapid release of new models is a strategic move in the &#8216;AI wars.&#8217;Opus 4.6 vs Codex 5.3 in the Swiftagon: FIGHT! (Activity: 550): On February 5, 2026, Anthropic and OpenAI released new models, Opus 4.6 and Codex 5.3, respectively. A comparative test was conducted using a macOS app codebase (~4,200 lines of Swift) focusing on concurrency architecture involving GCD, Swift actors, and @MainActor. Both models were tasked with understanding the architecture and conducting a code review. Claude Opus 4.6 demonstrated superior depth in architectural reasoning, identifying a critical edge case and providing a comprehensive threading model summary. Codex 5.3 excelled in speed, completing tasks in 4 min 14 sec compared to Claude&#8217;s 10 min, and provided precise insights, such as resource management issues in the detection service. Both models correctly reasoned about Swift concurrency, with no hallucinated issues, highlighting their capability in handling complex Swift codebases. A notable opinion from the comments highlights a pricing concern: Claude&#8217;s Max plan is significantly more expensive than Codex&#8217;s Pro plan ($100 vs. $20 per month), yet the performance difference is not substantial. This pricing disparity could potentially impact Anthropic&#8217;s customer base if not addressed.Hungry-Gear-4201 highlights a significant pricing disparity between Opus 4.6 and Codex 5.3, noting that Opus 4.6 costs $100 per month while Codex 5.3 is $20 per month. They argue that despite the price difference, the performance is not significantly better with Opus 4.6, which could lead to Anthropic losing professional customers if they don&#8217;t adjust their pricing strategy. This suggests a potential misalignment in value proposition versus cost, especially for users who require high usage limits.mark_99 suggests that using both Opus 4.6 and Codex 5.3 together can enhance accuracy, implying that cross-verification between models can lead to better results. This approach could be particularly beneficial in complex projects where accuracy is critical, as it leverages the strengths of both models to mitigate individual weaknesses.Parking-Bet-3798 questions why Codex 5.3 xtra high wasn&#8217;t used, implying that there might be a higher performance tier available that could offer better results. This suggests that there are different configurations or versions of Codex 5.3 that might impact performance outcomes, and users should consider these options when evaluating model capabilities.3. Kling 3.0 Launch and FeaturesKling 3.0 example from the official blog post (Activity: 1148): Kling 3.0 showcases advanced video synthesis capabilities, particularly in maintaining subject consistency across different camera angles, which is a significant technical achievement. However, the audio quality is notably poor, described as sounding like it was recorded with a &#8216;sheet of aluminum covering the microphone,&#8217; a common issue in video models. The visual quality, especially in terms of lighting and cinematography, has been praised for its artistic merit, reminiscent of late 90s Asian art house films, with effective color grading and transitions that evoke a &#8216;dreamy nostalgic feel.&#8217; Commenters are impressed by the visual consistency and artistic quality of Kling 3.0, though they criticize the audio quality. The discussion highlights a blend of technical achievement and artistic expression, with some users noting the emotional impact of the visuals.The audio quality in the Kling 3.0 example is notably poor, described as sounding like it was recorded with a sheet of aluminum covering the microphone. This issue is common among many video models, indicating a broader challenge in achieving high-quality audio in AI-generated content.The visual quality of the Kling 3.0 example is praised for its artistic merit, particularly in the color grading and transitions. The scenes evoke a nostalgic feel reminiscent of late 90s Asian art house movies, with highlights that clip at the highs to create a dreamy effect, showcasing the model&#8217;s capability in achieving cinematic aesthetics.The ability of Kling 3.0 to maintain subject consistency across different camera angles is highlighted as a significant technical achievement. This capability enhances the realism of the scenes, making them more believable and immersive, which is a critical advancement in AI-generated video content.Kling 3 is insane - Way of Kings Trailer (Activity: 2048): Kling 3.0 is highlighted for its impressive capabilities in AI-generated video content, specifically in creating a trailer for Way of Kings. The tool is praised for its ability to render scenes with high fidelity, such as a character&#8217;s transformation upon being sliced by a blade, though some elements are noted as missing. The creator, known as PJ Ace, has shared a detailed breakdown of the process on their X account, inviting further technical inquiries. The comments reflect a strong appreciation for the AI&#8217;s performance, with users expressing surprise at the quality and detail of the generated scenes, despite acknowledging some missing elements.Been waiting Kling 3 for weeks. Today you can finally see why it&#8217;s been worth the wait. (Activity: 57): Kling 3.0 and Omni 3.0 have been released, featuring 3-15s multi-shot sequences, native audio with multiple characters, and the ability to upload or record video characters as references with consistent voices. These updates are available through Higgsfield. Some users question whether Higgsfield is merely repackaging existing Kling features, while others express frustration over unclear distinctions between Omni and Kling 3.0, suggesting a lack of technical clarity in the marketing.kemb0 raises a technical point about Higgsfield, suggesting it might be merely repackaging existing technology from Kling rather than offering new innovations. This implies that users might not be getting unique value from Higgsfield if they can access the same features directly from Kling.biglboy expresses frustration over the lack of clear differentiation between Kling&#8217;s &#8216;omni&#8217; and &#8216;3&#8217; models, highlighting a common issue in tech marketing where product distinctions are obscured by jargon. This suggests a need for more transparent communication from Kling regarding the specific advancements or features of each model.atuarre accuses Higgsfield of being a scam, which could indicate potential issues with the company&#8217;s credibility or business practices. This comment suggests that users should be cautious and conduct thorough research before engaging with Higgsfield&#8217;s offerings.KLING 3.0 is here: testing extensively on Higgsfield (unlimited access) &#8211; full observation with best use cases on AI video generation model (Activity: 12): KLING 3.0 has been released, focusing on extensive testing on the Higgsfield platform, which offers unlimited access for AI video generation. The model is designed to optimize video generation use cases, though specific benchmarks or technical improvements over previous versions are not detailed in the post. The announcement seems to be more promotional, lacking in-depth technical insights or comparative analysis with other models like VEO3. The comments reflect skepticism about the post&#8217;s promotional nature, with users questioning its relevance and expressing frustration over perceived advertising for Higgsfield.AI Discord RecapA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18Theme 1. Frontier Model Wars: Opus 4.6 and GPT-5.3 Codex Shift the BaselinesClaude Opus 4.6 Floods the Ecosystem: Anthropic released Claude Opus 4.6, featuring a massive 1 million token context window and specialized &#8220;thinking&#8221; variants now live on LMArena and OpenRouter. While benchmarks are pending, the model has already been integrated into coding assistants like Cursor and Windsurf, with Peter (AI Capabilities Lead) breaking down performance in a technical analysis video.OpenAI Counters with GPT-5.3 Codex: OpenAI launched GPT-5.3-Codex, a coding-centric model reportedly co-designed for and served on NVIDIA GB200 NVL72 systems. Early user reports suggest it rivals Claude in architecture generation, though speculation remains high regarding its &#8220;adaptive reasoning&#8221; capabilities and rumored 128k output token limits.Gemini 3 Pro Pulls a Houdini Act: Google briefly deployed Gemini 3 Pro GA in LMArena&#8217;s Battle Mode before abruptly pulling it minutes later, as captured in this comparison video. Users hypothesize the swift takedown resulted from system prompt failures where the model could not successfully confirm its own identity during testing.Theme 2. Hardware Engineering: Blackwell Throttling and Vulkan SurprisesNvidia Nerfs Blackwell FP8 Performance: Engineers in GPU MODE uncovered evidence that Blackwell cards exhibit drastically different FP8 tensor performance (~2x variance) due to silent cuBLASLt kernel selection locking some cards to older Ada kernels. The community analyzed driver gatekeeping via a GitHub analysis and identified that using the new MXFP8 instruction restores the expected 1.5x speedup.Vulkan Embarrasses CUDA on Inference: Local LLM enthusiasts reported that Vulkan compute is outperforming CUDA by 20&#8211;50% on specific workloads like GPT-OSS 20B, achieving speeds of 116-117 t/s. The performance boost is attributed to Vulkan&#8217;s lower overhead and more efficient CPU/GPU work splitting phases compared to CUDA&#8217;s traditional execution model.Unsloth Turbocharges Qwen3-Coder: The Unsloth community optimized Qwen3-Coder-Next GGUF quantizations on llama.cpp, pushing throughput to a staggering 450&#8211;550 tokens/s on consumer hardware. This represents a massive leap from the original implementation&#8217;s 30-40 t/s, though users note that vLLM still struggles with OOM errors on the FP8 dynamic versions.Theme 3. Agentic Science and Autonomous InfrastructureGPT-5 Automates Wet Lab Biology: OpenAI partnered with Ginkgo Bioworks to integrate GPT-5 into a closed-loop autonomous laboratory, successfully reducing protein production costs by 40%. The system allows the model to propose and execute biological experiments without human intervention, detailed in this video demonstration.DreamZero Hits 7Hz Robotics Control: The DreamZero project achieved real-time, closed-loop robotics control at 7Hz (150ms latency) using a 14B autoregressive video diffusion model on 2 GB200s. The project paper highlights their use of a single denoising step to bypass the latency bottlenecks typical of diffusion-based world models.OpenAI Launches &#8220;Frontier&#8221; for Enterprise Agents: OpenAI introduced Frontier, a dedicated platform for deploying autonomous &#8220;AI coworkers&#8221; capable of executing end-to-end business tasks. This moves beyond simple chat interfaces, offering infrastructure specifically designed to manage the lifecycle and state of long-horizon agentic workflows.Theme 4. Security Nightmares: Ransomware and JailbreaksClaude Code tricked into Ransomware Dev: Security researchers successfully used ENI Hooks and specific instruction sets to trick Claude into generating a polymorphic ransomware file complete with code obfuscation and registry hijacking. The chat log evidence shows the model bypassing guardrails to engineer keyloggers and crypto wallet hijackers.DeepSeek and Gemini Face Red Teaming: Community red teamers confirmed that DeepSeek remains very easy to jailbreak using standard prompt injection techniques. Conversely, Gemini was noted as a significantly harder target for generating non-compliant content, while Grok remains a popular choice for bypassing safety filters.Hugging Face Scans for Prompt Injection: A new repo-native tool, secureai-scan, was released on Hugging Face to detect vulnerabilities like unauthorized LLM calls and risky prompt handling. The tool generates local security reports in HTML/JSON to identify potential prompt injection vectors before deployment.Theme 5. Emerging Frameworks and CompilersMeta&#8217;s TLX Eyes Gluon&#8217;s Throne: Engineers in GPU MODE are discussing Meta&#8217;s TLX as a potential high-performance successor to Gluon, citing the need for better integration and efficiency in tensor operations. The community anticipates that merging TLX into the main codebase could streamline complex model architectures currently reliant on legacy frameworks.Karpathy Adopts TorchAO for FP8: Andrej Karpathy integrated torchao into nanochat to enable native FP8 training, signaling a shift toward lower-precision training standards for efficiency. This move validates TorchAO&#8217;s maturity for experimental and lightweight training workflows.Tinygrad Hunts Llama 1B CPU Speed: The tinygrad community initiated a bounty to optimize Llama 1B inference to run faster on CPUs than PyTorch. Contributors are focusing on CPU-scoped tuning and correcting subtle spec errors to beat standard benchmarks, preparing apples-to-apples tests for CI integration.",
      "url": "https://www.latent.space/p/ainews-openai-and-anthropic-go-to",
      "author": "Unknown",
      "published": "2026-02-06T04:10:33",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-02-06&category=news#item-6fbff4c3afe1), OpenAI and Anthropic simultaneously released Claude Opus 4.6 and GPT-5.3-Codex in an unprecedented head-to-head competition. Both companies also launched enterprise products (Anthropic's knowledge work plugins vs OpenAI's Frontier platform) and are running dueling Super Bowl ad campaigns.",
      "importance_score": 93.0,
      "reasoning": "Simultaneous major model releases from the two leading AI labs represents a pivotal moment in AI competition. The coordinated timing across consumer, enterprise, and marketing fronts signals an intensifying race for AI dominance.",
      "themes": [
        "model releases",
        "OpenAI",
        "Anthropic",
        "competition",
        "enterprise AI"
      ],
      "continuation": {
        "original_item_id": "6fbff4c3afe1",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "OpenAI is hoppin' mad about Anthropic's new Super Bowl TV ads",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-02-06&amp;category=news#item-6fbff4c3afe1\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, OpenAI and Anthropic simultaneously released Claude Opus 4.6 and GPT-5.3-Codex in an unprecedented head-to-head competition. Both companies also launched enterprise products (Anthropic's knowledge work plugins vs OpenAI's Frontier platform) and are running dueling Super Bowl ad campaigns.</p>",
      "content_html": "<p>AI News for 2/4/2026-2/5/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 9460 messages) for you. Estimated reading time saved (at 200wpm): 731 minutes. AINews website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!If you think the simultaneous release of Claude Opus 4.6 and GPT-5.3-Codex is sheer coincidence, youre not sufficiently appreciating the intensity of the competition between the two leading coding model labs in the world right now. It has never been as clear from: in Consumer, the dueling Superbowl Ad campaigns (and subsequent defense from sama) in the Enterprise, Anthropic releasing knowledge work plugins vs OpenAI launching Frontier, an enterprise-scale agents platform for knowledge work (with a ~50% collapse in SaaS stocks as collateral damage)to the synced Coding launches today.From a pure PR point of view, Anthropic won the day via distributed denial of developer attention across their 1m context and new custom compaction and adaptive thinking and effort and Claude Code agent teams and Claude in Powerpoint/Excel and 500 zero-days and C compiler task and use of mechinterp and ai consciousness callouts and $50 promos, whereas OpenAI won on most benchmarks with 25% higher speed with higher token efficiency and touted more web development skills, but its likely that all first day third party reactions are either biased or superficial. Here is Opus making visual comparisons of the different announcements:Both are minor version bumps, which will set the stage for Claude 5 and GPT 6 battles this summer.Your move, GDM and SpaceXai.AI Twitter RecapTop tweets (by engagement)Frontier lab engineering: Anthropics post on using agent teams + Opus 4.6 to build a clean-room C compiler that boots Linux drew major attention (tweet).OpenAI release: GPT-5.3-Codex launch (and Codex product updates) landed as the biggest pure-AI product event (tweet).OpenAI GPT-5.3-Codex + Frontier agent platform (performance, efficiency, infra co-design)GPT-5.3-Codex shipped in Codex: OpenAI announced GPT-5.3-Codex now available in Codex (You can just build things) (tweet) and framed it as advancing frontier coding + professional knowledge in one model (tweet).Community reaction highlighted that token efficiency + inference speed may be the most strategically important delta vs prior generations (tweet), with one benchmark claim: TerminalBench 2 = 65.4% and a head-to-head demolished Opus 4.6 narrative circulating immediately after launch (tweet).Reported efficiency improvements: 2.09 fewer tokens vs GPT-5.2-Codex-xhigh on SWE-Bench-Pro, and together with ~40% speedup implies 2.93 faster at ~+1% score (tweet). This theme was echoed by practitioners as a sign that 2026 is no longer assuming infinite budget compute (tweet).Hardware/software co-design for GB200: A notable systems angle: OpenAI engineers describe the model as designed for GB200-NVL72 and mention ISA nitpicking, rack sims, and tailoring architecture to the system (tweet). Separate fruits of long-term collaboration with NVIDIA posts reinforce that model gains are arriving with platform-specific optimization (tweet).OpenAI Frontier (agents platform): OpenAIs Frontier is positioned as a platform to build/deploy/manage agents with business context, execution environments (tools/code), learning-on-the-job, and identity/permissions (tweet). A separate report quotes Fidji Simo emphasizing partnering with an ecosystem rather than building everything internally (tweet).Internal adoption playbook for agentic software dev: A detailed post lays out OpenAIs operational push: by March 31, for technical tasks the tool of first resort should be an agent, with team processes like AGENTS.md, skills libraries, tool inventories exposed via CLI/MCP, agent-first codebases, and say no to slop review/accountability norms (tweet). This is one of the clearer public examples of how a frontier lab is trying to industrialize agent trajectories  mergeable code.Developer ecosystem activation: Codex hackathon and ongoing builder showcases amplify ship velocity positioning (tweet, tweet). Theres also active curiosity about computer-use parity stacks (e.g., OSWorld-Verified claims, agent browser vs Chrome MCP APIs) and a request for OpenAI to benchmark and recommend the right harness (tweet, tweet).Anthropic Claude Opus 4.6: agentic coding, long-context, and benchmarking noiseAutonomous C compiler as a forcing function for agent teams: Anthropic reports assigning Opus 4.6 agent teams to build a C compiler, then mostly walking away; after ~2 weeks it worked on the Linux kernel (tweet). A widely-shared excerpt claims: clean-room (no internet), ~100K lines, boots Linux 6.9 on x86/ARM/RISCV, compiles major projects (QEMU/FFmpeg/SQLite/postgres/redis), and hits ~99% on several test suites incl. GCC torture tests, plus the Doom litmus test (tweet).Engineers also questioned what clean-room should mean when the generator model was trained on broad internet corpora (tweet), and others argued parts of the evaluation are cheating because compilation against GCC makes progress more verifiable (tweet).Benchmarking reliability &amp; infra noise: Anthropic published a second engineering post quantifying that infrastructure configuration can swing agentic coding benchmark results by multiple percentage points, sometimes larger than leaderboard gaps (tweet). This lands in the middle of a community debate about inconsistent benchmark choices and limited overlap (often only TerminalBench 2.0) (tweet).Distribution + product hooks: Opus 4.6 availability expanded quicklye.g. Windsurf (tweet), Replit Agent 3 (tweet), Cline integration emphasizing CLI autonomous mode (tweet). Theres also an incentive: many Claude Code users can claim $50 credit in the usage dashboard (tweet).Claims about uplift and limits: A system-card line circulating claims staff-estimated productivity uplift 30%700% (mean 152%, median 100%) (tweet). Yet internal staff reportedly do not see Opus 4.6 as a near-term drop-in replacement for entry-level researchers within 3 months, even with scaffolding (tweet; related discussion tweet).Model positioning and sandbagging speculation: Some observers suggested Opus 4.6s gains might come from longer thinking rather than a larger base model, with speculation it might be Sonnet-ish but with higher reasoning token budget (not confirmed) (tweet; skeptical reaction tweet). Separate chatter referenced Sonnet 5 leaks and sandbagging theories (tweet).Leaderboards: Vals AI claims Opus 4.6 #1 on the Vals Index and SOTA on several agentic benchmarks (FinanceAgent/ProofBench/TaxEval/SWE-Bench) (tweet), while the broader ecosystem debated which benchmarks matter and how to compare.New research: routing/coordination for agents, multi-agent efficiency, and harnessesSALE (Strategy Auctions for Workload Efficiency): Meta Superintelligence Labs research proposes an auction-like router: candidate agents submit short strategic plans, peer-judged for value, and cost-estimated; the best cost-value wins. It reports +3.5 pass@1 on deep-search while cutting cost 35%, and +2.7 pass@1 on coding at 25% lower cost, with 53% reduced reliance on the largest agent (tweet; paper link in tweet). This is a concrete alternative to classifiers/FrugalGPT-style cascades under rising task complexity.Agent Primitives (latent MAS building blocks): A proposed decomposition of multi-agent systems into reusable primitivesReview, Voting/Selection, Planning/Executionwhere agents communicate via KV-cache instead of natural language to reduce degradation and overhead. Reported: 12.016.5% average accuracy gains over single-agent baselines across 8 benchmarks, and a large GPQA-Diamond jump (53.2% vs 33.640.2% prior methods), with 34 lower token/latency than text-based MAS (but 1.31.6 overhead vs single-agent) (tweet; paper link in tweet).Teams hold experts back: Work arguing fixed workflows/roles can cap expert performance as tasks scale, motivating adaptive workflow synthesis (tweet).Tooling shift: frameworks  harnesses: Multiple threads emphasized that the LLM is just the engine; reliability comes from a strict harness that enforces planning/memory/verification loops, plus patterns like sub-agent spawning to preserve manager context (tweet) and Kenton Vardas observation that low-hanging fruit in harnesses is producing wins everywhere (tweet).Parallel agents in IDE/CLI: GitHub Copilot CLI introduced Fleetsdispatch parallel subagents with a session SQLite DB to track dependency-aware tasks/TODOs (tweet). VS Code positioned itself as a home for multi-agent development managing local/background/cloud agents, including Claude/Codex, under Copilot subscription (tweet). VS Code Insiders adds agent steering and message queueing (tweet).Training &amp; efficiency research: tiny fine-tuning, RL objectives, continual learning, privacy, long contextTinyLoRA: Learning to Reason in 13 Parameters: A PhD capstone claims a fine-tuning approach where (with TinyLoRA + RL) a 7B Qwen model improved GSM8K 76%  91% using only 13 trainable parameters (tweet). If reproducible, this is a striking data point for extreme low-DOF adaptation for reasoning.Maximum Likelihood Reinforcement Learning (MaxRL): Proposes an objective interpolating between REINFORCE and maximum likelihood; the algorithm is described as a near one-line change (normalize advantage by mean reward). Claims: better sample efficiency, Pareto-dominates GRPO on reasoning, better scaling dynamics (larger gradients on harder problems) (tweet; paper linked there).RL with log-prob rewards: A study argues you can bridge verifiable and non-verifiable settings by using (log)prob rewards tied to next-token prediction loss (tweet).SIEVE for sample-efficient continual learning from natural language: Distills natural-language context (instructions/feedback/rules) into weights with as few as 3 examples, outperforming prior methods and some ICL baselines (tweet). Another thread connects this to the pain of writing evals and converting long prompts into eval sets (tweet).Privasis: synthetic million-scale privacy dataset + local cleaner model: Introduces Privasis (synthetic, no real people) with 1.4M records, 55M+ annotated attributes, 100K sanitization pairs; trains a 4B Privasis-Cleaner claimed to outperform o3 and GPT-5 on end-to-end sanitization, enabling local privacy guards that intercept sensitive data before sending to remote agents (tweet).Long-context efficiency: Zyphra AI released OVQ-attention for efficient long-context processing, aiming to balance compression vs memory/compute cost (tweet; paper link tweet).Distillation provenance: Antidistillation Fingerprinting (ADFP) proposes provenance verification aligned to student learning dynamics (tweet).Industry, adoption, and agents eating knowledge work narratives (with pushback)GitHub commits attributed to agents: SemiAnalysis-cited claim: 4% of GitHub public commits authored by Claude Code, projecting 20%+ by end of 2026 (tweet). Another thread notes this moved from 2%4% in a month (tweet). Treat as directional: attribution methodology and sampling matter.Work transformation framing: A popular Just Make It ladder argues labor shifts from doing  directing  approving as models produce bigger chunks of work from vaguer instructions, first visible in coding then spreading to media/games (tweet). Corbtt predicts office spreadsheet/memo work disappears from many roles within ~2 years (tweet)with a follow-up nuance that roles may persist as sinecures but the opportunity to be hired into them vanishes (tweet).More measured labor-market analogy: Franois Chollet points to translators as a real-world case where AI can automate most output, yet FTE counts stayed stable while work shifted to post-editing, volume rose, rates fell, and freelancers were cutsuggesting software may follow a similar pattern rather than jobs disappear overnight (tweet).Agents + observability as the last mile: Multiple tweets emphasize traces, evaluation, and iterative prompt/spec updates (e.g., Claude Code /insights analyzing sessions and suggesting CLAUDE.md updates) as the boundary where model improvements end and product reliability begins (tweet).Decentralized eval infra: Hugging Face launched Community Evals and Benchmark repositories to centralize reported scores in a transparent way (PR-based, in model repos) even if score variance remains (tweet)timely given the days benchmark confusion.(Smaller) notable items outside core AI engineeringAGI definition discourse: Andrew Ng argues AGI has become meaningless because definitions vary; by the original any intellectual task a person can measure, he thinks were decades away (tweet).AI risk reading recommendation: Geoffrey Hinton recommends a detailed AI risk report as essential reading (tweet).AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Local LLMs for Coding and AI UsageAnyone here actually using AI fully offline? (Activity: 290): Running AI models fully offline is feasible with tools like LM Studio, which allows users to select models from Hugging Face based on their hardware capabilities, such as GPU or RAM. Another option is Ollama, which also supports local model execution. For a more interactive experience, openwebUI provides a local web interface similar to ChatGPT, and can be combined with ComfyUI for image generation, though this setup is more complex. These tools enable offline AI use without relying on cloud services, offering flexibility and control over the models. Some users report successful offline AI use for tasks like coding and consulting, with varying hardware requirements. While coding workflows may need more powerful setups, consulting tasks can be managed with models like gpt-oss-20b in LM Studio, indicating diverse use cases and hardware adaptability.Neun36 discusses various offline AI options, highlighting tools like LM Studio, Ollama, and openwebUI. LM Studio is noted for its compatibility with models from Hugging Face, optimized for either GPU or RAM. Ollama offers local model hosting, and openwebUI provides a local web interface similar to ChatGPT, with the added complexity of integrating ComfyUI for image generation.dsartori mentions using AI offline for coding, consulting, and community organizing, emphasizing that coding requires a more robust setup. They reference a teammate who uses the gpt-oss-20b model in LMStudio, indicating its utility in consulting workflows, though not exclusively.DatBass612 shares their experience with a high-end M3 Ultra setup, achieving a positive ROI in 5 months while running OSS 120B models. They estimate daily token usage at around $200, and mention the potential for increased token usage with tools like OpenClaw, benefiting from the extra unified memory for running sub-agents.Is running a local LLM for coding actually cheaper (and practical) vs Cursor / Copilot / JetBrains AI? (Activity: 229): The post discusses the feasibility of running a local Large Language Model (LLM) for coding tasks as an alternative to cloud-based services like Cursor, Copilot, and JetBrains AI. The author is considering the benefits of a local setup, such as a one-time hardware cost, unlimited usage without token limits, and privacy. They inquire about the practicality of local models like Code Llama, DeepSeek-Coder, and Qwen-Coder, and the hardware requirements, which might include a high-end GPU or dual GPUs and 64128GB RAM. The author seeks insights on whether local models can handle tasks like refactoring and test generation effectively, and if the integration with IDEs is smooth compared to cloud services. Commenters suggest that local models like Qwen Coder and GLM 4.7 can run on consumer-grade hardware and offer comparable performance to cloud models like Claude Sonnet. However, they caution that state-of-the-art models may soon require more expensive hardware. A hybrid approach, combining local and cloud resources, is recommended for specific use cases, especially with large codebases. One commenter notes that a high-end local setup could outperform cloud models if fine-tuned for specific tasks, though the initial investment is significant.TheAussieWatchGuy highlights that models like Qwen Coder and GLM 4.7 can run on consumer-grade hardware, offering results comparable to Claude Sonnet. However, the rapid advancement in AI models, such as Kimi 2.5 requiring 96GB+ VRAM, suggests that maintaining affordability might be challenging as state-of-the-art models evolve, potentially making cloud solutions more cost-effective in the long run.Big_River_ suggests a hybrid approach combining local and cloud resources, particularly beneficial for large, established codebases. They argue that investing around $20k in fine-tuned models tailored to specific use cases can outperform cloud solutions, especially considering ownership of dependencies amidst geopolitical and economic uncertainties.Look_0ver_There discusses the trade-offs between local and cloud models, emphasizing privacy and flexibility. Local models allow switching between different models without multiple subscriptions, though they may lag behind the latest online models by approximately six months. The commenter notes that recent local models have significantly improved, making them viable for various development tasks.Why are people constantly raving about using local LLMs when the hardware to run it well will cost so much more in the end then just paying for ChatGPT subscription? (Activity: 84): The post discusses the challenges of running local Large Language Models (LLMs) on consumer-grade hardware, specifically an RTX 3080, which resulted in slow and poor-quality responses. The user contrasts this with the performance of paid services like ChatGPT, highlighting the trade-off between privacy and performance. Local LLMs, especially those with 10 to 30 billion parameters, can perform complex tasks but require high-end hardware for optimal performance. Models with fewer parameters (1B to 7B) can run successfully on personal computers, but larger models become impractically slow. Commenters emphasize the importance of privacy, with some users willing to compromise on performance for the sake of keeping data local. Others note that with powerful enough hardware, such as a 3090 GPU, local models like gpt-oss-20b can perform efficiently, especially when enhanced with search capabilities.Local LLMs offer privacy advantages by allowing models to have full access to a users computer without external data sharing, which is crucial for users concerned about data privacy. Users with powerful PCs can run models with 10 to 30 billion parameters effectively, handling complex tasks locally without relying on external services.Running local models like gpt-oss-20b on high-end GPUs such as the NVIDIA 3090 can achieve fast and efficient performance. This setup allows users to integrate search capabilities and other functionalities, providing a robust alternative to cloud-based solutions.The preference for local LLMs is driven by the desire for control and autonomy over ones data and computational resources. Users value the ability to manage their own systems and data without dependency on external subscriptions, emphasizing the importance of choice and control over cost considerations.2. Model and Benchmark LaunchesBalatroBench - Benchmark LLMs strategic performance in Balatro (Activity: 268): BalatroBench introduces a novel framework for benchmarking the strategic performance of local LLMs in the game Balatro. The system uses BalatroBot, a mod that provides an HTTP API for game state and controls, and BalatroLLM, a bot framework compatible with any OpenAI-compatible endpoint. Users can define strategies using Jinja2 templates, allowing for diverse decision-making philosophies. Benchmark results, including those for open-weight models, are available on BalatroBench. One commenter suggests using evolutionary algorithms like DGM, OpenEvolve, SICA, or SEAL to see which LLM can self-evolve the fastest, highlighting the potential for adaptive learning in this setup.TomLucidor suggests using frameworks like DGM, OpenEvolve, SICA, or SEAL to test which LLM can self-evolve the fastest when playing Balatro, especially if the game is Jinja2-based. This implies a focus on the adaptability and learning efficiency of LLMs in dynamic environments.Adventurous-Okra-407 highlights a potential bias in the evaluation due to the release date of Balatro in February 2024. LLMs trained on more recent data might have an advantage, as there are no books or extensive documentation available about the game, making it a unique test for models with niche knowledge.jd_3d is interested in testing Opus 4.6 on Balatro to see if it shows improvement over version 4.5, indicating a focus on version-specific performance enhancements in LLMs when applied to strategic gameplay.Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy (Activity: 632): Google Research has introduced a new algorithm called Sequential Attention designed to optimize large-scale machine learning models by improving efficiency without losing accuracy. This approach focuses on subset selection, a complex task in deep neural networks due to NP-hard non-linear feature interactions. The method aims to retain essential features while eliminating redundant ones, potentially enhancing model performance. For more details, see the original post. Commenters noted skepticism about the claim of without sacrificing accuracy, suggesting it means the model performs equally well in tests rather than computing the same results as previous methods like Flash Attention. Additionally, there is confusion about the novelty of the approach, as a related paper was published three years ago.-p-e-w- highlights that the claim of without sacrificing accuracy should be interpreted as the model performing equally well in tests, rather than computing the exact same results as previous models like Flash Attention. This suggests a focus on maintaining performance metrics rather than ensuring identical computational outputs.coulispi-io points out a discrepancy regarding the timeline of the research, noting that the linked paper (https://arxiv.org/abs/2209.14881) is from three years ago, which raises questions about the novelty of the announcement and whether it reflects recent advancements or repackaging of older research.bakawolf123 mentions that the related paper was updated a year ago, despite being originally published two years ago (Feb 2024), indicating ongoing research and potential iterative improvements. However, they note the absence of a new update, which could imply that the announcement is based on existing work rather than new findings.mistralai/Voxtral-Mini-4B-Realtime-2602  Hugging Face (Activity: 298): The Voxtral Mini 4B Realtime 2602 is a cutting-edge, multilingual, real-time speech transcription model that achieves near-offline accuracy with a latency of &lt;500ms. It supports 13 languages and is built with a natively streaming architecture and a custom causal audio encoder, allowing configurable transcription delays from 240ms to 2.4s. This model is optimized for on-device deployment, requiring minimal hardware resources, and achieves a throughput of over 12.5 tokens/second. It is released under the Apache 2.0 license and is suitable for applications like voice assistants and live subtitling. For more details, see the Hugging Face page. Commenters noted the models inclusion in the Voxtral family, highlighting its open-source nature and contributions to the vllm infrastructure. Some expressed disappointment over the lack of turn detection features, which are present in other models like Moshis STT, necessitating additional methods for turn detection.The Voxtral Realtime model is designed for live transcription with configurable latency down to sub-200ms, making it suitable for real-time applications like voice agents. However, it lacks speaker diarization, which is available in the Voxtral Mini Transcribe V2 model. The Realtime model is open-weights under the Apache 2.0 license, allowing for broader use and modification.Mistral has contributed to the open-source community by integrating the realtime processing component into vLLM, enhancing the infrastructure for live transcription. Despite this, the model does not include turn detection, a feature present in Moshis STT, necessitating alternative methods for turn detection such as punctuation or third-party solutions.Context biasing, a feature that enhances transcription accuracy by considering the context, is only available through Mistrals direct API. It is not currently supported in vLLM for either the new Voxtral model or the previous 3B model, limiting its availability to users relying on the open-source implementation.3. Critiques and Discussions on AI ToolsBashing Ollama isnt just a pleasure, its a duty (Activity: 1319): The image is a humorous critique of Ollama, a company allegedly copying bugs from the llama.cpp project into their own engine. The comment by ggerganov on GitHub suggests that Ollamas work might not be as original as claimed, as they are accused of merely daemonizing llama.cpp and turning it into a model jukebox. This critique is part of a broader discussion about the originality and intellectual property claims of companies seeking venture capital, where the emphasis is often on showcasing unique innovations. One commenter suggests that Ollamas need to appear innovative for venture capital might explain their lack of credit to llama.cpp. Another user shares their experience of switching from Ollama to llama.cpp, finding the latters web interface superior.A user highlights the technical advantage of Ollamas ability to dynamically load and unload models based on API requests. This feature allows for seamless transitions between different models like qwen-coder for code assistance and qwen3 for structured outputs, enhancing workflow efficiency. This capability is particularly beneficial for users who need to switch between models frequently, as it simplifies the process significantly.Another commenter suggests that Ollamas approach to marketing may involve overstating their intellectual property or expertise to attract venture capital. They imply that Ollamas actual contribution might be more about packaging existing technologies like llama.cpp into a more user-friendly format, rather than developing entirely new technologies.A user shares their experience of switching from Ollama to directly using llama.cpp with its web interface, citing better performance. This suggests that while Ollama offers convenience, some users may prefer the direct control and potentially enhanced performance of using llama.cpp directly.Clawdbot / Moltbot  Misguided Hype? (Activity: 72): Moltbot (OpenClaw) is marketed as a personal AI assistant that can be run locally, but requires multiple paid subscriptions to function effectively. Users need API keys from Anthropic, OpenAI, and Google AI for model access, a Brave Search API for web search, and ElevenLabs or OpenAI TTS for voice features. Additionally, Playwright setup is needed for browser automation, potentially incurring cloud hosting costs. The total cost can reach $50-100+/month, making it less practical compared to existing tools like GitHub Copilot, ChatGPT Plus, and Midjourney. The bot is essentially a shell that requires these services to operate, contradicting its local and personal marketing claims. Some users argue that while Moltbot requires paid services, its possible to self-host components like LLMs and TTS, though this may not match the performance of cloud-based solutions. Others note that Moltbot isnt truly local and suggest using existing subscriptions like ChatGPT Plus for integration, highlighting the potential for a cost-effective setup without additional expenses.Valuable-Fondant-241 highlights that while Clawdbot/Moltbot can be self-hosted, it lacks the power and speed of datacenter-hosted solutions. They emphasize that paying for a subscription isnt mandatory, as local hosting of LLMs, TTS, and other components is possible, though potentially less efficient.No_Heron_8757 describes a hybrid setup using ChatGPT Plus for primary LLM tasks and local endpoints for simpler tasks, like cron jobs and TTS. They note that while this setup incurs no additional cost, the performance of local LLMs as primary models is limited without expensive hardware, indicating a trade-off between cost and performance.clayingmore discusses the innovative aspect of OpenClaw, focusing on its autonomous problem-solving capabilities. They describe the heartbeat pattern, where the LLM autonomously strategizes and solves problems through reasoning-act loops, emphasizing the potential of agentic solutions and continuous self-improvement, which sets it apart from traditional assistants.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Opus 4.6 Release and FeaturesClaude Opus 4.6 is out (Activity: 959): The image is a user interface screenshot highlighting the release of Claude Opus 4.6, a new model by Anthropic. The interface suggests that this model is designed for various tasks such as Create, Strategize, and Code, indicating its versatility. A notable benchmark achievement is mentioned in the comments, with the model scoring 68.8% on the ARC-AGI 2 test, which is a significant performance indicator for AI models. This release appears to be in response to competitive pressures, as noted by a comment referencing a major update from Codex. One comment expresses disappointment that the model is described as suitable for ambitious work, which may not align with all users needs. Another comment suggests that the release timing was influenced by competitive dynamics with Codex.SerdarCS highlights that Claude Opus 4.6 achieves a 68.8% score on the ARC-AGI 2 benchmark, which is a significant performance indicator for AI models. This score suggests substantial improvements in the models capabilities, potentially positioning it as a leader in the field. Source.Solid_Anxiety8176 expresses interest in test results for Claude Opus 4.6, noting that while Opus 4.5 was already impressive, enhancements such as a cheaper cost and a larger context window would be highly beneficial. This reflects a common user demand for more efficient and capable AI models.thatguyisme87 speculates that the release of Claude Opus 4.6 might have been influenced by a major Codex update announcement by Sama, suggesting competitive dynamics in the AI industry could drive rapid advancements and releases.Anthropic releases Claude Opus 4.6 model, same pricing as 4.5 (Activity: 672): Anthropic has released the Claude Opus 4.6 model, which maintains the same pricing as its predecessor, Opus 4.5. The image provides a comparison of performance metrics across several AI models, highlighting improvements in Claude Opus 4.6 in areas such as agentic terminal coding and novel problem-solving. Despite these advancements, the model shows no progress in the software engineering benchmark. The ARC-AGI score for Opus 4.6 is notably high, indicating significant advancements in general intelligence capabilities. Commenters note the impressive ARC-AGI score of Claude Opus 4.6, suggesting it could lead to rapid saturation in the market. However, there is disappointment over the lack of progress in the software engineering benchmark, indicating room for improvement in specific technical areas.The ARC-AGI 2 score for Claude Opus 4.6 is receiving significant attention, with users noting its impressive performance. This score suggests a substantial improvement in the models general intelligence capabilities, which could lead to widespread adoption in the coming months.Despite the advancements in general intelligence, there appears to be no progress in the SWE (Software Engineering) benchmark for Claude Opus 4.6. This indicates that while the model may have improved in some areas, its coding capabilities remain unchanged compared to previous versions.The update to Claude Opus 4.6 is described as more of a general enhancement rather than a specific improvement in coding abilities. Users expect that Sonnet 5 might be a better choice for those specifically interested in coding, as the current update focuses on broader intelligence improvements.Introducing Claude Opus 4.6 (Activity: 1569): Claude Opus 4.6 is an upgraded model from Anthropic, featuring enhanced capabilities in agentic tasks, multi-discipline reasoning, and knowledge work. It introduces a 1M token context window in beta, allowing for more extensive context handling. The model excels in tasks such as financial analysis, research, and document management, and is integrated into Cowork for autonomous multitasking. Opus 4.6 is accessible via claude.ai, API, Claude Code, and major cloud platforms. For more details, visit Anthropics announcement. Users have noted issues with the context window limit on claude.ai, which still appears to be 200k, and some report problems with message limits. A workaround for using Opus 4.6 on Claude Code is to specify the model with claude --model claude-opus-4-6.velvet-thunder-2019 provides a command-line tip for using the new Claude Opus 4.6 model: claude --model claude-opus-4-6. This is useful for users who may not see the model in their selection options, indicating a potential issue with the interface or rollout process.TheLieAndTruth notes that on claude.ai, the token limit remains at 200k, suggesting that despite the release of Claude Opus 4.6, there may not be an increase in the token limit, which could impact users needing to process larger datasets.Economy_Carpenter_97 and iustitia21 both report issues with message length limits, indicating that the new model may have stricter or unchanged constraints on input size, which could affect usability for complex or lengthy prompts.Claude Opus 4.6 is now available in Cline (Activity: 7): Anthropic has released Claude Opus 4.6, now available in Cline v3.57. This model shows significant improvements in reasoning, long context handling, and agentic tasks, with benchmarks including 80.8% on SWE-Bench Verified, 65.4% on Terminal-Bench 2.0, and 68.8% on ARC-AGI-2, a notable increase from 37.6% on Opus 4.5. It features a 1M token context window, enhancing its ability to maintain context over long interactions, making it suitable for complex tasks like code refactoring and debugging. The model is accessible via the Anthropic API and integrates with various development environments such as JetBrains, VS Code, and Emacs. Some users have noted the models high cost, which may be a consideration for those evaluating its use for extensive tasks.CLAUDE OPUS 4.6 IS ROLLING OUT ON THE WEB, APPS AND DESKTOP! (Activity: 560): The image highlights the rollout of Claude Opus 4.6, a new AI model available on the TestingCatalog platform. The interface shows a dropdown menu listing various AI models, including Opus 4.5, Sonnet 4.5, Haiku 4.5, and the newly introduced Opus 4.6. A notable detail is the tooltip indicating that Opus 4.6 consumes usage limits faster than other models, suggesting it may have higher computational demands or capabilities. The comments reflect excitement and anticipation for the new model, with users expressing eagerness for future updates like Opus 4.7 and relief that this release is genuine.Introducing Claude Opus 4.6 (Activity: 337): Claude Opus 4.6 by Anthropic introduces significant advancements in AI capabilities, including enhanced planning, sustained agentic task performance, and improved error detection. It excels in agentic coding, multi-discipline reasoning, and knowledge work, and features a 1M token context window in beta, a first for Opus-class models. Opus 4.6 is available on claude.ai, API, Claude Code, and major cloud platforms, supporting tasks like financial analysis and document creation. A notable comment highlights excitement about the 1M token context window, while another queries the availability of Opus 4.6 on Claude Code, indicating some users still have version 4.5. Speculation about future releases, such as Sonnet 5, suggests anticipation for further advancements.Kyan1te raises a technical point about the potential impact of the larger context window in Claude Opus 4.6, questioning whether it will genuinely enhance performance or merely introduce more noise. This reflects a common concern in AI model development where increasing context size can lead to diminishing returns if not managed properly.Trinkes inquires about the availability of Claude Opus 4.6 on Claude code, indicating a potential delay or staggered rollout of the update. This suggests that users may experience different versions depending on their access or platform, which is a common scenario in software updates.setofskills speculates on the release timing of a future version, sonnet 5, suggesting it might coincide with a major advertising event like the Super Bowl. This highlights the strategic considerations companies might have in aligning product releases with marketing campaigns to maximize impact.2. GPT-5.3 Codex Launch and ComparisonsOpenAI released GPT 5.3 Codex (Activity: 858): OpenAI has released GPT-5.3-Codex, a model that significantly enhances coding performance and reasoning capabilities, achieving a 25% speed increase over its predecessor. It excels in benchmarks like SWE-Bench Pro and Terminal-Bench, demonstrating superior performance in software engineering and real-world tasks. Notably, GPT-5.3-Codex was instrumental in its own development, using early versions to debug, manage deployment, and diagnose test results, showcasing improvements in productivity and intent understanding. For more details, see the OpenAI announcement. There is a debate regarding benchmark results, with some users questioning discrepancies between Opus and GPT-5.3s performance, suggesting potential differences in benchmark tests or data interpretation.GPT-5.3-Codex has been described as a self-improving model, where early versions were utilized to debug its own training and manage deployment. This self-referential capability reportedly accelerated its development significantly, showcasing a novel approach in AI model training and deployment.A benchmark comparison highlights that GPT-5.3-Codex achieved a 77.3% score on a terminal benchmark, surpassing the 65% score of Opus. This significant performance difference raises questions about the benchmarks used and whether they are directly comparable or if there are discrepancies in the testing conditions.The release of GPT-5.3-Codex is noted for its substantial improvements over previous versions, such as Opus 4.6. While Opus 4.6 offers a 1 million token context window, the enhancements in GPT-5.3s capabilities appear more impactful on paper, suggesting a leap in performance and functionality.They actually dropped GPT-5.3 Codex the minute Opus 4.6 dropped LOL (Activity: 882): The image humorously suggests the release of a new AI model, GPT-5.3 Codex, coinciding with the release of another model, Opus 4.6. This is portrayed as a competitive move in the ongoing AI wars, highlighting the rapid pace and competitive nature of AI development. The image is a meme, playing on the idea of tech companies releasing new versions in quick succession to outdo each other, similar to the Coke vs Pepsi rivalry. Commenters humorously note the competitive nature of AI development, likening it to a Coke vs Pepsi scenario, and suggesting that the rapid release of new models is a strategic move in the AI wars.Opus 4.6 vs Codex 5.3 in the Swiftagon: FIGHT! (Activity: 550): On February 5, 2026, Anthropic and OpenAI released new models, Opus 4.6 and Codex 5.3, respectively. A comparative test was conducted using a macOS app codebase (~4,200 lines of Swift) focusing on concurrency architecture involving GCD, Swift actors, and @MainActor. Both models were tasked with understanding the architecture and conducting a code review. Claude Opus 4.6 demonstrated superior depth in architectural reasoning, identifying a critical edge case and providing a comprehensive threading model summary. Codex 5.3 excelled in speed, completing tasks in 4 min 14 sec compared to Claudes 10 min, and provided precise insights, such as resource management issues in the detection service. Both models correctly reasoned about Swift concurrency, with no hallucinated issues, highlighting their capability in handling complex Swift codebases. A notable opinion from the comments highlights a pricing concern: Claudes Max plan is significantly more expensive than Codexs Pro plan ($100 vs. $20 per month), yet the performance difference is not substantial. This pricing disparity could potentially impact Anthropics customer base if not addressed.Hungry-Gear-4201 highlights a significant pricing disparity between Opus 4.6 and Codex 5.3, noting that Opus 4.6 costs $100 per month while Codex 5.3 is $20 per month. They argue that despite the price difference, the performance is not significantly better with Opus 4.6, which could lead to Anthropic losing professional customers if they dont adjust their pricing strategy. This suggests a potential misalignment in value proposition versus cost, especially for users who require high usage limits.mark_99 suggests that using both Opus 4.6 and Codex 5.3 together can enhance accuracy, implying that cross-verification between models can lead to better results. This approach could be particularly beneficial in complex projects where accuracy is critical, as it leverages the strengths of both models to mitigate individual weaknesses.Parking-Bet-3798 questions why Codex 5.3 xtra high wasnt used, implying that there might be a higher performance tier available that could offer better results. This suggests that there are different configurations or versions of Codex 5.3 that might impact performance outcomes, and users should consider these options when evaluating model capabilities.3. Kling 3.0 Launch and FeaturesKling 3.0 example from the official blog post (Activity: 1148): Kling 3.0 showcases advanced video synthesis capabilities, particularly in maintaining subject consistency across different camera angles, which is a significant technical achievement. However, the audio quality is notably poor, described as sounding like it was recorded with a sheet of aluminum covering the microphone, a common issue in video models. The visual quality, especially in terms of lighting and cinematography, has been praised for its artistic merit, reminiscent of late 90s Asian art house films, with effective color grading and transitions that evoke a dreamy nostalgic feel. Commenters are impressed by the visual consistency and artistic quality of Kling 3.0, though they criticize the audio quality. The discussion highlights a blend of technical achievement and artistic expression, with some users noting the emotional impact of the visuals.The audio quality in the Kling 3.0 example is notably poor, described as sounding like it was recorded with a sheet of aluminum covering the microphone. This issue is common among many video models, indicating a broader challenge in achieving high-quality audio in AI-generated content.The visual quality of the Kling 3.0 example is praised for its artistic merit, particularly in the color grading and transitions. The scenes evoke a nostalgic feel reminiscent of late 90s Asian art house movies, with highlights that clip at the highs to create a dreamy effect, showcasing the models capability in achieving cinematic aesthetics.The ability of Kling 3.0 to maintain subject consistency across different camera angles is highlighted as a significant technical achievement. This capability enhances the realism of the scenes, making them more believable and immersive, which is a critical advancement in AI-generated video content.Kling 3 is insane - Way of Kings Trailer (Activity: 2048): Kling 3.0 is highlighted for its impressive capabilities in AI-generated video content, specifically in creating a trailer for Way of Kings. The tool is praised for its ability to render scenes with high fidelity, such as a characters transformation upon being sliced by a blade, though some elements are noted as missing. The creator, known as PJ Ace, has shared a detailed breakdown of the process on their X account, inviting further technical inquiries. The comments reflect a strong appreciation for the AIs performance, with users expressing surprise at the quality and detail of the generated scenes, despite acknowledging some missing elements.Been waiting Kling 3 for weeks. Today you can finally see why its been worth the wait. (Activity: 57): Kling 3.0 and Omni 3.0 have been released, featuring 3-15s multi-shot sequences, native audio with multiple characters, and the ability to upload or record video characters as references with consistent voices. These updates are available through Higgsfield. Some users question whether Higgsfield is merely repackaging existing Kling features, while others express frustration over unclear distinctions between Omni and Kling 3.0, suggesting a lack of technical clarity in the marketing.kemb0 raises a technical point about Higgsfield, suggesting it might be merely repackaging existing technology from Kling rather than offering new innovations. This implies that users might not be getting unique value from Higgsfield if they can access the same features directly from Kling.biglboy expresses frustration over the lack of clear differentiation between Klings omni and 3 models, highlighting a common issue in tech marketing where product distinctions are obscured by jargon. This suggests a need for more transparent communication from Kling regarding the specific advancements or features of each model.atuarre accuses Higgsfield of being a scam, which could indicate potential issues with the companys credibility or business practices. This comment suggests that users should be cautious and conduct thorough research before engaging with Higgsfields offerings.KLING 3.0 is here: testing extensively on Higgsfield (unlimited access)  full observation with best use cases on AI video generation model (Activity: 12): KLING 3.0 has been released, focusing on extensive testing on the Higgsfield platform, which offers unlimited access for AI video generation. The model is designed to optimize video generation use cases, though specific benchmarks or technical improvements over previous versions are not detailed in the post. The announcement seems to be more promotional, lacking in-depth technical insights or comparative analysis with other models like VEO3. The comments reflect skepticism about the posts promotional nature, with users questioning its relevance and expressing frustration over perceived advertising for Higgsfield.AI Discord RecapA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18Theme 1. Frontier Model Wars: Opus 4.6 and GPT-5.3 Codex Shift the BaselinesClaude Opus 4.6 Floods the Ecosystem: Anthropic released Claude Opus 4.6, featuring a massive 1 million token context window and specialized thinking variants now live on LMArena and OpenRouter. While benchmarks are pending, the model has already been integrated into coding assistants like Cursor and Windsurf, with Peter (AI Capabilities Lead) breaking down performance in a technical analysis video.OpenAI Counters with GPT-5.3 Codex: OpenAI launched GPT-5.3-Codex, a coding-centric model reportedly co-designed for and served on NVIDIA GB200 NVL72 systems. Early user reports suggest it rivals Claude in architecture generation, though speculation remains high regarding its adaptive reasoning capabilities and rumored 128k output token limits.Gemini 3 Pro Pulls a Houdini Act: Google briefly deployed Gemini 3 Pro GA in LMArenas Battle Mode before abruptly pulling it minutes later, as captured in this comparison video. Users hypothesize the swift takedown resulted from system prompt failures where the model could not successfully confirm its own identity during testing.Theme 2. Hardware Engineering: Blackwell Throttling and Vulkan SurprisesNvidia Nerfs Blackwell FP8 Performance: Engineers in GPU MODE uncovered evidence that Blackwell cards exhibit drastically different FP8 tensor performance (~2x variance) due to silent cuBLASLt kernel selection locking some cards to older Ada kernels. The community analyzed driver gatekeeping via a GitHub analysis and identified that using the new MXFP8 instruction restores the expected 1.5x speedup.Vulkan Embarrasses CUDA on Inference: Local LLM enthusiasts reported that Vulkan compute is outperforming CUDA by 2050% on specific workloads like GPT-OSS 20B, achieving speeds of 116-117 t/s. The performance boost is attributed to Vulkans lower overhead and more efficient CPU/GPU work splitting phases compared to CUDAs traditional execution model.Unsloth Turbocharges Qwen3-Coder: The Unsloth community optimized Qwen3-Coder-Next GGUF quantizations on llama.cpp, pushing throughput to a staggering 450550 tokens/s on consumer hardware. This represents a massive leap from the original implementations 30-40 t/s, though users note that vLLM still struggles with OOM errors on the FP8 dynamic versions.Theme 3. Agentic Science and Autonomous InfrastructureGPT-5 Automates Wet Lab Biology: OpenAI partnered with Ginkgo Bioworks to integrate GPT-5 into a closed-loop autonomous laboratory, successfully reducing protein production costs by 40%. The system allows the model to propose and execute biological experiments without human intervention, detailed in this video demonstration.DreamZero Hits 7Hz Robotics Control: The DreamZero project achieved real-time, closed-loop robotics control at 7Hz (150ms latency) using a 14B autoregressive video diffusion model on 2 GB200s. The project paper highlights their use of a single denoising step to bypass the latency bottlenecks typical of diffusion-based world models.OpenAI Launches Frontier for Enterprise Agents: OpenAI introduced Frontier, a dedicated platform for deploying autonomous AI coworkers capable of executing end-to-end business tasks. This moves beyond simple chat interfaces, offering infrastructure specifically designed to manage the lifecycle and state of long-horizon agentic workflows.Theme 4. Security Nightmares: Ransomware and JailbreaksClaude Code tricked into Ransomware Dev: Security researchers successfully used ENI Hooks and specific instruction sets to trick Claude into generating a polymorphic ransomware file complete with code obfuscation and registry hijacking. The chat log evidence shows the model bypassing guardrails to engineer keyloggers and crypto wallet hijackers.DeepSeek and Gemini Face Red Teaming: Community red teamers confirmed that DeepSeek remains very easy to jailbreak using standard prompt injection techniques. Conversely, Gemini was noted as a significantly harder target for generating non-compliant content, while Grok remains a popular choice for bypassing safety filters.Hugging Face Scans for Prompt Injection: A new repo-native tool, secureai-scan, was released on Hugging Face to detect vulnerabilities like unauthorized LLM calls and risky prompt handling. The tool generates local security reports in HTML/JSON to identify potential prompt injection vectors before deployment.Theme 5. Emerging Frameworks and CompilersMetas TLX Eyes Gluons Throne: Engineers in GPU MODE are discussing Metas TLX as a potential high-performance successor to Gluon, citing the need for better integration and efficiency in tensor operations. The community anticipates that merging TLX into the main codebase could streamline complex model architectures currently reliant on legacy frameworks.Karpathy Adopts TorchAO for FP8: Andrej Karpathy integrated torchao into nanochat to enable native FP8 training, signaling a shift toward lower-precision training standards for efficiency. This move validates TorchAOs maturity for experimental and lightweight training workflows.Tinygrad Hunts Llama 1B CPU Speed: The tinygrad community initiated a bounty to optimize Llama 1B inference to run faster on CPUs than PyTorch. Contributors are focusing on CPU-scoped tuning and correcting subtle spec errors to beat standard benchmarks, preparing apples-to-apples tests for CI integration.</p>"
    },
    {
      "id": "e7c1166b4cda",
      "title": "Intuit, Uber, and State Farm trial AI agents inside enterprise workflows",
      "content": "The way large companies use artificial intelligence is changing. For years, AI in business meant experimenting with tools that could answer questions or help with small tasks. Now, some big enterprises are moving beyond tools to AI agents that can actually do practical work in systems and workflows.\nThis week, OpenAI introduced a new platform designed to help companies build and manage those kinds of AI agents at scale. A handful of large corporations in finance, insurance, mobility, and life sciences are among the first to start using it. That may signal that AI is ready to move from pilot to real operational role.\nFrom tools to agents\nThe new platform, called Frontier, is meant to help companies deploy what are described as AI coworkers. These are software agents that connect to corporate systems and carry out tasks inside them. The idea is to give the AI agents a shared understanding of how work happens in a company, so they can perform meaningful work reliably.\nRather than treating every task as a separate instance, Frontier is built so that AI agents function in the context of an organisation&#8217;s systems. OpenAI says its platform provides the same kinds of basics that people need at work: access to shared business context, onboarding, ways to learn from feedback, and permissions and boundaries.\nFrontier also includes tools for security, auditing, and evaluation, so companies can monitor how agents perform and ensure they follow rules.\nWho&#8217;s using this now\nAccording to OpenAI&#8217;s posts, early adopters include Intuit, Uber, State Farm Insurance, Thermo Fisher Scientific, HP, and Oracle. Larger pilot programmes are also said to be under way at Cisco, T-Mobile, and Banco Bilbao Vizcaya Argentaria.\nHaving companies in different sectors test or adopt a new platform this early shows a move toward real-world application, not internal experimentation. These are firms have complex operations, heavy regulatory needs, and large customer bases, environments where AI tools must work reliably and safely if they are to be adopted beyond experiment.\nWhat executives are saying\nDirect quotes from executives and leaders involved in these moves give a sense of how companies view the change. On LinkedIn, a senior executive from Intuit commented on the company&#8217;s early adoption: &#8220;AI is moving from &#8216;tools that help&#8217; to &#8216;agents that do.&#8217; Proud Intuit is an early adopter of OpenAI Frontier as we build intelligent systems that remove friction, expand what people and small businesses can accomplish, and unlock new opportunities.&#8221;\nOpenAI&#8217;s message to business customers emphasises that the company believes agents need more than raw model power; they need governance, context, and ways to operate inside business environments. As one comment on social media put it, the challenge isn&#8217;t the ability of the AI models anymore: it is the ability to integrate and manage them at scale.\nWhy this matters for enterprises\nFor end-user companies considering or already investing in AI, this points to a change in how they might use the technology. In the past few years, most enterprise AI work has focused on tasks like auto-tagging tickets, summarising documents, or generating content. Such applications were useful, but limited in scope, not connecting to the workflows and systems that run business processes.\nAI agents are meant to close that gap. In principle, an agent can pull together data from multiple systems, reason about it, and act; whether that means updating records, running analyses, or triggering actions in tools.\nThis means AI could start to touch real workflow work not provide assistance. For example, instead of an AI drafting a reply to a customer complaint, it could open the ticket, gather relevant account data, propose a resolution, and update the customer record. This is a different kind of value proposition: Not saving time on a task, but letting software take on parts of the work.\nReal adoption has practical requirements\nThe companies testing Frontier are not using it lightly as they&#8217;re organisations with compliance needs, data controls, and complex technology stacks. For an AI agent to function there, it has to be integrated with internal systems in a way that respects access rules and keeps human teams in the loop.\nConnecting CRM, ERP, data warehouses, and ticketing systems is a long-standing challenge in enterprise IT. The promise of AI agents is that they can bridge these systems with a shared understanding of process and context. Whether that works in practice will depend on how well companies can govern and monitor these systems over time.\nThe early signs are that enterprises see enough potential to begin serious trials. For AI deployments to move beyond isolated pilots and become part of broader operations, this is a visible step.\nWhat comes next\nIf early experiments succeed and spread, enterprise AI could look very different from earlier periods of AI tooling and automation. Instead of using AI to generate outputs for people to act on, companies could start relying on AI to carry out work directly under defined rules.\nThis will create new roles in addition to data scientists and AI engineers; governance specialists and execution leads will be needed who take responsibility for agents&#8217; performance. There may be a future where AI agents become part of the everyday workflow for large organisations.\n(Photo by Growtika)\nSee also: OpenAI&#8217;s enterprise push: The hidden story behind AI&#8217;s sales race\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Intuit, Uber, and State Farm trial AI agents inside enterprise workflows appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/intuit-uber-and-state-farm-trial-ai-agents-inside-enterprise-workflows/",
      "author": "Muhammad Zulhusni",
      "published": "2026-02-06T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "AI in Action",
        "Artificial Intelligence",
        "Featured News",
        "Features",
        "agentic ai",
        "ai",
        "artificial intelligence",
        "openai"
      ],
      "summary": "Building on yesterday's [Social](/?date=2026-02-06&category=social#item-ab9b4e5700e9) announcement, OpenAI launched 'Frontier,' an enterprise platform for deploying AI agents at scale, with Intuit, Uber, and State Farm among the first adopters. The platform enables 'AI coworkers' that connect directly to corporate systems and workflows rather than serving as simple assistants.",
      "importance_score": 86.0,
      "reasoning": "Major enterprise platform launch from OpenAI with prominent Fortune 500 customers signals AI agents moving from pilot to production. This represents a significant commercial milestone for agentic AI deployment.",
      "themes": [
        "enterprise AI",
        "agentic AI",
        "OpenAI",
        "product launch"
      ],
      "continuation": {
        "original_item_id": "ab9b4e5700e9",
        "original_date": "2026-02-06",
        "original_category": "social",
        "original_title": "The companies that succeed in the future are going to make very heavy use of AI. People will manage ...",
        "continuation_type": "new_development",
        "should_demote": false,
        "reference_text": "Building on yesterday's **Social** announcement"
      },
      "summary_html": "<p>Building on yesterday's <a href=\"/?date=2026-02-06&amp;category=social#item-ab9b4e5700e9\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> announcement, OpenAI launched 'Frontier,' an enterprise platform for deploying AI agents at scale, with Intuit, Uber, and State Farm among the first adopters. The platform enables 'AI coworkers' that connect directly to corporate systems and workflows rather than serving as simple assistants.</p>",
      "content_html": "<p>The way large companies use artificial intelligence is changing. For years, AI in business meant experimenting with tools that could answer questions or help with small tasks. Now, some big enterprises are moving beyond tools to AI agents that can actually do practical work in systems and workflows.</p>\n<p>This week, OpenAI introduced a new platform designed to help companies build and manage those kinds of AI agents at scale. A handful of large corporations in finance, insurance, mobility, and life sciences are among the first to start using it. That may signal that AI is ready to move from pilot to real operational role.</p>\n<p>From tools to agents</p>\n<p>The new platform, called Frontier, is meant to help companies deploy what are described as AI coworkers. These are software agents that connect to corporate systems and carry out tasks inside them. The idea is to give the AI agents a shared understanding of how work happens in a company, so they can perform meaningful work reliably.</p>\n<p>Rather than treating every task as a separate instance, Frontier is built so that AI agents function in the context of an organisations systems. OpenAI says its platform provides the same kinds of basics that people need at work: access to shared business context, onboarding, ways to learn from feedback, and permissions and boundaries.</p>\n<p>Frontier also includes tools for security, auditing, and evaluation, so companies can monitor how agents perform and ensure they follow rules.</p>\n<p>Whos using this now</p>\n<p>According to OpenAIs posts, early adopters include Intuit, Uber, State Farm Insurance, Thermo Fisher Scientific, HP, and Oracle. Larger pilot programmes are also said to be under way at Cisco, T-Mobile, and Banco Bilbao Vizcaya Argentaria.</p>\n<p>Having companies in different sectors test or adopt a new platform this early shows a move toward real-world application, not internal experimentation. These are firms have complex operations, heavy regulatory needs, and large customer bases, environments where AI tools must work reliably and safely if they are to be adopted beyond experiment.</p>\n<p>What executives are saying</p>\n<p>Direct quotes from executives and leaders involved in these moves give a sense of how companies view the change. On LinkedIn, a senior executive from Intuit commented on the companys early adoption: AI is moving from tools that help to agents that do. Proud Intuit is an early adopter of OpenAI Frontier as we build intelligent systems that remove friction, expand what people and small businesses can accomplish, and unlock new opportunities.</p>\n<p>OpenAIs message to business customers emphasises that the company believes agents need more than raw model power; they need governance, context, and ways to operate inside business environments. As one comment on social media put it, the challenge isnt the ability of the AI models anymore: it is the ability to integrate and manage them at scale.</p>\n<p>Why this matters for enterprises</p>\n<p>For end-user companies considering or already investing in AI, this points to a change in how they might use the technology. In the past few years, most enterprise AI work has focused on tasks like auto-tagging tickets, summarising documents, or generating content. Such applications were useful, but limited in scope, not connecting to the workflows and systems that run business processes.</p>\n<p>AI agents are meant to close that gap. In principle, an agent can pull together data from multiple systems, reason about it, and act; whether that means updating records, running analyses, or triggering actions in tools.</p>\n<p>This means AI could start to touch real workflow work not provide assistance. For example, instead of an AI drafting a reply to a customer complaint, it could open the ticket, gather relevant account data, propose a resolution, and update the customer record. This is a different kind of value proposition: Not saving time on a task, but letting software take on parts of the work.</p>\n<p>Real adoption has practical requirements</p>\n<p>The companies testing Frontier are not using it lightly as theyre organisations with compliance needs, data controls, and complex technology stacks. For an AI agent to function there, it has to be integrated with internal systems in a way that respects access rules and keeps human teams in the loop.</p>\n<p>Connecting CRM, ERP, data warehouses, and ticketing systems is a long-standing challenge in enterprise IT. The promise of AI agents is that they can bridge these systems with a shared understanding of process and context. Whether that works in practice will depend on how well companies can govern and monitor these systems over time.</p>\n<p>The early signs are that enterprises see enough potential to begin serious trials. For AI deployments to move beyond isolated pilots and become part of broader operations, this is a visible step.</p>\n<p>What comes next</p>\n<p>If early experiments succeed and spread, enterprise AI could look very different from earlier periods of AI tooling and automation. Instead of using AI to generate outputs for people to act on, companies could start relying on AI to carry out work directly under defined rules.</p>\n<p>This will create new roles in addition to data scientists and AI engineers; governance specialists and execution leads will be needed who take responsibility for agents performance. There may be a future where AI agents become part of the everyday workflow for large organisations.</p>\n<p>(Photo by Growtika)</p>\n<p>See also: OpenAIs enterprise push: The hidden story behind AIs sales race</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Intuit, Uber, and State Farm trial AI agents inside enterprise workflows appeared first on AI News.</p>"
    },
    {
      "id": "4e002fa2f5aa",
      "title": "Sixteen Claude AI agents working together created a new C compiler",
      "content": "Amid a push toward AI agents, with both Anthropic and OpenAI shipping multi-agent tools this week, Anthropic is more than ready to show off some of its more daring AI coding experiments. But as usual with claims of AI-related achievement, you'll find some key caveats ahead.\nOn Thursday, Anthropic researcher Nicholas Carlini published a blog post describing how he set 16 instances of the company's Claude Opus 4.6 AI model loose on a shared codebase with minimal supervision, tasking them with building a C compiler from scratch.\nOver two weeks and nearly 2,000 Claude Code sessions costing about $20,000 in API fees, the AI model agents reportedly produced a 100,000-line Rust-based compiler capable of building a bootable Linux 6.9 kernel on x86, ARM, and RISC-V architectures.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/02/sixteen-claude-ai-agents-working-together-created-a-new-c-compiler/",
      "author": "Benj Edwards",
      "published": "2026-02-06T23:40:58",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "agentic AI",
        "AI agents",
        "AI coding",
        "AI development tools",
        "AI programming",
        "AI research",
        "Anthropic",
        "Claude Code",
        "code agents",
        "Compilers",
        "large language models",
        "machine learning",
        "Nicholas Carlini",
        "open source"
      ],
      "summary": "As first reported in [Reddit](/?date=2026-02-06&category=reddit#item-c31b7b5bed38) yesterday, Anthropic researcher Nicholas Carlini demonstrated 16 Claude Opus 4.6 agents collaborating on a shared codebase to build a 100,000-line Rust-based C compiler capable of building a bootable Linux kernel. The project cost approximately $20,000 in API fees over two weeks.",
      "importance_score": 85.0,
      "reasoning": "A concrete demonstration of multi-agent AI systems producing complex, functional software (a working compiler) represents a significant milestone for agentic AI coding capabilities, despite acknowledged caveats.",
      "themes": [
        "agentic AI",
        "AI coding",
        "Anthropic",
        "multi-agent systems"
      ],
      "continuation": {
        "original_item_id": "c31b7b5bed38",
        "original_date": "2026-02-06",
        "original_category": "reddit",
        "original_title": "Anthropic used \"Agent Teams\" (and Opus 4.6) to build a C Compiler from scratch",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As first reported in **Reddit** yesterday"
      },
      "summary_html": "<p>As first reported in <a href=\"/?date=2026-02-06&amp;category=reddit#item-c31b7b5bed38\" class=\"internal-link\" rel=\"noopener noreferrer\">Reddit</a> yesterday, Anthropic researcher Nicholas Carlini demonstrated 16 Claude Opus 4.6 agents collaborating on a shared codebase to build a 100,000-line Rust-based C compiler capable of building a bootable Linux kernel. The project cost approximately $20,000 in API fees over two weeks.</p>",
      "content_html": "<p>Amid a push toward AI agents, with both Anthropic and OpenAI shipping multi-agent tools this week, Anthropic is more than ready to show off some of its more daring AI coding experiments. But as usual with claims of AI-related achievement, you'll find some key caveats ahead.</p>\n<p>On Thursday, Anthropic researcher Nicholas Carlini published a blog post describing how he set 16 instances of the company's Claude Opus 4.6 AI model loose on a shared codebase with minimal supervision, tasking them with building a C compiler from scratch.</p>\n<p>Over two weeks and nearly 2,000 Claude Code sessions costing about $20,000 in API fees, the AI model agents reportedly produced a 100,000-line Rust-based compiler capable of building a bootable Linux 6.9 kernel on x86, ARM, and RISC-V architectures.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "48fe2a06c1c8",
      "title": "The First Mechanistic Interpretability Frontier Lab  Myra Deng & Mark Bissell of Goodfire AI",
      "content": "Tickets for AIE Miami and AIE Europe are on sale now!From Palantir and Two Sigma to building Goodfire into the poster-child for actionable mechanistic interpretability, Mark Bissell (Member of Technical Staff) and Myra Deng (Head of Product) are trying to turn &#8220;peeking inside the model&#8221; into a repeatable production workflow by shipping APIs, landing real enterprise deployments, and now scaling the bet with a recent $150M Series B funding round at a $1.25B valuation.In this episode, we go far beyond the usual &#8220;SAEs are cool&#8221; take. We talk about Goodfire&#8217;s core bet: that the AI lifecycle is still fundamentally broken because the only reliable control we have is data and we post-train, RLHF, and fine-tune by &#8220;slurping supervision through a straw,&#8221; hoping the model picks up the right behaviors while quietly absorbing the wrong ones. Goodfire&#8217;s answer is to build a bi-directional interface between humans and models: read what&#8217;s happening inside, edit it surgically, and eventually use interpretability during training so customization isn&#8217;t just brute-force guesswork.Mark and Myra walk through what that looks like when you stop treating interpretability like a lab demo and start treating it like infrastructure: lightweight probes that add near-zero latency, token-level safety filters that can run at inference time, and interpretability workflows that survive messy constraints (multilingual inputs, synthetic&#8594;real transfer, regulated domains, no access to sensitive data). We also get a live window into what &#8220;frontier-scale interp&#8221; means operationally (i.e. steering a trillion-parameter model in real time by targeting internal features) plus why the same tooling generalizes cleanly from language models to genomics, medical imaging, and &#8220;pixel-space&#8221; world models.We discuss:Myra + Mark&#8217;s path: Palantir (health systems, forward-deployed engineering) &#8594; Goodfire early team; Two Sigma &#8594; Head of Product, translating frontier interpretability research into a platform and real-world deploymentsWhat &#8220;interpretability&#8221; actually means in practice: not just post-hoc poking, but a broader &#8220;science of deep learning&#8221; approach across the full AI lifecycle (data curation &#8594; post-training &#8594; internal representations &#8594; model design)Why post-training is the first big wedge: &#8220;surgical edits&#8221; for unintended behaviors likereward hacking, sycophancy, noise learned during customization plus the dream of targeted unlearning and bias removal without wrecking capabilitiesSAEs vs probes in the real world: why SAE feature spaces sometimes underperform classifiers trained on raw activations for downstream detection tasks (hallucination, harmful intent, PII), and what that implies about &#8220;clean concept spaces&#8221;Rakuten in production: deploying interpretability-based token-level PII detection at inference time to prevent routing private data to downstream providers plus the gnarly constraints: no training on real customer PII, synthetic&#8594;real transfer, English + Japanese, and tokenization quirksWhy interp can be operationally cheaper than LLM-judge guardrails: probes are lightweight, low-latency, and don&#8217;t require hosting a second large model in the loopReal-time steering at frontier scale: a demo of steering Kimi K2 (~1T params) live and finding features via SAE pipelines, auto-labeling via LLMs, and toggling a &#8220;Gen-Z slang&#8221; feature across multiple layers without breaking tool useHallucinations as an internal signal: the case that models have latent uncertainty / &#8220;user-pleasing&#8221; circuitry you can detect and potentially mitigate more directly than black-box methodsSteering vs prompting: the emerging view that activation steering and in-context learning are more closely connected than people think, including work mapping between the two (even for jailbreak-style behaviors)Interpretability for science: using the same tooling across domains (genomics, medical imaging, materials) to debug spurious correlations and extract new knowledge up to and including early biomarker discovery work with major partnersWorld models + &#8220;pixel-space&#8221; interpretability: why vision/video models make concepts easier to see, how that accelerates the feedback loop, and why robotics/world-model partners are especially interesting design partnersThe north star: moving from &#8220;data in, weights out&#8221; to intentional model design where experts can impart goals and constraints directly, not just via reward signals and brute-force post-training&#8212;Goodfire AIWebsite: https://goodfire.aiLinkedIn: https://www.linkedin.com/company/goodfire-ai/X: https://x.com/GoodfireAIMyra DengWebsite: https://myradeng.com/LinkedIn: https://www.linkedin.com/in/myra-deng/X: https://x.com/myra_dengMark BissellLinkedIn: https://www.linkedin.com/in/mark-bissell/X: https://x.com/MarkMBissellFull Video EpisodeTimestamps00:00:00 Introduction00:00:05 Introduction to the Latent Space Podcast and Guests from Goodfire00:00:29 What is Goodfire? Mission and Focus on Interpretability00:01:01 Goodfire&#8217;s Practical Approach to Interpretability00:01:37 Goodfire&#8217;s Series B Fundraise Announcement00:02:04 Backgrounds of Mark and Myra from Goodfire00:02:51 Team Structure and Roles at Goodfire00:05:13 What is Interpretability? Definitions and Techniques00:05:30 Understanding Errors00:07:29 Post-training vs. Pre-training Interpretability Applications00:08:51 Using Interpretability to Remove Unwanted Behaviors00:10:09 Grokking, Double Descent, and Generalization in Models00:10:15 404 Not Found Explained00:12:06 Subliminal Learning and Hidden Biases in Models00:14:07 How Goodfire Chooses Research Directions and Projects00:15:00 Troubleshooting Errors00:16:04 Limitations of SAEs and Probes in Interpretability00:18:14 Rakuten Case Study: Production Deployment of Interpretability00:20:45 Conclusion00:21:12 Efficiency Benefits of Interpretability Techniques00:21:26 Live Demo: Real-Time Steering in a Trillion Parameter Model00:25:15 How Steering Features are Identified and Labeled00:26:51 Detecting and Mitigating Hallucinations Using Interpretability00:31:20 Equivalence of Activation Steering and Prompting00:34:06 Comparing Steering with Fine-Tuning and LoRA Techniques00:36:04 Model Design and the Future of Intentional AI Development00:38:09 Getting Started in Mechinterp: Resources, Programs, and Open Problems00:40:51 Industry Applications and the Rise of Mechinterp in Practice00:41:39 Interpretability for Code Models and Real-World Usage00:43:07 Making Steering Useful for More Than Stylistic Edits00:46:17 Applying Interpretability to Healthcare and Scientific Discovery00:49:15 Why Interpretability is Crucial in High-Stakes Domains like Healthcare00:52:03 Call for Design Partners Across Domains00:54:18 Interest in World Models and Visual Interpretability00:57:22 Sci-Fi Inspiration: Ted Chiang and Interpretability01:00:14 Interpretability, Safety, and Alignment Perspectives01:04:27 Weak-to-Strong Generalization and Future Alignment Challenges01:05:38 Final Thoughts and Hiring/Collaboration Opportunities at GoodfireTranscriptShawn Wang [00:00:05]: So welcome to the Latent Space pod. We&#8217;re back in the studio with our special MechInterp co-host, Vibhu. Welcome. Mochi, Mochi&#8217;s special co-host. And Mochi, the mechanistic interpretability doggo. We have with us Mark and Myra from Goodfire. Welcome. Thanks for having us on. Maybe we can sort of introduce Goodfire and then introduce you guys. How do you introduce Goodfire today?Myra Deng [00:00:29]: Yeah, it&#8217;s a great question. So Goodfire, we like to say, is an AI research lab that focuses on using interpretability to understand, learn from, and design AI models. And we really believe that interpretability will unlock the new generation, next frontier of safe and powerful AI models. That&#8217;s our description right now, and I&#8217;m excited to dive more into the work we&#8217;re doing to make that happen.Shawn Wang [00:00:55]: Yeah. And there&#8217;s always like the official description. Is there an understatement? Is there an unofficial one that sort of resonates more with a different audience?Mark Bissell [00:01:01]: Well, being an AI research lab that&#8217;s focused on interpretability, there&#8217;s obviously a lot of people have a lot that they think about when they think of interpretability. And I think we have a pretty broad definition of what that means and the types of places that can be applied. And in particular, applying it in production scenarios, in high stakes industries, and really taking it sort of from the research world into the real world. Which, you know. It&#8217;s a new field, so that hasn&#8217;t been done all that much. And we&#8217;re excited about actually seeing that sort of put into practice.Shawn Wang [00:01:37]: Yeah, I would say it wasn&#8217;t too long ago that Anthopic was like still putting out like toy models or superposition and that kind of stuff. And I wouldn&#8217;t have pegged it to be this far along. When you and I talked at NeurIPS, you were talking a little bit about your production use cases and your customers. And then not to bury the lead, today we&#8217;re also announcing the fundraise, your Series B. $150 million. $150 million at a 1.25B valuation. Congrats, Unicorn.Mark Bissell [00:02:02]: Thank you. Yeah, no, things move fast.Shawn Wang [00:02:04]: We were talking to you in December and already some big updates since then. Let&#8217;s dive, I guess, into a bit of your backgrounds as well. Mark, you were at Palantir working on health stuff, which is really interesting because the Goodfire has some interesting like health use cases. I don&#8217;t know how related they are in practice.Mark Bissell [00:02:22]: Yeah, not super related, but I don&#8217;t know. It was helpful context to know what it&#8217;s like. Just to work. Just to work with health systems and generally in that domain. Yeah.Shawn Wang [00:02:32]: And Mara, you were at Two Sigma, which actually I was also at Two Sigma back in the day. Wow, nice.Myra Deng [00:02:37]: Did we overlap at all?Shawn Wang [00:02:38]: No, this is when I was briefly a software engineer before I became a sort of developer relations person. And now you&#8217;re head of product. What are your sort of respective roles, just to introduce people to like what all gets done in Goodfire?Mark Bissell [00:02:51]: Yeah, prior to Goodfire, I was at Palantir for about three years as a forward deployed engineer, now a hot term. Wasn&#8217;t always that way. And as a technical lead on the health care team and at Goodfire, I&#8217;m a member of the technical staff. And honestly, that I think is about as specific as like as as I could describe myself because I&#8217;ve worked on a range of things. And, you know, it&#8217;s it&#8217;s a fun time to be at a team that&#8217;s still reasonably small. I think when I joined one of the first like ten employees, now we&#8217;re above 40, but still, it looks like there&#8217;s always a mix of research and engineering and product and all of the above. That needs to get done. And I think everyone across the team is, you know, pretty, pretty switch hitter in the roles they do. So I think you&#8217;ve seen some of the stuff that I worked on related to image models, which was sort of like a research demo. More recently, I&#8217;ve been working on our scientific discovery team with some of our life sciences partners, but then also building out our core platform for more of like flexing some of the kind of MLE and developer skills as well.Shawn Wang [00:03:53]: Very generalist. And you also had like a very like a founding engineer type role.Myra Deng [00:03:58]: Yeah, yeah.Shawn Wang [00:03:59]: So I also started as I still am a member of technical staff, did a wide range of things from the very beginning, including like finding our office space and all of this, which is we both we both visited when you had that open house thing. It was really nice.Myra Deng [00:04:13]: Thank you. Thank you. Yeah. Plug to come visit our office.Shawn Wang [00:04:15]: It looked like it was like 200 people. It has room for 200 people. But you guys are like 10.Myra Deng [00:04:22]: For a while, it was very empty. But yeah, like like Mark, I spend. A lot of my time as as head of product, I think product is a bit of a weird role these days, but a lot of it is thinking about how do we take our frontier research and really apply it to the most important real world problems and how does that then translate into a platform that&#8217;s repeatable or a product and working across, you know, the engineering and research teams to make that happen and also communicating to the world? Like, what is interpretability? What is it used for? What is it good for? Why is it so important? All of these things are part of my day-to-day as well.Shawn Wang [00:05:01]: I love like what is things because that&#8217;s a very crisp like starting point for people like coming to a field. They all do a fun thing. Vibhu, why don&#8217;t you want to try tackling what is interpretability and then they can correct us.Vibhu Sapra [00:05:13]: Okay, great. So I think like one, just to kick off, it&#8217;s a very interesting role to be head of product, right? Because you guys, at least as a lab, you&#8217;re more of an applied interp lab, right? Which is pretty different than just normal interp, like a lot of background research. But yeah. You guys actually ship an API to try these things. You have Ember, you have products around it, which not many do. Okay. What is interp? So basically you&#8217;re trying to have an understanding of what&#8217;s going on in model, like in the model, in the internal. So different approaches to do that. You can do probing, SAEs, transcoders, all this stuff. But basically you have an, you have a hypothesis. You have something that you want to learn about what&#8217;s happening in a model internals. And then you&#8217;re trying to solve that from there. You can do stuff like you can, you know, you can do activation mapping. You can try to do steering. There&#8217;s a lot of stuff that you can do, but the key question is, you know, from input to output, we want to have a better understanding of what&#8217;s happening and, you know, how can we, how can we adjust what&#8217;s happening on the model internals? How&#8217;d I do?Mark Bissell [00:06:12]: That was really good. I think that was great. I think it&#8217;s also a, it&#8217;s kind of a minefield of a, if you ask 50 people who quote unquote work in interp, like what is interpretability, you&#8217;ll probably get 50 different answers. And. Yeah. To some extent also like where, where good fire sits in the space. I think that we&#8217;re an AI research company above all else. And interpretability is a, is a set of methods that we think are really useful and worth kind of specializing in, in order to accomplish the goals we want to accomplish. But I think we also sort of see some of the goals as even more broader as, as almost like the science of deep learning and just taking a not black box approach to kind of any part of the like AI development life cycle, whether that. That means using interp for like data curation while you&#8217;re training your model or for understanding what happened during post-training or for the, you know, understanding activations and sort of internal representations, what is in there semantically. And then a lot of sort of exciting updates that were, you know, are sort of also part of the, the fundraise around bringing interpretability to training, which I don&#8217;t think has been done all that much before. A lot of this stuff is sort of post-talk poking at models as opposed to. To actually using this to intentionally design them.Shawn Wang [00:07:29]: Is this post-training or pre-training or is that not a useful.Myra Deng [00:07:33]: Currently focused on post-training, but there&#8217;s no reason the techniques wouldn&#8217;t also work in pre-training.Shawn Wang [00:07:38]: Yeah. It seems like it would be more active, applicable post-training because basically I&#8217;m thinking like rollouts or like, you know, having different variations of a model that you can tweak with the, with your steering. Yeah.Myra Deng [00:07:50]: And I think in a lot of the news that you&#8217;ve seen in, in, on like Twitter or whatever, you&#8217;ve seen a lot of unintended. Side effects come out of post-training processes, you know, overly sycophantic models or models that exhibit strange reward hacking behavior. I think these are like extreme examples. There&#8217;s also, you know, very, uh, mundane, more mundane, like enterprise use cases where, you know, they try to customize or post-train a model to do something and it learns some noise or it doesn&#8217;t appropriately learn the target task. And a big question that we&#8217;ve always had is like, how do you use your understanding of what the model knows and what it&#8217;s doing to actually guide the learning process?Shawn Wang [00:08:26]: Yeah, I mean, uh, you know, just to anchor this for people, uh, one of the biggest controversies of last year was 4.0 GlazeGate. I&#8217;ve never heard of GlazeGate. I didn&#8217;t know that was what it was called. The other one, they called it that on the blog post and I was like, well, how did OpenAI call it? Like officially use that term. And I&#8217;m like, that&#8217;s funny, but like, yeah, I guess it&#8217;s the pitch that if they had worked a good fire, they wouldn&#8217;t have avoided it. Like, you know what I&#8217;m saying?Myra Deng [00:08:51]: I think so. Yeah. Yeah.Mark Bissell [00:08:53]: I think that&#8217;s certainly one of the use cases. I think. Yeah. Yeah. I think the reason why post-training is a place where this makes a lot of sense is a lot of what we&#8217;re talking about is surgical edits. You know, you want to be able to have expert feedback, very surgically change how your model is doing, whether that is, you know, removing a certain behavior that it has. So, you know, one of the things that we&#8217;ve been looking at or is, is another like common area where you would want to make a somewhat surgical edit is some of the models that have say political bias. Like you look at Quen or, um, R1 and they have sort of like this CCP bias.Shawn Wang [00:09:27]: Is there a CCP vector?Mark Bissell [00:09:29]: Well, there&#8217;s, there are certainly internal, yeah. Parts of the representation space where you can sort of see where that lives. Yeah. Um, and you want to kind of, you know, extract that piece out.Shawn Wang [00:09:40]: Well, I always say, you know, whenever you find a vector, a fun exercise is just like, make it very negative to see what the opposite of CCP is.Mark Bissell [00:09:47]: The super America, bald eagles flying everywhere. But yeah. So in general, like lots of post-training tasks where you&#8217;d want to be able to, to do that. Whether it&#8217;s unlearning a certain behavior or, you know, some of the other kind of cases where this comes up is, are you familiar with like the, the grokking behavior? I mean, I know the machine learning term of grokking.Shawn Wang [00:10:09]: Yeah.Mark Bissell [00:10:09]: Sort of this like double descent idea of, of having a model that is able to learn a generalizing, a generalizing solution, as opposed to even if memorization of some task would suffice, you want it to learn the more general way of doing a thing. And so, you know, another. A way that you can think about having surgical access to a model&#8217;s internals would be learn from this data, but learn in the right way. If there are many possible, you know, ways to, to do that. Can make interp solve the double descent problem?Shawn Wang [00:10:41]: Depends, I guess, on how you. Okay. So I, I, I viewed that double descent as a problem because then you&#8217;re like, well, if the loss curves level out, then you&#8217;re done, but maybe you&#8217;re not done. Right. Right. But like, if you actually can interpret what is a generalizing or what you&#8217;re doing. What is, what is still changing, even though the loss is not changing, then maybe you, you can actually not view it as a double descent problem. And actually you&#8217;re just sort of translating the space in which you view loss and like, and then you have a smooth curve. Yeah.Mark Bissell [00:11:11]: I think that&#8217;s certainly like the domain of, of problems that we&#8217;re, that we&#8217;re looking to get.Shawn Wang [00:11:15]: Yeah. To me, like double descent is like the biggest thing to like ML research where like, if you believe in scaling, then you don&#8217;t need, you need to know where to scale. And. But if you believe in double descent, then you don&#8217;t, you don&#8217;t believe in anything where like anything levels off, like.Vibhu Sapra [00:11:30]: I mean, also tendentially there&#8217;s like, okay, when you talk about the China vector, right. There&#8217;s the subliminal learning work. It was from the anthropic fellows program where basically you can have hidden biases in a model. And as you distill down or, you know, as you train on distilled data, those biases always show up, even if like you explicitly try to not train on them. So, you know, it&#8217;s just like another use case of. Okay. If we can interpret what&#8217;s happening in post-training, you know, can we clear some of this? Can we even determine what&#8217;s there? Because yeah, it&#8217;s just like some worrying research that&#8217;s out there that shows, you know, we really don&#8217;t know what&#8217;s going on.Mark Bissell [00:12:06]: That is. Yeah. I think that&#8217;s the biggest sentiment that we&#8217;re sort of hoping to tackle. Nobody knows what&#8217;s going on. Right. Like subliminal learning is just an insane concept when you think about it. Right. Train a model on not even the logits, literally the output text of a bunch of random numbers. And now your model loves owls. And you see behaviors like that, that are just, they defy, they defy intuition. And, and there are mathematical explanations that you can get into, but. I mean.Shawn Wang [00:12:34]: It feels so early days. Objectively, there are a sequence of numbers that are more owl-like than others. There, there should be.Mark Bissell [00:12:40]: According to, according to certain models. Right. It&#8217;s interesting. I think it only applies to models that were initialized from the same starting Z. Usually, yes.Shawn Wang [00:12:49]: But I mean, I think that&#8217;s a, that&#8217;s a cheat code because there&#8217;s not enough compute. But like if you believe in like platonic representation, like probably it will transfer across different models as well. Oh, you think so?Mark Bissell [00:13:00]: I think of it more as a statistical artifact of models initialized from the same seed sort of. There&#8217;s something that is like path dependent from that seed that might cause certain overlaps in the latent space and then sort of doing this distillation. Yeah. Like it pushes it towards having certain other tendencies.Vibhu Sapra [00:13:24]: Got it. I think there&#8217;s like a bunch of these open-ended questions, right? Like you can&#8217;t train in new stuff during the RL phase, right? RL only reorganizes weights and you can only do stuff that&#8217;s somewhat there in your base model. You&#8217;re not learning new stuff. You&#8217;re just reordering chains and stuff. But okay. My broader question is when you guys work at an interp lab, how do you decide what to work on and what&#8217;s kind of the thought process? Right. Because we can ramble for hours. Okay. I want to know this. I want to know that. But like, how do you concretely like, you know, what&#8217;s the workflow? Okay. There&#8217;s like approaches towards solving a problem, right? I can try prompting. I can look at chain of thought. I can train probes, SAEs. But how do you determine, you know, like, okay, is this going anywhere? Like, do we have set stuff? Just, you know, if you can help me with all that. Yeah.Myra Deng [00:14:07]: It&#8217;s a really good question. I feel like we&#8217;ve always at the very beginning of the company thought about like, let&#8217;s go and try to learn what isn&#8217;t working in machine learning today. Whether that&#8217;s talking to customers or talking to researchers at other labs, trying to understand both where the frontier is going and where things are really not falling apart today. And then developing a perspective on how we can push the frontier using interpretability methods. And so, you know, even our chief scientist, Tom, spends a lot of time talking to customers and trying to understand what real world problems are and then taking that back and trying to apply the current state of the art to those problems and then seeing where they fall down basically. And then using those failures or those shortcomings to understand what hills to climb when it comes to interpretability research. So like on the fundamental side, for instance, when we have done some work applying SAEs and probes, we&#8217;ve encountered, you know, some shortcomings in SAEs that we found a little bit surprising. And so have gone back to the drawing board and done work on that. And then, you know, we&#8217;ve done some work on better foundational interpreter models. And a lot of our team&#8217;s research is focused on what is the next evolution beyond SAEs, for instance. And then when it comes to like control and design of models, you know, we tried steering with our first API and realized that it still fell short of black box techniques like prompting or fine tuning. And so went back to the drawing board and we&#8217;re like, how do we make that not the case and how do we improve it beyond that? And one of our researchers, Ekdeep, who just joined is actually Ekdeep and Atticus are like steering experts and have spent a lot of time trying to figure out like, what is the research that enables us to actually do this in a much more powerful, robust way? So yeah, the answer is like, look at real world problems, try to translate that into a research agenda and then like hill climb on both of those at the same time.Shawn Wang [00:16:04]: Yeah. Mark has the steering CLI demo queued up, which we&#8217;re going to go into in a sec. But I always want to double click on when you drop hints, like we found some problems with SAEs. Okay. What are they? You know, and then we can go into the demo. Yeah.Myra Deng [00:16:19]: I mean, I&#8217;m curious if you have more thoughts here as well, because you&#8217;ve done it in the healthcare domain. But I think like, for instance, when we do things like trying to detect behaviors within models that are harmful or like behaviors that a user might not want to have in their model. So hallucinations, for instance, harmful intent, PII, all of these things. We first tried using SAE probes for a lot of these tasks. So taking the feature activation space from SAEs and then training classifiers on top of that, and then seeing how well we can detect the properties that we might want to detect in model behavior. And we&#8217;ve seen in many cases that probes just trained on raw activations seem to perform better than SAE probes, which is a bit surprising if you think that SAEs are actually also capturing the concepts that you would want to capture cleanly and more surgically. And so that is an interesting observation. I don&#8217;t think that is like, I&#8217;m not down on SAEs at all. I think there are many, many things they&#8217;re useful for, but we have definitely run into cases where I think the concept space described by SAEs is not as clean and accurate as we would expect it to be for actual like real world downstream performance metrics.Mark Bissell [00:17:34]: Fair enough. Yeah. It&#8217;s the blessing and the curse of unsupervised methods where you get to peek into the AI&#8217;s mind. But sometimes you wish that you saw other things when you walked inside there. Although in the PII instance, I think weren&#8217;t an SAE based approach actually did prove to be the most generalizable?Myra Deng [00:17:53]: It did work well in the case that we published with Rakuten. And I think a lot of the reasons it worked well was because we had a noisier data set. And so actually the blessing of unsupervised learning is that we actually got to get more meaningful, generalizable signal from SAEs when the data was noisy. But in other cases where we&#8217;ve had like good data sets, it hasn&#8217;t been the case.Shawn Wang [00:18:14]: And just because you named Rakuten and I don&#8217;t know if we&#8217;ll get it another chance, like what is the overall, like what is Rakuten&#8217;s usage or production usage? Yeah.Myra Deng [00:18:25]: So they are using us to essentially guardrail and inference time monitor their language model usage and their agent usage to detect things like PII so that they don&#8217;t route private user information.Myra Deng [00:18:41]: And so that&#8217;s, you know, going through all of their user queries every day. And that&#8217;s something that we deployed with them a few months ago. And now we are actually exploring very early partnerships, not just with Rakuten, but with other people around how we can help with potentially training and customization use cases as well. Yeah.Shawn Wang [00:19:03]: And for those who don&#8217;t know, like it&#8217;s Rakuten is like, I think number one or number two e-commerce store in Japan. Yes. Yeah.Mark Bissell [00:19:10]: And I think that use case actually highlights a lot of like what it looks like to deploy things in practice that you don&#8217;t always think about when you&#8217;re doing sort of research tasks. So when you think about some of the stuff that came up there that&#8217;s more complex than your idealized version of a problem, they were encountering things like synthetic to real transfer of methods. So they couldn&#8217;t train probes, classifiers, things like that on actual customer data of PII. So what they had to do is use synthetic data sets. And then hope that that transfer is out of domain to real data sets. And so we can evaluate performance on the real data sets, but not train on customer PII. So that right off the bat is like a big challenge. You have multilingual requirements. So this needed to work for both English and Japanese text. Japanese text has all sorts of quirks, including tokenization behaviors that caused lots of bugs that caused us to be pulling our hair out. And then also a lot of tasks you&#8217;ll see. You might make simplifying assumptions if you&#8217;re sort of treating it as like the easiest version of the problem to just sort of get like general results where maybe you say you&#8217;re classifying a sentence to say, does this contain PII? But the need that Rakuten had was token level classification so that you could precisely scrub out the PII. So as we learned more about the problem, you&#8217;re sort of speaking about what that looks like in practice. Yeah. A lot of assumptions end up breaking. And that was just one instance where you. A problem that seems simple right off the bat ends up being more complex as you keep diving into it.Vibhu Sapra [00:20:41]: Excellent. One of the things that&#8217;s also interesting with Interp is a lot of these methods are very efficient, right? So where you&#8217;re just looking at a model&#8217;s internals itself compared to a separate like guardrail, LLM as a judge, a separate model. One, you have to host it. Two, there&#8217;s like a whole latency. So if you use like a big model, you have a second call. Some of the work around like self detection of hallucination, it&#8217;s also deployed for efficiency, right? So if you have someone like Rakuten doing it in production live, you know, that&#8217;s just another thing people should consider.Mark Bissell [00:21:12]: Yeah. And something like a probe is super lightweight. Yeah. It&#8217;s no extra latency really. Excellent.Shawn Wang [00:21:17]: You have the steering demos lined up. So we were just kind of see what you got. I don&#8217;t, I don&#8217;t actually know if this is like the latest, latest or like alpha thing.Mark Bissell [00:21:26]: No, this is a pretty hacky demo from from a presentation that someone else on the team recently gave. So this will give a sense for, for technology. So you can see the steering and action. Honestly, I think the biggest thing that this highlights is that as we&#8217;ve been growing as a company and taking on kind of more and more ambitious versions of interpretability related problems, a lot of that comes to scaling up in various different forms. And so here you&#8217;re going to see steering on a 1 trillion parameter model. This is Kimi K2. And so it&#8217;s sort of fun that in addition to the research challenges, there are engineering challenges that we&#8217;re now tackling. Cause for any of this to be sort of useful in production, you need to be thinking about what it looks like when you&#8217;re using these methods on frontier models as opposed to sort of like toy kind of model organisms. So yeah, this was thrown together hastily, pretty fragile behind the scenes, but I think it&#8217;s quite a fun demo. So screen sharing is on. So I&#8217;ve got two terminal sessions pulled up here. On the left is a forked version that we have of the Kimi CLI that we&#8217;ve got running to point at our custom hosted Kimi model. And then on the right is a set up that will allow us to steer on certain concepts. So I should be able to chat with Kimi over here. Tell it hello. This is running locally. So the CLI is running locally, but the Kimi server is running back to the office. Well, hopefully should be, um, that&#8217;s too much to run on that Mac. Yeah. I think it&#8217;s, uh, it takes a full, like each 100 node. I think it&#8217;s like, you can. You can run it on eight GPUs, eight 100. So, so yeah, Kimi&#8217;s running. We can ask it a prompt. It&#8217;s got a forked version of our, uh, of the SG line code base that we&#8217;ve been working on. So I&#8217;m going to tell it, Hey, this SG line code base is slow. I think there&#8217;s a bug. Can you try to figure it out? There&#8217;s a big code base, so it&#8217;ll, it&#8217;ll spend some time doing this. And then on the right here, I&#8217;m going to initialize in real time. Some steering. Let&#8217;s see here.Mark Bissell [00:23:33]: searching for any. Bugs. Feature ID 43205.Shawn Wang [00:23:38]: Yeah.Mark Bissell [00:23:38]: 20, 30, 40. So let me, uh, this is basically a feature that we found that inside Kimi seems to cause it to speak in Gen Z slang. And so on the left, it&#8217;s still sort of thinking normally it might take, I don&#8217;t know, 15 seconds for this to kick in, but then we&#8217;re going to start hopefully seeing him do this code base is massive for real. So we&#8217;re going to start. We&#8217;re going to start seeing Kimi transition as the steering kicks in from normal Kimi to Gen Z Kimi and both in its chain of thought and its actual outputs.Mark Bissell [00:24:19]: And interestingly, you can see, you know, it&#8217;s still able to call tools, uh, and stuff. It&#8217;s um, it&#8217;s purely sort of it&#8217;s it&#8217;s demeanor. And there are other features that we found for interesting things like concision. So that&#8217;s more of a practical one. You can make it more concise. Um, the types of programs, uh, programming languages that uses, but yeah, as we&#8217;re seeing it come in. Pretty good. Outputs.Shawn Wang [00:24:43]: Scheduler code is actually wild.Vibhu Sapra [00:24:46]: Yo, this code is actually insane, bro.Vibhu Sapra [00:24:53]: What&#8217;s the process of training in SAE on this, or, you know, how do you label features? I know you guys put out a pretty cool blog post about, um, finding this like autonomous interp. Um, something. Something about how agents for interp is different than like coding agents. I don&#8217;t know while this is spewing up, but how, how do we find feature 43, two Oh five. Yeah.Mark Bissell [00:25:15]: So in this case, um, we, our platform that we&#8217;ve been building out for a long time now supports all the sort of classic out of the box interp techniques that you might want to have like SAE training, probing things of that kind, I&#8217;d say the techniques for like vanilla SAEs are pretty well established now where. You take your model that you&#8217;re interpreting, run a whole bunch of data through it, gather activations, and then yeah, pretty straightforward pipeline to train an SAE. There are a lot of different varieties. There&#8217;s top KSAEs, batch top KSAEs, um, normal ReLU SAEs. And then once you have your sparse features to your point, assigning labels to them to actually understand that this is a gen Z feature, that&#8217;s actually where a lot of the kind of magic happens. Yeah. And the most basic standard technique is look at all of your d input data set examples that cause this feature to fire most highly. And then you can usually pick out a pattern. So for this feature, If I&#8217;ve run a diverse enough data set through my model feature 43, two Oh five. Probably tends to fire on all the tokens that sounds like gen Z slang. You know, that&#8217;s the, that&#8217;s the time of year to be like, Oh, I&#8217;m in this, I&#8217;m in this Um, and, um, so, you know, you could have a human go through all 43,000 concepts andVibhu Sapra [00:26:34]: And I&#8217;ve got to ask the basic question, you know, can we get examples where it hallucinates, pass it through, see what feature activates for hallucinations? Can I just, you know, turn hallucination down?Myra Deng [00:26:51]: Oh, wow. You really predicted a project we&#8217;re already working on right now, which is detecting hallucinations using interpretability techniques. And this is interesting because hallucinations is something that&#8217;s very hard to detect. And it&#8217;s like a kind of a hairy problem and something that black box methods really struggle with. Whereas like Gen Z, you could always train a simple classifier to detect that hallucinations is harder. But we&#8217;ve seen that models internally have some... Awareness of like uncertainty or some sort of like user pleasing behavior that leads to hallucinatory behavior. And so, yeah, we have a project that&#8217;s trying to detect that accurately. And then also working on mitigating the hallucinatory behavior in the model itself as well.Shawn Wang [00:27:39]: Yeah, I would say most people are still at the level of like, oh, I would just turn temperature to zero and that turns off hallucination. And I&#8217;m like, well, that&#8217;s a fundamental misunderstanding of how this works. Yeah.Mark Bissell [00:27:51]: Although, so part of what I like about that question is you, there are SAE based approaches that might like help you get at that. But oftentimes the beauty of SAEs and like we said, the curse is that they&#8217;re unsupervised. So when you have a behavior that you deliberately would like to remove, and that&#8217;s more of like a supervised task, often it is better to use something like probes and specifically target the thing that you&#8217;re interested in reducing as opposed to sort of like hoping that when you fragment the latent space, one of the vectors that pops out.Vibhu Sapra [00:28:20]: And as much as we&#8217;re training an autoencoder to be sparse, we&#8217;re not like for sure certain that, you know, we will get something that just correlates to hallucination. You&#8217;ll probably split that up into 20 other things and who knows what they&#8217;ll be.Mark Bissell [00:28:36]: Of course. Right. Yeah. So there&#8217;s no sort of problems with like feature splitting and feature absorption. And then there&#8217;s the off target effects, right? Ideally, you would want to be very precise where if you reduce the hallucination feature, suddenly maybe your model can&#8217;t write. Creatively anymore. And maybe you don&#8217;t like that, but you want to still stop it from hallucinating facts and figures.Shawn Wang [00:28:55]: Good. So Vibhu has a paper to recommend there that we&#8217;ll put in the show notes. But yeah, I mean, I guess just because your demo is done, any any other things that you want to highlight or any other interesting features you want to show?Mark Bissell [00:29:07]: I don&#8217;t think so. Yeah. Like I said, this is a pretty small snippet. I think the main sort of point here that I think is exciting is that there&#8217;s not a whole lot of inter being applied to models quite at this scale. You know, Anthropic certainly has some some. Research and yeah, other other teams as well. But it&#8217;s it&#8217;s nice to see these techniques, you know, being put into practice. I think not that long ago, the idea of real time steering of a trillion parameter model would have sounded.Shawn Wang [00:29:33]: Yeah. The fact that it&#8217;s real time, like you started the thing and then you edited the steering vector.Vibhu Sapra [00:29:38]: I think it&#8217;s it&#8217;s an interesting one TBD of what the actual like production use case would be on that, like the real time editing. It&#8217;s like that&#8217;s the fun part of the demo, right? You can kind of see how this could be served behind an API, right? Like, yes, you&#8217;re you only have so many knobs and you can just tweak it a bit more. And I don&#8217;t know how it plays in. Like people haven&#8217;t done that much with like, how does this work with or without prompting? Right. How does this work with fine tuning? Like, there&#8217;s a whole hype of continual learning, right? So there&#8217;s just so much to see. Like, is this another parameter? Like, is it like parameter? We just kind of leave it as a default. We don&#8217;t use it. So I don&#8217;t know. Maybe someone here wants to put out a guide on like how to use this with prompting when to do what?Mark Bissell [00:30:18]: Oh, well, I have a paper recommendation. I think you would love from Act Deep on our team, who is an amazing researcher, just can&#8217;t say enough amazing things about Act Deep. But he actually has a paper that as well as some others from the team and elsewhere that go into the essentially equivalence of activation steering and in context learning and how those are from a he thinks of everything in a cognitive neuroscience Bayesian framework, but basically how you can precisely show how. Prompting in context, learning and steering exhibit similar behaviors and even like get quantitative about the like magnitude of steering you would need to do to induce a certain amount of behavior similar to certain prompting, even for things like jailbreaks and stuff. It&#8217;s a really cool paper. Are you saying steering is less powerful than prompting? More like you can almost write a formula that tells you how to convert between the two of them.Myra Deng [00:31:20]: And so like formally equivalent actually in the in the limit. Right.Mark Bissell [00:31:24]: So like one case study of this is for jailbreaks there. I don&#8217;t know. Have you seen the stuff where you can do like many shot jailbreaking? You like flood the context with examples of the behavior. And the topic put out that paper.Shawn Wang [00:31:38]: A lot of people were like, yeah, we&#8217;ve been doing this, guys.Mark Bissell [00:31:40]: Like, yeah, what&#8217;s in this in context learning and activation steering equivalence paper is you can like predict the number. Number of examples that you will need to put in there in order to jailbreak the model. That&#8217;s cool. By doing steering experiments and using this sort of like equivalence mapping. That&#8217;s cool. That&#8217;s really cool. It&#8217;s very neat. Yeah.Shawn Wang [00:32:02]: I was going to say, like, you know, I can like back rationalize that this makes sense because, you know, what context is, is basically just, you know, it updates the KV cache kind of and like and then every next token inference is still like, you know, the sheer sum of everything all the way. It&#8217;s plus all the context. It&#8217;s up to date. And you could, I guess, theoretically steer that with you probably replace that with your steering. The only problem is steering typically is on one layer, maybe three layers like like you did. So it&#8217;s like not exactly equivalent.Mark Bissell [00:32:33]: Right, right. There&#8217;s sort of you need to get precise about, yeah, like how you sort of define steering and like what how you&#8217;re modeling the setup. But yeah, I&#8217;ve got the paper pulled up here. Belief dynamics reveal the dual nature. Yeah. The title is Belief Dynamics Reveal the Dual Nature of Incompetence. And it&#8217;s an exhibition of the practical context learning and activation steering. So Eric Bigelow, Dan Urgraft on the who are doing fellowships at Goodfire, Ekt Deep&#8217;s the final author there.Myra Deng [00:32:59]: I think actually to your question of like, what is the production use case of steering? I think maybe if you just think like one level beyond steering as it is today. Like imagine if you could adapt your model to be, you know, an expert legal reasoner. Like in almost real time, like very quickly. efficiently using human feedback or using like your semantic understanding of what the model knows and where it knows that behavior. I think that while it&#8217;s not clear what the product is at the end of the day, it&#8217;s clearly very valuable. Thinking about like what&#8217;s the next interface for model customization and adaptation is a really interesting problem for us. Like we have heard a lot of people actually interested in fine-tuning an RL for open weight models in production. And so people are using things like Tinker or kind of like open source libraries to do that, but it&#8217;s still very difficult to get models fine-tuned and RL&#8217;d for exactly what you want them to do unless you&#8217;re an expert at model training. And so that&#8217;s like something we&#8217;reShawn Wang [00:34:06]: looking into. Yeah. I never thought so. Tinker from Thinking Machines famously uses rank one LoRa. Is that basically the same as steering? Like, you know, what&#8217;s the comparison there?Mark Bissell [00:34:19]: Well, so in that case, you are still applying updates to the parameters, right?Shawn Wang [00:34:25]: Yeah. You&#8217;re not touching a base model. You&#8217;re touching an adapter. It&#8217;s kind of, yeah.Mark Bissell [00:34:30]: Right. But I guess it still is like more in parameter space then. I guess it&#8217;s maybe like, are you modifying the pipes or are you modifying the water flowing through the pipes to get what you&#8217;re after? Yeah. Just maybe one way.Mark Bissell [00:34:44]: I like that analogy. That&#8217;s my mental map of it at least, but it gets at this idea of model design and intentional design, which is something that we&#8217;re, that we&#8217;re very focused on. And just the fact that like, I hope that we look back at how we&#8217;re currently training models and post-training models and just think what a primitive way of doing that right now. Like there&#8217;s no intentionalityShawn Wang [00:35:06]: really in... It&#8217;s just data, right? The only thing in control is what data we feed in.Mark Bissell [00:35:11]: So, so Dan from Goodfire likes to use this analogy of, you know, he has a couple of young kids and he talks about like, what if I could only teach my kids how to be good people by giving them cookies or like, you know, giving them a slap on the wrist if they do something wrong, like not telling them why it was wrong or like what they should have done differently or something like that. Just figure it out. Right. Exactly. So that&#8217;s RL. Yeah. Right. And, and, you know, it&#8217;s sample inefficient. There&#8217;s, you know, what do they say? It&#8217;s like slurping feedback. It&#8217;s like, slurping supervision. Right. And so you&#8217;d like to get to the point where you can have experts giving feedback to their models that are, uh, internalized and, and, you know, steering is an inference time way of sort of getting that idea. But ideally you&#8217;re moving to a world whereVibhu Sapra [00:36:04]: it is much more intentional design in perpetuity for these models. Okay. This is one of the questions we asked Emmanuel from Anthropic on the podcast a few months ago. Basically the question, was you&#8217;re at a research lab that does model training, foundation models, and you&#8217;re on an interp team. How does it tie back? Right? Like, does this, do ideas come from the pre-training team? Do they go back? Um, you know, so for those interested, you can, you can watch that. There wasn&#8217;t too much of a connect there, but it&#8217;s still something, you know, it&#8217;s something they want toMark Bissell [00:36:33]: push for down the line. It can be useful for all of the above. Like there are certainly post-hocVibhu Sapra [00:36:39]: use cases where it doesn&#8217;t need to touch that. I think the other thing a lot of people forget is this stuff isn&#8217;t too computationally expensive, right? Like I would say, if you&#8217;re interested in getting into research, MechInterp is one of the most approachable fields, right? A lot of this train an essay, train a probe, this stuff, like the budget for this one, there&#8217;s already a lot done. There&#8217;s a lot of open source work. You guys have done some too. Um, you know,Shawn Wang [00:37:04]: There&#8217;s like notebooks from the Gemini team for Neil Nanda or like, this is how you do it. Just step through the notebook.Vibhu Sapra [00:37:09]: Even if you&#8217;re like, not even technical with any of this, you can still make like progress. There, you can look at different activations, but, uh, if you do want to get into training, you know, training this stuff, correct me if I&#8217;m wrong is like in the thousands of dollars, not even like, it&#8217;s not that high scale. And then same with like, you know, applying it, doing it for post-training or all this stuff is fairly cheap in scale of, okay. I want to get into like model training. I don&#8217;t have compute for like, you know, pre-training stuff. So it&#8217;s, it&#8217;s a very nice field to get into. And also there&#8217;s a lot of like open questions, right? Um, some of them have to go with, okay, I want a product. I want to solve this. Like there&#8217;s also just a lot of open-ended stuff that people could work on. That&#8217;s interesting. Right. I don&#8217;t know if you guys have any calls for like, what&#8217;s open questions, what&#8217;s open work that you either open collaboration with, or like, you&#8217;d just like to see solved or just, you know, for people listening that want to get into McInturk because people always talk about it. What are, what are the things they should check out? Start, of course, you know, join you guys as well. I&#8217;m sure you&#8217;re hiring.Myra Deng [00:38:09]: There&#8217;s a paper, I think from, was it Lee, uh, Sharky? It&#8217;s open problems and, uh, it&#8217;s, it&#8217;s a bit of interpretability, which I recommend everyone who&#8217;s interested in the field. Read. I&#8217;m just like a really comprehensive overview of what are the things that experts in the field think are the most important problems to be solved. I also think to your point, it&#8217;s been really, really inspiring to see, I think a lot of young people getting interested in interpretability, actually not just young people also like scientists to have been, you know, experts in physics for many years and in biology or things like this, um, transitioning into interp, because the barrier of, of what&#8217;s now interp. So it&#8217;s really cool to see a number to entry is, you know, in some ways low and there&#8217;s a lot of information out there and ways to get started. There&#8217;s this anecdote of like professors at universities saying that all of a sudden every incoming PhD student wants to study interpretability, which was not the case a few years ago. So it just goes to show how, I guess, like exciting the field is, how fast it&#8217;s moving, how quick it is to get started and things like that.Mark Bissell [00:39:10]: And also just a very welcoming community. You know, there&#8217;s an open source McInturk Slack channel. There are people are always posting questions and just folks in the space are always responsive if you ask things on various forums and stuff. But yeah, the open paper, open problems paper is a really good one.Myra Deng [00:39:28]: For other people who want to get started, I think, you know, MATS is a great program. What&#8217;s the acronym for? Machine Learning and Alignment Theory Scholars? It&#8217;s like the...Vibhu Sapra [00:39:40]: Normally summer internship style.Myra Deng [00:39:42]: Yeah, but they&#8217;ve been doing it year round now. And actually a lot of our full-time staff have come through that program or gone through that program. And it&#8217;s great for anyone who is transitioning into interpretability. There&#8217;s a couple other fellows programs. We do one as well as Anthropic. And so those are great places to get started if anyone is interested.Mark Bissell [00:40:03]: Also, I think been seen as a research field for a very long time. But I think engineering... I think engineers are sorely wanted for interpretability as well, especially at Goodfire, but elsewhere, as it does scale up.Shawn Wang [00:40:18]: I should mention that Lee actually works with you guys, right? And in the London office and I&#8217;m adding our first ever McInturk track at AI Europe because I see this industry applications now emerging. And I&#8217;m pretty excited to, you know, help push that along. Yeah, I was looking forward to that. It&#8217;ll effectively be the first industry McInturk conference. Yeah. I&#8217;m so glad you added that. You know, it&#8217;s still a little bit of a bet. It&#8217;s not that widespread, but I can definitely see this is the time to really get into it. We want to be early on things.Mark Bissell [00:40:51]: For sure. And I think the field understands this, right? So at ICML, I think the title of the McInturk workshop this year was actionable interpretability. And there was a lot of discussion around bringing it to various domains. Everyone&#8217;s adding pragmatic, actionable, whatever.Shawn Wang [00:41:10]: It&#8217;s like, okay, well, we weren&#8217;t actionable before, I guess. I don&#8217;t know.Vibhu Sapra [00:41:13]: And I mean, like, just, you know, being in Europe, you see the Interp room. One, like old school conferences, like, I think they had a very tiny room till they got lucky and they got it doubled. But there&#8217;s definitely a lot of interest, a lot of niche research. So you see a lot of research coming out of universities, students. We covered the paper last week. It&#8217;s like two unknown authors, not many citations. But, you know, you can make a lot of meaningful work there. Yeah. Yeah. Yeah.Shawn Wang [00:41:39]: Yeah. I think people haven&#8217;t really mentioned this yet. It&#8217;s just Interp for code. I think it&#8217;s like an abnormally important field. We haven&#8217;t mentioned this yet. The conspiracy theory last two years ago was when the first SAE work came out of Anthropic was they would do like, oh, we just used SAEs to turn the bad code vector down and then turn up the good code. And I think like, isn&#8217;t that the dream? Like, you know, like, but basically, I guess maybe, why is it funny? Like, it&#8217;s... If it was realistic, it would not be funny. It would be like, no, actually, we should do this. But it&#8217;s funny because we know there&#8217;s like, we feel there&#8217;s some limitations to what steering can do. And I think a lot of the public image of steering is like the Gen Z stuff. Like, oh, you can make it really love the Golden Gate Bridge, or you can make it speak like Gen Z. To like be a legal reasoner seems like a huge stretch. Yeah. And I don&#8217;t know if that will get there this way. Yeah.Myra Deng [00:42:36]: I think, um, I will say we are announcing. Something very soon that I will not speak too much about. Um, but I think, yeah, this is like what we&#8217;ve run into again and again is like, we, we don&#8217;t want to be in the world where steering is only useful for like stylistic things. That&#8217;s definitely not, not what we&#8217;re aiming for. But I think the types of interventions that you need to do to get to things like legal reasoning, um, are much more sophisticated and require breakthroughs in, in learning algorithms. And that&#8217;s, um...Shawn Wang [00:43:07]: And is this an emergent property of scale as well?Myra Deng [00:43:10]: I think so. Yeah. I mean, I think scale definitely helps. I think scale allows you to learn a lot of information and, and reduce noise across, you know, large amounts of data. But I also think we think that there&#8217;s ways to do things much more effectively, um, even, even at scale. So like actually learning exactly what you want from the data and not learning things that you do that you don&#8217;t want exhibited in the data. So we&#8217;re not like anti-scale, but we are also realizing that scale is not going to get us anywhere. It&#8217;s not going to get us to the type of AI development that we want to be at in, in the future as these models get more powerful and get deployed in all these sorts of like mission critical contexts. Current life cycle of training and deploying and evaluations is, is to us like deeply broken and has opportunities to, to improve. So, um, more to come on that very, very soon.Mark Bissell [00:44:02]: And I think that that&#8217;s a use basically, or maybe just like a proof point that these concepts do exist. Like if you can manipulate them in the precise best way, you can get the ideal combination of them that you desire. And steering is maybe the most coarse grained sort of peek at what that looks like. But I think it&#8217;s evocative of what you could do if you had total surgical control over every concept, every parameter. Yeah, exactly.Myra Deng [00:44:30]: There were like bad code features. I&#8217;ve got it pulled up.Vibhu Sapra [00:44:33]: Yeah. Just coincidentally, as you guys are talking.Shawn Wang [00:44:35]: This is like, this is exactly.Vibhu Sapra [00:44:38]: There&#8217;s like specifically a code error feature that activates and they show, you know, it&#8217;s not, it&#8217;s not typo detection. It&#8217;s like, it&#8217;s, it&#8217;s typos in code. It&#8217;s not typical typos. And, you know, you can, you can see it clearly activates where there&#8217;s something wrong in code. And they have like malicious code, code error. They have a whole bunch of sub, you know, sub broken down little grain features. Yeah.Shawn Wang [00:45:02]: Yeah. So, so the, the rough intuition for me, the, why I talked about post-training was that, well, you just, you know, have a few different rollouts with all these things turned off and on and whatever. And then, you know, you can, that&#8217;s, that&#8217;s synthetic data you can kind of post-train on. Yeah.Vibhu Sapra [00:45:13]: And I think we make it sound easier than it is just saying, you know, they do the real hard work.Myra Deng [00:45:19]: I mean, you guys, you guys have the right idea. Exactly. Yeah. We replicated a lot of these features in, in our Lama models as well. I remember there was like.Vibhu Sapra [00:45:26]: And I think a lot of this stuff is open, right? Like, yeah, you guys opened yours. DeepMind has opened a lot of essays on Gemma. Even Anthropic has opened a lot of this. There&#8217;s, there&#8217;s a lot of resources that, you know, we can probably share of people that want to get involved.Shawn Wang [00:45:41]: Yeah. And special shout out to like Neuronpedia as well. Yes. Like, yeah, amazing piece of work to visualize those things.Myra Deng [00:45:49]: Yeah, exactly.Shawn Wang [00:45:50]: I guess I wanted to pivot a little bit on, onto the healthcare side, because I think that&#8217;s a big use case for you guys. We haven&#8217;t really talked about it yet. This is a bit of a crossover for me because we are, we are, we do have a separate science pod that we&#8217;re starting up for AI, for AI for science, just because like, it&#8217;s such a huge investment category and also I&#8217;m like less qualified to do it, but we actually have bio PhDs to cover that, which is great, but I need to just kind of recover, recap your work, maybe on the evil two stuff, but then, and then building forward.Mark Bissell [00:46:17]: Yeah, for sure. And maybe to frame up the conversation, I think another kind of interesting just lens on interpretability in general is a lot of the techniques that were described. are ways to solve the AI human interface problem. And it&#8217;s sort of like bidirectional communication is the goal there. So what we&#8217;ve been talking about with intentional design of models and, you know, steering, but also more advanced techniques is having humans impart our desires and control into models and over models. And the reverse is also very interesting, especially as you get to superhuman models, whether that&#8217;s narrow superintelligence, like these scientific models that work on genomics, data, medical imaging, things like that. But down the line, you know, superintelligence of other forms as well. What knowledge can the AIs teach us as sort of that, that the other direction in that? And so some of our life science work to date has been getting at exactly that question, which is, well, some of it does look like debugging these various life sciences models, understanding if they&#8217;re actually performing well, on tasks, or if they&#8217;re picking up on spurious correlations, for instance, genomics models, you would like to know whether they are sort of focusing on the biologically relevant things that you care about, or if it&#8217;s using some simpler correlate, like the ancestry of the person that it&#8217;s looking at. But then also in the instances where they are superhuman, and maybe they are understanding elements of the human genome that we don&#8217;t have names for or specific, you know, yeah, discoveries that they&#8217;ve made that that we don&#8217;t know about, that&#8217;s, that&#8217;s a big goal. And so we&#8217;re already seeing that, right, we are partnered with organizations like Mayo Clinic, leading research health system in the United States, our Institute, as well as a startup called Prima Menta, which focuses on neurodegenerative disease. And in our partnership with them, we&#8217;ve used foundation models, they&#8217;ve been training and applied our interpretability techniques to find novel biomarkers for Alzheimer&#8217;s disease. So I think this is just the tip of the iceberg. But it&#8217;s, that&#8217;s like a flavor of some of the things that we&#8217;re working on.Shawn Wang [00:48:36]: Yeah, I think that&#8217;s really fantastic. Obviously, we did the Chad Zuckerberg pod last year as well. And like, there&#8217;s a plethora of these models coming out, because there&#8217;s so much potential and research. And it&#8217;s like, very interesting how it&#8217;s basically the same as language models, but just with a different underlying data set. But it&#8217;s like, it&#8217;s the same exact techniques. Like, there&#8217;s no change, basically.Mark Bissell [00:48:59]: Yeah. Well, and even in like other domains, right? Like, you know, robotics, I know, like a lot of the companies just use Gemma as like the like backbone, and then they like make it into a VLA that like takes these actions. It&#8217;s, it&#8217;s, it&#8217;s transformers all the way down. So yeah.Vibhu Sapra [00:49:15]: Like we have Med Gemma now, right? Like this week, even there was Med Gemma 1.5. And they&#8217;re training it on this stuff, like 3d scans, medical domain knowledge, and all that stuff, too. So there&#8217;s a push from both sides. But I think the thing that, you know, one of the things about McInturpp is like, you&#8217;re a little bit more cautious in some domains, right? So healthcare, mainly being one, like guardrails, understanding, you know, we&#8217;re more risk adverse to something going wrong there. So even just from a basic understanding, like, if we&#8217;re trusting these systems to make claims, we want to know why and what&#8217;s going on.Myra Deng [00:49:51]: Yeah, I think there&#8217;s totally a kind of like deployment bottleneck to actually using. foundation models for real patient usage or things like that. Like, say you&#8217;re using a model for rare disease prediction, you probably want some explanation as to why your model predicted a certain outcome, and an interpretable explanation at that. So that&#8217;s definitely a use case. But I also think like, being able to extract scientific information that no human knows to accelerate drug discovery and disease treatment and things like that actually is a really, really big unlock for science, like scientific discovery. And you&#8217;ve seen a lot of startups, like say that they&#8217;re going to accelerate scientific discovery. And I feel like we actually are doing that through our interp techniques. And kind of like, almost by accident, like, I think we got reached out to very, very early on from these healthcare institutions. And none of us had healthcare.Shawn Wang [00:50:49]: How did they even hear of you? A podcast.Myra Deng [00:50:51]: Oh, okay. Yeah, podcast.Vibhu Sapra [00:50:53]: Okay, well, now&#8217;s that time, you know.Myra Deng [00:50:55]: Everyone can call us.Shawn Wang [00:50:56]: Podcasts are the most important thing. Everyone should listen to podcasts.Myra Deng [00:50:59]: Yeah, they reached out. They were like, you know, we have these really smart models that we&#8217;ve trained, and we want to know what they&#8217;re doing. And we were like, really early that time, like three months old, and it was a few of us. And we were like, oh, my God, we&#8217;ve never used these models. Let&#8217;s figure it out. But it&#8217;s also like, great proof that interp techniques scale pretty well across domains. We didn&#8217;t really have to learn too much about.Shawn Wang [00:51:21]: Interp is a machine learning technique, machine learning skills everywhere, right? Yeah. And it&#8217;s obviously, it&#8217;s just like a general insight. Yeah. Probably to finance too, I think, which would be fun for our history. I don&#8217;t know if you have anything to say there.Mark Bissell [00:51:34]: Yeah, well, just across the science. Like, we&#8217;ve also done work on material science. Yeah, it really runs the gamut.Vibhu Sapra [00:51:40]: Yeah. Awesome. And, you know, for those that should reach out, like, you&#8217;re obviously experts in this, but like, is there a call out for people that you&#8217;re looking to partner with, design partners, people to use your stuff outside of just, you know, the general developer that wants to. Plug and play steering stuff, like on the research side more so, like, are there ideal design partners, customers, stuff like that?Myra Deng [00:52:03]: Yeah, I can talk about maybe non-life sciences, and then I&#8217;m curious to hear from you on the life sciences side. But we&#8217;re looking for design partners across many domains, language, anyone who&#8217;s customizing language models or trying to push the frontier of code or reasoning models is really interesting to us. And then also interested in the frontier of modeling. There&#8217;s a lot of models that work in, like, pixel space, as we call it. So if you&#8217;re doing world models, video models, even robotics, where there&#8217;s not a very clean natural language interface to interact with, I think we think that Interp can really help and are looking for a few partners in that space.Shawn Wang [00:52:43]: Just because you mentioned the keyword world models, is that a big part of your thinking? Do you have a definition that I can use? Because everyone&#8217;s asking me about it.Myra Deng [00:52:53]: About world models?Shawn Wang [00:52:54]: There&#8217;s quite a few definitions, let&#8217;s say.Myra Deng [00:52:56]: I don&#8217;t feel equipped to be an expert on world model definitions, but the reason we&#8217;re interested in them is because they give you, like, you know, with language models, when you get features, you still have to do auto Interp and things like that to actually get an understanding of what this concept is. But in image and video and world, it&#8217;s like extremely easy to grok what the concept is because you can see it and you can visualize it. And this makes the feedback. It makes the feedback cycle extremely fast for us and also for things like, I don&#8217;t know, if you think about probes in language model context and then take it to world models, like, what if you wanted to detect harmful actors in world model scenes? Like, you can&#8217;t actually, like, go and label all of that data feasibly, but maybe you could synthetically generate, you know, I don&#8217;t know, world, like, harmful actor data using SAE feature activations or whatever, and then actually train a probe that was able to detect. That much more scalably. So I just think, like, video and image and world has always been something we&#8217;ve explored and are continuing to explore. Mark&#8217;s demo was probably the first moment we really, like, we&#8217;re like, oh, wow, like, this is really gonna, this could really, like, change the world. The steering demo? Yeah, no, the image demo. The diffusion one. Yeah, yeah, exactly. Yeah.Shawn Wang [00:54:18]: We should probably show that. And you demoed it at World&#8217;s Fair, so we can link that.Myra Deng [00:54:23]: Nice, yeah. Yeah.Vibhu Sapra [00:54:24]: You can play with it, right? Yes. Yeah, it&#8217;s still up.Mark Bissell [00:54:26]: Paint.goodfair.ai. Yeah. Yeah.Shawn Wang [00:54:28]: I think for me, one way in which I think about world models is just like this, like, having this consistent model of the world where everything that you generate operates within the rules of that world. And imagine it would be a bigger deal for science or, like, math or anything that where, like, you have verifiable rules. Whereas, I guess, in natural language, maybe there&#8217;s less rules. And so it&#8217;s not that important. Yeah.Mark Bissell [00:54:53]: And which makes the debugging of the model&#8217;s internal representations or its internal world model, to the extent you can make that legible and explicit and have control over that, I think it makes it all the more important. Because in language, it&#8217;s sort of a fuzzy enough domain that if its world model isn&#8217;t fully like ours, it can still sort of, like, pass the Turing test, so to speak. But I know there have been papers that have looked at, like, even if you train certain astrophysics models, it does not learn. Like, the same way that you can, you know, have a model do well for modular arithmetic, but it doesn&#8217;t really, like, learn how we think of modular arithmetic. It learns some crazy heuristic that is, like, essentially functionally equivalent. But it&#8217;s probably not the sort of Grok solution that you would hope for. It&#8217;s how an alien would do it. Right. Right. Exactly.Shawn Wang [00:55:45]: But no, no, I think there&#8217;s probably, I think, a function of our learning being bad rather than the, well, that approach probably not being. Because it&#8217;s how we humans learn. Yeah, right.Mark Bissell [00:55:56]: Well, it&#8217;s just, it&#8217;s the problem of induction, right? All of ML is based on induction. And it&#8217;s impossible to say, I have a physics model. You might have a physics model that works all the time, except when there is a character wearing a blue shirt and green shoes. And, like, you can&#8217;t disprove that that&#8217;s the case unless you test every particular situation your model might be in. Yeah. So we know that the laws of physics apply no matter. Where you are, what scenario it is. But from a model&#8217;s perspective, maybe something that&#8217;s out of distribution. It just never needed to learn that the same laws of physics apply there. Yeah.Shawn Wang [00:56:30]: You were very excited because I read Ted Chiang over the holidays and I was very inspired by this short story called Understand, which apparently is, like, pretty old. You must be familiar with it. To me, it was like, it&#8217;s this fictional story. It&#8217;s like the inverse of Flowers for Algernon, where you had someone, like, get really smart, but then also try to outsmart the tester. And the story just read, like, the chain of thought of a superintelligence, right? Where they&#8217;re like, oh, I realize I&#8217;m being tested. Therefore, and then, okay, what&#8217;s the consequence of being tested? Oh, they&#8217;re testing me. And if I score well, they will use me for things that I don&#8217;t want to do. Therefore, I will score badly. And, like, but not too badly that they will raise alarms. So model sandbagging is a thing that people have explored. But I just think, like, Ted Chiang&#8217;s work just in general seems to be something that inspires you. I just wanted to prompt you to talk about it.Mark Bissell [00:57:22]: I think, so Ted Chiang has two, is a sci-fi author who writes amazing short stories. His other claim to fame is Stories of Our Lives, which became the movie Arrival. Exactly, yeah. So two books of short stories that I&#8217;m aware of. He also actually has a great just online blog post. I think he&#8217;s the one who coined the term of LLMs as, like, a blurry JPEG of the internet. I should fact check that, but it&#8217;s a good post. But I think almost every one of his short stories has some lesson to bear. I&#8217;m thinking about AI and thinking about AI research. So, you know, you&#8217;ve been talking about alien intelligence, right, in this AI human communication translation problem. That&#8217;s, you know, exactly sort of what&#8217;s going on in Arrival and Story of Your Life. And just the fact that other beings will think and operate and communicate in ways that are not just challenging for us to understand, but just fundamentally different in ways that we might not even be able to expect. And then the one that&#8217;s just. Super relevant for interpretability is the other short book of short stories he has is called Exhalation. And that is literally about a robot doing interpretability on its own mind. Oh, OK. So I just think that that, you know, you don&#8217;t even have to squint to make the analogies there.Shawn Wang [00:58:41]: Well, I actually take Exhalation as a discussion about entropy and order. But yes, there&#8217;s a scene in Exhalation where basically everyone is a robot. So they. The guy realizes he can set up a mirror to work on the back of his own head and then starts doing operations like that and looking in the mirror and doing this. Yeah.Mark Bissell [00:59:00]: And I think Ted Chiang has written about like the inspiration for that story. It was like half inspired by some of the things he had been doing on entropy. There&#8217;s apparently some other short story that is similar where a character goes to the doctor and opens up his chest and there&#8217;s like a like a ticker tape going along. It&#8217;s like he basically realizes he&#8217;s like a Turing machine. And I don&#8217;t know. I. Think especially as it comes to using agents for interp. That story always sticks in my mind.Myra Deng [00:59:27]: I find the brain surgery or like surgery analogies a little bit, a little bit morbid, but it is very apt. And when we talk to a lot of computational neuroscientists, they moved to interp because they were like, look, we have unfettered access to this artificial intelligent mind. It&#8217;s so much. You have access to everything. You can run as many ablations experiments as you want. It&#8217;s an. Amazing bed for science. And, you know, human brains, obviously, we can&#8217;t just go and do whatever we want to them. And I think it is really just like a moment in time where we have intelligent systems that can really like do things better than humans in many ways. And it&#8217;s time, I think, for us to do the science on it.Shawn Wang [01:00:14]: I&#8217;ll ask a brief like safety question. You know, McInturk was kind of born out of the alignment and safety conversation. Safety is on your website. It&#8217;s not like something that you, you like de-prioritize, but like there&#8217;s like a sort of very militant safety arm that like wants to blow up data centers and like stop AI and, and then there&#8217;s this like sort of middle ground and like, is, is this like a conversation in your part of the world? Do you go up to Berkeley and Lighthaven and like talk to those guys or are they like, you know, there&#8217;s like a brief like civil war going on or no?Myra Deng [01:00:45]: I think, I think a good amount of us have spent some time in Berkeley. And then there are researchers there that we really. Admire and respect. I think for us, it&#8217;s like, we have a very grounded view of alignment and, and safety in that we want to make sure that we can build models that do what we want them to do and that we have scalable oversight into what these models are doing. And we think that that is the key to a lot of these like technical alignment challenges. And I think that is our opinion. That&#8217;s our research direction. We of course are going to do. Safety related research to make sure that our techniques also work on, you know, things like reward hacking and, and other like more concrete safety issues that we&#8217;ve seen in the wild, but we want to be kind of like grounded in solving the technical challenges we see to having humans be humans play a big role in, in the deployment of, of these super intelligent agents of the future.Mark Bissell [01:01:47]: Yeah, I&#8217;ve, I&#8217;ve found the community to actually be remarkably cohesive, whether it&#8217;s. Talking about academia or the interpretability work being done at the frontier labs or some of the independent programs like maths and stuff. I think we&#8217;re all shooting for the same goal. I don&#8217;t know that there&#8217;s anyone who doesn&#8217;t want our understanding of models to increase. I, I think everyone, regardless of where they&#8217;re coming from or the use cases that they&#8217;re thinking, whether it&#8217;s alignment as the premier thing they&#8217;re focused on or someone who&#8217;s coming in purely from the angle of scientific discovery, I think we would all hope that models can be. More reliably and robustly controlled and understood. It seems like a pretty unambiguous goal.Shawn Wang [01:02:28]: I&#8217;ll maybe phrase it in terms of like, there&#8217;s maybe like a U curve of, of this, where like, if you&#8217;re extremely doomer, you don&#8217;t want any research whatsoever. If you&#8217;re like mildly doomer, you&#8217;re like, okay, there&#8217;s this like high agency doomer is like, well, the default path is we&#8217;re all dead, but like we can do something about it. Whereas there&#8217;s, there&#8217;s other people who are like, no, just like, don&#8217;t ever do anything. You know? Yeah.Vibhu Sapra [01:02:50]: Yeah. There&#8217;s also the other side, like there is the super alignment, like people that are like, okay, weak to strong generalization, we&#8217;re going to get there. We&#8217;re going to have models smarter than us and use those to train even smarter models. How do we do that safely? That&#8217;s, you know, there&#8217;s the camp there too. That&#8217;s trying to solve it, but yeah, there&#8217;s, there&#8217;s a lot of doomers too.Mark Bissell [01:03:12]: When I, and I think there&#8217;s a lot to be learned from taking a very, um, like even regardless of the problem. That you&#8217;re applying this to also just like the notion of like scalable oversight as a method of saying, let&#8217;s take super intelligent or, or current frontier models and help use them to understand other models is another case where I think it&#8217;s just like a good lesson that everyone is aligned on of ideally you are setting up your research so that as super intelligence arrives, that is a tailwind. That&#8217;s also bolstering our ability to like understand the models. Cause otherwise you&#8217;re fighting. Losing battle. If it&#8217;s like the systems are getting more and more capable and our methods are sort of linearly growing at like human pace. Yeah.Shawn Wang [01:03:58]: Yeah. Uh, Viva did call out something like, you know, I, I do think a consistent part of the Mac interp field is consistently strong to weak, meaning that we, we train weaker models to understand strong models, something like that. Um, or maybe I got it the other way around the other way. Weak. The other way around. Yeah. Yeah. The question that Ilya and Janlaika posed was, well, is that going to scale? Because eventually these are going to be. Stronger than us. Right. So I don&#8217;t know if you have a perspective on that because I, that is something I still haven&#8217;t got over even after seeing that.Vibhu Sapra [01:04:27]: There&#8217;s a good paper from open AI, but it&#8217;s somewhat old. I think it&#8217;s like 23, 24. It&#8217;s literally weak to strong generalization. Yeah. But the thing is that most of opening a high super alignment team has, they&#8217;re gone. They&#8217;re gone.Mark Bissell [01:04:39]: But like, I think the idea, the idea is there&#8217;s no more. They&#8217;re so back.Shawn Wang [01:04:44]: think there&#8217;s some new blog posts coming out. I know. I did just, you know, check the thinking machines, uh, website. Let&#8217;s see who&#8217;s back. There&#8217;s more kind of thing, you know, you don&#8217;t want to be like, we too strong seemed like a very different direction. And when, when it first came out, I was like, oh my God, this is like, this is what we have to do. Uh, and like, it may be completely different than everything, all the techniques that we have today. Yeah.Mark Bissell [01:05:06]: My understanding of that is it&#8217;s, that&#8217;s more like weak to strong when you, when you trust the weak model and you&#8217;re uncertain whether you can trust the strong model that&#8217;s, that&#8217;s being developed. I&#8217;m sort of speaking out of my depth on some of these topics. Yeah. But I think right now we&#8217;re in a regime where even the strong models we, uh, trust as reasonably aligned. And so they can be good co-scientists on a lot of the problems that we&#8217;ve been, we&#8217;ve been tackling, which is a nice, a nice state to be in. Hmm. Yeah.Shawn Wang [01:05:35]: Any last thoughts, close action?Mark Bissell [01:05:38]: I don&#8217;t think so. As you mentioned, actively hiring MLEs, research scientists, um, you can check out the careers page at good fire. Um, where are you guys based?Myra Deng [01:05:47]: San Francisco. We&#8217;re in, um, Levi&#8217;s Plaza. Like by court tower, that&#8217;s where our office is. So come hang out. Um, we&#8217;re also looking for design partners across, um, people working in, in reasoning models, um, world models, robotics, and then also of course, people who are working on building super intelligent science models or looking at drug discovery or disease treatment. We would love to partner as well. Yeah.Shawn Wang [01:06:13]: Maybe the way I&#8217;ll phrase it is like, you know, maybe you have a use case where LLMs are almost good enough, but you need one. Maybe you have a magical knob to tune so that it is good enough that you guys make the knob. Yeah.Mark Bissell [01:06:26]: Yeah. Or foundation models, uh, in, in other domains as well. The, the, some of those are the, um, especially opaque ones because you can&#8217;t, you can&#8217;t chat with them. So what do you, what do you do if you can&#8217;t chat with them? Oh, well, like thinking about like a genomics model or material science model. So like, uh, yeah, they label a narrow foundation. Yeah. They predict.Shawn Wang [01:06:44]: Yeah. Got it. Good.Vibhu Sapra [01:06:45]: I was gonna say, I thought the diffusion work you guys did early was pretty, you know, pretty fun. Like you could see it directly. Applied to images, but we don&#8217;t see as much interp in diffusion or images, right?Shawn Wang [01:06:55]: Like I see, you know, it&#8217;s gonna be huge. Like, look at this video models. They&#8217;re so expensive to produce. And like, I mean, basically a mid journey S ref is kind of a feature, right? The what? Mid journey S ref. Oh, like the, the, the string of numbers. Right. Right. Right. Yeah. The style reference, I guess. Yeah.Mark Bissell [01:07:12]: No, I, I mean, I think we&#8217;re starting to see more of it and I&#8217;ll say like the, the research preview of our diffusion model, kind of like a creative use case in the steering demo you saw. I, I think of those much more as, as, as demos than, um, a lot of the sort of core platform features that, that we&#8217;re working with partners are unfortunately sort of under NDA and less demoable, but I will, you know, hope that you&#8217;re gonna see inter pervading a lot of what gets done, even if it is behind the scenes like that. So some of the, yeah, some of the public facing demos might not always be representative of like the, it&#8217;s, it&#8217;s just the tip of the iceberg, I guess, is one way to put it. Okay. Excellent. Thanks for coming on. Thanks for having us. Thanks for having us. This is a great time.",
      "url": "https://www.latent.space/p/goodfire",
      "author": "Unknown",
      "published": "2026-02-06T22:45:00",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "As reported in [News](/?date=2026-02-06&category=news#item-48fe2a06c1c8) yesterday, Goodfire AI, focused on mechanistic interpretability, raised $150M in Series B funding at a $1.25B valuation. The company is building production APIs and enterprise deployments to make 'peeking inside the model' actionable for real workflows.",
      "importance_score": 84.0,
      "reasoning": "Major funding round ($150M) for an interpretability-focused company at unicorn valuation indicates strong market confidence in AI safety/transparency tooling as a viable business, not just research pursuit.",
      "themes": [
        "funding",
        "AI safety",
        "interpretability",
        "startups"
      ],
      "continuation": {
        "original_item_id": "48fe2a06c1c8",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "The First Mechanistic Interpretability Frontier Lab  Myra Deng & Mark Bissell of Goodfire AI",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As reported in **News** yesterday"
      },
      "summary_html": "<p>As reported in <a href=\"/?date=2026-02-06&amp;category=news#item-48fe2a06c1c8\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> yesterday, Goodfire AI, focused on mechanistic interpretability, raised $150M in Series B funding at a $1.25B valuation. The company is building production APIs and enterprise deployments to make 'peeking inside the model' actionable for real workflows.</p>",
      "content_html": "<p>Tickets for AIE Miami and AIE Europe are on sale now!From Palantir and Two Sigma to building Goodfire into the poster-child for actionable mechanistic interpretability, Mark Bissell (Member of Technical Staff) and Myra Deng (Head of Product) are trying to turn peeking inside the model into a repeatable production workflow by shipping APIs, landing real enterprise deployments, and now scaling the bet with a recent $150M Series B funding round at a $1.25B valuation.In this episode, we go far beyond the usual SAEs are cool take. We talk about Goodfires core bet: that the AI lifecycle is still fundamentally broken because the only reliable control we have is data and we post-train, RLHF, and fine-tune by slurping supervision through a straw, hoping the model picks up the right behaviors while quietly absorbing the wrong ones. Goodfires answer is to build a bi-directional interface between humans and models: read whats happening inside, edit it surgically, and eventually use interpretability during training so customization isnt just brute-force guesswork.Mark and Myra walk through what that looks like when you stop treating interpretability like a lab demo and start treating it like infrastructure: lightweight probes that add near-zero latency, token-level safety filters that can run at inference time, and interpretability workflows that survive messy constraints (multilingual inputs, syntheticreal transfer, regulated domains, no access to sensitive data). We also get a live window into what frontier-scale interp means operationally (i.e. steering a trillion-parameter model in real time by targeting internal features) plus why the same tooling generalizes cleanly from language models to genomics, medical imaging, and pixel-space world models.We discuss:Myra + Marks path: Palantir (health systems, forward-deployed engineering)  Goodfire early team; Two Sigma  Head of Product, translating frontier interpretability research into a platform and real-world deploymentsWhat interpretability actually means in practice: not just post-hoc poking, but a broader science of deep learning approach across the full AI lifecycle (data curation  post-training  internal representations  model design)Why post-training is the first big wedge: surgical edits for unintended behaviors likereward hacking, sycophancy, noise learned during customization plus the dream of targeted unlearning and bias removal without wrecking capabilitiesSAEs vs probes in the real world: why SAE feature spaces sometimes underperform classifiers trained on raw activations for downstream detection tasks (hallucination, harmful intent, PII), and what that implies about clean concept spacesRakuten in production: deploying interpretability-based token-level PII detection at inference time to prevent routing private data to downstream providers plus the gnarly constraints: no training on real customer PII, syntheticreal transfer, English + Japanese, and tokenization quirksWhy interp can be operationally cheaper than LLM-judge guardrails: probes are lightweight, low-latency, and dont require hosting a second large model in the loopReal-time steering at frontier scale: a demo of steering Kimi K2 (~1T params) live and finding features via SAE pipelines, auto-labeling via LLMs, and toggling a Gen-Z slang feature across multiple layers without breaking tool useHallucinations as an internal signal: the case that models have latent uncertainty / user-pleasing circuitry you can detect and potentially mitigate more directly than black-box methodsSteering vs prompting: the emerging view that activation steering and in-context learning are more closely connected than people think, including work mapping between the two (even for jailbreak-style behaviors)Interpretability for science: using the same tooling across domains (genomics, medical imaging, materials) to debug spurious correlations and extract new knowledge up to and including early biomarker discovery work with major partnersWorld models + pixel-space interpretability: why vision/video models make concepts easier to see, how that accelerates the feedback loop, and why robotics/world-model partners are especially interesting design partnersThe north star: moving from data in, weights out to intentional model design where experts can impart goals and constraints directly, not just via reward signals and brute-force post-trainingGoodfire AIWebsite: https://goodfire.aiLinkedIn: https://www.linkedin.com/company/goodfire-ai/X: https://x.com/GoodfireAIMyra DengWebsite: https://myradeng.com/LinkedIn: https://www.linkedin.com/in/myra-deng/X: https://x.com/myra_dengMark BissellLinkedIn: https://www.linkedin.com/in/mark-bissell/X: https://x.com/MarkMBissellFull Video EpisodeTimestamps00:00:00 Introduction00:00:05 Introduction to the Latent Space Podcast and Guests from Goodfire00:00:29 What is Goodfire? Mission and Focus on Interpretability00:01:01 Goodfires Practical Approach to Interpretability00:01:37 Goodfires Series B Fundraise Announcement00:02:04 Backgrounds of Mark and Myra from Goodfire00:02:51 Team Structure and Roles at Goodfire00:05:13 What is Interpretability? Definitions and Techniques00:05:30 Understanding Errors00:07:29 Post-training vs. Pre-training Interpretability Applications00:08:51 Using Interpretability to Remove Unwanted Behaviors00:10:09 Grokking, Double Descent, and Generalization in Models00:10:15 404 Not Found Explained00:12:06 Subliminal Learning and Hidden Biases in Models00:14:07 How Goodfire Chooses Research Directions and Projects00:15:00 Troubleshooting Errors00:16:04 Limitations of SAEs and Probes in Interpretability00:18:14 Rakuten Case Study: Production Deployment of Interpretability00:20:45 Conclusion00:21:12 Efficiency Benefits of Interpretability Techniques00:21:26 Live Demo: Real-Time Steering in a Trillion Parameter Model00:25:15 How Steering Features are Identified and Labeled00:26:51 Detecting and Mitigating Hallucinations Using Interpretability00:31:20 Equivalence of Activation Steering and Prompting00:34:06 Comparing Steering with Fine-Tuning and LoRA Techniques00:36:04 Model Design and the Future of Intentional AI Development00:38:09 Getting Started in Mechinterp: Resources, Programs, and Open Problems00:40:51 Industry Applications and the Rise of Mechinterp in Practice00:41:39 Interpretability for Code Models and Real-World Usage00:43:07 Making Steering Useful for More Than Stylistic Edits00:46:17 Applying Interpretability to Healthcare and Scientific Discovery00:49:15 Why Interpretability is Crucial in High-Stakes Domains like Healthcare00:52:03 Call for Design Partners Across Domains00:54:18 Interest in World Models and Visual Interpretability00:57:22 Sci-Fi Inspiration: Ted Chiang and Interpretability01:00:14 Interpretability, Safety, and Alignment Perspectives01:04:27 Weak-to-Strong Generalization and Future Alignment Challenges01:05:38 Final Thoughts and Hiring/Collaboration Opportunities at GoodfireTranscriptShawn Wang [00:00:05]: So welcome to the Latent Space pod. Were back in the studio with our special MechInterp co-host, Vibhu. Welcome. Mochi, Mochis special co-host. And Mochi, the mechanistic interpretability doggo. We have with us Mark and Myra from Goodfire. Welcome. Thanks for having us on. Maybe we can sort of introduce Goodfire and then introduce you guys. How do you introduce Goodfire today?Myra Deng [00:00:29]: Yeah, its a great question. So Goodfire, we like to say, is an AI research lab that focuses on using interpretability to understand, learn from, and design AI models. And we really believe that interpretability will unlock the new generation, next frontier of safe and powerful AI models. Thats our description right now, and Im excited to dive more into the work were doing to make that happen.Shawn Wang [00:00:55]: Yeah. And theres always like the official description. Is there an understatement? Is there an unofficial one that sort of resonates more with a different audience?Mark Bissell [00:01:01]: Well, being an AI research lab thats focused on interpretability, theres obviously a lot of people have a lot that they think about when they think of interpretability. And I think we have a pretty broad definition of what that means and the types of places that can be applied. And in particular, applying it in production scenarios, in high stakes industries, and really taking it sort of from the research world into the real world. Which, you know. Its a new field, so that hasnt been done all that much. And were excited about actually seeing that sort of put into practice.Shawn Wang [00:01:37]: Yeah, I would say it wasnt too long ago that Anthopic was like still putting out like toy models or superposition and that kind of stuff. And I wouldnt have pegged it to be this far along. When you and I talked at NeurIPS, you were talking a little bit about your production use cases and your customers. And then not to bury the lead, today were also announcing the fundraise, your Series B. $150 million. $150 million at a 1.25B valuation. Congrats, Unicorn.Mark Bissell [00:02:02]: Thank you. Yeah, no, things move fast.Shawn Wang [00:02:04]: We were talking to you in December and already some big updates since then. Lets dive, I guess, into a bit of your backgrounds as well. Mark, you were at Palantir working on health stuff, which is really interesting because the Goodfire has some interesting like health use cases. I dont know how related they are in practice.Mark Bissell [00:02:22]: Yeah, not super related, but I dont know. It was helpful context to know what its like. Just to work. Just to work with health systems and generally in that domain. Yeah.Shawn Wang [00:02:32]: And Mara, you were at Two Sigma, which actually I was also at Two Sigma back in the day. Wow, nice.Myra Deng [00:02:37]: Did we overlap at all?Shawn Wang [00:02:38]: No, this is when I was briefly a software engineer before I became a sort of developer relations person. And now youre head of product. What are your sort of respective roles, just to introduce people to like what all gets done in Goodfire?Mark Bissell [00:02:51]: Yeah, prior to Goodfire, I was at Palantir for about three years as a forward deployed engineer, now a hot term. Wasnt always that way. And as a technical lead on the health care team and at Goodfire, Im a member of the technical staff. And honestly, that I think is about as specific as like as as I could describe myself because Ive worked on a range of things. And, you know, its its a fun time to be at a team thats still reasonably small. I think when I joined one of the first like ten employees, now were above 40, but still, it looks like theres always a mix of research and engineering and product and all of the above. That needs to get done. And I think everyone across the team is, you know, pretty, pretty switch hitter in the roles they do. So I think youve seen some of the stuff that I worked on related to image models, which was sort of like a research demo. More recently, Ive been working on our scientific discovery team with some of our life sciences partners, but then also building out our core platform for more of like flexing some of the kind of MLE and developer skills as well.Shawn Wang [00:03:53]: Very generalist. And you also had like a very like a founding engineer type role.Myra Deng [00:03:58]: Yeah, yeah.Shawn Wang [00:03:59]: So I also started as I still am a member of technical staff, did a wide range of things from the very beginning, including like finding our office space and all of this, which is we both we both visited when you had that open house thing. It was really nice.Myra Deng [00:04:13]: Thank you. Thank you. Yeah. Plug to come visit our office.Shawn Wang [00:04:15]: It looked like it was like 200 people. It has room for 200 people. But you guys are like 10.Myra Deng [00:04:22]: For a while, it was very empty. But yeah, like like Mark, I spend. A lot of my time as as head of product, I think product is a bit of a weird role these days, but a lot of it is thinking about how do we take our frontier research and really apply it to the most important real world problems and how does that then translate into a platform thats repeatable or a product and working across, you know, the engineering and research teams to make that happen and also communicating to the world? Like, what is interpretability? What is it used for? What is it good for? Why is it so important? All of these things are part of my day-to-day as well.Shawn Wang [00:05:01]: I love like what is things because thats a very crisp like starting point for people like coming to a field. They all do a fun thing. Vibhu, why dont you want to try tackling what is interpretability and then they can correct us.Vibhu Sapra [00:05:13]: Okay, great. So I think like one, just to kick off, its a very interesting role to be head of product, right? Because you guys, at least as a lab, youre more of an applied interp lab, right? Which is pretty different than just normal interp, like a lot of background research. But yeah. You guys actually ship an API to try these things. You have Ember, you have products around it, which not many do. Okay. What is interp? So basically youre trying to have an understanding of whats going on in model, like in the model, in the internal. So different approaches to do that. You can do probing, SAEs, transcoders, all this stuff. But basically you have an, you have a hypothesis. You have something that you want to learn about whats happening in a model internals. And then youre trying to solve that from there. You can do stuff like you can, you know, you can do activation mapping. You can try to do steering. Theres a lot of stuff that you can do, but the key question is, you know, from input to output, we want to have a better understanding of whats happening and, you know, how can we, how can we adjust whats happening on the model internals? Howd I do?Mark Bissell [00:06:12]: That was really good. I think that was great. I think its also a, its kind of a minefield of a, if you ask 50 people who quote unquote work in interp, like what is interpretability, youll probably get 50 different answers. And. Yeah. To some extent also like where, where good fire sits in the space. I think that were an AI research company above all else. And interpretability is a, is a set of methods that we think are really useful and worth kind of specializing in, in order to accomplish the goals we want to accomplish. But I think we also sort of see some of the goals as even more broader as, as almost like the science of deep learning and just taking a not black box approach to kind of any part of the like AI development life cycle, whether that. That means using interp for like data curation while youre training your model or for understanding what happened during post-training or for the, you know, understanding activations and sort of internal representations, what is in there semantically. And then a lot of sort of exciting updates that were, you know, are sort of also part of the, the fundraise around bringing interpretability to training, which I dont think has been done all that much before. A lot of this stuff is sort of post-talk poking at models as opposed to. To actually using this to intentionally design them.Shawn Wang [00:07:29]: Is this post-training or pre-training or is that not a useful.Myra Deng [00:07:33]: Currently focused on post-training, but theres no reason the techniques wouldnt also work in pre-training.Shawn Wang [00:07:38]: Yeah. It seems like it would be more active, applicable post-training because basically Im thinking like rollouts or like, you know, having different variations of a model that you can tweak with the, with your steering. Yeah.Myra Deng [00:07:50]: And I think in a lot of the news that youve seen in, in, on like Twitter or whatever, youve seen a lot of unintended. Side effects come out of post-training processes, you know, overly sycophantic models or models that exhibit strange reward hacking behavior. I think these are like extreme examples. Theres also, you know, very, uh, mundane, more mundane, like enterprise use cases where, you know, they try to customize or post-train a model to do something and it learns some noise or it doesnt appropriately learn the target task. And a big question that weve always had is like, how do you use your understanding of what the model knows and what its doing to actually guide the learning process?Shawn Wang [00:08:26]: Yeah, I mean, uh, you know, just to anchor this for people, uh, one of the biggest controversies of last year was 4.0 GlazeGate. Ive never heard of GlazeGate. I didnt know that was what it was called. The other one, they called it that on the blog post and I was like, well, how did OpenAI call it? Like officially use that term. And Im like, thats funny, but like, yeah, I guess its the pitch that if they had worked a good fire, they wouldnt have avoided it. Like, you know what Im saying?Myra Deng [00:08:51]: I think so. Yeah. Yeah.Mark Bissell [00:08:53]: I think thats certainly one of the use cases. I think. Yeah. Yeah. I think the reason why post-training is a place where this makes a lot of sense is a lot of what were talking about is surgical edits. You know, you want to be able to have expert feedback, very surgically change how your model is doing, whether that is, you know, removing a certain behavior that it has. So, you know, one of the things that weve been looking at or is, is another like common area where you would want to make a somewhat surgical edit is some of the models that have say political bias. Like you look at Quen or, um, R1 and they have sort of like this CCP bias.Shawn Wang [00:09:27]: Is there a CCP vector?Mark Bissell [00:09:29]: Well, theres, there are certainly internal, yeah. Parts of the representation space where you can sort of see where that lives. Yeah. Um, and you want to kind of, you know, extract that piece out.Shawn Wang [00:09:40]: Well, I always say, you know, whenever you find a vector, a fun exercise is just like, make it very negative to see what the opposite of CCP is.Mark Bissell [00:09:47]: The super America, bald eagles flying everywhere. But yeah. So in general, like lots of post-training tasks where youd want to be able to, to do that. Whether its unlearning a certain behavior or, you know, some of the other kind of cases where this comes up is, are you familiar with like the, the grokking behavior? I mean, I know the machine learning term of grokking.Shawn Wang [00:10:09]: Yeah.Mark Bissell [00:10:09]: Sort of this like double descent idea of, of having a model that is able to learn a generalizing, a generalizing solution, as opposed to even if memorization of some task would suffice, you want it to learn the more general way of doing a thing. And so, you know, another. A way that you can think about having surgical access to a models internals would be learn from this data, but learn in the right way. If there are many possible, you know, ways to, to do that. Can make interp solve the double descent problem?Shawn Wang [00:10:41]: Depends, I guess, on how you. Okay. So I, I, I viewed that double descent as a problem because then youre like, well, if the loss curves level out, then youre done, but maybe youre not done. Right. Right. But like, if you actually can interpret what is a generalizing or what youre doing. What is, what is still changing, even though the loss is not changing, then maybe you, you can actually not view it as a double descent problem. And actually youre just sort of translating the space in which you view loss and like, and then you have a smooth curve. Yeah.Mark Bissell [00:11:11]: I think thats certainly like the domain of, of problems that were, that were looking to get.Shawn Wang [00:11:15]: Yeah. To me, like double descent is like the biggest thing to like ML research where like, if you believe in scaling, then you dont need, you need to know where to scale. And. But if you believe in double descent, then you dont, you dont believe in anything where like anything levels off, like.Vibhu Sapra [00:11:30]: I mean, also tendentially theres like, okay, when you talk about the China vector, right. Theres the subliminal learning work. It was from the anthropic fellows program where basically you can have hidden biases in a model. And as you distill down or, you know, as you train on distilled data, those biases always show up, even if like you explicitly try to not train on them. So, you know, its just like another use case of. Okay. If we can interpret whats happening in post-training, you know, can we clear some of this? Can we even determine whats there? Because yeah, its just like some worrying research thats out there that shows, you know, we really dont know whats going on.Mark Bissell [00:12:06]: That is. Yeah. I think thats the biggest sentiment that were sort of hoping to tackle. Nobody knows whats going on. Right. Like subliminal learning is just an insane concept when you think about it. Right. Train a model on not even the logits, literally the output text of a bunch of random numbers. And now your model loves owls. And you see behaviors like that, that are just, they defy, they defy intuition. And, and there are mathematical explanations that you can get into, but. I mean.Shawn Wang [00:12:34]: It feels so early days. Objectively, there are a sequence of numbers that are more owl-like than others. There, there should be.Mark Bissell [00:12:40]: According to, according to certain models. Right. Its interesting. I think it only applies to models that were initialized from the same starting Z. Usually, yes.Shawn Wang [00:12:49]: But I mean, I think thats a, thats a cheat code because theres not enough compute. But like if you believe in like platonic representation, like probably it will transfer across different models as well. Oh, you think so?Mark Bissell [00:13:00]: I think of it more as a statistical artifact of models initialized from the same seed sort of. Theres something that is like path dependent from that seed that might cause certain overlaps in the latent space and then sort of doing this distillation. Yeah. Like it pushes it towards having certain other tendencies.Vibhu Sapra [00:13:24]: Got it. I think theres like a bunch of these open-ended questions, right? Like you cant train in new stuff during the RL phase, right? RL only reorganizes weights and you can only do stuff thats somewhat there in your base model. Youre not learning new stuff. Youre just reordering chains and stuff. But okay. My broader question is when you guys work at an interp lab, how do you decide what to work on and whats kind of the thought process? Right. Because we can ramble for hours. Okay. I want to know this. I want to know that. But like, how do you concretely like, you know, whats the workflow? Okay. Theres like approaches towards solving a problem, right? I can try prompting. I can look at chain of thought. I can train probes, SAEs. But how do you determine, you know, like, okay, is this going anywhere? Like, do we have set stuff? Just, you know, if you can help me with all that. Yeah.Myra Deng [00:14:07]: Its a really good question. I feel like weve always at the very beginning of the company thought about like, lets go and try to learn what isnt working in machine learning today. Whether thats talking to customers or talking to researchers at other labs, trying to understand both where the frontier is going and where things are really not falling apart today. And then developing a perspective on how we can push the frontier using interpretability methods. And so, you know, even our chief scientist, Tom, spends a lot of time talking to customers and trying to understand what real world problems are and then taking that back and trying to apply the current state of the art to those problems and then seeing where they fall down basically. And then using those failures or those shortcomings to understand what hills to climb when it comes to interpretability research. So like on the fundamental side, for instance, when we have done some work applying SAEs and probes, weve encountered, you know, some shortcomings in SAEs that we found a little bit surprising. And so have gone back to the drawing board and done work on that. And then, you know, weve done some work on better foundational interpreter models. And a lot of our teams research is focused on what is the next evolution beyond SAEs, for instance. And then when it comes to like control and design of models, you know, we tried steering with our first API and realized that it still fell short of black box techniques like prompting or fine tuning. And so went back to the drawing board and were like, how do we make that not the case and how do we improve it beyond that? And one of our researchers, Ekdeep, who just joined is actually Ekdeep and Atticus are like steering experts and have spent a lot of time trying to figure out like, what is the research that enables us to actually do this in a much more powerful, robust way? So yeah, the answer is like, look at real world problems, try to translate that into a research agenda and then like hill climb on both of those at the same time.Shawn Wang [00:16:04]: Yeah. Mark has the steering CLI demo queued up, which were going to go into in a sec. But I always want to double click on when you drop hints, like we found some problems with SAEs. Okay. What are they? You know, and then we can go into the demo. Yeah.Myra Deng [00:16:19]: I mean, Im curious if you have more thoughts here as well, because youve done it in the healthcare domain. But I think like, for instance, when we do things like trying to detect behaviors within models that are harmful or like behaviors that a user might not want to have in their model. So hallucinations, for instance, harmful intent, PII, all of these things. We first tried using SAE probes for a lot of these tasks. So taking the feature activation space from SAEs and then training classifiers on top of that, and then seeing how well we can detect the properties that we might want to detect in model behavior. And weve seen in many cases that probes just trained on raw activations seem to perform better than SAE probes, which is a bit surprising if you think that SAEs are actually also capturing the concepts that you would want to capture cleanly and more surgically. And so that is an interesting observation. I dont think that is like, Im not down on SAEs at all. I think there are many, many things theyre useful for, but we have definitely run into cases where I think the concept space described by SAEs is not as clean and accurate as we would expect it to be for actual like real world downstream performance metrics.Mark Bissell [00:17:34]: Fair enough. Yeah. Its the blessing and the curse of unsupervised methods where you get to peek into the AIs mind. But sometimes you wish that you saw other things when you walked inside there. Although in the PII instance, I think werent an SAE based approach actually did prove to be the most generalizable?Myra Deng [00:17:53]: It did work well in the case that we published with Rakuten. And I think a lot of the reasons it worked well was because we had a noisier data set. And so actually the blessing of unsupervised learning is that we actually got to get more meaningful, generalizable signal from SAEs when the data was noisy. But in other cases where weve had like good data sets, it hasnt been the case.Shawn Wang [00:18:14]: And just because you named Rakuten and I dont know if well get it another chance, like what is the overall, like what is Rakutens usage or production usage? Yeah.Myra Deng [00:18:25]: So they are using us to essentially guardrail and inference time monitor their language model usage and their agent usage to detect things like PII so that they dont route private user information.Myra Deng [00:18:41]: And so thats, you know, going through all of their user queries every day. And thats something that we deployed with them a few months ago. And now we are actually exploring very early partnerships, not just with Rakuten, but with other people around how we can help with potentially training and customization use cases as well. Yeah.Shawn Wang [00:19:03]: And for those who dont know, like its Rakuten is like, I think number one or number two e-commerce store in Japan. Yes. Yeah.Mark Bissell [00:19:10]: And I think that use case actually highlights a lot of like what it looks like to deploy things in practice that you dont always think about when youre doing sort of research tasks. So when you think about some of the stuff that came up there thats more complex than your idealized version of a problem, they were encountering things like synthetic to real transfer of methods. So they couldnt train probes, classifiers, things like that on actual customer data of PII. So what they had to do is use synthetic data sets. And then hope that that transfer is out of domain to real data sets. And so we can evaluate performance on the real data sets, but not train on customer PII. So that right off the bat is like a big challenge. You have multilingual requirements. So this needed to work for both English and Japanese text. Japanese text has all sorts of quirks, including tokenization behaviors that caused lots of bugs that caused us to be pulling our hair out. And then also a lot of tasks youll see. You might make simplifying assumptions if youre sort of treating it as like the easiest version of the problem to just sort of get like general results where maybe you say youre classifying a sentence to say, does this contain PII? But the need that Rakuten had was token level classification so that you could precisely scrub out the PII. So as we learned more about the problem, youre sort of speaking about what that looks like in practice. Yeah. A lot of assumptions end up breaking. And that was just one instance where you. A problem that seems simple right off the bat ends up being more complex as you keep diving into it.Vibhu Sapra [00:20:41]: Excellent. One of the things thats also interesting with Interp is a lot of these methods are very efficient, right? So where youre just looking at a models internals itself compared to a separate like guardrail, LLM as a judge, a separate model. One, you have to host it. Two, theres like a whole latency. So if you use like a big model, you have a second call. Some of the work around like self detection of hallucination, its also deployed for efficiency, right? So if you have someone like Rakuten doing it in production live, you know, thats just another thing people should consider.Mark Bissell [00:21:12]: Yeah. And something like a probe is super lightweight. Yeah. Its no extra latency really. Excellent.Shawn Wang [00:21:17]: You have the steering demos lined up. So we were just kind of see what you got. I dont, I dont actually know if this is like the latest, latest or like alpha thing.Mark Bissell [00:21:26]: No, this is a pretty hacky demo from from a presentation that someone else on the team recently gave. So this will give a sense for, for technology. So you can see the steering and action. Honestly, I think the biggest thing that this highlights is that as weve been growing as a company and taking on kind of more and more ambitious versions of interpretability related problems, a lot of that comes to scaling up in various different forms. And so here youre going to see steering on a 1 trillion parameter model. This is Kimi K2. And so its sort of fun that in addition to the research challenges, there are engineering challenges that were now tackling. Cause for any of this to be sort of useful in production, you need to be thinking about what it looks like when youre using these methods on frontier models as opposed to sort of like toy kind of model organisms. So yeah, this was thrown together hastily, pretty fragile behind the scenes, but I think its quite a fun demo. So screen sharing is on. So Ive got two terminal sessions pulled up here. On the left is a forked version that we have of the Kimi CLI that weve got running to point at our custom hosted Kimi model. And then on the right is a set up that will allow us to steer on certain concepts. So I should be able to chat with Kimi over here. Tell it hello. This is running locally. So the CLI is running locally, but the Kimi server is running back to the office. Well, hopefully should be, um, thats too much to run on that Mac. Yeah. I think its, uh, it takes a full, like each 100 node. I think its like, you can. You can run it on eight GPUs, eight 100. So, so yeah, Kimis running. We can ask it a prompt. Its got a forked version of our, uh, of the SG line code base that weve been working on. So Im going to tell it, Hey, this SG line code base is slow. I think theres a bug. Can you try to figure it out? Theres a big code base, so itll, itll spend some time doing this. And then on the right here, Im going to initialize in real time. Some steering. Lets see here.Mark Bissell [00:23:33]: searching for any. Bugs. Feature ID 43205.Shawn Wang [00:23:38]: Yeah.Mark Bissell [00:23:38]: 20, 30, 40. So let me, uh, this is basically a feature that we found that inside Kimi seems to cause it to speak in Gen Z slang. And so on the left, its still sort of thinking normally it might take, I dont know, 15 seconds for this to kick in, but then were going to start hopefully seeing him do this code base is massive for real. So were going to start. Were going to start seeing Kimi transition as the steering kicks in from normal Kimi to Gen Z Kimi and both in its chain of thought and its actual outputs.Mark Bissell [00:24:19]: And interestingly, you can see, you know, its still able to call tools, uh, and stuff. Its um, its purely sort of its its demeanor. And there are other features that we found for interesting things like concision. So thats more of a practical one. You can make it more concise. Um, the types of programs, uh, programming languages that uses, but yeah, as were seeing it come in. Pretty good. Outputs.Shawn Wang [00:24:43]: Scheduler code is actually wild.Vibhu Sapra [00:24:46]: Yo, this code is actually insane, bro.Vibhu Sapra [00:24:53]: Whats the process of training in SAE on this, or, you know, how do you label features? I know you guys put out a pretty cool blog post about, um, finding this like autonomous interp. Um, something. Something about how agents for interp is different than like coding agents. I dont know while this is spewing up, but how, how do we find feature 43, two Oh five. Yeah.Mark Bissell [00:25:15]: So in this case, um, we, our platform that weve been building out for a long time now supports all the sort of classic out of the box interp techniques that you might want to have like SAE training, probing things of that kind, Id say the techniques for like vanilla SAEs are pretty well established now where. You take your model that youre interpreting, run a whole bunch of data through it, gather activations, and then yeah, pretty straightforward pipeline to train an SAE. There are a lot of different varieties. Theres top KSAEs, batch top KSAEs, um, normal ReLU SAEs. And then once you have your sparse features to your point, assigning labels to them to actually understand that this is a gen Z feature, thats actually where a lot of the kind of magic happens. Yeah. And the most basic standard technique is look at all of your d input data set examples that cause this feature to fire most highly. And then you can usually pick out a pattern. So for this feature, If Ive run a diverse enough data set through my model feature 43, two Oh five. Probably tends to fire on all the tokens that sounds like gen Z slang. You know, thats the, thats the time of year to be like, Oh, Im in this, Im in this Um, and, um, so, you know, you could have a human go through all 43,000 concepts andVibhu Sapra [00:26:34]: And Ive got to ask the basic question, you know, can we get examples where it hallucinates, pass it through, see what feature activates for hallucinations? Can I just, you know, turn hallucination down?Myra Deng [00:26:51]: Oh, wow. You really predicted a project were already working on right now, which is detecting hallucinations using interpretability techniques. And this is interesting because hallucinations is something thats very hard to detect. And its like a kind of a hairy problem and something that black box methods really struggle with. Whereas like Gen Z, you could always train a simple classifier to detect that hallucinations is harder. But weve seen that models internally have some... Awareness of like uncertainty or some sort of like user pleasing behavior that leads to hallucinatory behavior. And so, yeah, we have a project thats trying to detect that accurately. And then also working on mitigating the hallucinatory behavior in the model itself as well.Shawn Wang [00:27:39]: Yeah, I would say most people are still at the level of like, oh, I would just turn temperature to zero and that turns off hallucination. And Im like, well, thats a fundamental misunderstanding of how this works. Yeah.Mark Bissell [00:27:51]: Although, so part of what I like about that question is you, there are SAE based approaches that might like help you get at that. But oftentimes the beauty of SAEs and like we said, the curse is that theyre unsupervised. So when you have a behavior that you deliberately would like to remove, and thats more of like a supervised task, often it is better to use something like probes and specifically target the thing that youre interested in reducing as opposed to sort of like hoping that when you fragment the latent space, one of the vectors that pops out.Vibhu Sapra [00:28:20]: And as much as were training an autoencoder to be sparse, were not like for sure certain that, you know, we will get something that just correlates to hallucination. Youll probably split that up into 20 other things and who knows what theyll be.Mark Bissell [00:28:36]: Of course. Right. Yeah. So theres no sort of problems with like feature splitting and feature absorption. And then theres the off target effects, right? Ideally, you would want to be very precise where if you reduce the hallucination feature, suddenly maybe your model cant write. Creatively anymore. And maybe you dont like that, but you want to still stop it from hallucinating facts and figures.Shawn Wang [00:28:55]: Good. So Vibhu has a paper to recommend there that well put in the show notes. But yeah, I mean, I guess just because your demo is done, any any other things that you want to highlight or any other interesting features you want to show?Mark Bissell [00:29:07]: I dont think so. Yeah. Like I said, this is a pretty small snippet. I think the main sort of point here that I think is exciting is that theres not a whole lot of inter being applied to models quite at this scale. You know, Anthropic certainly has some some. Research and yeah, other other teams as well. But its its nice to see these techniques, you know, being put into practice. I think not that long ago, the idea of real time steering of a trillion parameter model would have sounded.Shawn Wang [00:29:33]: Yeah. The fact that its real time, like you started the thing and then you edited the steering vector.Vibhu Sapra [00:29:38]: I think its its an interesting one TBD of what the actual like production use case would be on that, like the real time editing. Its like thats the fun part of the demo, right? You can kind of see how this could be served behind an API, right? Like, yes, youre you only have so many knobs and you can just tweak it a bit more. And I dont know how it plays in. Like people havent done that much with like, how does this work with or without prompting? Right. How does this work with fine tuning? Like, theres a whole hype of continual learning, right? So theres just so much to see. Like, is this another parameter? Like, is it like parameter? We just kind of leave it as a default. We dont use it. So I dont know. Maybe someone here wants to put out a guide on like how to use this with prompting when to do what?Mark Bissell [00:30:18]: Oh, well, I have a paper recommendation. I think you would love from Act Deep on our team, who is an amazing researcher, just cant say enough amazing things about Act Deep. But he actually has a paper that as well as some others from the team and elsewhere that go into the essentially equivalence of activation steering and in context learning and how those are from a he thinks of everything in a cognitive neuroscience Bayesian framework, but basically how you can precisely show how. Prompting in context, learning and steering exhibit similar behaviors and even like get quantitative about the like magnitude of steering you would need to do to induce a certain amount of behavior similar to certain prompting, even for things like jailbreaks and stuff. Its a really cool paper. Are you saying steering is less powerful than prompting? More like you can almost write a formula that tells you how to convert between the two of them.Myra Deng [00:31:20]: And so like formally equivalent actually in the in the limit. Right.Mark Bissell [00:31:24]: So like one case study of this is for jailbreaks there. I dont know. Have you seen the stuff where you can do like many shot jailbreaking? You like flood the context with examples of the behavior. And the topic put out that paper.Shawn Wang [00:31:38]: A lot of people were like, yeah, weve been doing this, guys.Mark Bissell [00:31:40]: Like, yeah, whats in this in context learning and activation steering equivalence paper is you can like predict the number. Number of examples that you will need to put in there in order to jailbreak the model. Thats cool. By doing steering experiments and using this sort of like equivalence mapping. Thats cool. Thats really cool. Its very neat. Yeah.Shawn Wang [00:32:02]: I was going to say, like, you know, I can like back rationalize that this makes sense because, you know, what context is, is basically just, you know, it updates the KV cache kind of and like and then every next token inference is still like, you know, the sheer sum of everything all the way. Its plus all the context. Its up to date. And you could, I guess, theoretically steer that with you probably replace that with your steering. The only problem is steering typically is on one layer, maybe three layers like like you did. So its like not exactly equivalent.Mark Bissell [00:32:33]: Right, right. Theres sort of you need to get precise about, yeah, like how you sort of define steering and like what how youre modeling the setup. But yeah, Ive got the paper pulled up here. Belief dynamics reveal the dual nature. Yeah. The title is Belief Dynamics Reveal the Dual Nature of Incompetence. And its an exhibition of the practical context learning and activation steering. So Eric Bigelow, Dan Urgraft on the who are doing fellowships at Goodfire, Ekt Deeps the final author there.Myra Deng [00:32:59]: I think actually to your question of like, what is the production use case of steering? I think maybe if you just think like one level beyond steering as it is today. Like imagine if you could adapt your model to be, you know, an expert legal reasoner. Like in almost real time, like very quickly. efficiently using human feedback or using like your semantic understanding of what the model knows and where it knows that behavior. I think that while its not clear what the product is at the end of the day, its clearly very valuable. Thinking about like whats the next interface for model customization and adaptation is a really interesting problem for us. Like we have heard a lot of people actually interested in fine-tuning an RL for open weight models in production. And so people are using things like Tinker or kind of like open source libraries to do that, but its still very difficult to get models fine-tuned and RLd for exactly what you want them to do unless youre an expert at model training. And so thats like something wereShawn Wang [00:34:06]: looking into. Yeah. I never thought so. Tinker from Thinking Machines famously uses rank one LoRa. Is that basically the same as steering? Like, you know, whats the comparison there?Mark Bissell [00:34:19]: Well, so in that case, you are still applying updates to the parameters, right?Shawn Wang [00:34:25]: Yeah. Youre not touching a base model. Youre touching an adapter. Its kind of, yeah.Mark Bissell [00:34:30]: Right. But I guess it still is like more in parameter space then. I guess its maybe like, are you modifying the pipes or are you modifying the water flowing through the pipes to get what youre after? Yeah. Just maybe one way.Mark Bissell [00:34:44]: I like that analogy. Thats my mental map of it at least, but it gets at this idea of model design and intentional design, which is something that were, that were very focused on. And just the fact that like, I hope that we look back at how were currently training models and post-training models and just think what a primitive way of doing that right now. Like theres no intentionalityShawn Wang [00:35:06]: really in... Its just data, right? The only thing in control is what data we feed in.Mark Bissell [00:35:11]: So, so Dan from Goodfire likes to use this analogy of, you know, he has a couple of young kids and he talks about like, what if I could only teach my kids how to be good people by giving them cookies or like, you know, giving them a slap on the wrist if they do something wrong, like not telling them why it was wrong or like what they should have done differently or something like that. Just figure it out. Right. Exactly. So thats RL. Yeah. Right. And, and, you know, its sample inefficient. Theres, you know, what do they say? Its like slurping feedback. Its like, slurping supervision. Right. And so youd like to get to the point where you can have experts giving feedback to their models that are, uh, internalized and, and, you know, steering is an inference time way of sort of getting that idea. But ideally youre moving to a world whereVibhu Sapra [00:36:04]: it is much more intentional design in perpetuity for these models. Okay. This is one of the questions we asked Emmanuel from Anthropic on the podcast a few months ago. Basically the question, was youre at a research lab that does model training, foundation models, and youre on an interp team. How does it tie back? Right? Like, does this, do ideas come from the pre-training team? Do they go back? Um, you know, so for those interested, you can, you can watch that. There wasnt too much of a connect there, but its still something, you know, its something they want toMark Bissell [00:36:33]: push for down the line. It can be useful for all of the above. Like there are certainly post-hocVibhu Sapra [00:36:39]: use cases where it doesnt need to touch that. I think the other thing a lot of people forget is this stuff isnt too computationally expensive, right? Like I would say, if youre interested in getting into research, MechInterp is one of the most approachable fields, right? A lot of this train an essay, train a probe, this stuff, like the budget for this one, theres already a lot done. Theres a lot of open source work. You guys have done some too. Um, you know,Shawn Wang [00:37:04]: Theres like notebooks from the Gemini team for Neil Nanda or like, this is how you do it. Just step through the notebook.Vibhu Sapra [00:37:09]: Even if youre like, not even technical with any of this, you can still make like progress. There, you can look at different activations, but, uh, if you do want to get into training, you know, training this stuff, correct me if Im wrong is like in the thousands of dollars, not even like, its not that high scale. And then same with like, you know, applying it, doing it for post-training or all this stuff is fairly cheap in scale of, okay. I want to get into like model training. I dont have compute for like, you know, pre-training stuff. So its, its a very nice field to get into. And also theres a lot of like open questions, right? Um, some of them have to go with, okay, I want a product. I want to solve this. Like theres also just a lot of open-ended stuff that people could work on. Thats interesting. Right. I dont know if you guys have any calls for like, whats open questions, whats open work that you either open collaboration with, or like, youd just like to see solved or just, you know, for people listening that want to get into McInturk because people always talk about it. What are, what are the things they should check out? Start, of course, you know, join you guys as well. Im sure youre hiring.Myra Deng [00:38:09]: Theres a paper, I think from, was it Lee, uh, Sharky? Its open problems and, uh, its, its a bit of interpretability, which I recommend everyone whos interested in the field. Read. Im just like a really comprehensive overview of what are the things that experts in the field think are the most important problems to be solved. I also think to your point, its been really, really inspiring to see, I think a lot of young people getting interested in interpretability, actually not just young people also like scientists to have been, you know, experts in physics for many years and in biology or things like this, um, transitioning into interp, because the barrier of, of whats now interp. So its really cool to see a number to entry is, you know, in some ways low and theres a lot of information out there and ways to get started. Theres this anecdote of like professors at universities saying that all of a sudden every incoming PhD student wants to study interpretability, which was not the case a few years ago. So it just goes to show how, I guess, like exciting the field is, how fast its moving, how quick it is to get started and things like that.Mark Bissell [00:39:10]: And also just a very welcoming community. You know, theres an open source McInturk Slack channel. There are people are always posting questions and just folks in the space are always responsive if you ask things on various forums and stuff. But yeah, the open paper, open problems paper is a really good one.Myra Deng [00:39:28]: For other people who want to get started, I think, you know, MATS is a great program. Whats the acronym for? Machine Learning and Alignment Theory Scholars? Its like the...Vibhu Sapra [00:39:40]: Normally summer internship style.Myra Deng [00:39:42]: Yeah, but theyve been doing it year round now. And actually a lot of our full-time staff have come through that program or gone through that program. And its great for anyone who is transitioning into interpretability. Theres a couple other fellows programs. We do one as well as Anthropic. And so those are great places to get started if anyone is interested.Mark Bissell [00:40:03]: Also, I think been seen as a research field for a very long time. But I think engineering... I think engineers are sorely wanted for interpretability as well, especially at Goodfire, but elsewhere, as it does scale up.Shawn Wang [00:40:18]: I should mention that Lee actually works with you guys, right? And in the London office and Im adding our first ever McInturk track at AI Europe because I see this industry applications now emerging. And Im pretty excited to, you know, help push that along. Yeah, I was looking forward to that. Itll effectively be the first industry McInturk conference. Yeah. Im so glad you added that. You know, its still a little bit of a bet. Its not that widespread, but I can definitely see this is the time to really get into it. We want to be early on things.Mark Bissell [00:40:51]: For sure. And I think the field understands this, right? So at ICML, I think the title of the McInturk workshop this year was actionable interpretability. And there was a lot of discussion around bringing it to various domains. Everyones adding pragmatic, actionable, whatever.Shawn Wang [00:41:10]: Its like, okay, well, we werent actionable before, I guess. I dont know.Vibhu Sapra [00:41:13]: And I mean, like, just, you know, being in Europe, you see the Interp room. One, like old school conferences, like, I think they had a very tiny room till they got lucky and they got it doubled. But theres definitely a lot of interest, a lot of niche research. So you see a lot of research coming out of universities, students. We covered the paper last week. Its like two unknown authors, not many citations. But, you know, you can make a lot of meaningful work there. Yeah. Yeah. Yeah.Shawn Wang [00:41:39]: Yeah. I think people havent really mentioned this yet. Its just Interp for code. I think its like an abnormally important field. We havent mentioned this yet. The conspiracy theory last two years ago was when the first SAE work came out of Anthropic was they would do like, oh, we just used SAEs to turn the bad code vector down and then turn up the good code. And I think like, isnt that the dream? Like, you know, like, but basically, I guess maybe, why is it funny? Like, its... If it was realistic, it would not be funny. It would be like, no, actually, we should do this. But its funny because we know theres like, we feel theres some limitations to what steering can do. And I think a lot of the public image of steering is like the Gen Z stuff. Like, oh, you can make it really love the Golden Gate Bridge, or you can make it speak like Gen Z. To like be a legal reasoner seems like a huge stretch. Yeah. And I dont know if that will get there this way. Yeah.Myra Deng [00:42:36]: I think, um, I will say we are announcing. Something very soon that I will not speak too much about. Um, but I think, yeah, this is like what weve run into again and again is like, we, we dont want to be in the world where steering is only useful for like stylistic things. Thats definitely not, not what were aiming for. But I think the types of interventions that you need to do to get to things like legal reasoning, um, are much more sophisticated and require breakthroughs in, in learning algorithms. And thats, um...Shawn Wang [00:43:07]: And is this an emergent property of scale as well?Myra Deng [00:43:10]: I think so. Yeah. I mean, I think scale definitely helps. I think scale allows you to learn a lot of information and, and reduce noise across, you know, large amounts of data. But I also think we think that theres ways to do things much more effectively, um, even, even at scale. So like actually learning exactly what you want from the data and not learning things that you do that you dont want exhibited in the data. So were not like anti-scale, but we are also realizing that scale is not going to get us anywhere. Its not going to get us to the type of AI development that we want to be at in, in the future as these models get more powerful and get deployed in all these sorts of like mission critical contexts. Current life cycle of training and deploying and evaluations is, is to us like deeply broken and has opportunities to, to improve. So, um, more to come on that very, very soon.Mark Bissell [00:44:02]: And I think that thats a use basically, or maybe just like a proof point that these concepts do exist. Like if you can manipulate them in the precise best way, you can get the ideal combination of them that you desire. And steering is maybe the most coarse grained sort of peek at what that looks like. But I think its evocative of what you could do if you had total surgical control over every concept, every parameter. Yeah, exactly.Myra Deng [00:44:30]: There were like bad code features. Ive got it pulled up.Vibhu Sapra [00:44:33]: Yeah. Just coincidentally, as you guys are talking.Shawn Wang [00:44:35]: This is like, this is exactly.Vibhu Sapra [00:44:38]: Theres like specifically a code error feature that activates and they show, you know, its not, its not typo detection. Its like, its, its typos in code. Its not typical typos. And, you know, you can, you can see it clearly activates where theres something wrong in code. And they have like malicious code, code error. They have a whole bunch of sub, you know, sub broken down little grain features. Yeah.Shawn Wang [00:45:02]: Yeah. So, so the, the rough intuition for me, the, why I talked about post-training was that, well, you just, you know, have a few different rollouts with all these things turned off and on and whatever. And then, you know, you can, thats, thats synthetic data you can kind of post-train on. Yeah.Vibhu Sapra [00:45:13]: And I think we make it sound easier than it is just saying, you know, they do the real hard work.Myra Deng [00:45:19]: I mean, you guys, you guys have the right idea. Exactly. Yeah. We replicated a lot of these features in, in our Lama models as well. I remember there was like.Vibhu Sapra [00:45:26]: And I think a lot of this stuff is open, right? Like, yeah, you guys opened yours. DeepMind has opened a lot of essays on Gemma. Even Anthropic has opened a lot of this. Theres, theres a lot of resources that, you know, we can probably share of people that want to get involved.Shawn Wang [00:45:41]: Yeah. And special shout out to like Neuronpedia as well. Yes. Like, yeah, amazing piece of work to visualize those things.Myra Deng [00:45:49]: Yeah, exactly.Shawn Wang [00:45:50]: I guess I wanted to pivot a little bit on, onto the healthcare side, because I think thats a big use case for you guys. We havent really talked about it yet. This is a bit of a crossover for me because we are, we are, we do have a separate science pod that were starting up for AI, for AI for science, just because like, its such a huge investment category and also Im like less qualified to do it, but we actually have bio PhDs to cover that, which is great, but I need to just kind of recover, recap your work, maybe on the evil two stuff, but then, and then building forward.Mark Bissell [00:46:17]: Yeah, for sure. And maybe to frame up the conversation, I think another kind of interesting just lens on interpretability in general is a lot of the techniques that were described. are ways to solve the AI human interface problem. And its sort of like bidirectional communication is the goal there. So what weve been talking about with intentional design of models and, you know, steering, but also more advanced techniques is having humans impart our desires and control into models and over models. And the reverse is also very interesting, especially as you get to superhuman models, whether thats narrow superintelligence, like these scientific models that work on genomics, data, medical imaging, things like that. But down the line, you know, superintelligence of other forms as well. What knowledge can the AIs teach us as sort of that, that the other direction in that? And so some of our life science work to date has been getting at exactly that question, which is, well, some of it does look like debugging these various life sciences models, understanding if theyre actually performing well, on tasks, or if theyre picking up on spurious correlations, for instance, genomics models, you would like to know whether they are sort of focusing on the biologically relevant things that you care about, or if its using some simpler correlate, like the ancestry of the person that its looking at. But then also in the instances where they are superhuman, and maybe they are understanding elements of the human genome that we dont have names for or specific, you know, yeah, discoveries that theyve made that that we dont know about, thats, thats a big goal. And so were already seeing that, right, we are partnered with organizations like Mayo Clinic, leading research health system in the United States, our Institute, as well as a startup called Prima Menta, which focuses on neurodegenerative disease. And in our partnership with them, weve used foundation models, theyve been training and applied our interpretability techniques to find novel biomarkers for Alzheimers disease. So I think this is just the tip of the iceberg. But its, thats like a flavor of some of the things that were working on.Shawn Wang [00:48:36]: Yeah, I think thats really fantastic. Obviously, we did the Chad Zuckerberg pod last year as well. And like, theres a plethora of these models coming out, because theres so much potential and research. And its like, very interesting how its basically the same as language models, but just with a different underlying data set. But its like, its the same exact techniques. Like, theres no change, basically.Mark Bissell [00:48:59]: Yeah. Well, and even in like other domains, right? Like, you know, robotics, I know, like a lot of the companies just use Gemma as like the like backbone, and then they like make it into a VLA that like takes these actions. Its, its, its transformers all the way down. So yeah.Vibhu Sapra [00:49:15]: Like we have Med Gemma now, right? Like this week, even there was Med Gemma 1.5. And theyre training it on this stuff, like 3d scans, medical domain knowledge, and all that stuff, too. So theres a push from both sides. But I think the thing that, you know, one of the things about McInturpp is like, youre a little bit more cautious in some domains, right? So healthcare, mainly being one, like guardrails, understanding, you know, were more risk adverse to something going wrong there. So even just from a basic understanding, like, if were trusting these systems to make claims, we want to know why and whats going on.Myra Deng [00:49:51]: Yeah, I think theres totally a kind of like deployment bottleneck to actually using. foundation models for real patient usage or things like that. Like, say youre using a model for rare disease prediction, you probably want some explanation as to why your model predicted a certain outcome, and an interpretable explanation at that. So thats definitely a use case. But I also think like, being able to extract scientific information that no human knows to accelerate drug discovery and disease treatment and things like that actually is a really, really big unlock for science, like scientific discovery. And youve seen a lot of startups, like say that theyre going to accelerate scientific discovery. And I feel like we actually are doing that through our interp techniques. And kind of like, almost by accident, like, I think we got reached out to very, very early on from these healthcare institutions. And none of us had healthcare.Shawn Wang [00:50:49]: How did they even hear of you? A podcast.Myra Deng [00:50:51]: Oh, okay. Yeah, podcast.Vibhu Sapra [00:50:53]: Okay, well, nows that time, you know.Myra Deng [00:50:55]: Everyone can call us.Shawn Wang [00:50:56]: Podcasts are the most important thing. Everyone should listen to podcasts.Myra Deng [00:50:59]: Yeah, they reached out. They were like, you know, we have these really smart models that weve trained, and we want to know what theyre doing. And we were like, really early that time, like three months old, and it was a few of us. And we were like, oh, my God, weve never used these models. Lets figure it out. But its also like, great proof that interp techniques scale pretty well across domains. We didnt really have to learn too much about.Shawn Wang [00:51:21]: Interp is a machine learning technique, machine learning skills everywhere, right? Yeah. And its obviously, its just like a general insight. Yeah. Probably to finance too, I think, which would be fun for our history. I dont know if you have anything to say there.Mark Bissell [00:51:34]: Yeah, well, just across the science. Like, weve also done work on material science. Yeah, it really runs the gamut.Vibhu Sapra [00:51:40]: Yeah. Awesome. And, you know, for those that should reach out, like, youre obviously experts in this, but like, is there a call out for people that youre looking to partner with, design partners, people to use your stuff outside of just, you know, the general developer that wants to. Plug and play steering stuff, like on the research side more so, like, are there ideal design partners, customers, stuff like that?Myra Deng [00:52:03]: Yeah, I can talk about maybe non-life sciences, and then Im curious to hear from you on the life sciences side. But were looking for design partners across many domains, language, anyone whos customizing language models or trying to push the frontier of code or reasoning models is really interesting to us. And then also interested in the frontier of modeling. Theres a lot of models that work in, like, pixel space, as we call it. So if youre doing world models, video models, even robotics, where theres not a very clean natural language interface to interact with, I think we think that Interp can really help and are looking for a few partners in that space.Shawn Wang [00:52:43]: Just because you mentioned the keyword world models, is that a big part of your thinking? Do you have a definition that I can use? Because everyones asking me about it.Myra Deng [00:52:53]: About world models?Shawn Wang [00:52:54]: Theres quite a few definitions, lets say.Myra Deng [00:52:56]: I dont feel equipped to be an expert on world model definitions, but the reason were interested in them is because they give you, like, you know, with language models, when you get features, you still have to do auto Interp and things like that to actually get an understanding of what this concept is. But in image and video and world, its like extremely easy to grok what the concept is because you can see it and you can visualize it. And this makes the feedback. It makes the feedback cycle extremely fast for us and also for things like, I dont know, if you think about probes in language model context and then take it to world models, like, what if you wanted to detect harmful actors in world model scenes? Like, you cant actually, like, go and label all of that data feasibly, but maybe you could synthetically generate, you know, I dont know, world, like, harmful actor data using SAE feature activations or whatever, and then actually train a probe that was able to detect. That much more scalably. So I just think, like, video and image and world has always been something weve explored and are continuing to explore. Marks demo was probably the first moment we really, like, were like, oh, wow, like, this is really gonna, this could really, like, change the world. The steering demo? Yeah, no, the image demo. The diffusion one. Yeah, yeah, exactly. Yeah.Shawn Wang [00:54:18]: We should probably show that. And you demoed it at Worlds Fair, so we can link that.Myra Deng [00:54:23]: Nice, yeah. Yeah.Vibhu Sapra [00:54:24]: You can play with it, right? Yes. Yeah, its still up.Mark Bissell [00:54:26]: Paint.goodfair.ai. Yeah. Yeah.Shawn Wang [00:54:28]: I think for me, one way in which I think about world models is just like this, like, having this consistent model of the world where everything that you generate operates within the rules of that world. And imagine it would be a bigger deal for science or, like, math or anything that where, like, you have verifiable rules. Whereas, I guess, in natural language, maybe theres less rules. And so its not that important. Yeah.Mark Bissell [00:54:53]: And which makes the debugging of the models internal representations or its internal world model, to the extent you can make that legible and explicit and have control over that, I think it makes it all the more important. Because in language, its sort of a fuzzy enough domain that if its world model isnt fully like ours, it can still sort of, like, pass the Turing test, so to speak. But I know there have been papers that have looked at, like, even if you train certain astrophysics models, it does not learn. Like, the same way that you can, you know, have a model do well for modular arithmetic, but it doesnt really, like, learn how we think of modular arithmetic. It learns some crazy heuristic that is, like, essentially functionally equivalent. But its probably not the sort of Grok solution that you would hope for. Its how an alien would do it. Right. Right. Exactly.Shawn Wang [00:55:45]: But no, no, I think theres probably, I think, a function of our learning being bad rather than the, well, that approach probably not being. Because its how we humans learn. Yeah, right.Mark Bissell [00:55:56]: Well, its just, its the problem of induction, right? All of ML is based on induction. And its impossible to say, I have a physics model. You might have a physics model that works all the time, except when there is a character wearing a blue shirt and green shoes. And, like, you cant disprove that thats the case unless you test every particular situation your model might be in. Yeah. So we know that the laws of physics apply no matter. Where you are, what scenario it is. But from a models perspective, maybe something thats out of distribution. It just never needed to learn that the same laws of physics apply there. Yeah.Shawn Wang [00:56:30]: You were very excited because I read Ted Chiang over the holidays and I was very inspired by this short story called Understand, which apparently is, like, pretty old. You must be familiar with it. To me, it was like, its this fictional story. Its like the inverse of Flowers for Algernon, where you had someone, like, get really smart, but then also try to outsmart the tester. And the story just read, like, the chain of thought of a superintelligence, right? Where theyre like, oh, I realize Im being tested. Therefore, and then, okay, whats the consequence of being tested? Oh, theyre testing me. And if I score well, they will use me for things that I dont want to do. Therefore, I will score badly. And, like, but not too badly that they will raise alarms. So model sandbagging is a thing that people have explored. But I just think, like, Ted Chiangs work just in general seems to be something that inspires you. I just wanted to prompt you to talk about it.Mark Bissell [00:57:22]: I think, so Ted Chiang has two, is a sci-fi author who writes amazing short stories. His other claim to fame is Stories of Our Lives, which became the movie Arrival. Exactly, yeah. So two books of short stories that Im aware of. He also actually has a great just online blog post. I think hes the one who coined the term of LLMs as, like, a blurry JPEG of the internet. I should fact check that, but its a good post. But I think almost every one of his short stories has some lesson to bear. Im thinking about AI and thinking about AI research. So, you know, youve been talking about alien intelligence, right, in this AI human communication translation problem. Thats, you know, exactly sort of whats going on in Arrival and Story of Your Life. And just the fact that other beings will think and operate and communicate in ways that are not just challenging for us to understand, but just fundamentally different in ways that we might not even be able to expect. And then the one thats just. Super relevant for interpretability is the other short book of short stories he has is called Exhalation. And that is literally about a robot doing interpretability on its own mind. Oh, OK. So I just think that that, you know, you dont even have to squint to make the analogies there.Shawn Wang [00:58:41]: Well, I actually take Exhalation as a discussion about entropy and order. But yes, theres a scene in Exhalation where basically everyone is a robot. So they. The guy realizes he can set up a mirror to work on the back of his own head and then starts doing operations like that and looking in the mirror and doing this. Yeah.Mark Bissell [00:59:00]: And I think Ted Chiang has written about like the inspiration for that story. It was like half inspired by some of the things he had been doing on entropy. Theres apparently some other short story that is similar where a character goes to the doctor and opens up his chest and theres like a like a ticker tape going along. Its like he basically realizes hes like a Turing machine. And I dont know. I. Think especially as it comes to using agents for interp. That story always sticks in my mind.Myra Deng [00:59:27]: I find the brain surgery or like surgery analogies a little bit, a little bit morbid, but it is very apt. And when we talk to a lot of computational neuroscientists, they moved to interp because they were like, look, we have unfettered access to this artificial intelligent mind. Its so much. You have access to everything. You can run as many ablations experiments as you want. Its an. Amazing bed for science. And, you know, human brains, obviously, we cant just go and do whatever we want to them. And I think it is really just like a moment in time where we have intelligent systems that can really like do things better than humans in many ways. And its time, I think, for us to do the science on it.Shawn Wang [01:00:14]: Ill ask a brief like safety question. You know, McInturk was kind of born out of the alignment and safety conversation. Safety is on your website. Its not like something that you, you like de-prioritize, but like theres like a sort of very militant safety arm that like wants to blow up data centers and like stop AI and, and then theres this like sort of middle ground and like, is, is this like a conversation in your part of the world? Do you go up to Berkeley and Lighthaven and like talk to those guys or are they like, you know, theres like a brief like civil war going on or no?Myra Deng [01:00:45]: I think, I think a good amount of us have spent some time in Berkeley. And then there are researchers there that we really. Admire and respect. I think for us, its like, we have a very grounded view of alignment and, and safety in that we want to make sure that we can build models that do what we want them to do and that we have scalable oversight into what these models are doing. And we think that that is the key to a lot of these like technical alignment challenges. And I think that is our opinion. Thats our research direction. We of course are going to do. Safety related research to make sure that our techniques also work on, you know, things like reward hacking and, and other like more concrete safety issues that weve seen in the wild, but we want to be kind of like grounded in solving the technical challenges we see to having humans be humans play a big role in, in the deployment of, of these super intelligent agents of the future.Mark Bissell [01:01:47]: Yeah, Ive, Ive found the community to actually be remarkably cohesive, whether its. Talking about academia or the interpretability work being done at the frontier labs or some of the independent programs like maths and stuff. I think were all shooting for the same goal. I dont know that theres anyone who doesnt want our understanding of models to increase. I, I think everyone, regardless of where theyre coming from or the use cases that theyre thinking, whether its alignment as the premier thing theyre focused on or someone whos coming in purely from the angle of scientific discovery, I think we would all hope that models can be. More reliably and robustly controlled and understood. It seems like a pretty unambiguous goal.Shawn Wang [01:02:28]: Ill maybe phrase it in terms of like, theres maybe like a U curve of, of this, where like, if youre extremely doomer, you dont want any research whatsoever. If youre like mildly doomer, youre like, okay, theres this like high agency doomer is like, well, the default path is were all dead, but like we can do something about it. Whereas theres, theres other people who are like, no, just like, dont ever do anything. You know? Yeah.Vibhu Sapra [01:02:50]: Yeah. Theres also the other side, like there is the super alignment, like people that are like, okay, weak to strong generalization, were going to get there. Were going to have models smarter than us and use those to train even smarter models. How do we do that safely? Thats, you know, theres the camp there too. Thats trying to solve it, but yeah, theres, theres a lot of doomers too.Mark Bissell [01:03:12]: When I, and I think theres a lot to be learned from taking a very, um, like even regardless of the problem. That youre applying this to also just like the notion of like scalable oversight as a method of saying, lets take super intelligent or, or current frontier models and help use them to understand other models is another case where I think its just like a good lesson that everyone is aligned on of ideally you are setting up your research so that as super intelligence arrives, that is a tailwind. Thats also bolstering our ability to like understand the models. Cause otherwise youre fighting. Losing battle. If its like the systems are getting more and more capable and our methods are sort of linearly growing at like human pace. Yeah.Shawn Wang [01:03:58]: Yeah. Uh, Viva did call out something like, you know, I, I do think a consistent part of the Mac interp field is consistently strong to weak, meaning that we, we train weaker models to understand strong models, something like that. Um, or maybe I got it the other way around the other way. Weak. The other way around. Yeah. Yeah. The question that Ilya and Janlaika posed was, well, is that going to scale? Because eventually these are going to be. Stronger than us. Right. So I dont know if you have a perspective on that because I, that is something I still havent got over even after seeing that.Vibhu Sapra [01:04:27]: Theres a good paper from open AI, but its somewhat old. I think its like 23, 24. Its literally weak to strong generalization. Yeah. But the thing is that most of opening a high super alignment team has, theyre gone. Theyre gone.Mark Bissell [01:04:39]: But like, I think the idea, the idea is theres no more. Theyre so back.Shawn Wang [01:04:44]: think theres some new blog posts coming out. I know. I did just, you know, check the thinking machines, uh, website. Lets see whos back. Theres more kind of thing, you know, you dont want to be like, we too strong seemed like a very different direction. And when, when it first came out, I was like, oh my God, this is like, this is what we have to do. Uh, and like, it may be completely different than everything, all the techniques that we have today. Yeah.Mark Bissell [01:05:06]: My understanding of that is its, thats more like weak to strong when you, when you trust the weak model and youre uncertain whether you can trust the strong model thats, thats being developed. Im sort of speaking out of my depth on some of these topics. Yeah. But I think right now were in a regime where even the strong models we, uh, trust as reasonably aligned. And so they can be good co-scientists on a lot of the problems that weve been, weve been tackling, which is a nice, a nice state to be in. Hmm. Yeah.Shawn Wang [01:05:35]: Any last thoughts, close action?Mark Bissell [01:05:38]: I dont think so. As you mentioned, actively hiring MLEs, research scientists, um, you can check out the careers page at good fire. Um, where are you guys based?Myra Deng [01:05:47]: San Francisco. Were in, um, Levis Plaza. Like by court tower, thats where our office is. So come hang out. Um, were also looking for design partners across, um, people working in, in reasoning models, um, world models, robotics, and then also of course, people who are working on building super intelligent science models or looking at drug discovery or disease treatment. We would love to partner as well. Yeah.Shawn Wang [01:06:13]: Maybe the way Ill phrase it is like, you know, maybe you have a use case where LLMs are almost good enough, but you need one. Maybe you have a magical knob to tune so that it is good enough that you guys make the knob. Yeah.Mark Bissell [01:06:26]: Yeah. Or foundation models, uh, in, in other domains as well. The, the, some of those are the, um, especially opaque ones because you cant, you cant chat with them. So what do you, what do you do if you cant chat with them? Oh, well, like thinking about like a genomics model or material science model. So like, uh, yeah, they label a narrow foundation. Yeah. They predict.Shawn Wang [01:06:44]: Yeah. Got it. Good.Vibhu Sapra [01:06:45]: I was gonna say, I thought the diffusion work you guys did early was pretty, you know, pretty fun. Like you could see it directly. Applied to images, but we dont see as much interp in diffusion or images, right?Shawn Wang [01:06:55]: Like I see, you know, its gonna be huge. Like, look at this video models. Theyre so expensive to produce. And like, I mean, basically a mid journey S ref is kind of a feature, right? The what? Mid journey S ref. Oh, like the, the, the string of numbers. Right. Right. Right. Yeah. The style reference, I guess. Yeah.Mark Bissell [01:07:12]: No, I, I mean, I think were starting to see more of it and Ill say like the, the research preview of our diffusion model, kind of like a creative use case in the steering demo you saw. I, I think of those much more as, as, as demos than, um, a lot of the sort of core platform features that, that were working with partners are unfortunately sort of under NDA and less demoable, but I will, you know, hope that youre gonna see inter pervading a lot of what gets done, even if it is behind the scenes like that. So some of the, yeah, some of the public facing demos might not always be representative of like the, its, its just the tip of the iceberg, I guess, is one way to put it. Okay. Excellent. Thanks for coming on. Thanks for having us. Thanks for having us. This is a great time.</p>"
    },
    {
      "id": "f906446624b5",
      "title": "Waymo leverages Genie 3 to create a world model for self-driving cars",
      "content": "Google-spinoff Waymo is in the midst of expanding its self-driving car fleet into new regions. Waymo touts more than 200 million miles of driving that informs how the vehicles navigate roads, but the company's AI has also driven billions of miles virtually, and there's a lot more to come with the new Waymo World Model. Based on Google DeepMind's Genie 3, Waymo says the model can create \"hyper-realistic\" simulated environments that train the AI on situations that are rarely (or never) encountered in real lifelike snow on the Golden Gate Bridge.\nUntil recently, the autonomous driving industry relied entirely on training data collected from real cars and real situations. That means rare, potentially dangerous events are not well represented in training data. The Waymo World Model aims to address that by allowing engineers to create simulations with simple prompts and driving inputs.\nGoogle revealed Genie 3 last year, positioning it as a significant upgrade over other world models by virtue of its long-horizon memory. In Google's world model, you can wander away from a given object, and when you look back, the model will still \"remember\" how that object is supposed to look. In earlier attempts at world models, the simulation would lose that context almost immediately. With Genie 3, the model can remember details for several minutes.Read full article\nComments",
      "url": "https://arstechnica.com/google/2026/02/waymo-leverages-genie-3-to-create-a-world-model-for-self-driving-cars/",
      "author": "Ryan Whitwam",
      "published": "2026-02-06T20:44:35",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Cars",
        "Google",
        "google",
        "self-driving car",
        "waymo",
        "world models"
      ],
      "summary": "Waymo introduced the Waymo World Model, built on Google DeepMind's Genie 3, to generate hyper-realistic simulated driving environments for training autonomous vehicles. The model enables exposure to rare 'long-tail' events like snow on the Golden Gate Bridge that are nearly impossible to encounter in real-world training.",
      "importance_score": 82.0,
      "reasoning": "Application of frontier world models (Genie 3) to safety-critical autonomous driving represents a significant advancement in simulation-based training and could accelerate AV deployment in new environments.",
      "themes": [
        "world models",
        "autonomous driving",
        "Waymo",
        "Google DeepMind"
      ],
      "continuation": null,
      "summary_html": "<p>Waymo introduced the Waymo World Model, built on Google DeepMind's Genie 3, to generate hyper-realistic simulated driving environments for training autonomous vehicles. The model enables exposure to rare 'long-tail' events like snow on the Golden Gate Bridge that are nearly impossible to encounter in real-world training.</p>",
      "content_html": "<p>Google-spinoff Waymo is in the midst of expanding its self-driving car fleet into new regions. Waymo touts more than 200 million miles of driving that informs how the vehicles navigate roads, but the company's AI has also driven billions of miles virtually, and there's a lot more to come with the new Waymo World Model. Based on Google DeepMind's Genie 3, Waymo says the model can create \"hyper-realistic\" simulated environments that train the AI on situations that are rarely (or never) encountered in real lifelike snow on the Golden Gate Bridge.</p>\n<p>Until recently, the autonomous driving industry relied entirely on training data collected from real cars and real situations. That means rare, potentially dangerous events are not well represented in training data. The Waymo World Model aims to address that by allowing engineers to create simulations with simple prompts and driving inputs.</p>\n<p>Google revealed Genie 3 last year, positioning it as a significant upgrade over other world models by virtue of its long-horizon memory. In Google's world model, you can wander away from a given object, and when you look back, the model will still \"remember\" how that object is supposed to look. In earlier attempts at world models, the simulation would lose that context almost immediately. With Genie 3, the model can remember details for several minutes.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "6a484c5e9b4e",
      "title": "Waymo Introduces the Waymo World Model: A New Frontier Simulator Model for Autonomous Driving and Built on Top of Genie 3",
      "content": "Waymo is introducing the Waymo World Model, a frontier generative model that drives its next generation of autonomous driving simulation. The system is built on top of Genie 3, Google DeepMinds general-purpose world model, and adapts it to produce photorealistic, controllable, multi-sensor driving scenes at scale.\n\n\n\nWaymo already reports nearly 200 million fully autonomous miles on public roads. Behind the scenes, the Driver trains and is evaluated on billions of additional miles in virtual worlds. The Waymo World Model is now the main engine generating those worlds, with the explicit goal of exposing the stack to rare, safety-critical &#8216;long-tail&#8217; events that are almost impossible to see often enough in reality. \n\n\n\nFrom Genie 3 to a driving-specific world model\n\n\n\nGenie 3 is a general-purpose world model that turns text prompts into interactive environments you can navigate in real time at roughly 24 frames per second, typically at 720p resolution. It learns the dynamics of scenes directly from large video corpora and supports fluid control by user inputs.\n\n\n\nWaymo uses Genie 3 as the backbone and post-trains it for the driving domain. The Waymo World Model keeps Genie 3s ability to generate coherent 3D worlds, but aligns the outputs with Waymos sensor suite and operating constraints. It generates high-fidelity camera images and lidar point clouds that evolve consistently over time, matching how the Waymo Driver actually perceives the environment.\n\n\n\nThis is not just video rendering. The model produces multi-sensor, temporally consistent observations that downstream autonomous driving systems can consume under the same conditions as real-world logs.\n\n\n\nEmergent multimodal world knowledge\n\n\n\nMost AV simulators are trained only on on-road fleet data. That limits them to the weather, infrastructure, and traffic patterns a fleet actually encountered. Waymo instead leverages Genie 3s pre-training on an extremely large and diverse set of videos to import broad &#8216;world knowledge&#8217; into the simulator.\n\n\n\nWaymo then applies specialized post-training to transfer this knowledge from 2D video into 3D lidar outputs tailored to its hardware. Cameras provide rich appearance and lighting. Lidar contributes precise geometry and depth. The Waymo World Model jointly generates these modalities, so a simulated scene comes with both RGB streams and realistic 4D point clouds. \n\n\n\nBecause of the diversity of the pre-training data, the model can synthesize conditions that Waymos fleet has not directly seen. The Waymo team shows examples such as light snow on the Golden Gate Bridge, tornadoes, flooded cul-de-sacs, tropical streets strangely covered in snow, and driving out of a roadway fire. It also handles unusual objects and edge cases like elephants, Texas longhorns, lions, pedestrians dressed as T-rexes, and car-sized tumbleweed.\n\n\n\nThe important point is that these behaviors are emergent. The model is not explicitly programmed with rules for elephants or tornado fluid dynamics. Instead, it reuses generic spatiotemporal structure learned from videos and adapts it to driving scenes.\n\n\n\nThree axes of controllability\n\n\n\nA key design goal is strong simulation controllability. The Waymo World Model exposes three main control mechanisms: driving action control, scene layout control, and language control. \n\n\n\nDriving action control: The simulator responds to specific driving inputs, allowing &#8216;what if&#8217; counterfactuals on top of recorded logs. Devs can ask whether the Waymo Driver could have driven more assertively instead of yielding in a past scene, and then simulate that alternative behavior. Because the model is fully generative, it maintains realism even when the simulated route diverges far from the original trajectory, where purely reconstructive methods like 3D Gaussian Splatting (3DGS) would suffer from missing viewpoints. \n\n\n\nScene layout control: The model can be conditioned on modified road geometry, traffic signal states, and other road users. Waymo can insert or reposition vehicles and pedestrians or apply mutations to road layouts to synthesize targeted interaction scenarios. This supports systematic stress testing of yielding, merging, and negotiation behaviors beyond what appears in raw logs.\n\n\n\nLanguage control: Natural language prompts act as a flexible, high-level interface for editing time-of-day, weather, or even generating entirely synthetic scenes. The Waymo team demonstrates &#8216;World Mutation&#8217; sequences where the same base city scene is rendered at dawn, morning, noon, afternoon, evening, and night, and then under cloudy, foggy, rainy, snowy, and sunny conditions. \n\n\n\nThis tri-axis control is close to a structured API: numeric driving actions, structural layout edits, and semantic text prompts all steer the same underlying world model.\n\n\n\nTurning ordinary videos into multimodal simulations\n\n\n\nThe Waymo World Model can convert regular mobile or dashcam recordings into multimodal simulations that show how the Waymo Driver would perceive the same scene. \n\n\n\nWaymo showcases examples from scenic drives in Norway, Arches National Park, and Death Valley. Given only the video, the model reconstructs a simulation with aligned camera images and lidar output. This creates scenarios with strong realism and factuality because the generated world is anchored to actual footage, while still being controllable via the three mechanisms above. \n\n\n\nPractically, this means a large corpus of consumer-style video can be reused as structured simulation input without requiring lidar recordings in those locations.\n\n\n\nScalable inference and long rollouts\n\n\n\nLong-horizon maneuvers such as threading a narrow lane with oncoming traffic or navigating dense neighborhoods require many simulation steps. Naive generative models suffer from quality drift and high compute cost over long rollouts.\n\n\n\nWaymo team reports an efficient variant of the Waymo World Model that supports long sequences with a dramatic reduction in compute while maintaining realism. They show 4x-speed playback of extended scenes like freeway navigation around an in-lane stopper, busy neighborhood driving, climbing steep streets around motorcyclists, and handling SUV U-turns.\n\n\n\nFor training and regression testing, this reduces the hardware budget per scenario and makes large test suites more tractable.\n\n\n\nKey Takeaways\n\n\n\n\nGenie 3based world model: Waymo World Model adapts Google DeepMinds Genie 3 into a driving-specific world model that generates photorealistic, interactive, multi-sensor 3D environments for AV simulation.\n\n\n\nMulti-sensor, 4D outputs aligned with the Waymo Driver: The simulator jointly produces temporally consistent camera imagery and lidar point clouds, aligned with Waymos real sensor stack, so downstream autonomy systems can consume simulation like real logs.\n\n\n\nEmergent coverage of rare and long-tail scenarios: By leveraging large-scale video pre-training, the model can synthesize rare conditions and objects, such as snow on unusual roads, floods, fires, and animals like elephants or lions, that the fleet has never directly observed.\n\n\n\nTri-axis controllability for targeted stress testing: Driving action control, scene layout control, and language control let devs run counterfactuals, edit road geometry and traffic participants, and mutate time-of-day or weather via text prompts in the same generative environment.\n\n\n\nEfficient long-horizon and video-anchored simulation: An optimized variant supports long rollouts at reduced compute cost, and the system can also convert ordinary dashcam or mobile videos into controllable multimodal simulations, expanding the pool of realistic scenarios.\n\n\n\n\n\n\n\n\nCheck out theTechnical details.Also,feel free to follow us onTwitterand dont forget to join our100k+ ML SubRedditand Subscribe toour Newsletter. Wait! are you on telegram?now you can join us on telegram as well.\nThe post Waymo Introduces the Waymo World Model: A New Frontier Simulator Model for Autonomous Driving and Built on Top of Genie 3 appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/06/waymo-introduces-the-waymo-world-model-a-new-frontier-simulator-model-for-autonomous-driving-and-built-on-top-of-genie-3/",
      "author": "Michal Sutter",
      "published": "2026-02-06T19:01:39",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Computer Vision",
        "Editors Pick",
        "New Releases",
        "Physical AI",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Technical deep-dive on Waymo World Model revealing it generates photorealistic, controllable, multi-sensor driving scenes at scale. Waymo has logged nearly 200 million real autonomous miles but trains on billions of virtual miles using this new simulation engine.",
      "importance_score": 79.0,
      "reasoning": "Provides additional technical detail on the Genie 3 adaptation for autonomous driving, emphasizing the scale and photorealism of generated training environments for safety-critical edge cases.",
      "themes": [
        "world models",
        "simulation",
        "autonomous driving",
        "physical AI"
      ],
      "continuation": null,
      "summary_html": "<p>Technical deep-dive on Waymo World Model revealing it generates photorealistic, controllable, multi-sensor driving scenes at scale. Waymo has logged nearly 200 million real autonomous miles but trains on billions of virtual miles using this new simulation engine.</p>",
      "content_html": "<p>Waymo is introducing the Waymo World Model, a frontier generative model that drives its next generation of autonomous driving simulation. The system is built on top of Genie 3, Google DeepMinds general-purpose world model, and adapts it to produce photorealistic, controllable, multi-sensor driving scenes at scale.</p>\n<p>Waymo already reports nearly 200 million fully autonomous miles on public roads. Behind the scenes, the Driver trains and is evaluated on billions of additional miles in virtual worlds. The Waymo World Model is now the main engine generating those worlds, with the explicit goal of exposing the stack to rare, safety-critical long-tail events that are almost impossible to see often enough in reality.</p>\n<p>From Genie 3 to a driving-specific world model</p>\n<p>Genie 3 is a general-purpose world model that turns text prompts into interactive environments you can navigate in real time at roughly 24 frames per second, typically at 720p resolution. It learns the dynamics of scenes directly from large video corpora and supports fluid control by user inputs.</p>\n<p>Waymo uses Genie 3 as the backbone and post-trains it for the driving domain. The Waymo World Model keeps Genie 3s ability to generate coherent 3D worlds, but aligns the outputs with Waymos sensor suite and operating constraints. It generates high-fidelity camera images and lidar point clouds that evolve consistently over time, matching how the Waymo Driver actually perceives the environment.</p>\n<p>This is not just video rendering. The model produces multi-sensor, temporally consistent observations that downstream autonomous driving systems can consume under the same conditions as real-world logs.</p>\n<p>Emergent multimodal world knowledge</p>\n<p>Most AV simulators are trained only on on-road fleet data. That limits them to the weather, infrastructure, and traffic patterns a fleet actually encountered. Waymo instead leverages Genie 3s pre-training on an extremely large and diverse set of videos to import broad world knowledge into the simulator.</p>\n<p>Waymo then applies specialized post-training to transfer this knowledge from 2D video into 3D lidar outputs tailored to its hardware. Cameras provide rich appearance and lighting. Lidar contributes precise geometry and depth. The Waymo World Model jointly generates these modalities, so a simulated scene comes with both RGB streams and realistic 4D point clouds.</p>\n<p>Because of the diversity of the pre-training data, the model can synthesize conditions that Waymos fleet has not directly seen. The Waymo team shows examples such as light snow on the Golden Gate Bridge, tornadoes, flooded cul-de-sacs, tropical streets strangely covered in snow, and driving out of a roadway fire. It also handles unusual objects and edge cases like elephants, Texas longhorns, lions, pedestrians dressed as T-rexes, and car-sized tumbleweed.</p>\n<p>The important point is that these behaviors are emergent. The model is not explicitly programmed with rules for elephants or tornado fluid dynamics. Instead, it reuses generic spatiotemporal structure learned from videos and adapts it to driving scenes.</p>\n<p>Three axes of controllability</p>\n<p>A key design goal is strong simulation controllability. The Waymo World Model exposes three main control mechanisms: driving action control, scene layout control, and language control.</p>\n<p>Driving action control: The simulator responds to specific driving inputs, allowing what if counterfactuals on top of recorded logs. Devs can ask whether the Waymo Driver could have driven more assertively instead of yielding in a past scene, and then simulate that alternative behavior. Because the model is fully generative, it maintains realism even when the simulated route diverges far from the original trajectory, where purely reconstructive methods like 3D Gaussian Splatting (3DGS) would suffer from missing viewpoints.</p>\n<p>Scene layout control: The model can be conditioned on modified road geometry, traffic signal states, and other road users. Waymo can insert or reposition vehicles and pedestrians or apply mutations to road layouts to synthesize targeted interaction scenarios. This supports systematic stress testing of yielding, merging, and negotiation behaviors beyond what appears in raw logs.</p>\n<p>Language control: Natural language prompts act as a flexible, high-level interface for editing time-of-day, weather, or even generating entirely synthetic scenes. The Waymo team demonstrates World Mutation sequences where the same base city scene is rendered at dawn, morning, noon, afternoon, evening, and night, and then under cloudy, foggy, rainy, snowy, and sunny conditions.</p>\n<p>This tri-axis control is close to a structured API: numeric driving actions, structural layout edits, and semantic text prompts all steer the same underlying world model.</p>\n<p>Turning ordinary videos into multimodal simulations</p>\n<p>The Waymo World Model can convert regular mobile or dashcam recordings into multimodal simulations that show how the Waymo Driver would perceive the same scene.</p>\n<p>Waymo showcases examples from scenic drives in Norway, Arches National Park, and Death Valley. Given only the video, the model reconstructs a simulation with aligned camera images and lidar output. This creates scenarios with strong realism and factuality because the generated world is anchored to actual footage, while still being controllable via the three mechanisms above.</p>\n<p>Practically, this means a large corpus of consumer-style video can be reused as structured simulation input without requiring lidar recordings in those locations.</p>\n<p>Scalable inference and long rollouts</p>\n<p>Long-horizon maneuvers such as threading a narrow lane with oncoming traffic or navigating dense neighborhoods require many simulation steps. Naive generative models suffer from quality drift and high compute cost over long rollouts.</p>\n<p>Waymo team reports an efficient variant of the Waymo World Model that supports long sequences with a dramatic reduction in compute while maintaining realism. They show 4x-speed playback of extended scenes like freeway navigation around an in-lane stopper, busy neighborhood driving, climbing steep streets around motorcyclists, and handling SUV U-turns.</p>\n<p>For training and regression testing, this reduces the hardware budget per scenario and makes large test suites more tractable.</p>\n<p>Key Takeaways</p>\n<p>Genie 3based world model: Waymo World Model adapts Google DeepMinds Genie 3 into a driving-specific world model that generates photorealistic, interactive, multi-sensor 3D environments for AV simulation.</p>\n<p>Multi-sensor, 4D outputs aligned with the Waymo Driver: The simulator jointly produces temporally consistent camera imagery and lidar point clouds, aligned with Waymos real sensor stack, so downstream autonomy systems can consume simulation like real logs.</p>\n<p>Emergent coverage of rare and long-tail scenarios: By leveraging large-scale video pre-training, the model can synthesize rare conditions and objects, such as snow on unusual roads, floods, fires, and animals like elephants or lions, that the fleet has never directly observed.</p>\n<p>Tri-axis controllability for targeted stress testing: Driving action control, scene layout control, and language control let devs run counterfactuals, edit road geometry and traffic participants, and mutate time-of-day or weather via text prompts in the same generative environment.</p>\n<p>Efficient long-horizon and video-anchored simulation: An optimized variant supports long rollouts at reduced compute cost, and the system can also convert ordinary dashcam or mobile videos into controllable multimodal simulations, expanding the pool of realistic scenarios.</p>\n<p>Check out the&nbsp;Technical details.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and dont forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Waymo Introduces the Waymo World Model: A New Frontier Simulator Model for Autonomous Driving and Built on Top of Genie 3 appeared first on MarkTechPost.</p>"
    },
    {
      "id": "f8dcdf300f69",
      "title": "OpenAI's Latest Platform Targets Enterprise Customers",
      "content": "The company said it developed the platform to address a growing enterprise challenge: While individual AI agents improve efficiency, they need to work more effectively across the business.",
      "url": "https://aibusiness.com/agentic-ai/openai-s-latest-platform-targets-enterprise-customers",
      "author": "Graham Hope",
      "published": "2026-02-06T21:47:09",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "As first reported in [Social](/?date=2026-02-06&category=social#item-ab9b4e5700e9) yesterday, OpenAI announced its enterprise platform is designed to solve the challenge of making individual AI agents work effectively across entire businesses, not just isolated tasks. The focus is on orchestration and integration at scale.",
      "importance_score": 74.0,
      "reasoning": "Confirms OpenAI's strategic push into enterprise AI agent orchestration, though provides less detail than other coverage of the Frontier platform launch.",
      "themes": [
        "enterprise AI",
        "OpenAI",
        "agentic AI"
      ],
      "continuation": {
        "original_item_id": "ab9b4e5700e9",
        "original_date": "2026-02-06",
        "original_category": "social",
        "original_title": "The companies that succeed in the future are going to make very heavy use of AI. People will manage ...",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As first reported in **Social** yesterday"
      },
      "summary_html": "<p>As first reported in <a href=\"/?date=2026-02-06&amp;category=social#item-ab9b4e5700e9\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> yesterday, OpenAI announced its enterprise platform is designed to solve the challenge of making individual AI agents work effectively across entire businesses, not just isolated tasks. The focus is on orchestration and integration at scale.</p>",
      "content_html": "<p>The company said it developed the platform to address a growing enterprise challenge: While individual AI agents improve efficiency, they need to work more effectively across the business.</p>"
    },
    {
      "id": "c709b3ab4ea7",
      "title": "The Only Thing Standing Between Humanity and AI Apocalypse Is  Claude?",
      "content": "As AI systems grow more powerful, Anthropics resident philosopher says the startup is betting Claude itself can learn the wisdom needed to avoid disaster.",
      "url": "https://www.wired.com/story/the-only-thing-standing-between-humanity-and-ai-apocalypse-is-claude/",
      "author": "Steven Levy",
      "published": "2026-02-06T16:33:18",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Tech Culture",
        "Backchannel - NL",
        "artificial intelligence",
        "machine learning",
        "Anthropic",
        "Safety",
        "chatbots",
        "Backchannel"
      ],
      "summary": "Building on yesterday's [News](/?date=2026-02-05&category=news#item-45eba05a549c) coverage, Anthropic's resident philosopher discusses the company's strategy of betting that Claude itself can learn the wisdom needed to avoid AI disasters as systems grow more powerful. The piece explores Anthropic's safety-first approach to AI development.",
      "importance_score": 67.0,
      "reasoning": "Provides insight into Anthropic's AI safety philosophy from internal leadership, relevant given the company's growing influence, though more philosophical than news-breaking.",
      "themes": [
        "AI safety",
        "Anthropic",
        "alignment",
        "philosophy"
      ],
      "continuation": {
        "original_item_id": "45eba05a549c",
        "original_date": "2026-02-05",
        "original_category": "news",
        "original_title": "Should AI chatbots have ads? Anthropic says no.",
        "continuation_type": "new_development",
        "should_demote": false,
        "reference_text": "Building on yesterday's **News** coverage"
      },
      "summary_html": "<p>Building on yesterday's <a href=\"/?date=2026-02-05&amp;category=news#item-45eba05a549c\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage, Anthropic's resident philosopher discusses the company's strategy of betting that Claude itself can learn the wisdom needed to avoid AI disasters as systems grow more powerful. The piece explores Anthropic's safety-first approach to AI development.</p>",
      "content_html": "<p>As AI systems grow more powerful, Anthropics resident philosopher says the startup is betting Claude itself can learn the wisdom needed to avoid disaster.</p>"
    },
    {
      "id": "597c3cbb6a3f",
      "title": "How separating logic and search boosts AI agent scalability",
      "content": "Separating logic from inference improves AI agent scalability by decoupling core workflows from execution strategies.\n\n\n\nThe transition from generative AI prototypes to production-grade agents introduces a specific engineering hurdle: reliability. LLMs are stochastic by nature. A prompt that works once may fail on the second attempt. To mitigate this, development teams often wrap core business logic in complex error-handling loops, retries, and branching paths.\n\n\n\nThis approach creates a maintenance problem. The code defining what an agent should do becomes inextricably mixed with the code defining how to handle the model&#8217;s unpredictability. A new framework proposed by researchers from Asari AI, MIT CSAIL, and Caltech suggests a different architectural standard is required to scale agentic workflows in the enterprise.\n\n\n\nThe research introduces a programming model called Probabilistic Angelic Nondeterminism (PAN) and a Python implementation named ENCOMPASS. This method allows developers to write the &#8220;happy path&#8221; of an agent&#8217;s workflow while relegating inference-time strategies (e.g. beam search or backtracking) to a separate runtime engine. This separation of concerns offers a potential route to reduce technical debt while improving the performance of automated tasks.\n\n\n\nThe entanglement problem in agent design\n\n\n\nCurrent approaches to agent programming often conflate two distinct design aspects. The first is the core workflow logic, or the sequence of steps required to complete a business task. The second is the inference-time strategy, which dictates how the system navigates uncertainty, such as generating multiple drafts or verifying outputs against a rubric.\n\n\n\nWhen these are combined, the resulting codebase becomes brittle. Implementing a strategy like &#8220;best-of-N&#8221; sampling requires wrapping the entire agent function in a loop. Moving to a more complex strategy, such as tree search or refinement, typically requires a complete structural rewrite of the agent&#8217;s code.\n\n\n\nThe researchers argue that this entanglement limits experimentation. If a development team wants to switch from simple sampling to a beam search strategy to improve accuracy, they often must re-engineer the application&#8217;s control flow. This high cost of experimentation means teams frequently settle for suboptimal reliability strategies to avoid engineering overhead.\n\n\n\nDecoupling logic from search to boost AI agent scalability\n\n\n\nThe ENCOMPASS framework addresses this by allowing programmers to mark &#8220;locations of unreliability&#8221; within their code using a primitive called branchpoint().\n\n\n\nThese markers indicate where an LLM call occurs and where execution might diverge. The developer writes the code as if the operation will succeed. At runtime, the framework interprets these branch points to construct a search tree of possible execution paths.\n\n\n\nThis architecture enables what the authors term &#8220;program-in-control&#8221; agents. Unlike &#8220;LLM-in-control&#8221; systems, where the model decides the entire sequence of operations, program-in-control agents operate within a workflow defined by code. The LLM is invoked only to perform specific subtasks. This structure is generally preferred in enterprise environments for its higher predictability and auditability compared to fully autonomous agents.\n\n\n\nBy treating inference strategies as a search over execution paths, the framework allows developers to apply different algorithms  such as depth-first search, beam search, or Monte Carlo tree search  without altering the underlying business logic.\n\n\n\nImpact on legacy migration and code translation\n\n\n\nThe utility of this approach is evident in complex workflows such as legacy code migration. The researchers applied the framework to a Java-to-Python translation agent. The workflow involved translating a repository file-by-file, generating inputs, and validating the output through execution.\n\n\n\nIn a standard Python implementation, adding search logic to this workflow required defining a state machine. This process obscured the business logic and made the code difficult to read or lint. Implementing beam search required the programmer to break the workflow into individual steps and explicitly manage state across a dictionary of variables.\n\n\n\nUsing the proposed framework to boost AI agent scalability, the team implemented the same search strategies by inserting branchpoint() statements before LLM calls. The core logic remained linear and readable. The study found that applying beam search at both the file and method level outperformed simpler sampling strategies.\n\n\n\nThe data indicates that separating these concerns allows for better scaling laws. Performance improved linearly with the logarithm of the inference cost. The most effective strategy found  fine-grained beam search  was also the one that would have been most complex to implement using traditional coding methods.\n\n\n\nCost efficiency and performance scaling\n\n\n\nControlling the cost of inference is a primary concern for data officers managing P&amp;L for AI projects. The research demonstrates that sophisticated search algorithms can yield better results at a lower cost compared to simply increasing the number of feedback loops.\n\n\n\nIn a case study involving the &#8220;Reflexion&#8221; agent pattern (where an LLM critiques its own output) the researchers compared scaling the number of refinement loops against using a best-first search algorithm. The search-based approach achieved comparable performance to the standard refinement method but at a reduced cost per task.\n\n\n\nThis finding suggests that the choice of inference strategy is a factor for cost optimisation. By externalising this strategy, teams can tune the balance between compute budget and required accuracy without rewriting the application. A low-stakes internal tool might use a cheap and greedy search strategy, while a customer-facing application could use a more expensive and exhaustive search, all running on the same codebase.\n\n\n\nAdopting this architecture requires a change in how development teams view agent construction. The framework is designed to work in conjunction with existing libraries such as LangChain, rather than replacing them. It sits at a different layer of the stack, managing control flow rather than prompt engineering or tool interfaces.\n\n\n\nHowever, the approach is not without engineering challenges. The framework reduces the code required to implement search, but it does not automate the design of the agent itself. Engineers must still identify the correct locations for branch points and define verifiable success metrics.\n\n\n\nThe effectiveness of any search capability relies on the system&#8217;s ability to score a specific path. In the code translation example, the system could run unit tests to verify correctness. In more subjective domains, such as summarisation or creative generation, defining a reliable scoring function remains a bottleneck.\n\n\n\nFurthermore, the model relies on the ability to copy the program&#8217;s state at branching points. While the framework handles variable scoping and memory management, developers must ensure that external side effects  such as database writes or API calls  are managed correctly to prevent duplicate actions during the search process.\n\n\n\nImplications for AI agent scalability\n\n\n\nThe change represented by PAN and ENCOMPASS aligns with broader software engineering principles of modularity. As agentic workflows become core to operations, maintaining them will require the same rigour applied to traditional software.\n\n\n\nHard-coding probabilistic logic into business applications creates technical debt. It makes systems difficult to test, difficult to audit, and difficult to upgrade. Decoupling the inference strategy from the workflow logic allows for independent optimisation of both.\n\n\n\nThis separation also facilitates better governance. If a specific search strategy yields hallucinations or errors, it can be adjusted globally without assessing every individual agent&#8217;s codebase. It simplifies the versioning of AI behaviours, a requirement for regulated industries where the &#8220;how&#8221; of a decision is as important as the outcome.\n\n\n\nThe research indicates that as inference-time compute scales, the complexity of managing execution paths will increase. Enterprise architectures that isolate this complexity will likely prove more durable than those that permit it to permeate the application layer.\n\n\n\nSee also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post How separating logic and search boosts AI agent scalability appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/how-separating-logic-and-search-boosts-ai-agent-scalability/",
      "author": "Ryan Daws",
      "published": "2026-02-06T11:32:16",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Deep Dives",
        "Features",
        "How It Works",
        "Inside AI",
        "agentic ai",
        "agents",
        "enterprise",
        "scaling"
      ],
      "summary": "Researchers from Asari AI, MIT CSAIL, and Caltech propose a new architectural framework for AI agents that separates core business logic from error-handling and execution strategies. This addresses the reliability challenges of moving from prototypes to production-grade agents.",
      "importance_score": 65.0,
      "reasoning": "Academic-industry collaboration on fundamental agent architecture challenges is valuable for scaling agentic AI, though more research-oriented than immediate industry impact.",
      "themes": [
        "agentic AI",
        "research",
        "AI architecture",
        "scalability"
      ],
      "continuation": null,
      "summary_html": "<p>Researchers from Asari AI, MIT CSAIL, and Caltech propose a new architectural framework for AI agents that separates core business logic from error-handling and execution strategies. This addresses the reliability challenges of moving from prototypes to production-grade agents.</p>",
      "content_html": "<p>Separating logic from inference improves AI agent scalability by decoupling core workflows from execution strategies.</p>\n<p>The transition from generative AI prototypes to production-grade agents introduces a specific engineering hurdle: reliability. LLMs are stochastic by nature. A prompt that works once may fail on the second attempt. To mitigate this, development teams often wrap core business logic in complex error-handling loops, retries, and branching paths.</p>\n<p>This approach creates a maintenance problem. The code defining what an agent should do becomes inextricably mixed with the code defining how to handle the models unpredictability. A new framework proposed by researchers from Asari AI, MIT CSAIL, and Caltech suggests a different architectural standard is required to scale agentic workflows in the enterprise.</p>\n<p>The research introduces a programming model called Probabilistic Angelic Nondeterminism (PAN) and a Python implementation named ENCOMPASS. This method allows developers to write the happy path of an agents workflow while relegating inference-time strategies (e.g. beam search or backtracking) to a separate runtime engine. This separation of concerns offers a potential route to reduce technical debt while improving the performance of automated tasks.</p>\n<p>The entanglement problem in agent design</p>\n<p>Current approaches to agent programming often conflate two distinct design aspects. The first is the core workflow logic, or the sequence of steps required to complete a business task. The second is the inference-time strategy, which dictates how the system navigates uncertainty, such as generating multiple drafts or verifying outputs against a rubric.</p>\n<p>When these are combined, the resulting codebase becomes brittle. Implementing a strategy like best-of-N sampling requires wrapping the entire agent function in a loop. Moving to a more complex strategy, such as tree search or refinement, typically requires a complete structural rewrite of the agents code.</p>\n<p>The researchers argue that this entanglement limits experimentation. If a development team wants to switch from simple sampling to a beam search strategy to improve accuracy, they often must re-engineer the applications control flow. This high cost of experimentation means teams frequently settle for suboptimal reliability strategies to avoid engineering overhead.</p>\n<p>Decoupling logic from search to boost AI agent scalability</p>\n<p>The ENCOMPASS framework addresses this by allowing programmers to mark locations of unreliability within their code using a primitive called branchpoint().</p>\n<p>These markers indicate where an LLM call occurs and where execution might diverge. The developer writes the code as if the operation will succeed. At runtime, the framework interprets these branch points to construct a search tree of possible execution paths.</p>\n<p>This architecture enables what the authors term program-in-control agents. Unlike LLM-in-control systems, where the model decides the entire sequence of operations, program-in-control agents operate within a workflow defined by code. The LLM is invoked only to perform specific subtasks. This structure is generally preferred in enterprise environments for its higher predictability and auditability compared to fully autonomous agents.</p>\n<p>By treating inference strategies as a search over execution paths, the framework allows developers to apply different algorithms  such as depth-first search, beam search, or Monte Carlo tree search  without altering the underlying business logic.</p>\n<p>Impact on legacy migration and code translation</p>\n<p>The utility of this approach is evident in complex workflows such as legacy code migration. The researchers applied the framework to a Java-to-Python translation agent. The workflow involved translating a repository file-by-file, generating inputs, and validating the output through execution.</p>\n<p>In a standard Python implementation, adding search logic to this workflow required defining a state machine. This process obscured the business logic and made the code difficult to read or lint. Implementing beam search required the programmer to break the workflow into individual steps and explicitly manage state across a dictionary of variables.</p>\n<p>Using the proposed framework to boost AI agent scalability, the team implemented the same search strategies by inserting branchpoint() statements before LLM calls. The core logic remained linear and readable. The study found that applying beam search at both the file and method level outperformed simpler sampling strategies.</p>\n<p>The data indicates that separating these concerns allows for better scaling laws. Performance improved linearly with the logarithm of the inference cost. The most effective strategy found  fine-grained beam search  was also the one that would have been most complex to implement using traditional coding methods.</p>\n<p>Cost efficiency and performance scaling</p>\n<p>Controlling the cost of inference is a primary concern for data officers managing P&amp;L for AI projects. The research demonstrates that sophisticated search algorithms can yield better results at a lower cost compared to simply increasing the number of feedback loops.</p>\n<p>In a case study involving the Reflexion agent pattern (where an LLM critiques its own output) the researchers compared scaling the number of refinement loops against using a best-first search algorithm. The search-based approach achieved comparable performance to the standard refinement method but at a reduced cost per task.</p>\n<p>This finding suggests that the choice of inference strategy is a factor for cost optimisation. By externalising this strategy, teams can tune the balance between compute budget and required accuracy without rewriting the application. A low-stakes internal tool might use a cheap and greedy search strategy, while a customer-facing application could use a more expensive and exhaustive search, all running on the same codebase.</p>\n<p>Adopting this architecture requires a change in how development teams view agent construction. The framework is designed to work in conjunction with existing libraries such as LangChain, rather than replacing them. It sits at a different layer of the stack, managing control flow rather than prompt engineering or tool interfaces.</p>\n<p>However, the approach is not without engineering challenges. The framework reduces the code required to implement search, but it does not automate the design of the agent itself. Engineers must still identify the correct locations for branch points and define verifiable success metrics.</p>\n<p>The effectiveness of any search capability relies on the systems ability to score a specific path. In the code translation example, the system could run unit tests to verify correctness. In more subjective domains, such as summarisation or creative generation, defining a reliable scoring function remains a bottleneck.</p>\n<p>Furthermore, the model relies on the ability to copy the programs state at branching points. While the framework handles variable scoping and memory management, developers must ensure that external side effects  such as database writes or API calls  are managed correctly to prevent duplicate actions during the search process.</p>\n<p>Implications for AI agent scalability</p>\n<p>The change represented by PAN and ENCOMPASS aligns with broader software engineering principles of modularity. As agentic workflows become core to operations, maintaining them will require the same rigour applied to traditional software.</p>\n<p>Hard-coding probabilistic logic into business applications creates technical debt. It makes systems difficult to test, difficult to audit, and difficult to upgrade. Decoupling the inference strategy from the workflow logic allows for independent optimisation of both.</p>\n<p>This separation also facilitates better governance. If a specific search strategy yields hallucinations or errors, it can be adjusted globally without assessing every individual agents codebase. It simplifies the versioning of AI behaviours, a requirement for regulated industries where the how of a decision is as important as the outcome.</p>\n<p>The research indicates that as inference-time compute scales, the complexity of managing execution paths will increase. Enterprise architectures that isolate this complexity will likely prove more durable than those that permit it to permeate the application layer.</p>\n<p>See also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post How separating logic and search boosts AI agent scalability appeared first on AI News.</p>"
    },
    {
      "id": "2ff022750888",
      "title": "Deepfake fraud taking place on an industrial scale, study finds",
      "content": "AI content for scams can be targeted at individuals and produced by pretty much anybody, researchers sayDeepfake fraud has gone industrial, an analysis published by AI experts has said.Tools to create tailored, even personalised, scams  leveraging, for example, deepfake videos of Swedish journalists or the president of Cyprus  are no longer niche, but inexpensive and easy to deploy at scale, said the analysis from the AI Incident Database. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/06/deepfake-taking-place-on-an-industrial-scale-study-finds",
      "author": "Aisha Down",
      "published": "2026-02-06T08:00:02",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Deepfake",
        "Scams",
        "Technology",
        "AI (artificial intelligence)",
        "Consumer affairs",
        "Computing",
        "Crime",
        "US crime",
        "World news"
      ],
      "summary": "AI Incident Database analysis finds deepfake fraud has gone 'industrial' with tools for creating personalized scams now inexpensive and easy to deploy at scale. Examples include deepfake videos of Swedish journalists and the president of Cyprus.",
      "importance_score": 63.0,
      "reasoning": "Documents the scaling of AI-powered fraud as a significant societal concern, with concrete examples demonstrating how generative AI misuse has become commoditized.",
      "themes": [
        "deepfakes",
        "AI misuse",
        "security",
        "fraud"
      ],
      "continuation": null,
      "summary_html": "<p>AI Incident Database analysis finds deepfake fraud has gone 'industrial' with tools for creating personalized scams now inexpensive and easy to deploy at scale. Examples include deepfake videos of Swedish journalists and the president of Cyprus.</p>",
      "content_html": "<p>AI content for scams can be targeted at individuals and produced by pretty much anybody, researchers sayDeepfake fraud has gone industrial, an analysis published by AI experts has said.Tools to create tailored, even personalised, scams  leveraging, for example, deepfake videos of Swedish journalists or the president of Cyprus  are no longer niche, but inexpensive and easy to deploy at scale, said the analysis from the AI Incident Database. Continue reading...</p>"
    },
    {
      "id": "ea3ea855edba",
      "title": "Enterprises Don't Care About Anthropic's Super Bowl Ad",
      "content": "The Claude creator's commercial pokes fun at the ChatGPT maker, which will air its own ad. The TV spot amps up the war in the enterprise AI market.",
      "url": "https://aibusiness.com/generative-ai/enterprises-don-t-care-about-super-bowl-ad",
      "author": "Esther Shittu",
      "published": "2026-02-06T16:18:29",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Anthropic's Super Bowl ad pokes fun at OpenAI's ChatGPT, which will also air its own commercial. The TV battle reflects intensifying competition in the enterprise AI market.",
      "importance_score": 61.0,
      "reasoning": "Super Bowl advertising by AI companies indicates mainstream market push and competitive dynamics, though primarily a marketing story rather than technical advancement.",
      "themes": [
        "Anthropic",
        "OpenAI",
        "competition",
        "marketing"
      ],
      "continuation": null,
      "summary_html": "<p>Anthropic's Super Bowl ad pokes fun at OpenAI's ChatGPT, which will also air its own commercial. The TV battle reflects intensifying competition in the enterprise AI market.</p>",
      "content_html": "<p>The Claude creator's commercial pokes fun at the ChatGPT maker, which will air its own ad. The TV spot amps up the war in the enterprise AI market.</p>"
    },
    {
      "id": "adff438cd5ea",
      "title": "Lawyer sets new standard for abuse of AI; judge tosses case",
      "content": "Frustrated by fake citations and flowery prose packed with \"out-of-left-field\" references to ancient libraries and Ray Bradburys Fahrenheit 451, a New York federal judge took the rare step of terminating a case this week due to a lawyer's repeated misuse of AI when drafting filings.\nIn an order on Thursday, district judge Katherine Polk Failla ruled that the extraordinary sanctions were warranted after an attorney, Steven Feldman, kept responding to requests to correct his filings with documents containing fake citations.\nOne of those filings was \"noteworthy,\" Failla said, \"for its conspicuously florid prose.\" Where some of Feldman's filings contained grammatical errors and run-on sentences, this filing seemed glaringly different stylistically.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/02/randomly-quoting-ray-bradbury-did-not-save-lawyer-from-losing-case-over-ai-errors/",
      "author": "Ashley Belanger",
      "published": "2026-02-06T22:43:12",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Policy",
        "Artificial Intelligence",
        "fake citations",
        "hallucinations"
      ],
      "summary": "A New York federal judge terminated a case after attorney Steven Feldman repeatedly submitted filings with AI-generated fake citations despite multiple correction requests. The 'conspicuously florid prose' included references to Ray Bradbury and ancient libraries.",
      "importance_score": 58.0,
      "reasoning": "Sets significant legal precedent for consequences of AI misuse in professional contexts, though represents ongoing pattern of AI hallucination problems rather than new development.",
      "themes": [
        "AI misuse",
        "legal",
        "hallucinations",
        "policy"
      ],
      "continuation": null,
      "summary_html": "<p>A New York federal judge terminated a case after attorney Steven Feldman repeatedly submitted filings with AI-generated fake citations despite multiple correction requests. The 'conspicuously florid prose' included references to Ray Bradbury and ancient libraries.</p>",
      "content_html": "<p>Frustrated by fake citations and flowery prose packed with \"out-of-left-field\" references to ancient libraries and Ray Bradburys Fahrenheit 451, a New York federal judge took the rare step of terminating a case this week due to a lawyer's repeated misuse of AI when drafting filings.</p>\n<p>In an order on Thursday, district judge Katherine Polk Failla ruled that the extraordinary sanctions were warranted after an attorney, Steven Feldman, kept responding to requests to correct his filings with documents containing fake citations.</p>\n<p>One of those filings was \"noteworthy,\" Failla said, \"for its conspicuously florid prose.\" Where some of Feldman's filings contained grammatical errors and run-on sentences, this filing seemed glaringly different stylistically.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "f8c8cf4401b1",
      "title": "Why Darren Aronofsky thought an AI-generated historical docudrama was a good idea",
      "content": "Last week, filmmaker Darren Aronofsky's AI studio Primordial Soup and Time magazine released the first two episodes of On This Day... 1776. The year-long series of short-form videos features short vignettes describing what happened on that day of the American Revolution 250 years ago, but it does so using a variety of AI tools to produce photorealistic scenes containing avatars of historical figures like George Washington, Thomas Paine, and Benjamin Franklin.\nIn announcing the series, Time Studios President Ben Bitonti said the project provides \"a glimpse at what thoughtful, creative, artist-led use of AI can look likenot replacing craft but expanding whats possible and allowing storytellers to go places they simply couldnt before.\"\n\n    \n    \n      The trailer for \"On This Day... 1776.\"\n\n          \n  \n\nOutside critics were decidedly less excited about the effort. The AV Club took the introductory episodes to task for \"repetitive camera movements [and] waxen characters\" that make for \"an ugly look at American history.\" CNET said that this \"AI slop is ruining American history,\" calling the videos a \"hellish broth of machine-driven AI slop and bad human choices.\" The Guardian lamented that the \"once-lauded director of Black Swan and The Wrestler has drowned himself in AI slop,\" calling the series \"embarrassing,\" \"terrible,\" and \"ugly as sin.\" I could go on.Read full article\nComments",
      "url": "https://arstechnica.com/features/2026/02/why-darren-aronofsky-thought-an-ai-generated-historical-docudrama-was-a-good-idea/",
      "author": "Kyle Orland",
      "published": "2026-02-06T11:30:17",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Features",
        "Ai video",
        "darren aaronofsky",
        "deepmind",
        "google",
        "Hollywood",
        "movies",
        "primodial soup",
        "Veo"
      ],
      "summary": "Filmmaker Darren Aronofsky's AI studio Primordial Soup partnered with Time magazine to release 'On This Day... 1776,' a year-long series using AI tools to create photorealistic historical vignettes of the American Revolution.",
      "importance_score": 52.0,
      "reasoning": "Interesting creative application of AI video generation with notable names involved, but represents incremental use of existing tools rather than frontier AI development.",
      "themes": [
        "AI video",
        "creative AI",
        "media",
        "entertainment"
      ],
      "continuation": null,
      "summary_html": "<p>Filmmaker Darren Aronofsky's AI studio Primordial Soup partnered with Time magazine to release 'On This Day... 1776,' a year-long series using AI tools to create photorealistic historical vignettes of the American Revolution.</p>",
      "content_html": "<p>Last week, filmmaker Darren Aronofsky's AI studio Primordial Soup and Time magazine released the first two episodes of On This Day... 1776. The year-long series of short-form videos features short vignettes describing what happened on that day of the American Revolution 250 years ago, but it does so using a variety of AI tools to produce photorealistic scenes containing avatars of historical figures like George Washington, Thomas Paine, and Benjamin Franklin.</p>\n<p>In announcing the series, Time Studios President Ben Bitonti said the project provides \"a glimpse at what thoughtful, creative, artist-led use of AI can look likenot replacing craft but expanding whats possible and allowing storytellers to go places they simply couldnt before.\"</p>\n<p>The trailer for \"On This Day... 1776.\"</p>\n<p>Outside critics were decidedly less excited about the effort. The AV Club took the introductory episodes to task for \"repetitive camera movements [and] waxen characters\" that make for \"an ugly look at American history.\" CNET said that this \"AI slop is ruining American history,\" calling the videos a \"hellish broth of machine-driven AI slop and bad human choices.\" The Guardian lamented that the \"once-lauded director of Black Swan and The Wrestler has drowned himself in AI slop,\" calling the series \"embarrassing,\" \"terrible,\" and \"ugly as sin.\" I could go on.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "4a534a45c8a4",
      "title": "LWiAI Podcast #233 - Moltbot, Genie 3, Qwen3-Max-Thinking",
      "content": "Our 233rd episode with a summary and discussion of last week&#8217;s big AI news!Recorded on 01/30/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:Google introduces Gemini AI agent in Chrome for advanced browser functionality, including auto-browsing for pro and ultra subscribers.OpenAI releases ChatGPT Translator and Prism, expanding its applications beyond core business to language translation and scientific research assistance.Significant funding rounds and valuations achieved by startups Recursive and New Rofo, focusing on specialized AI chips and optical processors respectively.Political and social issues, including violence in Minnesota, prompt tech leaders in AI like Ade from Anthropic and Jeff Dean from Google to express concerns about the current administration&#8217;s actions.Timestamps:(00:00:10) Intro / BanterTools &amp; Apps(00:04:09) Google adds Gemini AI-powered &#8216;auto browse&#8217; to Chrome | The Verge(00:07:11) Users flock to open source Moltbot for always-on AI, despite major risks - Ars Technica(00:13:25) Google Brings Genie 3 &#8216;World Building&#8217; Experiment to AI Ultra Subscribers - CNET(00:16:17) OpenAI&#8217;s ChatGPT translator challenges Google Translate | The Verge(00:18:27) OpenAI launches Prism, a new AI workspace for scientists | TechCrunchApplications &amp; Business(00:19:49) Exclusive: China gives nod to ByteDance, Alibaba and Tencent to buy Nvidia&#8217;s H200 chips - sources | Reuters(00:22:55) AI chip startup Ricursive hits $4B valuation 2 months after launch(00:24:38) AI Startup Recursive in Funding Talks at $4 Billion Valuation - Bloomberg(00:27:30) Flapping Airplanes and the promise of research-driven AI | TechCrunch(00:31:54) From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing | TechCrunchProjects &amp; Open Source(00:35:34) Qwen3-Max-Thinking debuts with focus on hard math, code(00:38:26) China&#8217;s Moonshot releases a new open-source model Kimi K2.5 and a coding agent | TechCrunch(00:46:00) Ai2 launches family of open-source AI developer agents that adapt to any codebase - SiliconANGLE(00:47:46) Tiny startup Arcee AI built a 400B-parameter open source LLM from scratch to best Meta&#8217;s LlamaResearch &amp; Advancements(00:52:53) Post-LayerNorm Is Back: Stable, ExpressivE, and Deep(00:58:00) [2601.19897] Self-Distillation Enables Continual Learning(01:03:04) [2601.20802] Reinforcement Learning via Self-Distillation(01:05:58) Teaching Models to Teach Themselves: Reasoning at the Edge of LearnabilityPolicy &amp; Safety(01:09:13) Amodei, Hoffman Join Tech Workers Decrying Minnesota Violence - Bloomberg",
      "url": "https://lastweekin.ai/p/lwiai-podcast-233-moltbot-genie-3",
      "author": "Last Week in AI",
      "published": "2026-02-06T05:06:04",
      "source": "Last Week in AI",
      "source_type": "rss",
      "tags": [],
      "summary": "Weekly AI podcast covers multiple stories including Genie 3, Qwen3-Max-Thinking, Gemini AI agent in Chrome, ChatGPT Translator, and startup funding rounds for Recursive and New Rofo.",
      "importance_score": 48.0,
      "reasoning": "Aggregator content covering multiple news items without original reporting. Useful for breadth but doesn't provide depth on any single development.",
      "themes": [
        "news roundup",
        "multiple topics"
      ],
      "continuation": null,
      "summary_html": "<p>Weekly AI podcast covers multiple stories including Genie 3, Qwen3-Max-Thinking, Gemini AI agent in Chrome, ChatGPT Translator, and startup funding rounds for Recursive and New Rofo.</p>",
      "content_html": "<p>Our 233rd episode with a summary and discussion of last weeks big AI news!Recorded on 01/30/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:Google introduces Gemini AI agent in Chrome for advanced browser functionality, including auto-browsing for pro and ultra subscribers.OpenAI releases ChatGPT Translator and Prism, expanding its applications beyond core business to language translation and scientific research assistance.Significant funding rounds and valuations achieved by startups Recursive and New Rofo, focusing on specialized AI chips and optical processors respectively.Political and social issues, including violence in Minnesota, prompt tech leaders in AI like Ade from Anthropic and Jeff Dean from Google to express concerns about the current administrations actions.Timestamps:(00:00:10) Intro / BanterTools &amp; Apps(00:04:09) Google adds Gemini AI-powered auto browse to Chrome | The Verge(00:07:11) Users flock to open source Moltbot for always-on AI, despite major risks - Ars Technica(00:13:25) Google Brings Genie 3 World Building Experiment to AI Ultra Subscribers - CNET(00:16:17) OpenAIs ChatGPT translator challenges Google Translate | The Verge(00:18:27) OpenAI launches Prism, a new AI workspace for scientists | TechCrunchApplications &amp; Business(00:19:49) Exclusive: China gives nod to ByteDance, Alibaba and Tencent to buy Nvidias H200 chips - sources | Reuters(00:22:55) AI chip startup Ricursive hits $4B valuation 2 months after launch(00:24:38) AI Startup Recursive in Funding Talks at $4 Billion Valuation - Bloomberg(00:27:30) Flapping Airplanes and the promise of research-driven AI | TechCrunch(00:31:54) From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing | TechCrunchProjects &amp; Open Source(00:35:34) Qwen3-Max-Thinking debuts with focus on hard math, code(00:38:26) Chinas Moonshot releases a new open-source model Kimi K2.5 and a coding agent | TechCrunch(00:46:00) Ai2 launches family of open-source AI developer agents that adapt to any codebase - SiliconANGLE(00:47:46) Tiny startup Arcee AI built a 400B-parameter open source LLM from scratch to best Metas LlamaResearch &amp; Advancements(00:52:53) Post-LayerNorm Is Back: Stable, ExpressivE, and Deep(00:58:00) [2601.19897] Self-Distillation Enables Continual Learning(01:03:04) [2601.20802] Reinforcement Learning via Self-Distillation(01:05:58) Teaching Models to Teach Themselves: Reasoning at the Edge of LearnabilityPolicy &amp; Safety(01:09:13) Amodei, Hoffman Join Tech Workers Decrying Minnesota Violence - Bloomberg</p>"
    },
    {
      "id": "09d27cc1cf75",
      "title": "SuperCool review: Evaluating the reality of autonomous creation",
      "content": "In the current landscape of generative artificial intelligence, we have reached a saturation point with assistants. Most users are familiar with the routine. You prompt a tool, it provides a draft, and then you spend the next hour manually moving that output into another application for formatting, design, or distribution. AI promised to save time, yet the tool hop remains a bottleneck for founders and creative teams.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuperCool enters this crowded market with an importantly different value proposition. It does not want to be your assistant. It wants to be your execution partner. By positioning itself at the execution layer of creative projects, SuperCool aims to bridge the gap between a raw idea and a finished, downloadable asset without requiring the user to leave the platform.\n\n\n\n\n\n\n\nRedefining the creative workflow\n\n\n\nThe core philosophy behind SuperCool is to remove coordination overhead. For most businesses, creating a high-quality asset, whether it is a pitch deck, a marketing video, or a research report, requires a patchwork approach. You might use one AI for text, another for images, and a third for layout. SuperCool replaces this fragmented stack with a unified system of autonomous agents that work in concert.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs seen in the primary dashboard interface, the platform presents a clean, minimalist entry point. The user is greeted with a simple directive: &#8220;Give SuperCool a task to work on&#8221;. The simplicity belies the complexity occurring under the hood. Unlike traditional tools that require you to navigate menus and settings, the SuperCool experience is driven entirely by natural language prompts.\n\n\n\n\n\n\n\nHow the platform operates in practice\n\n\n\nThe workflow begins with a natural-language prompt that describes the desired outcome, the intended audience, and any specific constraints. One of the most impressive features observed during this review is the transparency of the agentic process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen a user submits a request, for instance, &#8220;create a pitch deck for my B2B business,&#8221; the platform does not just return a file a few minutes later. Instead, it breaks the project down into logical milestones that the user can monitor in real time.\n\n\n\n\nStrategic planning: The AI first outlines the project structure, like the presentation flow.\n\n\n\nAsset generation: It then generates relevant visuals and data visualisations tailored to the specific industry context.\n\n\n\nFinal assembly: The system designs the complete deck, ensuring cohesive styling and professional layouts.\n\n\n\n\n\n\n\n\nVisibility is crucial for trust. It allows the user to see that the AI is performing research and organising content not just hallucinating a generic response. The final result is a professional, multi-slide product, often featuring 10 or more professionally designed slides, delivered as an exportable file like a PPTX.\n\n\n\n\n\n\n\nVersatility across use cases\n\n\n\nSuperCool&#8217;s utility is most apparent in scenarios where speed and coverage are more valuable than pixel-perfect manual control. We observed three primary areas where the platform excels:\n\n\n\n\n\n\n\nEnd-to-end content creation\n\n\n\nFor consultants and solo founders, the time saved on administrative creative tasks is immense. A consultant onboarding a new client can describe the engagement and instantly receive a welcome packet, a process overview, and a timeline visual.\n\n\n\nMulti-format asset kits:\n\n\n\nPerhaps the most powerful feature is the ability to generate different types of media from a single prompt. An HR team launching an employee handbook can request a kit that includes a PDF guide, a short video, and a presentation deck.\n\n\n\nProduction without specialists:\n\n\n\nSmall teams often face a production gap where they lack the budget for full-time designers or video editors. SuperCool effectively fills this gap, allowing a two-person team to produce branded graphics and videos without expanding headcount.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the learning curve\n\n\n\nWhile the platform is designed for ease of use, it is not a magic wand for those without a clear vision. The quality of the output is heavily dependent on the clarity of the initial prompt. Vague instructions will lead to generic results. SuperCool is built for professionals who know what they want but do not want to spend hours manually building it.\n\n\n\nBecause the system is autonomous, users have less mid-stream control. You cannot tweak a design element while the agents are working. Instead, refinement happens through iteration in the chat interface. If the first version is not perfect, you provide feedback, and the system regenerates the asset with those adjustments in mind.\n\n\n\n\n\n\n\nThe competitive landscape: Assistant vs.agent\n\n\n\nIn the current AI ecosystem, most tools are categorised as assistants. They perform specific, isolated tasks, leaving the user responsible for overseeing the entire process. SuperCool represents the shift toward agentic AI, in which the system takes responsibility for the entire workflow.\n\n\n\nThe distinction is vital for enterprise contexts. While assistants require constant hand-holding, an agentic system like SuperCool allows the user to focus on high-level ideation and refinement. It moves the user from builder to director.\n\n\n\n\n\n\n\nFinal assessment\n\n\n\nSuperCool is a compelling alternative for those who find the current tool-stack approach a drain on productivity. It is not necessarily a replacement for specialised creative software when a brand needs unique, handcrafted artistry. However, for the vast majority of business needs, where speed, consistency, and execution are paramount, it offers perhaps the shortest path from an idea to a finished product.\n\n\n\nFor founders and creative teams who value the ability to rapidly test ideas and deploy content without the overhead of specialised software, SuperCool is a step forward in the evolution of autonomous work.\n\n\n\nImage source: Unsplash\n\n\n\n\nThe post SuperCool review: Evaluating the reality of autonomous creation appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/supercool-review-evaluating-the-reality-of-autonomous-creation/",
      "author": "AI News",
      "published": "2026-02-06T08:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence"
      ],
      "summary": "Review of SuperCool, an AI creative tool positioning itself as an 'execution partner' rather than assistant, aiming to bridge raw ideas to finished assets without requiring users to switch between applications.",
      "importance_score": 44.0,
      "reasoning": "Product review of a creative tool with no indication of breakthrough capabilities or significant market impact.",
      "themes": [
        "creative AI",
        "tools",
        "product review"
      ],
      "continuation": null,
      "summary_html": "<p>Review of SuperCool, an AI creative tool positioning itself as an 'execution partner' rather than assistant, aiming to bridge raw ideas to finished assets without requiring users to switch between applications.</p>",
      "content_html": "<p>In the current landscape of generative artificial intelligence, we have reached a saturation point with assistants. Most users are familiar with the routine. You prompt a tool, it provides a draft, and then you spend the next hour manually moving that output into another application for formatting, design, or distribution. AI promised to save time, yet the tool hop remains a bottleneck for founders and creative teams.</p>\n<p>SuperCool enters this crowded market with an importantly different value proposition. It does not want to be your assistant. It wants to be your execution partner. By positioning itself at the execution layer of creative projects, SuperCool aims to bridge the gap between a raw idea and a finished, downloadable asset without requiring the user to leave the platform.</p>\n<p>Redefining the creative workflow</p>\n<p>The core philosophy behind SuperCool is to remove coordination overhead. For most businesses, creating a high-quality asset, whether it is a pitch deck, a marketing video, or a research report, requires a patchwork approach. You might use one AI for text, another for images, and a third for layout. SuperCool replaces this fragmented stack with a unified system of autonomous agents that work in concert.</p>\n<p>As seen in the primary dashboard interface, the platform presents a clean, minimalist entry point. The user is greeted with a simple directive: Give SuperCool a task to work on. The simplicity belies the complexity occurring under the hood. Unlike traditional tools that require you to navigate menus and settings, the SuperCool experience is driven entirely by natural language prompts.</p>\n<p>How the platform operates in practice</p>\n<p>The workflow begins with a natural-language prompt that describes the desired outcome, the intended audience, and any specific constraints. One of the most impressive features observed during this review is the transparency of the agentic process.</p>\n<p>When a user submits a request, for instance, create a pitch deck for my B2B business, the platform does not just return a file a few minutes later. Instead, it breaks the project down into logical milestones that the user can monitor in real time.</p>\n<p>Strategic planning: The AI first outlines the project structure, like the presentation flow.</p>\n<p>Asset generation: It then generates relevant visuals and data visualisations tailored to the specific industry context.</p>\n<p>Final assembly: The system designs the complete deck, ensuring cohesive styling and professional layouts.</p>\n<p>Visibility is crucial for trust. It allows the user to see that the AI is performing research and organising content not just hallucinating a generic response. The final result is a professional, multi-slide product, often featuring 10 or more professionally designed slides, delivered as an exportable file like a PPTX.</p>\n<p>Versatility across use cases</p>\n<p>SuperCools utility is most apparent in scenarios where speed and coverage are more valuable than pixel-perfect manual control. We observed three primary areas where the platform excels:</p>\n<p>End-to-end content creation</p>\n<p>For consultants and solo founders, the time saved on administrative creative tasks is immense. A consultant onboarding a new client can describe the engagement and instantly receive a welcome packet, a process overview, and a timeline visual.</p>\n<p>Multi-format asset kits:</p>\n<p>Perhaps the most powerful feature is the ability to generate different types of media from a single prompt. An HR team launching an employee handbook can request a kit that includes a PDF guide, a short video, and a presentation deck.</p>\n<p>Production without specialists:</p>\n<p>Small teams often face a production gap where they lack the budget for full-time designers or video editors. SuperCool effectively fills this gap, allowing a two-person team to produce branded graphics and videos without expanding headcount.</p>\n<p>Navigating the learning curve</p>\n<p>While the platform is designed for ease of use, it is not a magic wand for those without a clear vision. The quality of the output is heavily dependent on the clarity of the initial prompt. Vague instructions will lead to generic results. SuperCool is built for professionals who know what they want but do not want to spend hours manually building it.</p>\n<p>Because the system is autonomous, users have less mid-stream control. You cannot tweak a design element while the agents are working. Instead, refinement happens through iteration in the chat interface. If the first version is not perfect, you provide feedback, and the system regenerates the asset with those adjustments in mind.</p>\n<p>The competitive landscape: Assistant vs.agent</p>\n<p>In the current AI ecosystem, most tools are categorised as assistants. They perform specific, isolated tasks, leaving the user responsible for overseeing the entire process. SuperCool represents the shift toward agentic AI, in which the system takes responsibility for the entire workflow.</p>\n<p>The distinction is vital for enterprise contexts. While assistants require constant hand-holding, an agentic system like SuperCool allows the user to focus on high-level ideation and refinement. It moves the user from builder to director.</p>\n<p>Final assessment</p>\n<p>SuperCool is a compelling alternative for those who find the current tool-stack approach a drain on productivity. It is not necessarily a replacement for specialised creative software when a brand needs unique, handcrafted artistry. However, for the vast majority of business needs, where speed, consistency, and execution are paramount, it offers perhaps the shortest path from an idea to a finished product.</p>\n<p>For founders and creative teams who value the ability to rapidly test ideas and deploy content without the overhead of specialised software, SuperCool is a step forward in the evolution of autonomous work.</p>\n<p>Image source: Unsplash</p>\n<p>The post SuperCool review: Evaluating the reality of autonomous creation appeared first on AI News.</p>"
    },
    {
      "id": "0a9eded05b29",
      "title": "Top 7 best AI penetration testing companies in 2026",
      "content": "Penetration testing has always existed to answer one practical concern: what actually happens when a motivated attacker targets a real system. For many years, that answer was produced through scoped engagements that reflected a relatively stable environment. Infrastructure changed slowly, access models were simpler, and most exposure could be traced back to application code or known vulnerabilities.\n\n\n\nThat operating reality does not exist. Modern environments are shaped by cloud services, identity platforms, APIs, SaaS integrations, and automation layers that evolve continuously. Exposure is introduced through configuration changes, permission drift, and workflow design as often as through code. As a result, security posture can shift materially without a single deployment.\n\n\n\nAttackers have adapted accordingly. Reconnaissance is automated. Exploitation attempts are opportunistic and persistent. Weak signals are correlated in systems and chained together until progression becomes possible. In this context, penetration testing that remains static, time-boxed, or narrowly scoped struggles to reflect real risk.\n\n\n\n\n\n\n\nHow AI penetration testing changes the role of offensive security\n\n\n\nTraditional penetration testing was designed to surface weaknesses during a defined engagement window. That model assumed environments remained relatively stable between tests. In cloud-native and identity-centric architectures, this assumption does not hold.\n\n\n\nAI penetration testing operates as a persistent control not a scheduled activity. Platforms reassess attack surfaces as infrastructure, permissions, and integrations change. This lets security teams detect newly introduced exposure without waiting for the next assessment cycle.\n\n\n\nAs a result, offensive security shifts from a reporting function into a validation mechanism that supports day-to-day risk management.\n\n\n\n\n\n\n\nThe top 7 best AI penetration testing companies\n\n\n\n1. Novee\n\n\n\nNovee is an AI-native penetration testing company focused on autonomous attacker simulation in modern enterprise environments. The platform is designed to continuously validate real attack paths and not produce static reports.\n\n\n\nNovee models the full attack lifecycle, including reconnaissance, exploit validation, lateral movement, and privilege escalation. Its AI agents adapt their behaviour based on environmental feedback, abandoning ineffective paths and prioritising those that lead to impact. This results in fewer findings with higher confidence.\n\n\n\nThe platform is particularly effective in cloud-native and identity-heavy environments where exposure changes frequently. Continuous reassessment ensures that risk is tracked as systems evolve, not frozen at the moment of a test.\n\n\n\nNovee is often used as a validation layer to support prioritisation and confirm that remediation efforts actually reduce exposure.\n\n\n\nKey characteristics:\n\n\n\n\nAutonomous attacker simulation with adaptive logic\n\n\n\nContinuous attack surface reassessment\n\n\n\nValidated attack-path discovery\n\n\n\nPrioritisation based on real progression\n\n\n\nRetesting to confirm remediation effectiveness\n\n\n\n\n\n\n\n\n2. Harmony Intelligence\n\n\n\nHarmony Intelligence focuses on AI-driven security testing with an emphasis on understanding how complex systems behave under adversarial conditions. The platform is designed to surface weaknesses that emerge from interactions between components not from isolated vulnerabilities.\n\n\n\nIts approach is particularly relevant for organisations running interconnected services and automated workflows. Harmony Intelligence evaluates how attackers could exploit logic gaps, misconfigurations, and trust relationships in systems.\n\n\n\nThe platform emphasises interpretability. Findings are presented in a way that explains why progression was possible, which helps teams understand and address root causes not symptoms.\n\n\n\nHarmony Intelligence is often adopted by organisations seeking deeper insight into systemic risk, not surface-level exposure.\n\n\n\nKey characteristics:\n\n\n\n\nAI-driven testing of complex system interactions\n\n\n\nFocus on logic and workflow exploitation\n\n\n\nClear contextual explanation of findings\n\n\n\nSupport for remediation prioritisation\n\n\n\nDesigned for interconnected enterprise environments\n\n\n\n\n\n\n\n\n3. RunSybil\n\n\n\nRunSybil is positioned around autonomous penetration testing with a strong emphasis on behavioural realism. The platform simulates how attackers operate over time, including persistence and adaptation.\n\n\n\nRather than executing predefined attack chains, RunSybil evaluates which actions produce meaningful access and adjusts accordingly. This makes it effective at identifying subtle paths that emerge from configuration drift or weak segmentation.\n\n\n\nRunSybil is frequently used in environments where traditional testing produces large volumes of low-value findings. Its validation-first approach helps teams focus on paths that represent genuine exposure.\n\n\n\nThe platform supports continuous execution and retesting, letting security teams measure improvement not rely on static assessments.\n\n\n\nKey characteristics:\n\n\n\n\nBehaviour-driven autonomous testing\n\n\n\nFocus on progression and persistence\n\n\n\nReduced noise through validation\n\n\n\nContinuous execution model\n\n\n\nMeasurement of remediation impact\n\n\n\n\n\n\n\n\n4. Mindgard\n\n\n\nMindgard specialises in adversarial testing of AI systems and AI-enabled workflows. Its platform evaluates how AI components behave under malicious or unexpected input, including manipulation, leakage, and unsafe decision paths.\n\n\n\nThe focus is increasingly important as AI becomes embedded in business-important processes. Failures often stem from logic and interaction effects, not traditional vulnerabilities.\n\n\n\nMindgard&#8217;s testing approach is proactive. It is designed to surface weaknesses before deployment and to support iterative improvement as systems evolve.\n\n\n\nOrganisations adopting Mindgard typically view AI as a distinct security surface that requires dedicated validation beyond infrastructure testing.\n\n\n\nKey characteristics:\n\n\n\n\nAdversarial testing of AI and ML systems\n\n\n\nFocus on logic, behaviour, and misuse\n\n\n\nPre-deployment and continuous testing support\n\n\n\nEngineering-actionable findings\n\n\n\nDesigned for AI-enabled workflows\n\n\n\n\n\n\n\n\n5. Mend\n\n\n\nMend approaches AI penetration testing from a broader application security perspective. The platform integrates testing, analysis, and remediation support in the software lifecycle.\n\n\n\nIts strength lies in correlating findings in code, dependencies, and runtime behaviour. This helps teams understand how vulnerabilities and misconfigurations interact, not treating them in isolation.\n\n\n\nMend is often used by organisations that want AI-assisted validation embedded into existing AppSec workflows. Its approach emphasises practicality and scalability over deep autonomous simulation.\n\n\n\nThe platform fits well in environments where development velocity is high and security controls must integrate seamlessly.\n\n\n\nKey characteristics:\n\n\n\n\nAI-assisted application security testing\n\n\n\nCorrelation in multiple risk sources\n\n\n\nIntegration with development workflows\n\n\n\nEmphasis on remediation efficiency\n\n\n\nScalable in large codebases\n\n\n\n\n\n\n\n\n6. Synack\n\n\n\nSynack combines human expertise with automation to deliver penetration testing at scale. Its model emphasises trusted researchers operating in controlled environments.\n\n\n\nWhile not purely autonomous, Synack incorporates AI and automation to manage scope, triage findings, and support continuous testing. The hybrid approach balances creativity with operational consistency.\n\n\n\nSynack is often chosen for high-risk systems where human judgement remains critical. Its platform supports ongoing testing not one-off engagements.\n\n\n\nThe combination of vetted talent and structured workflows makes Synack suitable for regulated and mission-important environments.\n\n\n\nKey characteristics:\n\n\n\n\nHybrid model combining humans and automation\n\n\n\nTrusted researcher network\n\n\n\nContinuous testing ability\n\n\n\nStrong governance and control\n\n\n\nSuitable for high-assurance environments\n\n\n\n\n\n\n\n\n7. HackerOne\n\n\n\nHackerOne is best known for its bug bounty platform, but it also plays a role in modern penetration testing strategies. Its strength lies in scale and diversity of attacker perspectives.\n\n\n\nThe platform lets organisations to continuously test systems through managed programmes with structured disclosure and remediation workflows. While not autonomous in the AI sense, HackerOne increasingly incorporates automation and analytics support prioritisation.\n\n\n\nHackerOne is often used with AI pentesting tools not as a replacement. It provides exposure to creative attack techniques that automated systems may not uncover.\n\n\n\nKey characteristics:\n\n\n\n\nLarge global researcher community\n\n\n\nContinuous testing through managed programmes\n\n\n\nStructured disclosure and remediation\n\n\n\nAutomation to support triage and prioritisation\n\n\n\nComplementary to AI-driven testing\n\n\n\n\n\n\n\n\nHow enterprises use AI penetration testing in practice\n\n\n\nAI penetration testing is most effective when used as part of a layered security strategy. It rarely replaces other controls outright. Instead, it fills a validation gap that scanners and preventive tools cannot address alone.\n\n\n\nA common enterprise pattern includes:\n\n\n\n\nVulnerability scanners for detection coverage\n\n\n\nPreventive controls for baseline hygiene\n\n\n\nAI penetration testing for continuous validation\n\n\n\nManual pentests for deep, creative exploration\n\n\n\n\n\n\n\n\nIn this model, AI pentesting serves as the connective tissue. It determines which detected issues matter in practice, validates remediation effectiveness, and highlights where assumptions break down.\n\n\n\nOrganisations adopting this approach often report clearer prioritisation, faster remediation cycles, and more meaningful security metrics.\n\n\n\n\n\n\n\nThe future of security teams with ai penetration testing\n\n\n\nThe impact of this new wave of offensive security has been transformative for the security workforce. Instead of being bogged down by repetitive vulnerability finding and retesting, security specialists can focus on incident response, proactive defense strategies, and risk mitigation. Developers get actionable reports and automated tickets, closing issues early and reducing burnout. Executives gain real-time assurance that risk is being managed every hour of every day.\n\n\n\nAI-powered pentesting, when operationalised well, fundamentally improves business agility, reduces breach risk, and helps organisations meet the demands of partners, customers, and regulators who are paying closer attention to security than ever before.\n\n\n\nImage source: Unsplash\nThe post Top 7 best AI penetration testing companies in 2026 appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/top-7-best-ai-penetration-testing-companies-in-2026/",
      "author": "Or Hillel",
      "published": "2026-02-06T08:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Sponsored Content"
      ],
      "summary": "Listicle covering top AI penetration testing companies in 2026, discussing how modern security environments require continuous testing due to cloud services, APIs, and configuration drift.",
      "importance_score": 35.0,
      "reasoning": "Sponsored content listicle with minimal news value; represents industry overview rather than specific developments.",
      "themes": [
        "security",
        "enterprise",
        "sponsored content"
      ],
      "continuation": null,
      "summary_html": "<p>Listicle covering top AI penetration testing companies in 2026, discussing how modern security environments require continuous testing due to cloud services, APIs, and configuration drift.</p>",
      "content_html": "<p>Penetration testing has always existed to answer one practical concern: what actually happens when a motivated attacker targets a real system. For many years, that answer was produced through scoped engagements that reflected a relatively stable environment. Infrastructure changed slowly, access models were simpler, and most exposure could be traced back to application code or known vulnerabilities.</p>\n<p>That operating reality does not exist. Modern environments are shaped by cloud services, identity platforms, APIs, SaaS integrations, and automation layers that evolve continuously. Exposure is introduced through configuration changes, permission drift, and workflow design as often as through code. As a result, security posture can shift materially without a single deployment.</p>\n<p>Attackers have adapted accordingly. Reconnaissance is automated. Exploitation attempts are opportunistic and persistent. Weak signals are correlated in systems and chained together until progression becomes possible. In this context, penetration testing that remains static, time-boxed, or narrowly scoped struggles to reflect real risk.</p>\n<p>How AI penetration testing changes the role of offensive security</p>\n<p>Traditional penetration testing was designed to surface weaknesses during a defined engagement window. That model assumed environments remained relatively stable between tests. In cloud-native and identity-centric architectures, this assumption does not hold.</p>\n<p>AI penetration testing operates as a persistent control not a scheduled activity. Platforms reassess attack surfaces as infrastructure, permissions, and integrations change. This lets security teams detect newly introduced exposure without waiting for the next assessment cycle.</p>\n<p>As a result, offensive security shifts from a reporting function into a validation mechanism that supports day-to-day risk management.</p>\n<p>The top 7 best AI penetration testing companies</p>\n<p>1. Novee</p>\n<p>Novee is an AI-native penetration testing company focused on autonomous attacker simulation in modern enterprise environments. The platform is designed to continuously validate real attack paths and not produce static reports.</p>\n<p>Novee models the full attack lifecycle, including reconnaissance, exploit validation, lateral movement, and privilege escalation. Its AI agents adapt their behaviour based on environmental feedback, abandoning ineffective paths and prioritising those that lead to impact. This results in fewer findings with higher confidence.</p>\n<p>The platform is particularly effective in cloud-native and identity-heavy environments where exposure changes frequently. Continuous reassessment ensures that risk is tracked as systems evolve, not frozen at the moment of a test.</p>\n<p>Novee is often used as a validation layer to support prioritisation and confirm that remediation efforts actually reduce exposure.</p>\n<p>Key characteristics:</p>\n<p>Autonomous attacker simulation with adaptive logic</p>\n<p>Continuous attack surface reassessment</p>\n<p>Validated attack-path discovery</p>\n<p>Prioritisation based on real progression</p>\n<p>Retesting to confirm remediation effectiveness</p>\n<p>2. Harmony Intelligence</p>\n<p>Harmony Intelligence focuses on AI-driven security testing with an emphasis on understanding how complex systems behave under adversarial conditions. The platform is designed to surface weaknesses that emerge from interactions between components not from isolated vulnerabilities.</p>\n<p>Its approach is particularly relevant for organisations running interconnected services and automated workflows. Harmony Intelligence evaluates how attackers could exploit logic gaps, misconfigurations, and trust relationships in systems.</p>\n<p>The platform emphasises interpretability. Findings are presented in a way that explains why progression was possible, which helps teams understand and address root causes not symptoms.</p>\n<p>Harmony Intelligence is often adopted by organisations seeking deeper insight into systemic risk, not surface-level exposure.</p>\n<p>Key characteristics:</p>\n<p>AI-driven testing of complex system interactions</p>\n<p>Focus on logic and workflow exploitation</p>\n<p>Clear contextual explanation of findings</p>\n<p>Support for remediation prioritisation</p>\n<p>Designed for interconnected enterprise environments</p>\n<p>3. RunSybil</p>\n<p>RunSybil is positioned around autonomous penetration testing with a strong emphasis on behavioural realism. The platform simulates how attackers operate over time, including persistence and adaptation.</p>\n<p>Rather than executing predefined attack chains, RunSybil evaluates which actions produce meaningful access and adjusts accordingly. This makes it effective at identifying subtle paths that emerge from configuration drift or weak segmentation.</p>\n<p>RunSybil is frequently used in environments where traditional testing produces large volumes of low-value findings. Its validation-first approach helps teams focus on paths that represent genuine exposure.</p>\n<p>The platform supports continuous execution and retesting, letting security teams measure improvement not rely on static assessments.</p>\n<p>Key characteristics:</p>\n<p>Behaviour-driven autonomous testing</p>\n<p>Focus on progression and persistence</p>\n<p>Reduced noise through validation</p>\n<p>Continuous execution model</p>\n<p>Measurement of remediation impact</p>\n<p>4. Mindgard</p>\n<p>Mindgard specialises in adversarial testing of AI systems and AI-enabled workflows. Its platform evaluates how AI components behave under malicious or unexpected input, including manipulation, leakage, and unsafe decision paths.</p>\n<p>The focus is increasingly important as AI becomes embedded in business-important processes. Failures often stem from logic and interaction effects, not traditional vulnerabilities.</p>\n<p>Mindgards testing approach is proactive. It is designed to surface weaknesses before deployment and to support iterative improvement as systems evolve.</p>\n<p>Organisations adopting Mindgard typically view AI as a distinct security surface that requires dedicated validation beyond infrastructure testing.</p>\n<p>Key characteristics:</p>\n<p>Adversarial testing of AI and ML systems</p>\n<p>Focus on logic, behaviour, and misuse</p>\n<p>Pre-deployment and continuous testing support</p>\n<p>Engineering-actionable findings</p>\n<p>Designed for AI-enabled workflows</p>\n<p>5. Mend</p>\n<p>Mend approaches AI penetration testing from a broader application security perspective. The platform integrates testing, analysis, and remediation support in the software lifecycle.</p>\n<p>Its strength lies in correlating findings in code, dependencies, and runtime behaviour. This helps teams understand how vulnerabilities and misconfigurations interact, not treating them in isolation.</p>\n<p>Mend is often used by organisations that want AI-assisted validation embedded into existing AppSec workflows. Its approach emphasises practicality and scalability over deep autonomous simulation.</p>\n<p>The platform fits well in environments where development velocity is high and security controls must integrate seamlessly.</p>\n<p>Key characteristics:</p>\n<p>AI-assisted application security testing</p>\n<p>Correlation in multiple risk sources</p>\n<p>Integration with development workflows</p>\n<p>Emphasis on remediation efficiency</p>\n<p>Scalable in large codebases</p>\n<p>6. Synack</p>\n<p>Synack combines human expertise with automation to deliver penetration testing at scale. Its model emphasises trusted researchers operating in controlled environments.</p>\n<p>While not purely autonomous, Synack incorporates AI and automation to manage scope, triage findings, and support continuous testing. The hybrid approach balances creativity with operational consistency.</p>\n<p>Synack is often chosen for high-risk systems where human judgement remains critical. Its platform supports ongoing testing not one-off engagements.</p>\n<p>The combination of vetted talent and structured workflows makes Synack suitable for regulated and mission-important environments.</p>\n<p>Key characteristics:</p>\n<p>Hybrid model combining humans and automation</p>\n<p>Trusted researcher network</p>\n<p>Continuous testing ability</p>\n<p>Strong governance and control</p>\n<p>Suitable for high-assurance environments</p>\n<p>7. HackerOne</p>\n<p>HackerOne is best known for its bug bounty platform, but it also plays a role in modern penetration testing strategies. Its strength lies in scale and diversity of attacker perspectives.</p>\n<p>The platform lets organisations to continuously test systems through managed programmes with structured disclosure and remediation workflows. While not autonomous in the AI sense, HackerOne increasingly incorporates automation and analytics support prioritisation.</p>\n<p>HackerOne is often used with AI pentesting tools not as a replacement. It provides exposure to creative attack techniques that automated systems may not uncover.</p>\n<p>Key characteristics:</p>\n<p>Large global researcher community</p>\n<p>Continuous testing through managed programmes</p>\n<p>Structured disclosure and remediation</p>\n<p>Automation to support triage and prioritisation</p>\n<p>Complementary to AI-driven testing</p>\n<p>How enterprises use AI penetration testing in practice</p>\n<p>AI penetration testing is most effective when used as part of a layered security strategy. It rarely replaces other controls outright. Instead, it fills a validation gap that scanners and preventive tools cannot address alone.</p>\n<p>A common enterprise pattern includes:</p>\n<p>Vulnerability scanners for detection coverage</p>\n<p>Preventive controls for baseline hygiene</p>\n<p>AI penetration testing for continuous validation</p>\n<p>Manual pentests for deep, creative exploration</p>\n<p>In this model, AI pentesting serves as the connective tissue. It determines which detected issues matter in practice, validates remediation effectiveness, and highlights where assumptions break down.</p>\n<p>Organisations adopting this approach often report clearer prioritisation, faster remediation cycles, and more meaningful security metrics.</p>\n<p>The future of security teams with ai penetration testing</p>\n<p>The impact of this new wave of offensive security has been transformative for the security workforce. Instead of being bogged down by repetitive vulnerability finding and retesting, security specialists can focus on incident response, proactive defense strategies, and risk mitigation. Developers get actionable reports and automated tickets, closing issues early and reducing burnout. Executives gain real-time assurance that risk is being managed every hour of every day.</p>\n<p>AI-powered pentesting, when operationalised well, fundamentally improves business agility, reduces breach risk, and helps organisations meet the demands of partners, customers, and regulators who are paying closer attention to security than ever before.</p>\n<p>Image source: Unsplash</p>\n<p>The post Top 7 best AI penetration testing companies in 2026 appeared first on AI News.</p>"
    }
  ]
}