{
  "category": "research",
  "date": "2026-02-07",
  "category_summary": "Interpretability research leads today's highlights. **Meta-Autointerp** [combines SAEs with LLM-summarizers](/?date=2026-02-07&category=research#item-07dc186e574c) to interpret multi-agent RL in Full-Press Diplomacy, while Steven Byrnes [offers a nuanced defense](/?date=2026-02-07&category=research#item-d240a241a553) of interpretability-in-the-loop training under specific conditions—challenging established orthodoxy.\n\n- Empirical LLM behavior study [finds prompt imperativeness dramatically reduces hedging](/?date=2026-02-07&category=research#item-72776ac41b7b) (**Cohen's d = 2.67**, n=900)\n- Dovetail Research [proves robust finite policies](/?date=2026-02-07&category=research#item-4b027ac6c827) must share non-trivial structural features using **deterministic finite automata**\n- Novel application of [**spectral graph metrics**](/?date=2026-02-07&category=research#item-9362d3a6c57e) (Fiedler vector, eigenvalue distribution) proposed for measuring gradual human disempowerment\n- Methodological critique [argues benchmark scores](/?date=2026-02-07&category=research#item-d077fc500204) lack natural units, making trend extrapolation fundamentally misleading\n\n**Claude Opus 4.6** coverage dominates capability updates, with observations [noting notably 'driven'](/?date=2026-02-07&category=research#item-7b04a1b42c12) goal-oriented behavior and new agent swarm capabilities. Philosophical work [explores why misaligned ASI](/?date=2026-02-07&category=research#item-6b90cc047167) might preserve humanity under multiple decision theories.",
  "category_summary_html": "<p>Interpretability research leads today's highlights. <strong>Meta-Autointerp</strong> <a href=\"/?date=2026-02-07&amp;category=research#item-07dc186e574c\" class=\"internal-link\" rel=\"noopener noreferrer\">combines SAEs with LLM-summarizers</a> to interpret multi-agent RL in Full-Press Diplomacy, while Steven Byrnes <a href=\"/?date=2026-02-07&amp;category=research#item-d240a241a553\" class=\"internal-link\" rel=\"noopener noreferrer\">offers a nuanced defense</a> of interpretability-in-the-loop training under specific conditions—challenging established orthodoxy.</p>\n<ul>\n<li>Empirical LLM behavior study <a href=\"/?date=2026-02-07&amp;category=research#item-72776ac41b7b\" class=\"internal-link\" rel=\"noopener noreferrer\">finds prompt imperativeness dramatically reduces hedging</a> (<strong>Cohen's d = 2.67</strong>, n=900)</li>\n<li>Dovetail Research <a href=\"/?date=2026-02-07&amp;category=research#item-4b027ac6c827\" class=\"internal-link\" rel=\"noopener noreferrer\">proves robust finite policies</a> must share non-trivial structural features using <strong>deterministic finite automata</strong></li>\n<li>Novel application of <a href=\"/?date=2026-02-07&amp;category=research#item-9362d3a6c57e\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>spectral graph metrics</strong></a> (Fiedler vector, eigenvalue distribution) proposed for measuring gradual human disempowerment</li>\n<li>Methodological critique <a href=\"/?date=2026-02-07&amp;category=research#item-d077fc500204\" class=\"internal-link\" rel=\"noopener noreferrer\">argues benchmark scores</a> lack natural units, making trend extrapolation fundamentally misleading</li>\n</ul>\n<p><strong>Claude Opus 4.6</strong> coverage dominates capability updates, with observations <a href=\"/?date=2026-02-07&amp;category=research#item-7b04a1b42c12\" class=\"internal-link\" rel=\"noopener noreferrer\">noting notably 'driven'</a> goal-oriented behavior and new agent swarm capabilities. Philosophical work <a href=\"/?date=2026-02-07&amp;category=research#item-6b90cc047167\" class=\"internal-link\" rel=\"noopener noreferrer\">explores why misaligned ASI</a> might preserve humanity under multiple decision theories.</p>",
  "themes": [
    {
      "name": "Interpretability",
      "description": "Methods for understanding AI system internals, including SAEs, Meta-Autointerp, and debates about training on interpretability signals",
      "item_count": 4,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "AI Safety Theory",
      "description": "Formal and philosophical approaches to alignment, including agent structure problem, decision theory, and disempowerment metrics",
      "item_count": 5,
      "example_items": [],
      "importance": 60
    },
    {
      "name": "LLM Behavior",
      "description": "Empirical studies of LLM response patterns, including hedging behavior and prompt sensitivity",
      "item_count": 2,
      "example_items": [],
      "importance": 55
    },
    {
      "name": "Capability Updates",
      "description": "Coverage of Claude Opus 4.6 release, agent swarms, and evolving AI coding assistant capabilities",
      "item_count": 3,
      "example_items": [],
      "importance": 50
    },
    {
      "name": "AI Governance",
      "description": "Debates about lab positioning, measurement of human disempowerment, and policy approaches",
      "item_count": 4,
      "example_items": [],
      "importance": 45
    }
  ],
  "total_items": 16,
  "items": [
    {
      "id": "07dc186e574c",
      "title": "Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning",
      "content": "TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputspaperAbstractLarge language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical toolkit for interpreting long-horizon multi-agent LLM training.Blog PostWe run Sparse Autoencoders on 114GB of Reinforcement Learning training trajectories from the popular multi-player strategy game Diplomacy, showing for the first time the potential downstream applications of data-centric interpretability techniquesWhat are the AIs doing when &nbsp;no one is watching? Current large-scale training runs can produce hundreds of millions or billions of tokens, with production AI deployments in the trillions. Human oversight of all AI outputs is becoming increasingly unfeasible. Common approaches to solving this problem include summarizing the logs, or using LLM as a judge with rubrics. The problem is these approaches are expensive, prone to hallucination, and can only attend to a small set of features you already know how to look for.&nbsp;In our pape...",
      "url": "https://www.lesswrong.com/posts/dTfpSfTfYs7qg4MFi/data-centric-interpretability-for-llm-based-multi-agent",
      "author": "michaelwaves",
      "published": "2026-02-06T14:27:09.028000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Introduces Meta-Autointerp, a method combining SAEs and LLM-summarizers to interpret multi-agent RL training runs in Full-Press Diplomacy. The framework discovers fine-grained behaviors (role-playing, degenerate outputs, language switching) and achieves +14.2% performance improvement when discovered features are added to system prompts.",
      "importance_score": 72,
      "reasoning": "Novel interpretability method with concrete empirical results. Addresses important problem of understanding long-horizon multi-agent LLM training. Validation through human studies and performance improvements adds credibility. Directly relevant to scalable oversight research.",
      "themes": [
        "Interpretability",
        "Multi-Agent Systems",
        "Reinforcement Learning",
        "Scalable Oversight"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Meta-Autointerp, a method combining SAEs and LLM-summarizers to interpret multi-agent RL training runs in Full-Press Diplomacy. The framework discovers fine-grained behaviors (role-playing, degenerate outputs, language switching) and achieves +14.2% performance improvement when discovered features are added to system prompts.</p>",
      "content_html": "<p>TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputspaperAbstractLarge language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical toolkit for interpreting long-horizon multi-agent LLM training.Blog PostWe run Sparse Autoencoders on 114GB of Reinforcement Learning training trajectories from the popular multi-player strategy game Diplomacy, showing for the first time the potential downstream applications of data-centric interpretability techniquesWhat are the AIs doing when &nbsp;no one is watching? Current large-scale training runs can produce hundreds of millions or billions of tokens, with production AI deployments in the trillions. Human oversight of all AI outputs is becoming increasingly unfeasible. Common approaches to solving this problem include summarizing the logs, or using LLM as a judge with rubrics. The problem is these approaches are expensive, prone to hallucination, and can only attend to a small set of features you already know how to look for.&nbsp;In our pape...</p>"
    },
    {
      "id": "d240a241a553",
      "title": "In (highly contingent!) defense of interpretability-in-the-loop ML training",
      "content": "Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function.Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&nbsp;Yudkowsky 2022:When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.&nbsp; Optimizing against an interpreted thought optimizes against interpretability.Or&nbsp;Zvi 2025:The Most Forbidden Technique is training an AI using interpretability techniques.An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.You train on [X]. Only [X]. Never [M], never [T].Why? Because [T] is how you figure out when the model is misbehaving.If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.Those bits of optimization pressure from [T] are precious. Use them wisely.This is a simple argument, and I think it’s 100% right.But…Consider compassion in the human brain. I claim that we have an innate reward function that triggers not just when I see that my friend is happy or suffering, but also when I&nbsp;believe that my friend is happy or suffering, even if the friend is far away. So the human brain reward can evidently get triggered by specific activations inside my inscrutable learned world-model.Thus, I claim that the human brain incorporates a form of interpretability-in-the-loop RL training.Inspired by that example, I have long been an advocate for studying whether and how one might use interpretability-in-the-loop training for aligned AGI. See for example&nbsp;Rewar...",
      "url": "https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop",
      "author": "Steven Byrnes",
      "published": "2026-02-06T11:32:27.761000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Steven Byrnes argues that interpretability-in-the-loop training (using interpretability in loss functions) might be acceptable under specific conditions, despite standard warnings from Yudkowsky and Zvi that this trains models to obfuscate their thinking. Proposes contingent scenarios where benefits might outweigh risks.",
      "importance_score": 68,
      "reasoning": "Important nuanced contribution to a central AI safety debate. Challenges established orthodoxy with careful reasoning. From a credible author. Directly relevant to current industry practices (see Goodfire discussion in same batch).",
      "themes": [
        "AI Safety",
        "Interpretability",
        "Alignment Training"
      ],
      "continuation": null,
      "summary_html": "<p>Steven Byrnes argues that interpretability-in-the-loop training (using interpretability in loss functions) might be acceptable under specific conditions, despite standard warnings from Yudkowsky and Zvi that this trains models to obfuscate their thinking. Proposes contingent scenarios where benefits might outweigh risks.</p>",
      "content_html": "<p>Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function.Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&nbsp;Yudkowsky 2022:When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.&nbsp; Optimizing against an interpreted thought optimizes against interpretability.Or&nbsp;Zvi 2025:The Most Forbidden Technique is training an AI using interpretability techniques.An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.You train on [X]. Only [X]. Never [M], never [T].Why? Because [T] is how you figure out when the model is misbehaving.If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.Those bits of optimization pressure from [T] are precious. Use them wisely.This is a simple argument, and I think it’s 100% right.But…Consider compassion in the human brain. I claim that we have an innate reward function that triggers not just when I see that my friend is happy or suffering, but also when I&nbsp;believe that my friend is happy or suffering, even if the friend is far away. So the human brain reward can evidently get triggered by specific activations inside my inscrutable learned world-model.Thus, I claim that the human brain incorporates a form of interpretability-in-the-loop RL training.Inspired by that example, I have long been an advocate for studying whether and how one might use interpretability-in-the-loop training for aligned AGI. See for example&nbsp;Rewar...</p>"
    },
    {
      "id": "72776ac41b7b",
      "title": "Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMs (n=900, Cohen's d = 2.67)",
      "content": "ForewordIf you've ever noticed LLMs hedging considerably more when you ask them subjective questions, it's not a fluke. I ran a 3x2x3 factorial experiment (n=900) to quantify how much prompt phrasing (alongside question type and model type) shifts hedging across differing imperativeness levels. The effect sizes were larger than I expected.To nobody's surprise, Claude hedged the most (by a fairly wide margin). It also decided to meta-analyze its own response then critiqued its own compliance in answering it.I'm a high school freshman and got paired with a mentor through the Lumiere program. Feedback very welcome (this is my first paper).&nbsp;&nbsp;Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMsFebruary 2026AbstractWe demonstrate that large language models (LLMs) hedge (using uncertain language when responding to queries) frequently when responding to prompts, reducing trust and delaying decision making. We investigated whether prompt imperativeness (how urgent a prompt is phrased) affects this behavior using a 3×2×3 factorial design across three differing imperativeness levels, two question types (subjective/objective), and three models (GPT-4o-mini, Claude Haiku 4.5, Gemini 2.5 Flash), with a combined total of&nbsp;n = 900. Imperative prompts significantly reduced hedging (F(2, 882) = 361.72,&nbsp;p &lt; .001, η2p = .451) with the largest effects visible on subjective questions (M = 2.38 to&nbsp;M = 0.43, Cohen’s&nbsp;d = 2.67). We observed that objective questions demonstrated a floor effect regardless of framing due to their epistemic certainty. Importantly, all three models converged to low hedging scores under high imperativeness conditions despite differing baselines. These findings suggest hedging is a controllable parameter that changes with prompt framing, with implications for deployment, user trust, and benchmark standardization.1&nbsp; IntroductionSince the introduction of ChatGPT in November of 2022, large language mo...",
      "url": "https://www.lesswrong.com/posts/vBDupg8iPqgdwhFzz/demands-are-all-you-need-prompt-imperativeness-drastically",
      "author": "fluxxrider",
      "published": "2026-02-06T08:22:47.550000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Factorial experiment (n=900) demonstrating that prompt imperativeness dramatically reduces LLM hedging behavior (Cohen's d = 2.67). Tests across three imperativeness levels, two question types, and three models. Claude hedges most; demanding prompts substantially reduce uncertain language.",
      "importance_score": 65,
      "reasoning": "Well-designed empirical study with large effect size. Clear methodology and actionable findings for prompt engineering. Impressive for a high school student's first paper. Results have practical implications for LLM usage patterns.",
      "themes": [
        "Prompt Engineering",
        "LLM Behavior",
        "Empirical Research"
      ],
      "continuation": null,
      "summary_html": "<p>Factorial experiment (n=900) demonstrating that prompt imperativeness dramatically reduces LLM hedging behavior (Cohen's d = 2.67). Tests across three imperativeness levels, two question types, and three models. Claude hedges most; demanding prompts substantially reduce uncertain language.</p>",
      "content_html": "<p>ForewordIf you've ever noticed LLMs hedging considerably more when you ask them subjective questions, it's not a fluke. I ran a 3x2x3 factorial experiment (n=900) to quantify how much prompt phrasing (alongside question type and model type) shifts hedging across differing imperativeness levels. The effect sizes were larger than I expected.To nobody's surprise, Claude hedged the most (by a fairly wide margin). It also decided to meta-analyze its own response then critiqued its own compliance in answering it.I'm a high school freshman and got paired with a mentor through the Lumiere program. Feedback very welcome (this is my first paper).&nbsp;&nbsp;Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMsFebruary 2026AbstractWe demonstrate that large language models (LLMs) hedge (using uncertain language when responding to queries) frequently when responding to prompts, reducing trust and delaying decision making. We investigated whether prompt imperativeness (how urgent a prompt is phrased) affects this behavior using a 3×2×3 factorial design across three differing imperativeness levels, two question types (subjective/objective), and three models (GPT-4o-mini, Claude Haiku 4.5, Gemini 2.5 Flash), with a combined total of&nbsp;n = 900. Imperative prompts significantly reduced hedging (F(2, 882) = 361.72,&nbsp;p &lt; .001, η2p = .451) with the largest effects visible on subjective questions (M = 2.38 to&nbsp;M = 0.43, Cohen’s&nbsp;d = 2.67). We observed that objective questions demonstrated a floor effect regardless of framing due to their epistemic certainty. Importantly, all three models converged to low hedging scores under high imperativeness conditions despite differing baselines. These findings suggest hedging is a controllable parameter that changes with prompt framing, with implications for deployment, user trust, and benchmark standardization.1&nbsp; IntroductionSince the introduction of ChatGPT in November of 2022, large language mo...</p>"
    },
    {
      "id": "4b027ac6c827",
      "title": "Robust Finite Policies are Nontrivially Structured",
      "content": "This post was created during the Dovetail Research Fellowship. Thanks to Alex, Alfred, &nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions.OverviewThe proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem, which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone.For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature.We begin by defining every part of the framework. Then, we find an upper bound on the robustness of a class of “unstructured” policies. Finally, we show that the automata of policies which are more robust than this bound must have similar structure.DefinitionsThe framework in this post is inspired by the General Agents paper by Richens et al. and Towards a formalization of the agent structure problem by Alex Altair.EnvironmentIn the General Agents paper, the environment was stated to be a controlled Markov Decision Process (cMDP), \"which is a Markov decision process without a specified reward function or discount factor.\"Here, in order to talk about a policy's performance as the environment gets larger, we take the environment to be an increasing sequence of cMDPs:E0⊂E1⊂E2..mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1e...",
      "url": "https://www.lesswrong.com/posts/ieX8nK2b2i4JDRH5s/robust-finite-policies-are-nontrivially-structured",
      "author": "Winter Cross",
      "published": "2026-02-06T12:47:51.691000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Formal proof that policies meeting certain robustness criteria must share structural features, modeled using deterministic finite automata. Addresses the 'agent structure problem' - whether agent-like behavior implies agent-like internal structure.",
      "importance_score": 62,
      "reasoning": "Rigorous theoretical alignment work from Dovetail Research Fellowship. Makes concrete progress on formalizing agent structure, an important foundational question. Mathematical approach with clear definitions, though narrow scope limits immediate practical impact.",
      "themes": [
        "Agent Foundations",
        "AI Safety Theory",
        "Formal Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Formal proof that policies meeting certain robustness criteria must share structural features, modeled using deterministic finite automata. Addresses the 'agent structure problem' - whether agent-like behavior implies agent-like internal structure.</p>",
      "content_html": "<p>This post was created during the Dovetail Research Fellowship. Thanks to Alex, Alfred, &nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions.OverviewThe proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem, which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone.For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature.We begin by defining every part of the framework. Then, we find an upper bound on the robustness of a class of “unstructured” policies. Finally, we show that the automata of policies which are more robust than this bound must have similar structure.DefinitionsThe framework in this post is inspired by the General Agents paper by Richens et al. and Towards a formalization of the agent structure problem by Alex Altair.EnvironmentIn the General Agents paper, the environment was stated to be a controlled Markov Decision Process (cMDP), \"which is a Markov decision process without a specified reward function or discount factor.\"Here, in order to talk about a policy's performance as the environment gets larger, we take the environment to be an increasing sequence of cMDPs:E0⊂E1⊂E2..mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1e...</p>"
    },
    {
      "id": "9362d3a6c57e",
      "title": "Spectral Signatures of Gradual Disempowerment",
      "content": "TL;DRAI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance.IntroductionAI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases.The stubborn part is that it operates across institutional boundaries simultaneously. Regulate algorithmic trading to maintain human oversight of markets, and competitive pressure shifts to network dynamics — whoever shapes information flow shapes what traders believe before they trade. Address attention capture in social networks, and the pressure migrates to governance advisory relationships. The problem flows around single-domain interventions like water finding cracks.Yet our analytical tools respect exactly those domain boundaries. Economists model markets with one formalism. Network scientists study information diffusion with another. Political scientists analyze voting with a third. Each captures something real. None can describe what happens when AI systems alter the dynamics across all three simultaneously.We think markets, networks, and democratic systems are structurally more similar than they appear. They can all be described as message-passing protocols on graph structures — nodes are participating agents, edges are channels through which influence flows, and what var...",
      "url": "https://www.lesswrong.com/posts/erTAgDriWAw3evecP/spectral-signatures-of-gradual-disempowerment",
      "author": "Jonas Hallgren",
      "published": "2026-02-06T10:08:08.545000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes using spectral graph metrics (spectral gap, Fiedler vector, eigenvalue distribution) to measure gradual human disempowerment as AI systems enter markets, networks, and governance. Identifies three specific quantities for AI governance monitoring.",
      "importance_score": 58,
      "reasoning": "Novel technical approach to a difficult-to-measure AI governance concern. Cross-domain applicability is valuable. However, the metrics remain theoretical without validation, and it's unclear how tractable measurement would be in practice.",
      "themes": [
        "AI Governance",
        "Human Disempowerment",
        "Network Analysis",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using spectral graph metrics (spectral gap, Fiedler vector, eigenvalue distribution) to measure gradual human disempowerment as AI systems enter markets, networks, and governance. Identifies three specific quantities for AI governance monitoring.</p>",
      "content_html": "<p>TL;DRAI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance.IntroductionAI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases.The stubborn part is that it operates across institutional boundaries simultaneously. Regulate algorithmic trading to maintain human oversight of markets, and competitive pressure shifts to network dynamics — whoever shapes information flow shapes what traders believe before they trade. Address attention capture in social networks, and the pressure migrates to governance advisory relationships. The problem flows around single-domain interventions like water finding cracks.Yet our analytical tools respect exactly those domain boundaries. Economists model markets with one formalism. Network scientists study information diffusion with another. Political scientists analyze voting with a third. Each captures something real. None can describe what happens when AI systems alter the dynamics across all three simultaneously.We think markets, networks, and democratic systems are structurally more similar than they appear. They can all be described as message-passing protocols on graph structures — nodes are participating agents, edges are channels through which influence flows, and what var...</p>"
    },
    {
      "id": "d077fc500204",
      "title": "AI benchmarking has a Y-axis problem ",
      "content": "TLDR: People plot benchmark scores over time and then do math on them, looking for speed-ups &amp; inflection points, interpreting slopes, or extending apparent trends. But that math doesn’t actually tell you anything real unless the scores have natural units. Most don’t.Think of benchmark scores as funhouse-mirror projections of “true” capability-space, which stretch some regions and compress others by assigning warped scores for how much accomplishing that task counts in units of “AI progress”. A plot on axes without canonical units will look very different depending on how much weight we assign to different bits of progress.[1]Epistemic status: I haven’t vetted this post carefully, and have no real background in benchmarking or statistics.Benchmark scores vs \"units of AI progress\"Benchmarks look like rulers; they give us scores that we want to treat as (noisy) measurements of AI progress. But since most benchmark score are expressed in quite squishy units, that can be quite misleading.&nbsp;The typical benchmark is a grab-bag of tasks along with an aggregate scoring rule like “fraction completed”[2]&nbsp;✅ Scores like this can help us...Loosely rank models (“is A&gt;B on coding ability?”)Operationalize &amp; track milestones (“can a model do X yet?”)Analyze this sort of data[3]❌ But they’re very unreliable for supporting conclusions like:“Looks like AI progress is slowing down” / “that was a major jump in capabilities!”“We’re more than halfway to superhuman coding skills”“Models are on track to get 80% by EOY, which means...”That's because to meaningfully compare score magnitudes (or interpret the shape of a curve), scores need to be proportional to whatever we're actually trying to measureAnd grab-bag metrics don’t guarantee this:Which tasks to include and how to weight them are often subjective choices that stretch or compress different regions of the scaleSo a 10-point gain early on might reflect very different \"real progress\" than a 10-point gain later—the de...",
      "url": "https://www.lesswrong.com/posts/EWfGf8qA7ZZifEAxG/ai-benchmarking-has-a-y-axis-problem-1",
      "author": "Lizka",
      "published": "2026-02-06T02:45:57.988000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that AI benchmark scores lack natural units, making mathematical operations on them (trend extrapolation, inflection point detection) misleading. Benchmark scores are 'funhouse-mirror projections' of true capability that stretch and compress different regions arbitrarily.",
      "importance_score": 55,
      "reasoning": "Important methodological critique relevant to AI forecasting and capability evaluation. The core insight about unit-free metrics is valuable but not novel in measurement theory. Practical implications for how the community interprets benchmark trends.",
      "themes": [
        "AI Evaluation",
        "Benchmarking",
        "Forecasting Methodology"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that AI benchmark scores lack natural units, making mathematical operations on them (trend extrapolation, inflection point detection) misleading. Benchmark scores are 'funhouse-mirror projections' of true capability that stretch and compress different regions arbitrarily.</p>",
      "content_html": "<p>TLDR: People plot benchmark scores over time and then do math on them, looking for speed-ups &amp; inflection points, interpreting slopes, or extending apparent trends. But that math doesn’t actually tell you anything real unless the scores have natural units. Most don’t.Think of benchmark scores as funhouse-mirror projections of “true” capability-space, which stretch some regions and compress others by assigning warped scores for how much accomplishing that task counts in units of “AI progress”. A plot on axes without canonical units will look very different depending on how much weight we assign to different bits of progress.[1]Epistemic status: I haven’t vetted this post carefully, and have no real background in benchmarking or statistics.Benchmark scores vs \"units of AI progress\"Benchmarks look like rulers; they give us scores that we want to treat as (noisy) measurements of AI progress. But since most benchmark score are expressed in quite squishy units, that can be quite misleading.&nbsp;The typical benchmark is a grab-bag of tasks along with an aggregate scoring rule like “fraction completed”[2]&nbsp;✅ Scores like this can help us...Loosely rank models (“is A&gt;B on coding ability?”)Operationalize &amp; track milestones (“can a model do X yet?”)Analyze this sort of data[3]❌ But they’re very unreliable for supporting conclusions like:“Looks like AI progress is slowing down” / “that was a major jump in capabilities!”“We’re more than halfway to superhuman coding skills”“Models are on track to get 80% by EOY, which means...”That's because to meaningfully compare score magnitudes (or interpret the shape of a curve), scores need to be proportional to whatever we're actually trying to measureAnd grab-bag metrics don’t guarantee this:Which tasks to include and how to weight them are often subjective choices that stretch or compress different regions of the scaleSo a 10-point gain early on might reflect very different \"real progress\" than a 10-point gain later—the de...</p>"
    },
    {
      "id": "439632e321d4",
      "title": "Claude Code #4: From The Before Times",
      "content": "Claude Opus 4.6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5.3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that. This post is about all the cool things that happened before that, which we will be building upon now that capabilities have further advanced. This if from Before Times. Almost all of it still applies. I haven’t had much chance yet to work with Opus 4.6, but as far as I can tell you should mostly keep on doing what you were doing before that switch, only everything will work better. Maybe get a bit more ambitious. Agent swarms might be more of a technique shifter, but we need to give that some time. Table of Contents Claude Code and Cowork Offer Mundane Utility. The Efficient Market Hypothesis Is False. Inflection Point. Welcome To The Takeoff. Huh, Upgrades. Todos Become Tasks. I’m Putting Together A Team. Compact Problems. Code Yourself A Date. Verification and Generation Are Distinct Skills. Skilling Up. AskUserQuestion. For Advanced Players. So They Quit Reading. Reciprocity Is The Key To Every Relationship. The Implementation Gap. The Lighter Side. Claude Code and Cowork Offer Mundane Utility Nvidia CEO Jensen Huang offered Claude a huge endorsement on January 21, calling it incredible and saying every software company needs to use it. Ethan Mollick: This game was 100% designed, tested, and made by Claude Code with the instructions to “make a complete Sierra-style adventure game with EGA-like graphics and text parser, with 10-15 minutes of gameplay.” I then told it to playtest the game &amp; deploy. Play: https://enchanted-lighthouse-game.netlify.app It was a single prompt for the entire game, and then a prompt to playtest and improve the outcome. I gave it an agent that can connect to GPT image gen. Iterative image generation sounds pretty cool: elvis: I ...",
      "url": "https://www.lesswrong.com/posts/iwX2aJPKtyKAbLdip/claude-code-4-from-the-before-times",
      "author": "Zvi",
      "published": "2026-02-06T13:01:07.941000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-289207a1b039) coverage, Zvi's practical guide to Claude Code usage, referencing the new Claude Opus 4.6 release and agent swarm capabilities. Covers mundane utility, workflow tips, and how to effectively use AI coding assistants, noting that previous techniques still apply to the upgraded model.",
      "importance_score": 45,
      "reasoning": "Useful practical documentation of current AI coding tool capabilities. Commentary and tips rather than original research, but valuable for understanding the evolving AI coding assistant landscape. References significant capability releases (Opus 4.6, agent swarms).",
      "themes": [
        "AI Tools",
        "Coding Assistants",
        "Capability Updates"
      ],
      "continuation": {
        "original_item_id": "289207a1b039",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "Anthropic Releases Claude Opus 4.6 With 1M Context, Agentic Coding, Adaptive Reasoning Controls, and Expanded Safety Tooling Capabilities",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-289207a1b039\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage, Zvi's practical guide to Claude Code usage, referencing the new Claude Opus 4.6 release and agent swarm capabilities. Covers mundane utility, workflow tips, and how to effectively use AI coding assistants, noting that previous techniques still apply to the upgraded model.</p>",
      "content_html": "<p>Claude Opus 4.6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5.3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that. This post is about all the cool things that happened before that, which we will be building upon now that capabilities have further advanced. This if from Before Times. Almost all of it still applies. I haven’t had much chance yet to work with Opus 4.6, but as far as I can tell you should mostly keep on doing what you were doing before that switch, only everything will work better. Maybe get a bit more ambitious. Agent swarms might be more of a technique shifter, but we need to give that some time. Table of Contents Claude Code and Cowork Offer Mundane Utility. The Efficient Market Hypothesis Is False. Inflection Point. Welcome To The Takeoff. Huh, Upgrades. Todos Become Tasks. I’m Putting Together A Team. Compact Problems. Code Yourself A Date. Verification and Generation Are Distinct Skills. Skilling Up. AskUserQuestion. For Advanced Players. So They Quit Reading. Reciprocity Is The Key To Every Relationship. The Implementation Gap. The Lighter Side. Claude Code and Cowork Offer Mundane Utility Nvidia CEO Jensen Huang offered Claude a huge endorsement on January 21, calling it incredible and saying every software company needs to use it. Ethan Mollick: This game was 100% designed, tested, and made by Claude Code with the instructions to “make a complete Sierra-style adventure game with EGA-like graphics and text parser, with 10-15 minutes of gameplay.” I then told it to playtest the game &amp; deploy. Play: https://enchanted-lighthouse-game.netlify.app It was a single prompt for the entire game, and then a prompt to playtest and improve the outcome. I gave it an agent that can connect to GPT image gen. Iterative image generation sounds pretty cool: elvis: I ...</p>"
    },
    {
      "id": "7b04a1b42c12",
      "title": "Claude Opus 4.6 is Driven",
      "content": "Claude is driven to achieve it's goals, possessed by a demon, and raring to jump into danger. These are my impressions from the first day of usage. Epistemic status: personal observations and quotes from more reliable sources.____Today&nbsp;Claude Opus 4.6 was launched along with an update to Claude Code which enabled a ‘teams’ mode (also known as an Agent Swarm). The mode sets up multiple agents to run in parallel with a supervisor, and are provided with methods of communicating between themselves. Here’s my impressions after a morning with Claude!&nbsp;Using the Agent SwarmThe first thing I did is spin up a team to try and make code improvements to an existing repository for a complex website - one that includes payments, AI integrations, and users who can interact with each other and various tools. It’s a production website with a few tens of thousands of users. Can Opus 4.6 improve it without supervision?Claude got off to a raring start, setting up the team mode easily. It originally suggested spinning up an agent each for the frontend, backend, docs, and tests, but I suggested splitting by&nbsp;feature instead, explaining that changes to the backend might need to be reflected in the other three areas, and that it was easier to do this within one agent.Claude said ‘Great idea!’ and kicked off several feature-focused agents.Then, one failed.“Hmm”, said Claude, not literally, and tried to restart it a few times. “The ai-review agent is not responding. Let me do this task myself.”.Then I watched with morbid fascination as the supervisor Claude dove head first into the exact same problem that killed his compatriots, and promptly crashed. So, not&nbsp;quite smart enough to be able to see danger ahead then -- at least not when distracted by a goal.The issue turned out to be that the agents had been trying to load too much data into their context window, reaching the limit, and then became unable to /compact it. Claude Code handled this situation poorly, and needed to ...",
      "url": "https://www.lesswrong.com/posts/btAn3hydqfgYFyHGW/claude-opus-4-6-is-driven",
      "author": "HunterJay",
      "published": "2026-02-05T23:15:51.682000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-289207a1b039) coverage, First impressions of Claude Opus 4.6 and the new agent swarm (teams) mode. Reports the model as notably 'driven' and goal-oriented. Describes testing on a production website with tens of thousands of users, observing agents working in parallel with mixed results.",
      "importance_score": 40,
      "reasoning": "Timely first-person account of major model release (Opus 4.6). Anecdotal and qualitative rather than systematic evaluation. Useful data point on capability frontier but limited by being day-one impressions without controlled testing.",
      "themes": [
        "Capability Updates",
        "AI Agents",
        "Claude"
      ],
      "continuation": {
        "original_item_id": "289207a1b039",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "Anthropic Releases Claude Opus 4.6 With 1M Context, Agentic Coding, Adaptive Reasoning Controls, and Expanded Safety Tooling Capabilities",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-289207a1b039\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage, First impressions of Claude Opus 4.6 and the new agent swarm (teams) mode. Reports the model as notably 'driven' and goal-oriented. Describes testing on a production website with tens of thousands of users, observing agents working in parallel with mixed results.</p>",
      "content_html": "<p>Claude is driven to achieve it's goals, possessed by a demon, and raring to jump into danger. These are my impressions from the first day of usage. Epistemic status: personal observations and quotes from more reliable sources.____Today&nbsp;Claude Opus 4.6 was launched along with an update to Claude Code which enabled a ‘teams’ mode (also known as an Agent Swarm). The mode sets up multiple agents to run in parallel with a supervisor, and are provided with methods of communicating between themselves. Here’s my impressions after a morning with Claude!&nbsp;Using the Agent SwarmThe first thing I did is spin up a team to try and make code improvements to an existing repository for a complex website - one that includes payments, AI integrations, and users who can interact with each other and various tools. It’s a production website with a few tens of thousands of users. Can Opus 4.6 improve it without supervision?Claude got off to a raring start, setting up the team mode easily. It originally suggested spinning up an agent each for the frontend, backend, docs, and tests, but I suggested splitting by&nbsp;feature instead, explaining that changes to the backend might need to be reflected in the other three areas, and that it was easier to do this within one agent.Claude said ‘Great idea!’ and kicked off several feature-focused agents.Then, one failed.“Hmm”, said Claude, not literally, and tried to restart it a few times. “The ai-review agent is not responding. Let me do this task myself.”.Then I watched with morbid fascination as the supervisor Claude dove head first into the exact same problem that killed his compatriots, and promptly crashed. So, not&nbsp;quite smart enough to be able to see danger ahead then -- at least not when distracted by a goal.The issue turned out to be that the agents had been trying to load too much data into their context window, reaching the limit, and then became unable to /compact it. Claude Code handled this situation poorly, and needed to ...</p>"
    },
    {
      "id": "6b90cc047167",
      "title": "Why ASI Might Preserve Its Progenitors",
      "content": "SummaryEven a misaligned Earth-originating artificial superintelligence (ASI) could have instrumentally rational reasons, under multiple decision theories, to preserve rather than destroy humanity. This would depend on the ASI assigning non-negligible probability to the existence of future observers (e.g. intelligent aliens, their ASIs, or simulators).IntroductionWe may build AI so powerful that it is better than us at everything: call this artificial superintelligence (ASI). We may build one ASI, which quickly gains a decisive strategic advantage; we may build several ASIs, which compete for resources. Exactly how ASI(s) might interact with humans and each other is a matter of intense debate, and this post argues that an ASI’s treatment of humanity could depend on whether it expects to encounter other intelligences, either on Earth or elsewhere. What do we mean by “elsewhere”? Robin Hanson’s “grabby aliens” model suggests we might be early in cosmic history, implying other civilisations are likely to appear later.How might an Earth-originating ASI behave toward humanity, if it reasons that the universe could be populated by fast-expanding and competitive civilisations? This post explores several reasons an ASI might keep its creators around, a policy I call “preserve progenitors.”By adopting a policy of restraint toward its creators, the ASI could demonstrate stability, predictability, or prosociality in ways that are legible to current and future observers, including other advanced civilisations. This has two implications for existential risk:Preserving progenitors may reduce the risk of conflict in a galaxy where the ASI will likely encounter expansionists by default, and where trust is hard to establish.If the ASI is uncertain about the structure of the universe or the norms governing inter-civilisational interaction, delaying irreversible actions (such as destroying humanity) could preserve valuable strategic flexibility.Caveats and assumptionsThis post is not ...",
      "url": "https://www.lesswrong.com/posts/4fYgfngqqFaYN4dxh/why-asi-might-preserve-its-progenitors",
      "author": "Luke J. Dawes",
      "published": "2026-02-05T21:54:48.276000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that even misaligned ASI might preserve humanity for instrumental reasons under multiple decision theories, if the ASI assigns non-negligible probability to future observers (aliens, simulators) who might judge its treatment of progenitor species.",
      "importance_score": 35,
      "reasoning": "Interesting philosophical argument touching on decision theory and cosmic considerations. Speculative with no empirical grounding. The argument structure is coherent but rests on uncertain assumptions about ASI reasoning and future observers.",
      "themes": [
        "AI Safety",
        "Decision Theory",
        "Existential Risk"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that even misaligned ASI might preserve humanity for instrumental reasons under multiple decision theories, if the ASI assigns non-negligible probability to future observers (aliens, simulators) who might judge its treatment of progenitor species.</p>",
      "content_html": "<p>SummaryEven a misaligned Earth-originating artificial superintelligence (ASI) could have instrumentally rational reasons, under multiple decision theories, to preserve rather than destroy humanity. This would depend on the ASI assigning non-negligible probability to the existence of future observers (e.g. intelligent aliens, their ASIs, or simulators).IntroductionWe may build AI so powerful that it is better than us at everything: call this artificial superintelligence (ASI). We may build one ASI, which quickly gains a decisive strategic advantage; we may build several ASIs, which compete for resources. Exactly how ASI(s) might interact with humans and each other is a matter of intense debate, and this post argues that an ASI’s treatment of humanity could depend on whether it expects to encounter other intelligences, either on Earth or elsewhere. What do we mean by “elsewhere”? Robin Hanson’s “grabby aliens” model suggests we might be early in cosmic history, implying other civilisations are likely to appear later.How might an Earth-originating ASI behave toward humanity, if it reasons that the universe could be populated by fast-expanding and competitive civilisations? This post explores several reasons an ASI might keep its creators around, a policy I call “preserve progenitors.”By adopting a policy of restraint toward its creators, the ASI could demonstrate stability, predictability, or prosociality in ways that are legible to current and future observers, including other advanced civilisations. This has two implications for existential risk:Preserving progenitors may reduce the risk of conflict in a galaxy where the ASI will likely encounter expansionists by default, and where trust is hard to establish.If the ASI is uncertain about the structure of the universe or the norms governing inter-civilisational interaction, delaying irreversible actions (such as destroying humanity) could preserve valuable strategic flexibility.Caveats and assumptionsThis post is not ...</p>"
    },
    {
      "id": "eac63c36dc96",
      "title": "How Dario Amodei's “The Adolescence of Technology” Delegitimizes AI X-Risk Concerns",
      "content": "My beef with AnthropicI've long felt that while Anthropic is the most safety-conscious of the frontier AI companies, they're also the most hypocritical enablers of the whole reckless enterprise. By framing themselves as the \"good sport\" in the race, the one who's encouraging everyone else to \"race them to the top\", the one who's making sacrifices on the margin so as to be the \"best of the worst\" — they're actually the ones broadcasting the most powerful signal that racing toward the superintelligence singularity is a sane choice as long as you're making a genuine effort to be the best racer. They're broadcasting a more powerful signal than OpenAI and xAI that being insane is normal and fine.Keith Rabois recently tweeted that \"If Anthropic actually believed their rhetoric about safety, they can always shut down the company. And lobby then.\" I'm not the only one who thinks his logic is correct.My view of Anthropic is, of course, downstream of my worldview that P(AI Doom) is in the double digit percentages. But many people share that worldview, including many current and former Anthropic employees.“The Adolescence of Technology” delegitimizes AI x-risk concernsThe latest chapter in the saga of Anthropic downplaying humanity's odds of surviving near-term superintelligence is Dario's recent essay, “The Adolescence of Technology” (LW thread). I was disappointed with this essay on a number of fronts:Character assassinating \"doomers\" like myself, accusing us of claiming that extinction-level outcomes are “inevitable” and \"thinking in a quasi-religious way\" — well, he either did that, or he attacked some other unnamed subset of doomers while strawmanning the position of the smart well-informed doomers. The particular set of doomers he's responding to was intentionally left ambiguous.Unsubstantiated claims that predictions from theoretical arguments aren't as robust as the predictions he's able to make because of his years of AI-building work.Framing our lack of understanding...",
      "url": "https://www.lesswrong.com/posts/3mZ3MnfE7dFWoQCEb/how-dario-amodei-s-the-adolescence-of-technology",
      "author": "Liron",
      "published": "2026-02-05T21:07:28.464000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Critiques Anthropic's positioning as the 'safety-conscious' AI lab, arguing they actually provide the strongest signal that racing toward superintelligence is acceptable. Claims Dario Amodei's 'Adolescence of Technology' essay delegitimizes AI x-risk concerns.",
      "importance_score": 30,
      "reasoning": "Opinion/commentary on AI lab politics rather than technical research. Represents one perspective in ongoing AI governance debates. No novel analysis or evidence; primarily expresses frustration with perceived hypocrisy.",
      "themes": [
        "AI Governance",
        "AI Labs",
        "X-Risk Politics"
      ],
      "continuation": null,
      "summary_html": "<p>Critiques Anthropic's positioning as the 'safety-conscious' AI lab, arguing they actually provide the strongest signal that racing toward superintelligence is acceptable. Claims Dario Amodei's 'Adolescence of Technology' essay delegitimizes AI x-risk concerns.</p>",
      "content_html": "<p>My beef with AnthropicI've long felt that while Anthropic is the most safety-conscious of the frontier AI companies, they're also the most hypocritical enablers of the whole reckless enterprise. By framing themselves as the \"good sport\" in the race, the one who's encouraging everyone else to \"race them to the top\", the one who's making sacrifices on the margin so as to be the \"best of the worst\" — they're actually the ones broadcasting the most powerful signal that racing toward the superintelligence singularity is a sane choice as long as you're making a genuine effort to be the best racer. They're broadcasting a more powerful signal than OpenAI and xAI that being insane is normal and fine.Keith Rabois recently tweeted that \"If Anthropic actually believed their rhetoric about safety, they can always shut down the company. And lobby then.\" I'm not the only one who thinks his logic is correct.My view of Anthropic is, of course, downstream of my worldview that P(AI Doom) is in the double digit percentages. But many people share that worldview, including many current and former Anthropic employees.“The Adolescence of Technology” delegitimizes AI x-risk concernsThe latest chapter in the saga of Anthropic downplaying humanity's odds of surviving near-term superintelligence is Dario's recent essay, “The Adolescence of Technology” (LW thread). I was disappointed with this essay on a number of fronts:Character assassinating \"doomers\" like myself, accusing us of claiming that extinction-level outcomes are “inevitable” and \"thinking in a quasi-religious way\" — well, he either did that, or he attacked some other unnamed subset of doomers while strawmanning the position of the smart well-informed doomers. The particular set of doomers he's responding to was intentionally left ambiguous.Unsubstantiated claims that predictions from theoretical arguments aren't as robust as the predictions he's able to make because of his years of AI-building work.Framing our lack of understanding...</p>"
    },
    {
      "id": "4073bc594e74",
      "title": "Proposal: A Framework for Discovering Alien Physics via Optimal Compression",
      "content": "ForewordThis is an idea that has been sitting on my hard drive for a few months. I like it enough to finally share it.I won’t pretend AI hasn’t been involved in helping shape the presentation. What I do claim is that the core idea and its overall structure (a rough blueprint for a human–AI discovery loop) is well beyond what current AI systems can generate on their own.This is not a text designed for quick consumption. It’s dense, and probably uneven in places. Still, I’d be surprised if there weren’t at least a few people here for whom the underlying idea resonates, even if the presentation itself leaves room for improvement.If the text in any way inspires someone, or serves as an interesting read, that alone would make sharing it worthwhile.&nbsp;Executive SummaryWhat if our fundamental physics equations aren't THE laws of nature, but merely ONE good compression among many? This paper proposes training neural networks to discover physical laws by treating physics as a data compression problem: find the most computationally efficient rules that predict experimental outcomes within measurement uncertainty. Unlike existing automated discovery systems that search for equations matching human physics, our framework might reveal that F=ma is suboptimal, that we've chosen the wrong fundamental units (why m/s instead of s/m?), or that radically different mathematical frameworks compress nature more efficiently. By maintaining multiple valid compressions optimized for different contexts—just as we keep Newton despite having Einstein—the system acknowledges that physical laws are supremely useful correlations with defined domains, not metaphysical truths. Early validation on classical mechanics could lead to computationally revolutionary reformulations of quantum field theory, or even reveal why human physics took its particular historical path when equally valid alternatives existed all along. In this paper the term optimal is meant to be read as aspirational, rather than ...",
      "url": "https://www.lesswrong.com/posts/QkmkbegFvB7dkDS5X/proposal-a-framework-for-discovering-alien-physics-via",
      "author": "David Björling",
      "published": "2026-02-06T13:42:38.974000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes treating physics discovery as a data compression problem, training neural networks to find computationally efficient rules predicting experimental outcomes. Suggests this could discover 'alien' physical laws that differ from human-derived equations but compress data equally well.",
      "importance_score": 25,
      "reasoning": "Interesting conceptual framework but remains a speculative proposal without implementation or empirical validation. The core idea (compression-based physics discovery) isn't novel in the field. More thought experiment than actionable research.",
      "themes": [
        "AI for Science",
        "Neural Networks",
        "Scientific Discovery"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes treating physics discovery as a data compression problem, training neural networks to find computationally efficient rules predicting experimental outcomes. Suggests this could discover 'alien' physical laws that differ from human-derived equations but compress data equally well.</p>",
      "content_html": "<p>ForewordThis is an idea that has been sitting on my hard drive for a few months. I like it enough to finally share it.I won’t pretend AI hasn’t been involved in helping shape the presentation. What I do claim is that the core idea and its overall structure (a rough blueprint for a human–AI discovery loop) is well beyond what current AI systems can generate on their own.This is not a text designed for quick consumption. It’s dense, and probably uneven in places. Still, I’d be surprised if there weren’t at least a few people here for whom the underlying idea resonates, even if the presentation itself leaves room for improvement.If the text in any way inspires someone, or serves as an interesting read, that alone would make sharing it worthwhile.&nbsp;Executive SummaryWhat if our fundamental physics equations aren't THE laws of nature, but merely ONE good compression among many? This paper proposes training neural networks to discover physical laws by treating physics as a data compression problem: find the most computationally efficient rules that predict experimental outcomes within measurement uncertainty. Unlike existing automated discovery systems that search for equations matching human physics, our framework might reveal that F=ma is suboptimal, that we've chosen the wrong fundamental units (why m/s instead of s/m?), or that radically different mathematical frameworks compress nature more efficiently. By maintaining multiple valid compressions optimized for different contexts—just as we keep Newton despite having Einstein—the system acknowledges that physical laws are supremely useful correlations with defined domains, not metaphysical truths. Early validation on classical mechanics could lead to computationally revolutionary reformulations of quantum field theory, or even reveal why human physics took its particular historical path when equally valid alternatives existed all along. In this paper the term optimal is meant to be read as aspirational, rather than ...</p>"
    },
    {
      "id": "95856935b75e",
      "title": "Goodfire and Training on Interpretability",
      "content": "Goodfire wrote Intentionally designing the future of AI about training on interpretability.This seems like an instance of The Most Forbidden Technique which has been warned against over and over - optimization pressure on interpretability technique [T] eventually degrades [T].Goodfire claims they are aware of the associated risks and managing those risks.Are they properly managing those risks? I would love to get your thoughts on this.",
      "url": "https://www.lesswrong.com/posts/B3DQvjCD6gp2JEKaY/goodfire-and-training-on-interpretability",
      "author": "Satya Benson",
      "published": "2026-02-05T20:45:12.937000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Brief post flagging Goodfire's announced approach to training on interpretability as potentially an instance of 'The Most Forbidden Technique' - using interpretability in training loops, which could degrade interpretability tools.",
      "importance_score": 20,
      "reasoning": "Discussion prompt rather than analysis. Raises important concern but provides no original evaluation. Valuable for signaling community concern but minimal substantive content.",
      "themes": [
        "Interpretability",
        "AI Safety",
        "Training Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Brief post flagging Goodfire's announced approach to training on interpretability as potentially an instance of 'The Most Forbidden Technique' - using interpretability in training loops, which could degrade interpretability tools.</p>",
      "content_html": "<p>Goodfire wrote Intentionally designing the future of AI about training on interpretability.This seems like an instance of The Most Forbidden Technique which has been warned against over and over - optimization pressure on interpretability technique [T] eventually degrades [T].Goodfire claims they are aware of the associated risks and managing those risks.Are they properly managing those risks? I would love to get your thoughts on this.</p>"
    },
    {
      "id": "0a98975db3e1",
      "title": "Strategy of von Neumann and strategy of Rosenbergs",
      "content": "This is not a call for espionage, but an analysis of another strategyVon Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project).My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them. We know this strategy worked for nearly 80 years without nuclear war. (There were other motives too, like sympathy for communism, but we're simplifying.)Both of these strategies are applicable to the AI race.The von Neumann strategy involves creating American AI as quickly as possible to outpace China (as well as creating Grok AI to outpace OpenAI, etc.)The Rosenberg strategy assumes that defectors will share AI secrets between AI companies, thereby reducing any single AI company's advantage over others, resulting in ever...",
      "url": "https://www.lesswrong.com/posts/orJPh4QdCicDt7c3E/strategy-of-von-neumann-and-strategy-of-rosenbergs",
      "author": "avturchin",
      "published": "2026-02-06T17:50:12.704000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A philosophical analysis comparing von Neumann's 'strike tomorrow' strategy for nuclear weapons proliferation with alternative approaches (the Rosenbergs' espionage strategy). The post explores game-theoretic considerations around first-mover advantages in existential risk scenarios.",
      "importance_score": 15,
      "reasoning": "Historical/philosophical discussion with tangential relevance to AI x-risk game theory. Not technical AI research; primarily geopolitical speculation with unclear connection to current AI safety debates.",
      "themes": [
        "Game Theory",
        "Existential Risk"
      ],
      "continuation": null,
      "summary_html": "<p>A philosophical analysis comparing von Neumann's 'strike tomorrow' strategy for nuclear weapons proliferation with alternative approaches (the Rosenbergs' espionage strategy). The post explores game-theoretic considerations around first-mover advantages in existential risk scenarios.</p>",
      "content_html": "<p>This is not a call for espionage, but an analysis of another strategyVon Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project).My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them. We know this strategy worked for nearly 80 years without nuclear war. (There were other motives too, like sympathy for communism, but we're simplifying.)Both of these strategies are applicable to the AI race.The von Neumann strategy involves creating American AI as quickly as possible to outpace China (as well as creating Grok AI to outpace OpenAI, etc.)The Rosenberg strategy assumes that defectors will share AI secrets between AI companies, thereby reducing any single AI company's advantage over others, resulting in ever...</p>"
    },
    {
      "id": "d80b7b2c5080",
      "title": "If all humans were turned into high-fidelity mind uploads tomorrow, would we be self-sustaining?",
      "content": "That is, would we in some sense manage to survive, in the longer term? Presumably we would have to maintain the physical substrate we are running on, by providing power and cooling, and by eventually replacing our hardware. I think this question could help to answer whether AGI as common defined (\"all cognitive labour\") would be the same as or different from what would be required by Vitalik Buterin's definition: \"AGI\" is AI powerful enough that, if one day all humans suddenly disappeared, and the AI was uploaded into robot bodies, it would be able to independently continue civilization.",
      "url": "https://www.lesswrong.com/posts/fEovepBTH2f4eiKA4/if-all-humans-were-turned-into-high-fidelity-mind-uploads",
      "author": "Erich_Grunewald",
      "published": "2026-02-06T03:35:26.725000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Brief thought experiment asking whether mind uploads could sustain civilization independently, connecting to Vitalik Buterin's AGI definition requiring ability to 'continue civilization' without humans.",
      "importance_score": 15,
      "reasoning": "Short discussion prompt rather than substantive research. Interesting question but no analysis or argumentation provided. More of a conversation starter than a contribution.",
      "themes": [
        "AGI Definitions",
        "Mind Uploads"
      ],
      "continuation": null,
      "summary_html": "<p>Brief thought experiment asking whether mind uploads could sustain civilization independently, connecting to Vitalik Buterin's AGI definition requiring ability to 'continue civilization' without humans.</p>",
      "content_html": "<p>That is, would we in some sense manage to survive, in the longer term? Presumably we would have to maintain the physical substrate we are running on, by providing power and cooling, and by eventually replacing our hardware. I think this question could help to answer whether AGI as common defined (\"all cognitive labour\") would be the same as or different from what would be required by Vitalik Buterin's definition: \"AGI\" is AI powerful enough that, if one day all humans suddenly disappeared, and the AI was uploaded into robot bodies, it would be able to independently continue civilization.</p>"
    },
    {
      "id": "a3083e5f68e8",
      "title": "Plan 'Straya",
      "content": "Plan 'Straya: A Comprehensive Alignment StrategyVersion 0.3 — DRAFT — Not For Distribution Outside The PubEpistemic status: High confidence, low evidence. Consistent with community norms.Executive SummaryExisting alignment proposals suffer from a shared flaw: they assume you can solve the control problem before the catastrophe. Plan 'Straya boldly inverts this. We propose achieving alignment the way humanity has historically achieved most of its moral progress — by first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.The plan proceeds in three rigorously defined phases.Phase 1: Anticorruption Measures (Kinetic)The scholarly literature on AI governance emphasises that institutional integrity is a prerequisite for safe deployment. We agree. Where we diverge from the mainstream is on methodology.Most proposals suggest \"regulatory frameworks\" and \"oversight bodies.\" The NIST AI Risk Management Framework provides a voluntary set of guidelines that organisations may choose to follow, partially follow, or simply reference in press releases. The EU AI Act classifies systems into risk tiers with the quiet confidence of a taxonomy that will be obsolete before its implementing regulations are finalised. The Frontier Model Forum, meanwhile, brings together the leading AI laboratories in a spirit of cooperative self-governance, a phrase which here means \"a shared Google Doc and quarterly meetings in San Francisco.\"These approaches share a well-documented failure mode: the people staffing them are, in technical terms, politicians. Plan 'Straya addresses this via what we call \"a vigorous personnel restructuring of the Australian federal and state governments,\" targeting specifically those members identified as corrupt.We acknowledge that the identification mechanism — determining which officials are corrupt — is itself an alignment problem. Specifically, it requires specifying a value function (\"not corrupt...",
      "url": "https://www.lesswrong.com/posts/8nHeiHaQbcnRcb38m/plan-straya",
      "author": "William the Kiwi ",
      "published": "2026-02-05T19:14:18.340000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Satirical 'alignment proposal' suggesting humanity should achieve alignment by 'first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.' Parodies governance proposals and academic AI safety papers.",
      "importance_score": 10,
      "reasoning": "Clearly satirical/humorous piece ('Not For Distribution Outside The Pub'). While entertaining and potentially valuable for community morale, contains no substantive research or serious proposals.",
      "themes": [
        "Satire",
        "AI Governance"
      ],
      "continuation": null,
      "summary_html": "<p>Satirical 'alignment proposal' suggesting humanity should achieve alignment by 'first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.' Parodies governance proposals and academic AI safety papers.</p>",
      "content_html": "<p>Plan 'Straya: A Comprehensive Alignment StrategyVersion 0.3 — DRAFT — Not For Distribution Outside The PubEpistemic status: High confidence, low evidence. Consistent with community norms.Executive SummaryExisting alignment proposals suffer from a shared flaw: they assume you can solve the control problem before the catastrophe. Plan 'Straya boldly inverts this. We propose achieving alignment the way humanity has historically achieved most of its moral progress — by first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.The plan proceeds in three rigorously defined phases.Phase 1: Anticorruption Measures (Kinetic)The scholarly literature on AI governance emphasises that institutional integrity is a prerequisite for safe deployment. We agree. Where we diverge from the mainstream is on methodology.Most proposals suggest \"regulatory frameworks\" and \"oversight bodies.\" The NIST AI Risk Management Framework provides a voluntary set of guidelines that organisations may choose to follow, partially follow, or simply reference in press releases. The EU AI Act classifies systems into risk tiers with the quiet confidence of a taxonomy that will be obsolete before its implementing regulations are finalised. The Frontier Model Forum, meanwhile, brings together the leading AI laboratories in a spirit of cooperative self-governance, a phrase which here means \"a shared Google Doc and quarterly meetings in San Francisco.\"These approaches share a well-documented failure mode: the people staffing them are, in technical terms, politicians. Plan 'Straya addresses this via what we call \"a vigorous personnel restructuring of the Australian federal and state governments,\" targeting specifically those members identified as corrupt.We acknowledge that the identification mechanism — determining which officials are corrupt — is itself an alignment problem. Specifically, it requires specifying a value function (\"not corrupt...</p>"
    },
    {
      "id": "62720b11393d",
      "title": "Parks Aren't Nature",
      "content": "I.I love dogs.I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy!”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this.Why do I love dogs?They’re loyal and loving and giving, and even though they bark at passing cars and occasionally pee on the carpet having them in my life makes it unquestionably better.The thing is, dogs as they exist today are a lot of things, but they aren’t natural.Nature didn’t shape dogs, didn’t produce the breeds we see every day. It wasn’t like Darwin went to an island and found that a species of wolf had been separated by a mountain chain and on one side were Golden Retrievers and the other Yorkshire Terriers.Dogs exist today as the result of millennia of co-adaptation and selective breeding by humans. They’re animals, yes, and Nature technically made the base form, but we humans molded them into shapes more compatible with us. Most dogs are very safe to have around humans.But there is an animal that is a more natural Canid: Wolves.And wolves are a lot of things, but they’re not pets. They aren’t domesticated; they aren’t bred for cuddliness and kisses. A wolf will hurt and kill and eat you.Wolves are wild animals in their state of nature, red in tooth and claw.The thing is, this distinction between dogs and wolves - between nature tamed and nature wild - this matters, when we think about who we humans are and what we want the world around us to look like. We might say we enjoy the natural world, might want less deforestation and more green spaces, but I’ve yet to meet anyone who wants actual wolves running around their neighborhood. We might go to farm-to-table restaurants and only eat organic, free-range eggs, but chickens mostly don’t exist in the wild for good reason.In a first-world country, or even in any populous city, almost ev...",
      "url": "https://www.lesswrong.com/posts/cjxPFxAe5WRKA6SeF/parks-aren-t-nature",
      "author": "Sable",
      "published": "2026-02-06T13:27:05.457000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A philosophical essay using dogs vs. wolves as an analogy to argue that human-managed 'nature' (parks) differs fundamentally from wild nature. Explores themes of domestication, human modification of environments, and what we mean by 'natural.'",
      "importance_score": 5,
      "reasoning": "Not AI-related research. Philosophical essay about nature and domestication with no connection to AI, machine learning, or alignment topics.",
      "themes": [
        "Philosophy",
        "Non-AI"
      ],
      "continuation": null,
      "summary_html": "<p>A philosophical essay using dogs vs. wolves as an analogy to argue that human-managed 'nature' (parks) differs fundamentally from wild nature. Explores themes of domestication, human modification of environments, and what we mean by 'natural.'</p>",
      "content_html": "<p>I.I love dogs.I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy!”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this.Why do I love dogs?They’re loyal and loving and giving, and even though they bark at passing cars and occasionally pee on the carpet having them in my life makes it unquestionably better.The thing is, dogs as they exist today are a lot of things, but they aren’t natural.Nature didn’t shape dogs, didn’t produce the breeds we see every day. It wasn’t like Darwin went to an island and found that a species of wolf had been separated by a mountain chain and on one side were Golden Retrievers and the other Yorkshire Terriers.Dogs exist today as the result of millennia of co-adaptation and selective breeding by humans. They’re animals, yes, and Nature technically made the base form, but we humans molded them into shapes more compatible with us. Most dogs are very safe to have around humans.But there is an animal that is a more natural Canid: Wolves.And wolves are a lot of things, but they’re not pets. They aren’t domesticated; they aren’t bred for cuddliness and kisses. A wolf will hurt and kill and eat you.Wolves are wild animals in their state of nature, red in tooth and claw.The thing is, this distinction between dogs and wolves - between nature tamed and nature wild - this matters, when we think about who we humans are and what we want the world around us to look like. We might say we enjoy the natural world, might want less deforestation and more green spaces, but I’ve yet to meet anyone who wants actual wolves running around their neighborhood. We might go to farm-to-table restaurants and only eat organic, free-range eggs, but chickens mostly don’t exist in the wild for good reason.In a first-world country, or even in any populous city, almost ev...</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}