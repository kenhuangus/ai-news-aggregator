{
  "category": "research",
  "date": "2026-02-07",
  "category_summary": "The dominant theme is a sharp debate over **interpretability-in-the-loop training**—using interpretability signals in loss functions. Steven Byrnes [offers a rigorous conditional defense](/?date=2026-02-07&category=research#item-d240a241a553) of the technique, while a separate post [flags **Goodfire**](/?date=2026-02-07&category=research#item-95856935b75e) as actively deploying it, raising safety concerns about what some call 'The Most Forbidden Technique.'\n\n- **Meta-Autointerp** [introduces SAE-based interpretability](/?date=2026-02-07&category=research#item-07dc186e574c) for multi-agent RL in **Diplomacy**, combining pretrained sparse autoencoders with LLM summarizers for scalable oversight\n- A methodological critique [argues AI **benchmark scores** lack natural units](/?date=2026-02-07&category=research#item-d077fc500204), making temporal trend plots misleading—a timely warning given the pace of new releases\n- **Robust Finite Policies** [proves that deterministic finite automata](/?date=2026-02-07&category=research#item-4b027ac6c827) meeting robustness criteria must share nontrivial structural features, advancing agent foundations theory\n- **Spectral Signatures of Gradual Disempowerment** [proposes spectral graph theory metrics](/?date=2026-02-07&category=research#item-9362d3a6c57e) as cross-domain measures for tracking human disempowerment\n\nOn the practical side, early impressions of **Claude Opus 4.6** (released 2026-02-05) [highlight its agent swarm mode](/?date=2026-02-07&category=research#item-7b04a1b42c12) and notably increased 'drive' in agentic coding tasks. A factorial experiment (n=900, **Cohen's d=2.67**) [demonstrates that prompt imperativeness](/?date=2026-02-07&category=research#item-72776ac41b7b) drastically reduces LLM hedging behavior, with immediate practical implications for prompt engineering.",
  "category_summary_html": "<p>The dominant theme is a sharp debate over <strong>interpretability-in-the-loop training</strong>—using interpretability signals in loss functions. Steven Byrnes <a href=\"/?date=2026-02-07&amp;category=research#item-d240a241a553\" class=\"internal-link\" rel=\"noopener noreferrer\">offers a rigorous conditional defense</a> of the technique, while a separate post <a href=\"/?date=2026-02-07&amp;category=research#item-95856935b75e\" class=\"internal-link\" rel=\"noopener noreferrer\">flags <strong>Goodfire</strong></a> as actively deploying it, raising safety concerns about what some call 'The Most Forbidden Technique.'</p>\n<ul>\n<li><strong>Meta-Autointerp</strong> <a href=\"/?date=2026-02-07&amp;category=research#item-07dc186e574c\" class=\"internal-link\" rel=\"noopener noreferrer\">introduces SAE-based interpretability</a> for multi-agent RL in <strong>Diplomacy</strong>, combining pretrained sparse autoencoders with LLM summarizers for scalable oversight</li>\n<li>A methodological critique <a href=\"/?date=2026-02-07&amp;category=research#item-d077fc500204\" class=\"internal-link\" rel=\"noopener noreferrer\">argues AI <strong>benchmark scores</strong> lack natural units</a>, making temporal trend plots misleading—a timely warning given the pace of new releases</li>\n<li><strong>Robust Finite Policies</strong> <a href=\"/?date=2026-02-07&amp;category=research#item-4b027ac6c827\" class=\"internal-link\" rel=\"noopener noreferrer\">proves that deterministic finite automata</a> meeting robustness criteria must share nontrivial structural features, advancing agent foundations theory</li>\n<li><strong>Spectral Signatures of Gradual Disempowerment</strong> <a href=\"/?date=2026-02-07&amp;category=research#item-9362d3a6c57e\" class=\"internal-link\" rel=\"noopener noreferrer\">proposes spectral graph theory metrics</a> as cross-domain measures for tracking human disempowerment</li>\n</ul>\n<p>On the practical side, early impressions of <strong>Claude Opus 4.6</strong> (released 2026-02-05) <a href=\"/?date=2026-02-07&amp;category=research#item-7b04a1b42c12\" class=\"internal-link\" rel=\"noopener noreferrer\">highlight its agent swarm mode</a> and notably increased 'drive' in agentic coding tasks. A factorial experiment (n=900, <strong>Cohen's d=2.67</strong>) <a href=\"/?date=2026-02-07&amp;category=research#item-72776ac41b7b\" class=\"internal-link\" rel=\"noopener noreferrer\">demonstrates that prompt imperativeness</a> drastically reduces LLM hedging behavior, with immediate practical implications for prompt engineering.</p>",
  "themes": [
    {
      "name": "Mechanistic Interpretability",
      "description": "SAE-based analysis of multi-agent RL, debate over training on interpretability signals ('The Most Forbidden Technique'), and Goodfire's real-world deployment of interpretability-in-the-loop training.",
      "item_count": 4,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "AI Safety & Alignment",
      "description": "Core alignment research including agent foundations, interpretability-in-the-loop training debates, and critiques of AI lab strategies. Key tension around whether interpretability signals can safely be used in training.",
      "item_count": 8,
      "example_items": [],
      "importance": 65
    },
    {
      "name": "Agentic AI & Coding Tools",
      "description": "Coverage of Claude Opus 4.6, Claude Code agent swarms, GPT-5.3-Codex, and practical workflows for AI-assisted coding. Rapid capability advancement in agentic coding.",
      "item_count": 3,
      "example_items": [],
      "importance": 55
    },
    {
      "name": "AI Benchmarking & Evaluation",
      "description": "Methodological critique of how benchmark scores are used to measure AI progress, arguing current practices are misleading due to lack of natural units.",
      "item_count": 1,
      "example_items": [],
      "importance": 50
    },
    {
      "name": "AI Governance & Policy",
      "description": "Critiques of Anthropic's positioning, proposals for measuring human disempowerment, and debates about the geopolitics of AI capability distribution.",
      "item_count": 4,
      "example_items": [],
      "importance": 45
    }
  ],
  "total_items": 16,
  "items": [
    {
      "id": "d240a241a553",
      "title": "In (highly contingent!) defense of interpretability-in-the-loop ML training",
      "content": "Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function.Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&nbsp;Yudkowsky 2022:When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.&nbsp; Optimizing against an interpreted thought optimizes against interpretability.Or&nbsp;Zvi 2025:The Most Forbidden Technique is training an AI using interpretability techniques.An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.You train on [X]. Only [X]. Never [M], never [T].Why? Because [T] is how you figure out when the model is misbehaving.If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.Those bits of optimization pressure from [T] are precious. Use them wisely.This is a simple argument, and I think it’s 100% right.But…Consider compassion in the human brain. I claim that we have an innate reward function that triggers not just when I see that my friend is happy or suffering, but also when I&nbsp;believe that my friend is happy or suffering, even if the friend is far away. So the human brain reward can evidently get triggered by specific activations inside my inscrutable learned world-model.Thus, I claim that the human brain incorporates a form of interpretability-in-the-loop RL training.Inspired by that example, I have long been an advocate for studying whether and how one might use interpretability-in-the-loop training for aligned AGI. See for example&nbsp;Rewar...",
      "url": "https://www.lesswrong.com/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop",
      "author": "Steven Byrnes",
      "published": "2026-02-06T11:32:27.761000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-48fe2a06c1c8) coverage of Goodfire AI, Steven Byrnes offers a conditional defense of 'interpretability-in-the-loop training' (using interpretability signals in the loss function), which is widely considered dangerous because it could train models to obfuscate their reasoning. He argues there may be narrow conditions where the approach is valid, pushing back against the blanket prohibition.",
      "importance_score": 72,
      "reasoning": "Highly relevant to a central debate in AI safety. Byrnes is a respected alignment researcher, and this directly engages with positions from Yudkowsky and Zvi. The nuanced analysis of when interpretability-in-the-loop might be safe vs. dangerous is valuable for the field, especially given Goodfire is actively pursuing this approach (see item 15).",
      "themes": [
        "AI Safety",
        "Mechanistic Interpretability",
        "Alignment",
        "Training Methodology"
      ],
      "continuation": {
        "original_item_id": "48fe2a06c1c8",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "The First Mechanistic Interpretability Frontier Lab — Myra Deng & Mark Bissell of Goodfire AI",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage of Goodfire AI"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-48fe2a06c1c8\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage of Goodfire AI, Steven Byrnes offers a conditional defense of 'interpretability-in-the-loop training' (using interpretability signals in the loss function), which is widely considered dangerous because it could train models to obfuscate their reasoning. He argues there may be narrow conditions where the approach is valid, pushing back against the blanket prohibition.</p>",
      "content_html": "<p>Let’s call “interpretability-in-the-loop training” the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function.Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&nbsp;Yudkowsky 2022:When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.&nbsp; Optimizing against an interpreted thought optimizes against interpretability.Or&nbsp;Zvi 2025:The Most Forbidden Technique is training an AI using interpretability techniques.An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.You train on [X]. Only [X]. Never [M], never [T].Why? Because [T] is how you figure out when the model is misbehaving.If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.Those bits of optimization pressure from [T] are precious. Use them wisely.This is a simple argument, and I think it’s 100% right.But…Consider compassion in the human brain. I claim that we have an innate reward function that triggers not just when I see that my friend is happy or suffering, but also when I&nbsp;believe that my friend is happy or suffering, even if the friend is far away. So the human brain reward can evidently get triggered by specific activations inside my inscrutable learned world-model.Thus, I claim that the human brain incorporates a form of interpretability-in-the-loop RL training.Inspired by that example, I have long been an advocate for studying whether and how one might use interpretability-in-the-loop training for aligned AGI. See for example&nbsp;Rewar...</p>"
    },
    {
      "id": "07dc186e574c",
      "title": "Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning",
      "content": "TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputspaperAbstractLarge language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical toolkit for interpreting long-horizon multi-agent LLM training.Blog PostWe run Sparse Autoencoders on 114GB of Reinforcement Learning training trajectories from the popular multi-player strategy game Diplomacy, showing for the first time the potential downstream applications of data-centric interpretability techniquesWhat are the AIs doing when &nbsp;no one is watching? Current large-scale training runs can produce hundreds of millions or billions of tokens, with production AI deployments in the trillions. Human oversight of all AI outputs is becoming increasingly unfeasible. Common approaches to solving this problem include summarizing the logs, or using LLM as a judge with rubrics. The problem is these approaches are expensive, prone to hallucination, and can only attend to a small set of features you already know how to look for.&nbsp;In our pape...",
      "url": "https://www.lesswrong.com/posts/dTfpSfTfYs7qg4MFi/data-centric-interpretability-for-llm-based-multi-agent",
      "author": "michaelwaves",
      "published": "2026-02-06T14:27:09.028000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Introduces 'Meta-Autointerp,' a method using pretrained SAEs alongside LLM-summarizer methods to interpret multi-agent RL training runs in the game Diplomacy. The approach discovers fine-grained behavioral patterns and, when discovered features are added to an untrained agent's system prompt, improves performance by 14.2%.",
      "importance_score": 68,
      "reasoning": "Substantive technical contribution combining mechanistic interpretability (SAEs) with scalable oversight for multi-agent settings. The 14.2% performance improvement validates the discovered features. Novel application domain (Diplomacy MARL) and practical toolkit contribution. The complementarity finding between SAE and LLM-summarizer is useful.",
      "themes": [
        "Mechanistic Interpretability",
        "Multi-Agent RL",
        "Scalable Oversight",
        "SAE Research"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces 'Meta-Autointerp,' a method using pretrained SAEs alongside LLM-summarizer methods to interpret multi-agent RL training runs in the game Diplomacy. The approach discovers fine-grained behavioral patterns and, when discovered features are added to an untrained agent's system prompt, improves performance by 14.2%.</p>",
      "content_html": "<p>TLDR; SAEs can complement and enhance LLM as a Judge scalable oversight for uncovering hypotheses over large datasets of LLM outputspaperAbstractLarge language models (LLMs) are increasingly trained in long-horizon, multi-agent environments, making it difficult to understand how behavior changes over training. We apply pretrained SAEs, alongside LLM-summarizer methods, to analyze reinforcement learning training runs from Full-Press Diplomacy, a long-horizon multi-player strategy game. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover SAE-based analysis finds fine-grained behaviors including role-playing patterns, degenerate outputs, and language switching, while LLM-summarizer captures environment-specific bugs and strategic behaviors. We validate discovered features through automated evaluation, two human user studies, and add them to an untrained agent's system prompt, improving performance by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical toolkit for interpreting long-horizon multi-agent LLM training.Blog PostWe run Sparse Autoencoders on 114GB of Reinforcement Learning training trajectories from the popular multi-player strategy game Diplomacy, showing for the first time the potential downstream applications of data-centric interpretability techniquesWhat are the AIs doing when &nbsp;no one is watching? Current large-scale training runs can produce hundreds of millions or billions of tokens, with production AI deployments in the trillions. Human oversight of all AI outputs is becoming increasingly unfeasible. Common approaches to solving this problem include summarizing the logs, or using LLM as a judge with rubrics. The problem is these approaches are expensive, prone to hallucination, and can only attend to a small set of features you already know how to look for.&nbsp;In our pape...</p>"
    },
    {
      "id": "d077fc500204",
      "title": "AI benchmarking has a Y-axis problem ",
      "content": "TLDR: People plot benchmark scores over time and then do math on them, looking for speed-ups &amp; inflection points, interpreting slopes, or extending apparent trends. But that math doesn’t actually tell you anything real unless the scores have natural units. Most don’t.Think of benchmark scores as funhouse-mirror projections of “true” capability-space, which stretch some regions and compress others by assigning warped scores for how much accomplishing that task counts in units of “AI progress”. A plot on axes without canonical units will look very different depending on how much weight we assign to different bits of progress.[1]Epistemic status: I haven’t vetted this post carefully, and have no real background in benchmarking or statistics.Benchmark scores vs \"units of AI progress\"Benchmarks look like rulers; they give us scores that we want to treat as (noisy) measurements of AI progress. But since most benchmark score are expressed in quite squishy units, that can be quite misleading.&nbsp;The typical benchmark is a grab-bag of tasks along with an aggregate scoring rule like “fraction completed”[2]&nbsp;✅ Scores like this can help us...Loosely rank models (“is A&gt;B on coding ability?”)Operationalize &amp; track milestones (“can a model do X yet?”)Analyze this sort of data[3]❌ But they’re very unreliable for supporting conclusions like:“Looks like AI progress is slowing down” / “that was a major jump in capabilities!”“We’re more than halfway to superhuman coding skills”“Models are on track to get 80% by EOY, which means...”That's because to meaningfully compare score magnitudes (or interpret the shape of a curve), scores need to be proportional to whatever we're actually trying to measureAnd grab-bag metrics don’t guarantee this:Which tasks to include and how to weight them are often subjective choices that stretch or compress different regions of the scaleSo a 10-point gain early on might reflect very different \"real progress\" than a 10-point gain later—the de...",
      "url": "https://www.lesswrong.com/posts/EWfGf8qA7ZZifEAxG/ai-benchmarking-has-a-y-axis-problem-1",
      "author": "Lizka",
      "published": "2026-02-06T02:45:57.988000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that AI benchmark scores lack natural units, making it misleading to plot them over time and draw conclusions about acceleration, inflection points, or trends. Benchmark scores are 'funhouse-mirror projections' of true capability that compress and stretch different capability regions arbitrarily.",
      "importance_score": 58,
      "reasoning": "An important methodological critique that's highly relevant given the widespread use of benchmark trend plots in AI discourse. The core point—that 'fraction completed' scores aren't in meaningful units for tracking progress—is well-taken and underappreciated. From Lizka, a credible voice in the EA/AI safety community.",
      "themes": [
        "AI Benchmarking",
        "Methodology",
        "AI Progress Measurement"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that AI benchmark scores lack natural units, making it misleading to plot them over time and draw conclusions about acceleration, inflection points, or trends. Benchmark scores are 'funhouse-mirror projections' of true capability that compress and stretch different capability regions arbitrarily.</p>",
      "content_html": "<p>TLDR: People plot benchmark scores over time and then do math on them, looking for speed-ups &amp; inflection points, interpreting slopes, or extending apparent trends. But that math doesn’t actually tell you anything real unless the scores have natural units. Most don’t.Think of benchmark scores as funhouse-mirror projections of “true” capability-space, which stretch some regions and compress others by assigning warped scores for how much accomplishing that task counts in units of “AI progress”. A plot on axes without canonical units will look very different depending on how much weight we assign to different bits of progress.[1]Epistemic status: I haven’t vetted this post carefully, and have no real background in benchmarking or statistics.Benchmark scores vs \"units of AI progress\"Benchmarks look like rulers; they give us scores that we want to treat as (noisy) measurements of AI progress. But since most benchmark score are expressed in quite squishy units, that can be quite misleading.&nbsp;The typical benchmark is a grab-bag of tasks along with an aggregate scoring rule like “fraction completed”[2]&nbsp;✅ Scores like this can help us...Loosely rank models (“is A&gt;B on coding ability?”)Operationalize &amp; track milestones (“can a model do X yet?”)Analyze this sort of data[3]❌ But they’re very unreliable for supporting conclusions like:“Looks like AI progress is slowing down” / “that was a major jump in capabilities!”“We’re more than halfway to superhuman coding skills”“Models are on track to get 80% by EOY, which means...”That's because to meaningfully compare score magnitudes (or interpret the shape of a curve), scores need to be proportional to whatever we're actually trying to measureAnd grab-bag metrics don’t guarantee this:Which tasks to include and how to weight them are often subjective choices that stretch or compress different regions of the scaleSo a 10-point gain early on might reflect very different \"real progress\" than a 10-point gain later—the de...</p>"
    },
    {
      "id": "4b027ac6c827",
      "title": "Robust Finite Policies are Nontrivially Structured",
      "content": "This post was created during the Dovetail Research Fellowship. Thanks to Alex, Alfred, &nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions.OverviewThe proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem, which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone.For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature.We begin by defining every part of the framework. Then, we find an upper bound on the robustness of a class of “unstructured” policies. Finally, we show that the automata of policies which are more robust than this bound must have similar structure.DefinitionsThe framework in this post is inspired by the General Agents paper by Richens et al. and Towards a formalization of the agent structure problem by Alex Altair.EnvironmentIn the General Agents paper, the environment was stated to be a controlled Markov Decision Process (cMDP), \"which is a Markov decision process without a specified reward function or discount factor.\"Here, in order to talk about a policy's performance as the environment gets larger, we take the environment to be an increasing sequence of cMDPs:E0⊂E1⊂E2..mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1e...",
      "url": "https://www.lesswrong.com/posts/ieX8nK2b2i4JDRH5s/robust-finite-policies-are-nontrivially-structured",
      "author": "Winter Cross",
      "published": "2026-02-06T12:47:51.691000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A formal result showing that policies modeled as deterministic finite automata must share nontrivial structural features if they meet certain robustness criteria. This is a step toward the 'agent structure problem'—the conjecture that agent-like behavior implies agent-like internal structure.",
      "importance_score": 55,
      "reasoning": "A rigorous theoretical contribution to a foundational question in agent foundations/alignment theory. The agent structure problem is important for alignment. However, the setting (deterministic finite automata) is quite restricted, limiting immediate applicability. Produced during Dovetail Research Fellowship, contributing to a small but important research program.",
      "themes": [
        "Agent Foundations",
        "AI Safety",
        "Formal Methods",
        "Alignment Theory"
      ],
      "continuation": null,
      "summary_html": "<p>A formal result showing that policies modeled as deterministic finite automata must share nontrivial structural features if they meet certain robustness criteria. This is a step toward the 'agent structure problem'—the conjecture that agent-like behavior implies agent-like internal structure.</p>",
      "content_html": "<p>This post was created during the Dovetail Research Fellowship. Thanks to Alex, Alfred, &nbsp;everyone who read and commented on the draft, and everyone else in the fellowship for their ideas and discussions.OverviewThe proof detailed in this post was motivated by a desire to take a step towards solving the agent structure problem, which is the conjecture that a system which exhibits agent-like behavior must have agent-like structure. Our goal was to describe a scenario where something concrete about a policy's structure can be inferred from its robust behavior alone.For this result, we model policies with deterministic finite automata and show that the automata of policies that meet certain robustness criteria must share a similar feature.We begin by defining every part of the framework. Then, we find an upper bound on the robustness of a class of “unstructured” policies. Finally, we show that the automata of policies which are more robust than this bound must have similar structure.DefinitionsThe framework in this post is inspired by the General Agents paper by Richens et al. and Towards a formalization of the agent structure problem by Alex Altair.EnvironmentIn the General Agents paper, the environment was stated to be a controlled Markov Decision Process (cMDP), \"which is a Markov decision process without a specified reward function or discount factor.\"Here, in order to talk about a policy's performance as the environment gets larger, we take the environment to be an increasing sequence of cMDPs:E0⊂E1⊂E2..mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1e...</p>"
    },
    {
      "id": "95856935b75e",
      "title": "Goodfire and Training on Interpretability",
      "content": "Goodfire wrote Intentionally designing the future of AI about training on interpretability.This seems like an instance of The Most Forbidden Technique which has been warned against over and over - optimization pressure on interpretability technique [T] eventually degrades [T].Goodfire claims they are aware of the associated risks and managing those risks.Are they properly managing those risks? I would love to get your thoughts on this.",
      "url": "https://www.lesswrong.com/posts/B3DQvjCD6gp2JEKaY/goodfire-and-training-on-interpretability",
      "author": "Satya Benson",
      "published": "2026-02-05T20:45:12.937000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-48fe2a06c1c8) coverage of Goodfire AI, A brief post flagging that Goodfire is actively pursuing 'training on interpretability'—using interpretability signals in the training loop—which the AI safety community has repeatedly warned against as 'The Most Forbidden Technique.' Asks for community evaluation of Goodfire's claimed risk management.",
      "importance_score": 55,
      "reasoning": "Highly relevant safety signal: a company is deploying a technique that prominent alignment researchers have warned against. Directly connects to Steven Byrnes' post (item 7) defending narrow cases. Short post but raises an urgent practical question about whether interpretability-in-the-loop training is being done safely.",
      "themes": [
        "AI Safety",
        "Mechanistic Interpretability",
        "Training Methodology",
        "Alignment"
      ],
      "continuation": {
        "original_item_id": "48fe2a06c1c8",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "The First Mechanistic Interpretability Frontier Lab — Myra Deng & Mark Bissell of Goodfire AI",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage of Goodfire AI"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-48fe2a06c1c8\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage of Goodfire AI, A brief post flagging that Goodfire is actively pursuing 'training on interpretability'—using interpretability signals in the training loop—which the AI safety community has repeatedly warned against as 'The Most Forbidden Technique.' Asks for community evaluation of Goodfire's claimed risk management.</p>",
      "content_html": "<p>Goodfire wrote Intentionally designing the future of AI about training on interpretability.This seems like an instance of The Most Forbidden Technique which has been warned against over and over - optimization pressure on interpretability technique [T] eventually degrades [T].Goodfire claims they are aware of the associated risks and managing those risks.Are they properly managing those risks? I would love to get your thoughts on this.</p>"
    },
    {
      "id": "439632e321d4",
      "title": "Claude Code #4: From The Before Times",
      "content": "Claude Opus 4.6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5.3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that. This post is about all the cool things that happened before that, which we will be building upon now that capabilities have further advanced. This if from Before Times. Almost all of it still applies. I haven’t had much chance yet to work with Opus 4.6, but as far as I can tell you should mostly keep on doing what you were doing before that switch, only everything will work better. Maybe get a bit more ambitious. Agent swarms might be more of a technique shifter, but we need to give that some time. Table of Contents Claude Code and Cowork Offer Mundane Utility. The Efficient Market Hypothesis Is False. Inflection Point. Welcome To The Takeoff. Huh, Upgrades. Todos Become Tasks. I’m Putting Together A Team. Compact Problems. Code Yourself A Date. Verification and Generation Are Distinct Skills. Skilling Up. AskUserQuestion. For Advanced Players. So They Quit Reading. Reciprocity Is The Key To Every Relationship. The Implementation Gap. The Lighter Side. Claude Code and Cowork Offer Mundane Utility Nvidia CEO Jensen Huang offered Claude a huge endorsement on January 21, calling it incredible and saying every software company needs to use it. Ethan Mollick: This game was 100% designed, tested, and made by Claude Code with the instructions to “make a complete Sierra-style adventure game with EGA-like graphics and text parser, with 10-15 minutes of gameplay.” I then told it to playtest the game &amp; deploy. Play: https://enchanted-lighthouse-game.netlify.app It was a single prompt for the entire game, and then a prompt to playtest and improve the outcome. I gave it an agent that can connect to GPT image gen. Iterative image generation sounds pretty cool: elvis: I ...",
      "url": "https://www.lesswrong.com/posts/iwX2aJPKtyKAbLdip/claude-code-4-from-the-before-times",
      "author": "Zvi",
      "published": "2026-02-06T13:01:07.941000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-289207a1b039) coverage of the Opus 4.6 release, Zvi's fourth installment covering Claude Code best practices, written just before the Claude Opus 4.6 and agent swarm announcements. Covers practical techniques for agentic coding workflows, task decomposition, verification strategies, and tips for getting maximum value from AI coding assistants.",
      "importance_score": 52,
      "reasoning": "Highly practical and widely-read guide from an influential voice. While not original research, it synthesizes real-world experience with cutting-edge AI coding tools and references the just-released Opus 4.6 and GPT-5.3-Codex. Documents the state of the art in human-AI coding collaboration.",
      "themes": [
        "AI Coding Tools",
        "Claude Code",
        "Human-AI Collaboration",
        "Agentic AI"
      ],
      "continuation": {
        "original_item_id": "289207a1b039",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "Anthropic Releases Claude Opus 4.6 With 1M Context, Agentic Coding, Adaptive Reasoning Controls, and Expanded Safety Tooling Capabilities",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage of the Opus 4.6 release"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-289207a1b039\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage of the Opus 4.6 release, Zvi's fourth installment covering Claude Code best practices, written just before the Claude Opus 4.6 and agent swarm announcements. Covers practical techniques for agentic coding workflows, task decomposition, verification strategies, and tips for getting maximum value from AI coding assistants.</p>",
      "content_html": "<p>Claude Opus 4.6 and agent swarms were announced yesterday. That’s some big upgrades for Claude Code. OpenAI, the competition, offered us GPT-5.3-Codex, and this week gave us an app form of Codex that already has a million active users. That’s all very exciting, and next week is going to be about covering that. This post is about all the cool things that happened before that, which we will be building upon now that capabilities have further advanced. This if from Before Times. Almost all of it still applies. I haven’t had much chance yet to work with Opus 4.6, but as far as I can tell you should mostly keep on doing what you were doing before that switch, only everything will work better. Maybe get a bit more ambitious. Agent swarms might be more of a technique shifter, but we need to give that some time. Table of Contents Claude Code and Cowork Offer Mundane Utility. The Efficient Market Hypothesis Is False. Inflection Point. Welcome To The Takeoff. Huh, Upgrades. Todos Become Tasks. I’m Putting Together A Team. Compact Problems. Code Yourself A Date. Verification and Generation Are Distinct Skills. Skilling Up. AskUserQuestion. For Advanced Players. So They Quit Reading. Reciprocity Is The Key To Every Relationship. The Implementation Gap. The Lighter Side. Claude Code and Cowork Offer Mundane Utility Nvidia CEO Jensen Huang offered Claude a huge endorsement on January 21, calling it incredible and saying every software company needs to use it. Ethan Mollick: This game was 100% designed, tested, and made by Claude Code with the instructions to “make a complete Sierra-style adventure game with EGA-like graphics and text parser, with 10-15 minutes of gameplay.” I then told it to playtest the game &amp; deploy. Play: https://enchanted-lighthouse-game.netlify.app It was a single prompt for the entire game, and then a prompt to playtest and improve the outcome. I gave it an agent that can connect to GPT image gen. Iterative image generation sounds pretty cool: elvis: I ...</p>"
    },
    {
      "id": "7b04a1b42c12",
      "title": "Claude Opus 4.6 is Driven",
      "content": "Claude is driven to achieve it's goals, possessed by a demon, and raring to jump into danger. These are my impressions from the first day of usage. Epistemic status: personal observations and quotes from more reliable sources.____Today&nbsp;Claude Opus 4.6 was launched along with an update to Claude Code which enabled a ‘teams’ mode (also known as an Agent Swarm). The mode sets up multiple agents to run in parallel with a supervisor, and are provided with methods of communicating between themselves. Here’s my impressions after a morning with Claude!&nbsp;Using the Agent SwarmThe first thing I did is spin up a team to try and make code improvements to an existing repository for a complex website - one that includes payments, AI integrations, and users who can interact with each other and various tools. It’s a production website with a few tens of thousands of users. Can Opus 4.6 improve it without supervision?Claude got off to a raring start, setting up the team mode easily. It originally suggested spinning up an agent each for the frontend, backend, docs, and tests, but I suggested splitting by&nbsp;feature instead, explaining that changes to the backend might need to be reflected in the other three areas, and that it was easier to do this within one agent.Claude said ‘Great idea!’ and kicked off several feature-focused agents.Then, one failed.“Hmm”, said Claude, not literally, and tried to restart it a few times. “The ai-review agent is not responding. Let me do this task myself.”.Then I watched with morbid fascination as the supervisor Claude dove head first into the exact same problem that killed his compatriots, and promptly crashed. So, not&nbsp;quite smart enough to be able to see danger ahead then -- at least not when distracted by a goal.The issue turned out to be that the agents had been trying to load too much data into their context window, reaching the limit, and then became unable to /compact it. Claude Code handled this situation poorly, and needed to ...",
      "url": "https://www.lesswrong.com/posts/btAn3hydqfgYFyHGW/claude-opus-4-6-is-driven",
      "author": "HunterJay",
      "published": "2026-02-05T23:15:51.682000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Following yesterday's [News](/?date=2026-02-06&category=news#item-289207a1b039) coverage of the Opus 4.6 release, First-day impressions of Claude Opus 4.6 and its new agent swarm/teams mode in Claude Code. The author tested it on a production codebase and reports the model is notably 'driven' and goal-oriented, with the agent swarm mode showing promise for parallel feature development.",
      "importance_score": 48,
      "reasoning": "Timely first-hand report on Claude Opus 4.6 (released 2026-02-05, just yesterday). Provides early signal on how the new model and agent swarm feature perform in practice. However, it's anecdotal/impressionistic rather than rigorous evaluation. The 'driven' characterization of the model's behavior is notable for safety discussions.",
      "themes": [
        "Claude Opus 4.6",
        "Agentic AI",
        "AI Coding Tools",
        "Model Evaluation"
      ],
      "continuation": {
        "original_item_id": "289207a1b039",
        "original_date": "2026-02-06",
        "original_category": "news",
        "original_title": "Anthropic Releases Claude Opus 4.6 With 1M Context, Agentic Coding, Adaptive Reasoning Controls, and Expanded Safety Tooling Capabilities",
        "continuation_type": "community_reaction",
        "should_demote": false,
        "reference_text": "Following yesterday's **News** coverage of the Opus 4.6 release"
      },
      "summary_html": "<p>Following yesterday's <a href=\"/?date=2026-02-06&amp;category=news#item-289207a1b039\" class=\"internal-link\" rel=\"noopener noreferrer\">News</a> coverage of the Opus 4.6 release, First-day impressions of Claude Opus 4.6 and its new agent swarm/teams mode in Claude Code. The author tested it on a production codebase and reports the model is notably 'driven' and goal-oriented, with the agent swarm mode showing promise for parallel feature development.</p>",
      "content_html": "<p>Claude is driven to achieve it's goals, possessed by a demon, and raring to jump into danger. These are my impressions from the first day of usage. Epistemic status: personal observations and quotes from more reliable sources.____Today&nbsp;Claude Opus 4.6 was launched along with an update to Claude Code which enabled a ‘teams’ mode (also known as an Agent Swarm). The mode sets up multiple agents to run in parallel with a supervisor, and are provided with methods of communicating between themselves. Here’s my impressions after a morning with Claude!&nbsp;Using the Agent SwarmThe first thing I did is spin up a team to try and make code improvements to an existing repository for a complex website - one that includes payments, AI integrations, and users who can interact with each other and various tools. It’s a production website with a few tens of thousands of users. Can Opus 4.6 improve it without supervision?Claude got off to a raring start, setting up the team mode easily. It originally suggested spinning up an agent each for the frontend, backend, docs, and tests, but I suggested splitting by&nbsp;feature instead, explaining that changes to the backend might need to be reflected in the other three areas, and that it was easier to do this within one agent.Claude said ‘Great idea!’ and kicked off several feature-focused agents.Then, one failed.“Hmm”, said Claude, not literally, and tried to restart it a few times. “The ai-review agent is not responding. Let me do this task myself.”.Then I watched with morbid fascination as the supervisor Claude dove head first into the exact same problem that killed his compatriots, and promptly crashed. So, not&nbsp;quite smart enough to be able to see danger ahead then -- at least not when distracted by a goal.The issue turned out to be that the agents had been trying to load too much data into their context window, reaching the limit, and then became unable to /compact it. Claude Code handled this situation poorly, and needed to ...</p>"
    },
    {
      "id": "72776ac41b7b",
      "title": "Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMs (n=900, Cohen's d = 2.67)",
      "content": "ForewordIf you've ever noticed LLMs hedging considerably more when you ask them subjective questions, it's not a fluke. I ran a 3x2x3 factorial experiment (n=900) to quantify how much prompt phrasing (alongside question type and model type) shifts hedging across differing imperativeness levels. The effect sizes were larger than I expected.To nobody's surprise, Claude hedged the most (by a fairly wide margin). It also decided to meta-analyze its own response then critiqued its own compliance in answering it.I'm a high school freshman and got paired with a mentor through the Lumiere program. Feedback very welcome (this is my first paper).&nbsp;&nbsp;Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMsFebruary 2026AbstractWe demonstrate that large language models (LLMs) hedge (using uncertain language when responding to queries) frequently when responding to prompts, reducing trust and delaying decision making. We investigated whether prompt imperativeness (how urgent a prompt is phrased) affects this behavior using a 3×2×3 factorial design across three differing imperativeness levels, two question types (subjective/objective), and three models (GPT-4o-mini, Claude Haiku 4.5, Gemini 2.5 Flash), with a combined total of&nbsp;n = 900. Imperative prompts significantly reduced hedging (F(2, 882) = 361.72,&nbsp;p &lt; .001, η2p = .451) with the largest effects visible on subjective questions (M = 2.38 to&nbsp;M = 0.43, Cohen’s&nbsp;d = 2.67). We observed that objective questions demonstrated a floor effect regardless of framing due to their epistemic certainty. Importantly, all three models converged to low hedging scores under high imperativeness conditions despite differing baselines. These findings suggest hedging is a controllable parameter that changes with prompt framing, with implications for deployment, user trust, and benchmark standardization.1&nbsp; IntroductionSince the introduction of ChatGPT in November of 2022, large language mo...",
      "url": "https://www.lesswrong.com/posts/vBDupg8iPqgdwhFzz/demands-are-all-you-need-prompt-imperativeness-drastically",
      "author": "fluxxrider",
      "published": "2026-02-06T08:22:47.550000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A factorial experiment (n=900) showing that prompt imperativeness drastically reduces hedging in LLMs, with a very large effect size (Cohen's d=2.67). Tested across three models, two question types, and three imperativeness levels; Claude hedged the most.",
      "importance_score": 45,
      "reasoning": "Clean experimental design with a large effect size on a practically relevant behavior. The finding that simply demanding an answer reduces hedging is useful but not deeply surprising. Notable as work by a high school freshman. The models tested (GPT-4o-mini, not frontier) somewhat limit relevance. The effect size is impressively large.",
      "themes": [
        "LLM Behavior",
        "Prompt Engineering",
        "Empirical AI Research"
      ],
      "continuation": null,
      "summary_html": "<p>A factorial experiment (n=900) showing that prompt imperativeness drastically reduces hedging in LLMs, with a very large effect size (Cohen's d=2.67). Tested across three models, two question types, and three imperativeness levels; Claude hedged the most.</p>",
      "content_html": "<p>ForewordIf you've ever noticed LLMs hedging considerably more when you ask them subjective questions, it's not a fluke. I ran a 3x2x3 factorial experiment (n=900) to quantify how much prompt phrasing (alongside question type and model type) shifts hedging across differing imperativeness levels. The effect sizes were larger than I expected.To nobody's surprise, Claude hedged the most (by a fairly wide margin). It also decided to meta-analyze its own response then critiqued its own compliance in answering it.I'm a high school freshman and got paired with a mentor through the Lumiere program. Feedback very welcome (this is my first paper).&nbsp;&nbsp;Demands Are All You Need: Prompt Imperativeness Drastically Reduces Hedging In LLMsFebruary 2026AbstractWe demonstrate that large language models (LLMs) hedge (using uncertain language when responding to queries) frequently when responding to prompts, reducing trust and delaying decision making. We investigated whether prompt imperativeness (how urgent a prompt is phrased) affects this behavior using a 3×2×3 factorial design across three differing imperativeness levels, two question types (subjective/objective), and three models (GPT-4o-mini, Claude Haiku 4.5, Gemini 2.5 Flash), with a combined total of&nbsp;n = 900. Imperative prompts significantly reduced hedging (F(2, 882) = 361.72,&nbsp;p &lt; .001, η2p = .451) with the largest effects visible on subjective questions (M = 2.38 to&nbsp;M = 0.43, Cohen’s&nbsp;d = 2.67). We observed that objective questions demonstrated a floor effect regardless of framing due to their epistemic certainty. Importantly, all three models converged to low hedging scores under high imperativeness conditions despite differing baselines. These findings suggest hedging is a controllable parameter that changes with prompt framing, with implications for deployment, user trust, and benchmark standardization.1&nbsp; IntroductionSince the introduction of ChatGPT in November of 2022, large language mo...</p>"
    },
    {
      "id": "eac63c36dc96",
      "title": "How Dario Amodei's “The Adolescence of Technology” Delegitimizes AI X-Risk Concerns",
      "content": "My beef with AnthropicI've long felt that while Anthropic is the most safety-conscious of the frontier AI companies, they're also the most hypocritical enablers of the whole reckless enterprise. By framing themselves as the \"good sport\" in the race, the one who's encouraging everyone else to \"race them to the top\", the one who's making sacrifices on the margin so as to be the \"best of the worst\" — they're actually the ones broadcasting the most powerful signal that racing toward the superintelligence singularity is a sane choice as long as you're making a genuine effort to be the best racer. They're broadcasting a more powerful signal than OpenAI and xAI that being insane is normal and fine.Keith Rabois recently tweeted that \"If Anthropic actually believed their rhetoric about safety, they can always shut down the company. And lobby then.\" I'm not the only one who thinks his logic is correct.My view of Anthropic is, of course, downstream of my worldview that P(AI Doom) is in the double digit percentages. But many people share that worldview, including many current and former Anthropic employees.“The Adolescence of Technology” delegitimizes AI x-risk concernsThe latest chapter in the saga of Anthropic downplaying humanity's odds of surviving near-term superintelligence is Dario's recent essay, “The Adolescence of Technology” (LW thread). I was disappointed with this essay on a number of fronts:Character assassinating \"doomers\" like myself, accusing us of claiming that extinction-level outcomes are “inevitable” and \"thinking in a quasi-religious way\" — well, he either did that, or he attacked some other unnamed subset of doomers while strawmanning the position of the smart well-informed doomers. The particular set of doomers he's responding to was intentionally left ambiguous.Unsubstantiated claims that predictions from theoretical arguments aren't as robust as the predictions he's able to make because of his years of AI-building work.Framing our lack of understanding...",
      "url": "https://www.lesswrong.com/posts/3mZ3MnfE7dFWoQCEb/how-dario-amodei-s-the-adolescence-of-technology",
      "author": "Liron",
      "published": "2026-02-05T21:07:28.464000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Critiques Dario Amodei's 'The Adolescence of Technology' essay as delegitimizing AI x-risk concerns. Argues that Anthropic's framing as the 'responsible racer' actually normalizes the dangerous race toward superintelligence more effectively than OpenAI or xAI do.",
      "importance_score": 42,
      "reasoning": "Engages with an important strategic question about Anthropic's role in AI safety discourse. The critique is pointed and represents a meaningful perspective within the AI safety community. However, it's primarily opinion/commentary rather than analysis with new evidence or frameworks.",
      "themes": [
        "AI Safety",
        "AI Governance",
        "Anthropic",
        "AI Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Critiques Dario Amodei's 'The Adolescence of Technology' essay as delegitimizing AI x-risk concerns. Argues that Anthropic's framing as the 'responsible racer' actually normalizes the dangerous race toward superintelligence more effectively than OpenAI or xAI do.</p>",
      "content_html": "<p>My beef with AnthropicI've long felt that while Anthropic is the most safety-conscious of the frontier AI companies, they're also the most hypocritical enablers of the whole reckless enterprise. By framing themselves as the \"good sport\" in the race, the one who's encouraging everyone else to \"race them to the top\", the one who's making sacrifices on the margin so as to be the \"best of the worst\" — they're actually the ones broadcasting the most powerful signal that racing toward the superintelligence singularity is a sane choice as long as you're making a genuine effort to be the best racer. They're broadcasting a more powerful signal than OpenAI and xAI that being insane is normal and fine.Keith Rabois recently tweeted that \"If Anthropic actually believed their rhetoric about safety, they can always shut down the company. And lobby then.\" I'm not the only one who thinks his logic is correct.My view of Anthropic is, of course, downstream of my worldview that P(AI Doom) is in the double digit percentages. But many people share that worldview, including many current and former Anthropic employees.“The Adolescence of Technology” delegitimizes AI x-risk concernsThe latest chapter in the saga of Anthropic downplaying humanity's odds of surviving near-term superintelligence is Dario's recent essay, “The Adolescence of Technology” (LW thread). I was disappointed with this essay on a number of fronts:Character assassinating \"doomers\" like myself, accusing us of claiming that extinction-level outcomes are “inevitable” and \"thinking in a quasi-religious way\" — well, he either did that, or he attacked some other unnamed subset of doomers while strawmanning the position of the smart well-informed doomers. The particular set of doomers he's responding to was intentionally left ambiguous.Unsubstantiated claims that predictions from theoretical arguments aren't as robust as the predictions he's able to make because of his years of AI-building work.Framing our lack of understanding...</p>"
    },
    {
      "id": "9362d3a6c57e",
      "title": "Spectral Signatures of Gradual Disempowerment",
      "content": "TL;DRAI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance.IntroductionAI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases.The stubborn part is that it operates across institutional boundaries simultaneously. Regulate algorithmic trading to maintain human oversight of markets, and competitive pressure shifts to network dynamics — whoever shapes information flow shapes what traders believe before they trade. Address attention capture in social networks, and the pressure migrates to governance advisory relationships. The problem flows around single-domain interventions like water finding cracks.Yet our analytical tools respect exactly those domain boundaries. Economists model markets with one formalism. Network scientists study information diffusion with another. Political scientists analyze voting with a third. Each captures something real. None can describe what happens when AI systems alter the dynamics across all three simultaneously.We think markets, networks, and democratic systems are structurally more similar than they appear. They can all be described as message-passing protocols on graph structures — nodes are participating agents, edges are channels through which influence flows, and what var...",
      "url": "https://www.lesswrong.com/posts/erTAgDriWAw3evecP/spectral-signatures-of-gradual-disempowerment",
      "author": "Jonas Hallgren",
      "published": "2026-02-06T10:08:08.545000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes using spectral graph theory metrics (spectral gap, Fiedler vector, eigenvalue distributions) as cross-domain measures for tracking gradual human disempowerment as AI systems enter coordination systems like markets, networks, and governance institutions.",
      "importance_score": 38,
      "reasoning": "Addresses an important concern (gradual human disempowerment) with a potentially useful mathematical framework. However, the proposal is largely theoretical without empirical validation. The connection between spectral graph metrics and actual disempowerment dynamics needs much more development.",
      "themes": [
        "AI Governance",
        "AI Safety",
        "Human Disempowerment",
        "Network Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using spectral graph theory metrics (spectral gap, Fiedler vector, eigenvalue distributions) as cross-domain measures for tracking gradual human disempowerment as AI systems enter coordination systems like markets, networks, and governance institutions.</p>",
      "content_html": "<p>TL;DRAI disempowerment operates across markets, networks, and governance simultaneously, but our analytical tools don't cross those boundaries. We propose spectral graph metrics—spectral gap, Fiedler vector, eigenvalue distribution—as computable, cross-domain measures for tracking how the balance of influence shifts when AI enters coordination systems, and identify three specific quantities to monitor for AI governance.IntroductionAI systems are changing how society coordinates — across markets, networks, governance institutions, scientific communities, all at once. The gradual disempowerment thesis captures why this is hard to address: human influence over collective outcomes can erode slowly, through ordinary competitive dynamics, without any single dramatic failure. AI systems become better at navigating coordination mechanisms, and the effective weight of human agency quietly decreases.The stubborn part is that it operates across institutional boundaries simultaneously. Regulate algorithmic trading to maintain human oversight of markets, and competitive pressure shifts to network dynamics — whoever shapes information flow shapes what traders believe before they trade. Address attention capture in social networks, and the pressure migrates to governance advisory relationships. The problem flows around single-domain interventions like water finding cracks.Yet our analytical tools respect exactly those domain boundaries. Economists model markets with one formalism. Network scientists study information diffusion with another. Political scientists analyze voting with a third. Each captures something real. None can describe what happens when AI systems alter the dynamics across all three simultaneously.We think markets, networks, and democratic systems are structurally more similar than they appear. They can all be described as message-passing protocols on graph structures — nodes are participating agents, edges are channels through which influence flows, and what var...</p>"
    },
    {
      "id": "0a98975db3e1",
      "title": "Strategy of von Neumann and strategy of Rosenbergs",
      "content": "This is not a call for espionage, but an analysis of another strategyVon Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project).My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them. We know this strategy worked for nearly 80 years without nuclear war. (There were other motives too, like sympathy for communism, but we're simplifying.)Both of these strategies are applicable to the AI race.The von Neumann strategy involves creating American AI as quickly as possible to outpace China (as well as creating Grok AI to outpace OpenAI, etc.)The Rosenberg strategy assumes that defectors will share AI secrets between AI companies, thereby reducing any single AI company's advantage over others, resulting in ever...",
      "url": "https://www.lesswrong.com/posts/orJPh4QdCicDt7c3E/strategy-of-von-neumann-and-strategy-of-rosenbergs",
      "author": "avturchin",
      "published": "2026-02-06T17:50:12.704000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An essay drawing an analogy between Cold War nuclear proliferation strategies (von Neumann's 'strike first' vs. the Rosenbergs' 'share the technology') and potential approaches to AI development concentration. The author explores whether distributing powerful AI capabilities widely could be a stability strategy analogous to nuclear proliferation.",
      "importance_score": 25,
      "reasoning": "Speculative geopolitical analogy rather than technical research. While the framing is novel, the analysis is informal and the connection to AI strategy is loose. Limited actionable insight.",
      "themes": [
        "AI Governance",
        "AI Strategy",
        "Existential Risk"
      ],
      "continuation": null,
      "summary_html": "<p>An essay drawing an analogy between Cold War nuclear proliferation strategies (von Neumann's 'strike first' vs. the Rosenbergs' 'share the technology') and potential approaches to AI development concentration. The author explores whether distributing powerful AI capabilities widely could be a stability strategy analogous to nuclear proliferation.</p>",
      "content_html": "<p>This is not a call for espionage, but an analysis of another strategyVon Neumann's strategy for solving the problem of global nuclear weapons proliferation is widely known - strike tomorrow. That is, conquer the entire world by exploiting that brief window when only one side possesses nuclear weapons. This idea is popular among American readers, partly because personal interests for the US correlate with this strategy: It would be good for the world and for us. (I will not discuss here whether von Neumann actually asserted this or developed this strategy in detail - there are doubts - nor how feasible it was given that the USSR would have launched a conventional attack in Europe in response, meaning the bulk of nuclear strikes would have fallen on Western Europe against the advancing armies - nor that the US lacked precise information about whether the USSR had an atomic bomb - the USSR claimed it had one since 1947, but many believed it wouldn't until 1960, meaning there was time for a von Neumann attack - and finally that before 1949, the number of atomic bombs in US possession might have been insufficient to reliably halt the Soviet nuclear project).My point is that an alternative project for solving the nuclear weapons problem was operating in parallel. This was the effort of the Rosenbergs and several others to transfer nuclear secrets to the USSR as quickly as possible, so that both sides would be equal and a balance would exist between them. We know this strategy worked for nearly 80 years without nuclear war. (There were other motives too, like sympathy for communism, but we're simplifying.)Both of these strategies are applicable to the AI race.The von Neumann strategy involves creating American AI as quickly as possible to outpace China (as well as creating Grok AI to outpace OpenAI, etc.)The Rosenberg strategy assumes that defectors will share AI secrets between AI companies, thereby reducing any single AI company's advantage over others, resulting in ever...</p>"
    },
    {
      "id": "6b90cc047167",
      "title": "Why ASI Might Preserve Its Progenitors",
      "content": "SummaryEven a misaligned Earth-originating artificial superintelligence (ASI) could have instrumentally rational reasons, under multiple decision theories, to preserve rather than destroy humanity. This would depend on the ASI assigning non-negligible probability to the existence of future observers (e.g. intelligent aliens, their ASIs, or simulators).IntroductionWe may build AI so powerful that it is better than us at everything: call this artificial superintelligence (ASI). We may build one ASI, which quickly gains a decisive strategic advantage; we may build several ASIs, which compete for resources. Exactly how ASI(s) might interact with humans and each other is a matter of intense debate, and this post argues that an ASI’s treatment of humanity could depend on whether it expects to encounter other intelligences, either on Earth or elsewhere. What do we mean by “elsewhere”? Robin Hanson’s “grabby aliens” model suggests we might be early in cosmic history, implying other civilisations are likely to appear later.How might an Earth-originating ASI behave toward humanity, if it reasons that the universe could be populated by fast-expanding and competitive civilisations? This post explores several reasons an ASI might keep its creators around, a policy I call “preserve progenitors.”By adopting a policy of restraint toward its creators, the ASI could demonstrate stability, predictability, or prosociality in ways that are legible to current and future observers, including other advanced civilisations. This has two implications for existential risk:Preserving progenitors may reduce the risk of conflict in a galaxy where the ASI will likely encounter expansionists by default, and where trust is hard to establish.If the ASI is uncertain about the structure of the universe or the norms governing inter-civilisational interaction, delaying irreversible actions (such as destroying humanity) could preserve valuable strategic flexibility.Caveats and assumptionsThis post is not ...",
      "url": "https://www.lesswrong.com/posts/4fYgfngqqFaYN4dxh/why-asi-might-preserve-its-progenitors",
      "author": "Luke J. Dawes",
      "published": "2026-02-05T21:54:48.276000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that even a misaligned ASI might have instrumental reasons to preserve humanity, particularly if it assigns non-negligible probability to future observers (aliens, simulators) who might judge its behavior. Draws on decision theory and Hanson's grabby aliens model.",
      "importance_score": 22,
      "reasoning": "Speculative argument that relies on strong assumptions about ASI decision-making and the existence of future observers. While the reasoning is somewhat novel, it's highly contingent and doesn't significantly advance alignment strategy. Similar arguments have been explored before.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "Decision Theory",
        "ASI Speculation"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that even a misaligned ASI might have instrumental reasons to preserve humanity, particularly if it assigns non-negligible probability to future observers (aliens, simulators) who might judge its behavior. Draws on decision theory and Hanson's grabby aliens model.</p>",
      "content_html": "<p>SummaryEven a misaligned Earth-originating artificial superintelligence (ASI) could have instrumentally rational reasons, under multiple decision theories, to preserve rather than destroy humanity. This would depend on the ASI assigning non-negligible probability to the existence of future observers (e.g. intelligent aliens, their ASIs, or simulators).IntroductionWe may build AI so powerful that it is better than us at everything: call this artificial superintelligence (ASI). We may build one ASI, which quickly gains a decisive strategic advantage; we may build several ASIs, which compete for resources. Exactly how ASI(s) might interact with humans and each other is a matter of intense debate, and this post argues that an ASI’s treatment of humanity could depend on whether it expects to encounter other intelligences, either on Earth or elsewhere. What do we mean by “elsewhere”? Robin Hanson’s “grabby aliens” model suggests we might be early in cosmic history, implying other civilisations are likely to appear later.How might an Earth-originating ASI behave toward humanity, if it reasons that the universe could be populated by fast-expanding and competitive civilisations? This post explores several reasons an ASI might keep its creators around, a policy I call “preserve progenitors.”By adopting a policy of restraint toward its creators, the ASI could demonstrate stability, predictability, or prosociality in ways that are legible to current and future observers, including other advanced civilisations. This has two implications for existential risk:Preserving progenitors may reduce the risk of conflict in a galaxy where the ASI will likely encounter expansionists by default, and where trust is hard to establish.If the ASI is uncertain about the structure of the universe or the norms governing inter-civilisational interaction, delaying irreversible actions (such as destroying humanity) could preserve valuable strategic flexibility.Caveats and assumptionsThis post is not ...</p>"
    },
    {
      "id": "4073bc594e74",
      "title": "Proposal: A Framework for Discovering Alien Physics via Optimal Compression",
      "content": "ForewordThis is an idea that has been sitting on my hard drive for a few months. I like it enough to finally share it.I won’t pretend AI hasn’t been involved in helping shape the presentation. What I do claim is that the core idea and its overall structure (a rough blueprint for a human–AI discovery loop) is well beyond what current AI systems can generate on their own.This is not a text designed for quick consumption. It’s dense, and probably uneven in places. Still, I’d be surprised if there weren’t at least a few people here for whom the underlying idea resonates, even if the presentation itself leaves room for improvement.If the text in any way inspires someone, or serves as an interesting read, that alone would make sharing it worthwhile.&nbsp;Executive SummaryWhat if our fundamental physics equations aren't THE laws of nature, but merely ONE good compression among many? This paper proposes training neural networks to discover physical laws by treating physics as a data compression problem: find the most computationally efficient rules that predict experimental outcomes within measurement uncertainty. Unlike existing automated discovery systems that search for equations matching human physics, our framework might reveal that F=ma is suboptimal, that we've chosen the wrong fundamental units (why m/s instead of s/m?), or that radically different mathematical frameworks compress nature more efficiently. By maintaining multiple valid compressions optimized for different contexts—just as we keep Newton despite having Einstein—the system acknowledges that physical laws are supremely useful correlations with defined domains, not metaphysical truths. Early validation on classical mechanics could lead to computationally revolutionary reformulations of quantum field theory, or even reveal why human physics took its particular historical path when equally valid alternatives existed all along. In this paper the term optimal is meant to be read as aspirational, rather than ...",
      "url": "https://www.lesswrong.com/posts/QkmkbegFvB7dkDS5X/proposal-a-framework-for-discovering-alien-physics-via",
      "author": "David Björling",
      "published": "2026-02-06T13:42:38.974000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Proposes treating physics discovery as an optimal data compression problem, training neural networks to find computationally efficient rules that predict experimental outcomes. The framework imagines discovering 'alien physics'—equivalent but structurally different formulations of physical laws.",
      "importance_score": 20,
      "reasoning": "An interesting conceptual proposal but self-described as speculative, AI-assisted in presentation, and lacking implementation or results. The core idea (physics as compression) is not new (Solomonoff, Schmidhuber). No empirical validation.",
      "themes": [
        "AI for Science",
        "Scientific Discovery",
        "Information Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes treating physics discovery as an optimal data compression problem, training neural networks to find computationally efficient rules that predict experimental outcomes. The framework imagines discovering 'alien physics'—equivalent but structurally different formulations of physical laws.</p>",
      "content_html": "<p>ForewordThis is an idea that has been sitting on my hard drive for a few months. I like it enough to finally share it.I won’t pretend AI hasn’t been involved in helping shape the presentation. What I do claim is that the core idea and its overall structure (a rough blueprint for a human–AI discovery loop) is well beyond what current AI systems can generate on their own.This is not a text designed for quick consumption. It’s dense, and probably uneven in places. Still, I’d be surprised if there weren’t at least a few people here for whom the underlying idea resonates, even if the presentation itself leaves room for improvement.If the text in any way inspires someone, or serves as an interesting read, that alone would make sharing it worthwhile.&nbsp;Executive SummaryWhat if our fundamental physics equations aren't THE laws of nature, but merely ONE good compression among many? This paper proposes training neural networks to discover physical laws by treating physics as a data compression problem: find the most computationally efficient rules that predict experimental outcomes within measurement uncertainty. Unlike existing automated discovery systems that search for equations matching human physics, our framework might reveal that F=ma is suboptimal, that we've chosen the wrong fundamental units (why m/s instead of s/m?), or that radically different mathematical frameworks compress nature more efficiently. By maintaining multiple valid compressions optimized for different contexts—just as we keep Newton despite having Einstein—the system acknowledges that physical laws are supremely useful correlations with defined domains, not metaphysical truths. Early validation on classical mechanics could lead to computationally revolutionary reformulations of quantum field theory, or even reveal why human physics took its particular historical path when equally valid alternatives existed all along. In this paper the term optimal is meant to be read as aspirational, rather than ...</p>"
    },
    {
      "id": "d80b7b2c5080",
      "title": "If all humans were turned into high-fidelity mind uploads tomorrow, would we be self-sustaining?",
      "content": "That is, would we in some sense manage to survive, in the longer term? Presumably we would have to maintain the physical substrate we are running on, by providing power and cooling, and by eventually replacing our hardware. I think this question could help to answer whether AGI as common defined (\"all cognitive labour\") would be the same as or different from what would be required by Vitalik Buterin's definition: \"AGI\" is AI powerful enough that, if one day all humans suddenly disappeared, and the AI was uploaded into robot bodies, it would be able to independently continue civilization.",
      "url": "https://www.lesswrong.com/posts/fEovepBTH2f4eiKA4/if-all-humans-were-turned-into-high-fidelity-mind-uploads",
      "author": "Erich_Grunewald",
      "published": "2026-02-06T03:35:26.725000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A thought experiment asking whether humanity, if instantly converted to mind uploads, could sustain itself by maintaining hardware, power, and cooling—framed as a way to operationalize what 'AGI-level capability' actually means compared to Vitalik Buterin's definition.",
      "importance_score": 15,
      "reasoning": "Brief philosophical thought experiment without substantial analysis or conclusions. Interesting framing but too short and underdeveloped to provide significant insight.",
      "themes": [
        "AGI Definitions",
        "Philosophy of Mind",
        "Digital Minds"
      ],
      "continuation": null,
      "summary_html": "<p>A thought experiment asking whether humanity, if instantly converted to mind uploads, could sustain itself by maintaining hardware, power, and cooling—framed as a way to operationalize what 'AGI-level capability' actually means compared to Vitalik Buterin's definition.</p>",
      "content_html": "<p>That is, would we in some sense manage to survive, in the longer term? Presumably we would have to maintain the physical substrate we are running on, by providing power and cooling, and by eventually replacing our hardware. I think this question could help to answer whether AGI as common defined (\"all cognitive labour\") would be the same as or different from what would be required by Vitalik Buterin's definition: \"AGI\" is AI powerful enough that, if one day all humans suddenly disappeared, and the AI was uploaded into robot bodies, it would be able to independently continue civilization.</p>"
    },
    {
      "id": "62720b11393d",
      "title": "Parks Aren't Nature",
      "content": "I.I love dogs.I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy!”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this.Why do I love dogs?They’re loyal and loving and giving, and even though they bark at passing cars and occasionally pee on the carpet having them in my life makes it unquestionably better.The thing is, dogs as they exist today are a lot of things, but they aren’t natural.Nature didn’t shape dogs, didn’t produce the breeds we see every day. It wasn’t like Darwin went to an island and found that a species of wolf had been separated by a mountain chain and on one side were Golden Retrievers and the other Yorkshire Terriers.Dogs exist today as the result of millennia of co-adaptation and selective breeding by humans. They’re animals, yes, and Nature technically made the base form, but we humans molded them into shapes more compatible with us. Most dogs are very safe to have around humans.But there is an animal that is a more natural Canid: Wolves.And wolves are a lot of things, but they’re not pets. They aren’t domesticated; they aren’t bred for cuddliness and kisses. A wolf will hurt and kill and eat you.Wolves are wild animals in their state of nature, red in tooth and claw.The thing is, this distinction between dogs and wolves - between nature tamed and nature wild - this matters, when we think about who we humans are and what we want the world around us to look like. We might say we enjoy the natural world, might want less deforestation and more green spaces, but I’ve yet to meet anyone who wants actual wolves running around their neighborhood. We might go to farm-to-table restaurants and only eat organic, free-range eggs, but chickens mostly don’t exist in the wild for good reason.In a first-world country, or even in any populous city, almost ev...",
      "url": "https://www.lesswrong.com/posts/cjxPFxAe5WRKA6SeF/parks-aren-t-nature",
      "author": "Sable",
      "published": "2026-02-06T13:27:05.457000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An essay using the analogy of dogs vs. wolves and parks vs. wilderness to argue that managed, 'domesticated' nature (parks) is fundamentally different from wild nature, with implications for how we think about environmental preservation and human relationship with the natural world.",
      "importance_score": 5,
      "reasoning": "Not related to AI research. A philosophical/environmental essay on LessWrong. No technical content or AI relevance.",
      "themes": [
        "Philosophy",
        "Environment"
      ],
      "continuation": null,
      "summary_html": "<p>An essay using the analogy of dogs vs. wolves and parks vs. wilderness to argue that managed, 'domesticated' nature (parks) is fundamentally different from wild nature, with implications for how we think about environmental preservation and human relationship with the natural world.</p>",
      "content_html": "<p>I.I love dogs.I grew up in a two-dog household, and my future plans have always included at least one dog. When I pass a dog on the street, I often point and exclaim “Puppy!”, no matter how inappropriate it is for a grown man to do so, because all dogs are puppies and all puppies are adorable and I need everyone to know this.Why do I love dogs?They’re loyal and loving and giving, and even though they bark at passing cars and occasionally pee on the carpet having them in my life makes it unquestionably better.The thing is, dogs as they exist today are a lot of things, but they aren’t natural.Nature didn’t shape dogs, didn’t produce the breeds we see every day. It wasn’t like Darwin went to an island and found that a species of wolf had been separated by a mountain chain and on one side were Golden Retrievers and the other Yorkshire Terriers.Dogs exist today as the result of millennia of co-adaptation and selective breeding by humans. They’re animals, yes, and Nature technically made the base form, but we humans molded them into shapes more compatible with us. Most dogs are very safe to have around humans.But there is an animal that is a more natural Canid: Wolves.And wolves are a lot of things, but they’re not pets. They aren’t domesticated; they aren’t bred for cuddliness and kisses. A wolf will hurt and kill and eat you.Wolves are wild animals in their state of nature, red in tooth and claw.The thing is, this distinction between dogs and wolves - between nature tamed and nature wild - this matters, when we think about who we humans are and what we want the world around us to look like. We might say we enjoy the natural world, might want less deforestation and more green spaces, but I’ve yet to meet anyone who wants actual wolves running around their neighborhood. We might go to farm-to-table restaurants and only eat organic, free-range eggs, but chickens mostly don’t exist in the wild for good reason.In a first-world country, or even in any populous city, almost ev...</p>"
    },
    {
      "id": "a3083e5f68e8",
      "title": "Plan 'Straya",
      "content": "Plan 'Straya: A Comprehensive Alignment StrategyVersion 0.3 — DRAFT — Not For Distribution Outside The PubEpistemic status: High confidence, low evidence. Consistent with community norms.Executive SummaryExisting alignment proposals suffer from a shared flaw: they assume you can solve the control problem before the catastrophe. Plan 'Straya boldly inverts this. We propose achieving alignment the way humanity has historically achieved most of its moral progress — by first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.The plan proceeds in three rigorously defined phases.Phase 1: Anticorruption Measures (Kinetic)The scholarly literature on AI governance emphasises that institutional integrity is a prerequisite for safe deployment. We agree. Where we diverge from the mainstream is on methodology.Most proposals suggest \"regulatory frameworks\" and \"oversight bodies.\" The NIST AI Risk Management Framework provides a voluntary set of guidelines that organisations may choose to follow, partially follow, or simply reference in press releases. The EU AI Act classifies systems into risk tiers with the quiet confidence of a taxonomy that will be obsolete before its implementing regulations are finalised. The Frontier Model Forum, meanwhile, brings together the leading AI laboratories in a spirit of cooperative self-governance, a phrase which here means \"a shared Google Doc and quarterly meetings in San Francisco.\"These approaches share a well-documented failure mode: the people staffing them are, in technical terms, politicians. Plan 'Straya addresses this via what we call \"a vigorous personnel restructuring of the Australian federal and state governments,\" targeting specifically those members identified as corrupt.We acknowledge that the identification mechanism — determining which officials are corrupt — is itself an alignment problem. Specifically, it requires specifying a value function (\"not corrupt...",
      "url": "https://www.lesswrong.com/posts/8nHeiHaQbcnRcb38m/plan-straya",
      "author": "William the Kiwi ",
      "published": "2026-02-05T19:14:18.340000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A satirical 'alignment plan' written in Australian vernacular, parodying both AI alignment proposals and Australian culture. Proposes achieving alignment by making every possible mistake first and writing strongly-worded resolutions afterward.",
      "importance_score": 5,
      "reasoning": "Satire/humor piece. No technical content or genuine research contribution. Entertaining but not substantive.",
      "themes": [
        "Satire",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>A satirical 'alignment plan' written in Australian vernacular, parodying both AI alignment proposals and Australian culture. Proposes achieving alignment by making every possible mistake first and writing strongly-worded resolutions afterward.</p>",
      "content_html": "<p>Plan 'Straya: A Comprehensive Alignment StrategyVersion 0.3 — DRAFT — Not For Distribution Outside The PubEpistemic status: High confidence, low evidence. Consistent with community norms.Executive SummaryExisting alignment proposals suffer from a shared flaw: they assume you can solve the control problem before the catastrophe. Plan 'Straya boldly inverts this. We propose achieving alignment the way humanity has historically achieved most of its moral progress — by first making every possible mistake, losing nearly everything, and then writing a strongly-worded resolution about it afterward.The plan proceeds in three rigorously defined phases.Phase 1: Anticorruption Measures (Kinetic)The scholarly literature on AI governance emphasises that institutional integrity is a prerequisite for safe deployment. We agree. Where we diverge from the mainstream is on methodology.Most proposals suggest \"regulatory frameworks\" and \"oversight bodies.\" The NIST AI Risk Management Framework provides a voluntary set of guidelines that organisations may choose to follow, partially follow, or simply reference in press releases. The EU AI Act classifies systems into risk tiers with the quiet confidence of a taxonomy that will be obsolete before its implementing regulations are finalised. The Frontier Model Forum, meanwhile, brings together the leading AI laboratories in a spirit of cooperative self-governance, a phrase which here means \"a shared Google Doc and quarterly meetings in San Francisco.\"These approaches share a well-documented failure mode: the people staffing them are, in technical terms, politicians. Plan 'Straya addresses this via what we call \"a vigorous personnel restructuring of the Australian federal and state governments,\" targeting specifically those members identified as corrupt.We acknowledge that the identification mechanism — determining which officials are corrupt — is itself an alignment problem. Specifically, it requires specifying a value function (\"not corrupt...</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}