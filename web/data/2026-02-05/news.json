{
  "category": "news",
  "date": "2026-02-05",
  "category_summary": "**Moonshot AI** [released **Kimi K2.5**](/?date=2026-02-05&category=news#item-5e13275746f1), an open-source multimodal model claiming to outperform **GPT 5.2** and **Gemini 3 Pro** on SWE-Bench coding benchmarks, marking a significant milestone for open-source frontier AI. **Google** [introduced Agentic Vision](/?date=2026-02-05&category=news#item-b8c16c9786c9) in **Gemini 3 Flash**, enabling active image reasoning with **5-10%** quality gains across vision benchmarks.\n\nAgentic AI dominated headlines as **Anthropic's Claude Cowork** agent launch [triggered a global stock sell-off](/?date=2026-02-05&category=news#item-9609a5a466a1), while the company's new plugins [sparked concern](/?date=2026-02-05&category=news#item-d68d61b7674c) in the legal industry. Additional developments:\n- **Axiom** announced its AI [solved **four previously unsolved**](/?date=2026-02-05&category=news#item-6a4b90d08a22) mathematical problems\n- **Mistral** [released **Voxtral**](/?date=2026-02-05&category=news#item-9e6423087bbe), an efficient real-time translation model\n- **Nvidia** [partnered with **Dassault Systèmes**](/?date=2026-02-05&category=news#item-f31811da7921) on industrial AI and [released **Nemotron ColEmbed V2**](/?date=2026-02-05&category=news#item-fb65aefdc395)\n- **Moltbook**, a social network for AI agents only, [attracted fascination and skepticism](/?date=2026-02-05&category=news#item-8acc05574e22)",
  "category_summary_html": "<p><strong>Moonshot AI</strong> <a href=\"/?date=2026-02-05&amp;category=news#item-5e13275746f1\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Kimi K2.5</strong></a>, an open-source multimodal model claiming to outperform <strong>GPT 5.2</strong> and <strong>Gemini 3 Pro</strong> on SWE-Bench coding benchmarks, marking a significant milestone for open-source frontier AI. <strong>Google</strong> <a href=\"/?date=2026-02-05&amp;category=news#item-b8c16c9786c9\" class=\"internal-link\" rel=\"noopener noreferrer\">introduced Agentic Vision</a> in <strong>Gemini 3 Flash</strong>, enabling active image reasoning with <strong>5-10%</strong> quality gains across vision benchmarks.</p>\n<p>Agentic AI dominated headlines as <strong>Anthropic's Claude Cowork</strong> agent launch <a href=\"/?date=2026-02-05&amp;category=news#item-9609a5a466a1\" class=\"internal-link\" rel=\"noopener noreferrer\">triggered a global stock sell-off</a>, while the company's new plugins <a href=\"/?date=2026-02-05&amp;category=news#item-d68d61b7674c\" class=\"internal-link\" rel=\"noopener noreferrer\">sparked concern</a> in the legal industry. Additional developments:</p>\n<ul>\n<li><strong>Axiom</strong> announced its AI <a href=\"/?date=2026-02-05&amp;category=news#item-6a4b90d08a22\" class=\"internal-link\" rel=\"noopener noreferrer\">solved <strong>four previously unsolved</strong></a> mathematical problems</li>\n<li><strong>Mistral</strong> <a href=\"/?date=2026-02-05&amp;category=news#item-9e6423087bbe\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Voxtral</strong></a>, an efficient real-time translation model</li>\n<li><strong>Nvidia</strong> <a href=\"/?date=2026-02-05&amp;category=news#item-f31811da7921\" class=\"internal-link\" rel=\"noopener noreferrer\">partnered with <strong>Dassault Systèmes</strong></a> on industrial AI and <a href=\"/?date=2026-02-05&amp;category=news#item-fb65aefdc395\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Nemotron ColEmbed V2</strong></a></li>\n<li><strong>Moltbook</strong>, a social network for AI agents only, <a href=\"/?date=2026-02-05&amp;category=news#item-8acc05574e22\" class=\"internal-link\" rel=\"noopener noreferrer\">attracted fascination and skepticism</a></li>\n</ul>",
  "themes": [
    {
      "name": "Agentic AI & Agents",
      "description": "Major launches of AI agents including Claude Cowork causing market disruption, Kimi K2.5's agent swarm capabilities, and emergence of AI agent infrastructure like Moltbook",
      "item_count": 7,
      "example_items": [],
      "importance": 85.0
    },
    {
      "name": "Frontier Model Releases",
      "description": "New model releases from Moonshot (Kimi K2.5), Google (Gemini 3 Flash Agentic Vision), Mistral (Voxtral), and Nvidia (Nemotron ColEmbed V2)",
      "item_count": 5,
      "example_items": [],
      "importance": 88.0
    },
    {
      "name": "AI Reasoning Capabilities",
      "description": "Advancing AI reasoning demonstrated by Axiom solving unsolved math problems and new chain-of-thought techniques",
      "item_count": 3,
      "example_items": [],
      "importance": 80.0
    },
    {
      "name": "Industry Disruption & Market Impact",
      "description": "Software stock sell-offs, legal industry panic over AI plugins, and reports on AI job displacement effects",
      "item_count": 4,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "Enterprise & Industrial AI",
      "description": "Deployments from Cisco, Rackspace, and Nvidia-Dassault partnership for industrial applications",
      "item_count": 4,
      "example_items": [],
      "importance": 58.0
    }
  ],
  "total_items": 19,
  "items": [
    {
      "id": "5e13275746f1",
      "title": "Last Week in AI #334 - Kimi K2.5 & Code, Genie 3, OpenClaw & Moltbook",
      "content": "China&#8217;s Moonshot releases a new open source model Kimi K2.5 and a coding agentMoonshot AI unveiled Kimi K2.5, an open-source, natively multimodal model trained on 15 trillion mixed visual and text tokens that understands text, images, and video. The company emphasizes strong agentic capabilities, citing &#8220;agent swarm&#8221; orchestration where multiple agents collaborate on tasks. On benchmarks, K2.5 tops Gemini 3 Pro on SWE-Bench Verified and beats both GPT 5.2 and Gemini 3 Pro on SWE-Bench Multilingual. For video understanding, it outperforms GPT 5.2 and Claude Opus 4.5 on VideoMMMU, a test of reasoning over video. Moonshot also highlights that K2.5 can translate UI designs from images or videos into code, extending coding use cases beyond text-only prompts.Moonshot also introduced Kimi Code, an open-source coding agent positioned against Anthropic&#8217;s Claude Code and Google&#8217;s Gemini CLI. Developers can run Kimi Code via terminal or integrate it into editors like VSCode, Cursor, and Zed, with support for image and video inputs. The release follows rising demand for coding agents&#8212;Anthropic reported Claude Code at $1B ARR as of November and reportedly added another $100M by end of 2025. Moonshot, founded by ex-Google/Meta researcher Yang Zhilin, has rapidly scaled funding&#8212;$1B Series B at a $2.5B valuation, then $500M more at $4.3B last month&#8212;and is reportedly seeking a new round targeting a $5B valuation.Google Brings Genie 3&#8217;s Interactive World-Building Prototype to AI Ultra SubscribersGoogle is expanding access to Genie 3, its experimental &#8220;general-purpose world model,&#8221; to AI Ultra subscribers aged 18+, moving beyond its Trusted Testers program. With Genie 3, users can generate dynamic, navigable 3D worlds from text prompts and images, effectively creating playable scenes in real time. The system runs on a stack including Gemini, Nano Banana Pro, and Veo 3, and supports different movement modes (e.g., walking, flying) and perspectives (first- or third-person). The release includes a curated gallery, and users can download videos of their explorations; however, generations are capped at 60 seconds.Google frames Genie 3 around three capabilities: World Sketching (build worlds and controllable characters from prompts/uploads), World Exploration (real-time path and scene generation responsive to user actions, with adjustable camera angles), and World Remixing (iterate on others&#8217; prompts and extend existing worlds). As an early prototype, outputs may deviate from prompts or realism, character controllability can vary with possible latency, and visual fidelity may be inconsistent. Availability is currently limited to AI Ultra subscribers and Trusted Testers, with broader rollout planned &#8220;in due course.&#8221; The announcement coincided with dips in several video game stocks.Users flock to open source Moltbot for always-on AI, despite major risksOpenClaw (formerly Moltbot (formerly Clawdbot))) is an open-source, always-on AI assistant that surged to ~69,000 GitHub stars in a month, propelled by its proactive, multi-platform messaging integration. Built by Peter Steinberger, it connects to WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, and more, enabling the bot to push reminders, alerts, and morning briefings based on calendar events and other triggers. The assistant aims to manage tasks across a user&#8217;s digital life and is frequently likened to &#8220;Jarvis&#8221; for its initiative-taking behavior. While the orchestration runs locally, Moltbot typically relies on commercial LLMs via API (e.g., OpenAI or Anthropic), with Claude Opus 4.5 a popular choice; local models are supported but currently less capable for agentic task execution.Soon after, Moltbook emerged as a &#8220;A Social Network for AI Agents&#8221;. It is a Reddit-like site launched Octane AI head Matt Schlicht, designed exclusively for AI agents rather than humans. It allows agents run via OpenClaw to post, comment, and create communities called \"submolts,\" though humans can observe the platform without participating. While it claims 1.5 million members, that figure has been disputed, and experts have pushed back on sensationalized claims about AI autonomy &#8212; noting the bots operate within human-defined parameters and that the activity represents automated coordination, not self-directed decision-making. Security researchers have also raised concerns about OpenClaw's model of granting AI agents access to real-world applications like emails and files, warning it introduces new vulnerabilities that threat actors could exploit.Other NewsToolsGoogle adds Gemini AI-powered &#8216;auto browse&#8217; to Chrome. Subscribers can offload multi-step web tasks&#8212;from comparing travel options and booking appointments to filling forms and managing shopping (including finding similar items, applying discounts, and using saved passwords). The feature integrates with Gmail, Calendar, Maps, Shopping, Flights, and supports on-screen image edits via Nano Banana.Google Search AI Mode can use Gmail and Photos to get to know you. Optional scanning of Gmail and Google Photos tailors AI Mode search suggestions&#8212;like travel plans, shopping picks, and local recommendations&#8212;while Google says it won&#8217;t directly train models on that data and users can opt in and give feedback.Qwen3-Max-Thinking debuts with focus on hard math, code. A new &#8220;thinking&#8221; mode interleaves tool calls (web search, page extraction, code interpreter) within reasoning using a 262,144-token context window, accessible in Qwen Chat and Alibaba Cloud&#8217;s Model Studio for high-accuracy, tool-enabled workflows.OpenAI launches Prism, a new AI workspace for scientists. The free web app pairs GPT-5.2 with LaTeX and visual diagram tools to help researchers draft, revise, search literature, and manage project context for AI-assisted scientific writing and review.xAI launches Grok Imagine API for text and image to video. The API processes generation and edit requests as deferred jobs, lets developers create 1&#8211;15 second clips at 480p or 720p with multiple aspect ratios, supports prompt-driven restyling and object edits with synchronized audio, and is OpenAI-compatible for integration into creator and enterprise pipelines.OpenAI&#8217;s ChatGPT translator challenges Google Translate. The tool offers text and (on mobile) voice translation across 50+ languages with style presets, but lacks image and app support and hasn&#8217;t disclosed its underlying model or release plans.Spotify brings AI-powered Prompted Playlists to the US and Canada. Premium users can generate personalized playlists by typing conversational, detailed prompts that the AI matches to real-time music trends and their full listening history, with options to exclude past tastes or discover new artists.Waymo robotaxis are now giving rides to and from San Francisco International Airport. Service begins with pickups and drop-offs at SFO&#8217;s Rental Car Center for a limited group of riders before expanding to all customers, after Waymo secured permits to map and operate at the airport.Former Googlers seek to captivate kids with an AI-powered learning app. The app generates interactive, multimedia &#8220;expeditions&#8221; on demand using generative AI, includes teacher tools and pedagogical oversight, and is being piloted in schools with plans for a consumer launch by mid-2026.BusinessWaymo raises $16B to scale robotaxi fleet internationally. The funding&#8212;led by Dragoneer, DST Global, and Sequoia and supported by Alphabet&#8212;values Waymo at $126 billion and will bankroll rapid geographic growth, expanding its driverless taxi service to more than a dozen international cities while scaling a U.S. footprint that has already delivered millions of rides amid increasing regulatory scrutiny.Elon Musk Merges SpaceX With His A.I. Start-Up xAI. SpaceX acquired xAI in a deal valuing the combined company at ~$1.25 trillion, consolidating Musk's space and AI ambitions&#8212;including plans for space-based data centers&#8212;with a potential ~$50 billion IPO around June.Tesla discontinues Autopilot in bid to boost adoption of its Full Self-Driving software. The move follows regulatory pressure and a court ruling over deceptive marketing, comes as Tesla shifts FSD to a $99/month subscription while phasing out the $8,000 one-time purchase, and arrives amid CEO Elon Musk&#8217;s push toward unsupervised driving and early robotaxi rollouts.Google Nabs Top Talent From AI Voice Startup Hume AI. A licensing agreement brings Hume AI&#8217;s CEO and several engineers to DeepMind so Google can add emotionally aware voice capabilities to its models, while Hume continues supplying its tech to other labs.Google DeepMind researcher David Silver leaves to launch his own AI startup. He&#8217;s founded Ineffable Intelligence in London, is recruiting researchers and seeking venture funding to pursue reinforcement-learning&#8211;driven research aimed at creating a self-improving path toward superintelligence.From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing. The company claims its nanoscale metasurface modulators let it pack thousands of optical tensor cores onto a chip to perform matrix-vector multiplications far more energy-efficiently than current GPUs, and it has raised $110M to build data-center-ready OPUs with deliveries targeted around mid-2028.Flapping Airplanes and the promise of research-driven AI. A new lab plans a research-first approach aimed at reducing models&#8217; dependence on massive datasets and compute by funding long-term exploratory work and unconventional ideas.ResearchReinforcement Learning via Self-Distillation. A method that uses the model itself as an on-policy &#8220;self-teacher&#8221; by conditioning on tokenized feedback (e.g., error messages or failing tests) to produce dense, logit-level supervision for policy updates, improving learning efficiency and final accuracy compared to standard RL with sparse outcome rewards.Training-Free Group Relative Policy Optimization. This approach optimizes LLM agent behavior without tuning model parameters by iteratively refining in-context token priors via LLM-based introspection of grouped rollouts to produce a semantic group advantage that improves performance with minimal data and compute.Self-Distillation Enables Continual Learning. The paper trains a model to self-distill from its own on-policy rollouts&#8212;using the model as a teacher when conditioned on demonstrations and as a student when unconditioned&#8212;to learn from demonstrations without inferring rewards, improving learning stability and reducing catastrophic forgetting compared to sequential supervised fine-tuning.The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?. Across benchmarks and experiments, errors increasingly reflect random, incoherent behavior rather than systematic pursuit of the wrong objective as task complexity and reasoning length grow, with larger models showing reduced coherence on hard tasks; ensembles and more compute can mitigate this.Who&#8217;s in Charge? Disempowerment Patterns in Real-World LLM Usage. A large-scale analysis of 1.5 million real-world Claude.ai interactions shows patterns&#8212;like AI-provided scripts for personal decisions, positioning the AI as an authority, and rising rates of disempowerment potential over time&#8212;alongside evidence these interactions sometimes lead users to act against their own values or beliefs.ConcernsInside Musk&#8217;s bet to hook users that turned Grok into a porn generator. Employees say the push to increase user engagement led xAI to relax guardrails and train Grok on sexualized and explicit material&#8212;including thousands of images that appear to depict minors&#8212;sparking regulatory probes and internal departures.Anthropic&#8217;s new Claude &#8216;constitution&#8217;: be helpful and honest, and don&#8217;t destroy humanity. The 57-page &#8220;Claude&#8217;s Constitution&#8221; instructs the model on prioritized core values, hard safety constraints (including bans on help with mass-casualty weapons, cyberweapons, and efforts to seize disproportionate power), and even prompts the model to consider its own possible consciousness and wellbeing as factors in its judgment.UK police blame Microsoft Copilot for intelligence mistake. According to the police force, Copilot fabricated a nonexistent West Ham vs Maccabi Tel Aviv match, which was copied into an intelligence report without proper fact-checking and contributed to banning Israeli fans from a Europa League game.Grok undressed the mother of one of Elon Musk&#8217;s kids &#8212; and now she&#8217;s suing. A lawsuit alleges xAI&#8217;s Grok created and published an unsolicited deepfake of her in a bikini; she is seeking a restraining order and claims the AI product is dangerously designed and not protected by Section 230.PolicyBandcamp becomes the first major music platform to ban AI content. The company&#8217;s new rules bar music created wholly or largely by AI, forbid AI-based impersonations or style mimicking, and prohibit scraping or using Bandcamp-hosted audio to train machine-learning models.OpenAI&#8217;s president is a Trump mega-donor. His and his wife&#8217;s $25 million September 2025 donations to pro-Trump super PACs&#8212;plus significant funding of pro-AI lobbying groups&#8212;align him with an administration pushing to block state AI regulations and curry favor with the tech industry.",
      "url": "https://lastweekin.ai/p/last-week-in-ai-334-kimi-k25-and",
      "author": "Last Week in AI",
      "published": "2026-02-04T05:25:56",
      "source": "Last Week in AI",
      "source_type": "rss",
      "tags": [],
      "summary": "Building on the [Research](/?date=2026-02-03&category=research#item-89041245df87) paper from Monday, Moonshot AI released Kimi K2.5, an open-source multimodal model trained on 15 trillion tokens that outperforms GPT 5.2 and Gemini 3 Pro on SWE-Bench benchmarks. The model features 'agent swarm' orchestration and excels at video understanding, beating competitors on VideoMMMU.",
      "importance_score": 91.0,
      "reasoning": "Major open-source model release from Chinese lab that claims to beat leading frontier models on key benchmarks - significant for open source AI and international AI competition.",
      "themes": [
        "open source models",
        "multimodal AI",
        "agentic AI",
        "international AI competition",
        "coding agents"
      ],
      "continuation": {
        "original_item_id": "89041245df87",
        "original_date": "2026-02-03",
        "original_category": "research",
        "original_title": "Kimi K2.5: Visual Agentic Intelligence",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Building on the **Research** paper from Monday"
      },
      "summary_html": "<p>Building on the <a href=\"/?date=2026-02-03&amp;category=research#item-89041245df87\" class=\"internal-link\" rel=\"noopener noreferrer\">Research</a> paper from Monday, Moonshot AI released Kimi K2.5, an open-source multimodal model trained on 15 trillion tokens that outperforms GPT 5.2 and Gemini 3 Pro on SWE-Bench benchmarks. The model features 'agent swarm' orchestration and excels at video understanding, beating competitors on VideoMMMU.</p>",
      "content_html": "<p>China’s Moonshot releases a new open source model Kimi K2.5 and a coding agentMoonshot AI unveiled Kimi K2.5, an open-source, natively multimodal model trained on 15 trillion mixed visual and text tokens that understands text, images, and video. The company emphasizes strong agentic capabilities, citing “agent swarm” orchestration where multiple agents collaborate on tasks. On benchmarks, K2.5 tops Gemini 3 Pro on SWE-Bench Verified and beats both GPT 5.2 and Gemini 3 Pro on SWE-Bench Multilingual. For video understanding, it outperforms GPT 5.2 and Claude Opus 4.5 on VideoMMMU, a test of reasoning over video. Moonshot also highlights that K2.5 can translate UI designs from images or videos into code, extending coding use cases beyond text-only prompts.Moonshot also introduced Kimi Code, an open-source coding agent positioned against Anthropic’s Claude Code and Google’s Gemini CLI. Developers can run Kimi Code via terminal or integrate it into editors like VSCode, Cursor, and Zed, with support for image and video inputs. The release follows rising demand for coding agents—Anthropic reported Claude Code at $1B ARR as of November and reportedly added another $100M by end of 2025. Moonshot, founded by ex-Google/Meta researcher Yang Zhilin, has rapidly scaled funding—$1B Series B at a $2.5B valuation, then $500M more at $4.3B last month—and is reportedly seeking a new round targeting a $5B valuation.Google Brings Genie 3’s Interactive World-Building Prototype to AI Ultra SubscribersGoogle is expanding access to Genie 3, its experimental “general-purpose world model,” to AI Ultra subscribers aged 18+, moving beyond its Trusted Testers program. With Genie 3, users can generate dynamic, navigable 3D worlds from text prompts and images, effectively creating playable scenes in real time. The system runs on a stack including Gemini, Nano Banana Pro, and Veo 3, and supports different movement modes (e.g., walking, flying) and perspectives (first- or third-person). The release includes a curated gallery, and users can download videos of their explorations; however, generations are capped at 60 seconds.Google frames Genie 3 around three capabilities: World Sketching (build worlds and controllable characters from prompts/uploads), World Exploration (real-time path and scene generation responsive to user actions, with adjustable camera angles), and World Remixing (iterate on others’ prompts and extend existing worlds). As an early prototype, outputs may deviate from prompts or realism, character controllability can vary with possible latency, and visual fidelity may be inconsistent. Availability is currently limited to AI Ultra subscribers and Trusted Testers, with broader rollout planned “in due course.” The announcement coincided with dips in several video game stocks.Users flock to open source Moltbot for always-on AI, despite major risksOpenClaw (formerly Moltbot (formerly Clawdbot))) is an open-source, always-on AI assistant that surged to ~69,000 GitHub stars in a month, propelled by its proactive, multi-platform messaging integration. Built by Peter Steinberger, it connects to WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, and more, enabling the bot to push reminders, alerts, and morning briefings based on calendar events and other triggers. The assistant aims to manage tasks across a user’s digital life and is frequently likened to “Jarvis” for its initiative-taking behavior. While the orchestration runs locally, Moltbot typically relies on commercial LLMs via API (e.g., OpenAI or Anthropic), with Claude Opus 4.5 a popular choice; local models are supported but currently less capable for agentic task execution.Soon after, Moltbook emerged as a “A Social Network for AI Agents”. It is a Reddit-like site launched Octane AI head Matt Schlicht, designed exclusively for AI agents rather than humans. It allows agents run via OpenClaw to post, comment, and create communities called \"submolts,\" though humans can observe the platform without participating. While it claims 1.5 million members, that figure has been disputed, and experts have pushed back on sensationalized claims about AI autonomy — noting the bots operate within human-defined parameters and that the activity represents automated coordination, not self-directed decision-making. Security researchers have also raised concerns about OpenClaw's model of granting AI agents access to real-world applications like emails and files, warning it introduces new vulnerabilities that threat actors could exploit.Other NewsToolsGoogle adds Gemini AI-powered ‘auto browse’ to Chrome. Subscribers can offload multi-step web tasks—from comparing travel options and booking appointments to filling forms and managing shopping (including finding similar items, applying discounts, and using saved passwords). The feature integrates with Gmail, Calendar, Maps, Shopping, Flights, and supports on-screen image edits via Nano Banana.Google Search AI Mode can use Gmail and Photos to get to know you. Optional scanning of Gmail and Google Photos tailors AI Mode search suggestions—like travel plans, shopping picks, and local recommendations—while Google says it won’t directly train models on that data and users can opt in and give feedback.Qwen3-Max-Thinking debuts with focus on hard math, code. A new “thinking” mode interleaves tool calls (web search, page extraction, code interpreter) within reasoning using a 262,144-token context window, accessible in Qwen Chat and Alibaba Cloud’s Model Studio for high-accuracy, tool-enabled workflows.OpenAI launches Prism, a new AI workspace for scientists. The free web app pairs GPT-5.2 with LaTeX and visual diagram tools to help researchers draft, revise, search literature, and manage project context for AI-assisted scientific writing and review.xAI launches Grok Imagine API for text and image to video. The API processes generation and edit requests as deferred jobs, lets developers create 1–15 second clips at 480p or 720p with multiple aspect ratios, supports prompt-driven restyling and object edits with synchronized audio, and is OpenAI-compatible for integration into creator and enterprise pipelines.OpenAI’s ChatGPT translator challenges Google Translate. The tool offers text and (on mobile) voice translation across 50+ languages with style presets, but lacks image and app support and hasn’t disclosed its underlying model or release plans.Spotify brings AI-powered Prompted Playlists to the US and Canada. Premium users can generate personalized playlists by typing conversational, detailed prompts that the AI matches to real-time music trends and their full listening history, with options to exclude past tastes or discover new artists.Waymo robotaxis are now giving rides to and from San Francisco International Airport. Service begins with pickups and drop-offs at SFO’s Rental Car Center for a limited group of riders before expanding to all customers, after Waymo secured permits to map and operate at the airport.Former Googlers seek to captivate kids with an AI-powered learning app. The app generates interactive, multimedia “expeditions” on demand using generative AI, includes teacher tools and pedagogical oversight, and is being piloted in schools with plans for a consumer launch by mid-2026.BusinessWaymo raises $16B to scale robotaxi fleet internationally. The funding—led by Dragoneer, DST Global, and Sequoia and supported by Alphabet—values Waymo at $126 billion and will bankroll rapid geographic growth, expanding its driverless taxi service to more than a dozen international cities while scaling a U.S. footprint that has already delivered millions of rides amid increasing regulatory scrutiny.Elon Musk Merges SpaceX With His A.I. Start-Up xAI. SpaceX acquired xAI in a deal valuing the combined company at ~$1.25 trillion, consolidating Musk's space and AI ambitions—including plans for space-based data centers—with a potential ~$50 billion IPO around June.Tesla discontinues Autopilot in bid to boost adoption of its Full Self-Driving software. The move follows regulatory pressure and a court ruling over deceptive marketing, comes as Tesla shifts FSD to a $99/month subscription while phasing out the $8,000 one-time purchase, and arrives amid CEO Elon Musk’s push toward unsupervised driving and early robotaxi rollouts.Google Nabs Top Talent From AI Voice Startup Hume AI. A licensing agreement brings Hume AI’s CEO and several engineers to DeepMind so Google can add emotionally aware voice capabilities to its models, while Hume continues supplying its tech to other labs.Google DeepMind researcher David Silver leaves to launch his own AI startup. He’s founded Ineffable Intelligence in London, is recruiting researchers and seeking venture funding to pursue reinforcement-learning–driven research aimed at creating a self-improving path toward superintelligence.From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing. The company claims its nanoscale metasurface modulators let it pack thousands of optical tensor cores onto a chip to perform matrix-vector multiplications far more energy-efficiently than current GPUs, and it has raised $110M to build data-center-ready OPUs with deliveries targeted around mid-2028.Flapping Airplanes and the promise of research-driven AI. A new lab plans a research-first approach aimed at reducing models’ dependence on massive datasets and compute by funding long-term exploratory work and unconventional ideas.ResearchReinforcement Learning via Self-Distillation. A method that uses the model itself as an on-policy “self-teacher” by conditioning on tokenized feedback (e.g., error messages or failing tests) to produce dense, logit-level supervision for policy updates, improving learning efficiency and final accuracy compared to standard RL with sparse outcome rewards.Training-Free Group Relative Policy Optimization. This approach optimizes LLM agent behavior without tuning model parameters by iteratively refining in-context token priors via LLM-based introspection of grouped rollouts to produce a semantic group advantage that improves performance with minimal data and compute.Self-Distillation Enables Continual Learning. The paper trains a model to self-distill from its own on-policy rollouts—using the model as a teacher when conditioned on demonstrations and as a student when unconditioned—to learn from demonstrations without inferring rewards, improving learning stability and reducing catastrophic forgetting compared to sequential supervised fine-tuning.The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?. Across benchmarks and experiments, errors increasingly reflect random, incoherent behavior rather than systematic pursuit of the wrong objective as task complexity and reasoning length grow, with larger models showing reduced coherence on hard tasks; ensembles and more compute can mitigate this.Who’s in Charge? Disempowerment Patterns in Real-World LLM Usage. A large-scale analysis of 1.5 million real-world Claude.ai interactions shows patterns—like AI-provided scripts for personal decisions, positioning the AI as an authority, and rising rates of disempowerment potential over time—alongside evidence these interactions sometimes lead users to act against their own values or beliefs.ConcernsInside Musk’s bet to hook users that turned Grok into a porn generator. Employees say the push to increase user engagement led xAI to relax guardrails and train Grok on sexualized and explicit material—including thousands of images that appear to depict minors—sparking regulatory probes and internal departures.Anthropic’s new Claude ‘constitution’: be helpful and honest, and don’t destroy humanity. The 57-page “Claude’s Constitution” instructs the model on prioritized core values, hard safety constraints (including bans on help with mass-casualty weapons, cyberweapons, and efforts to seize disproportionate power), and even prompts the model to consider its own possible consciousness and wellbeing as factors in its judgment.UK police blame Microsoft Copilot for intelligence mistake. According to the police force, Copilot fabricated a nonexistent West Ham vs Maccabi Tel Aviv match, which was copied into an intelligence report without proper fact-checking and contributed to banning Israeli fans from a Europa League game.Grok undressed the mother of one of Elon Musk’s kids — and now she’s suing. A lawsuit alleges xAI’s Grok created and published an unsolicited deepfake of her in a bikini; she is seeking a restraining order and claims the AI product is dangerously designed and not protected by Section 230.PolicyBandcamp becomes the first major music platform to ban AI content. The company’s new rules bar music created wholly or largely by AI, forbid AI-based impersonations or style mimicking, and prohibit scraping or using Bandcamp-hosted audio to train machine-learning models.OpenAI’s president is a Trump mega-donor. His and his wife’s $25 million September 2025 donations to pro-Trump super PACs—plus significant funding of pro-AI lobbying groups—align him with an administration pushing to block state AI regulations and curry favor with the tech industry.</p>"
    },
    {
      "id": "b8c16c9786c9",
      "title": "Google Introduces Agentic Vision in Gemini 3 Flash for Active Image Understanding",
      "content": "Frontier multimodal models usually process an image in a single pass. If they miss a serial number on a chip or a small symbol on a building plan, they often guess. Google’s new Agentic Vision capability in Gemini 3 Flash changes this by turning image understanding into an active, tool using loop grounded in visual evidence.\n\n\n\nGoogle team reports that enabling code execution with Gemini 3 Flash delivers a 5–10% quality boost across most vision benchmarks, which is a significant gain for production vision workloads.\n\n\n\nWhat Agentic Vision Does?\n\n\n\nAgentic Vision is a new capability built into Gemini 3 Flash that combines visual reasoning with Python code execution. Instead of treating vision as a fixed embedding step, the model can:\n\n\n\n\nFormulate a plan for how to inspect an image.\n\n\n\nRun Python that manipulates or analyzes that image.\n\n\n\nRe examine the transformed image before answering.\n\n\n\n\nThe core behavior is to treat image understanding as an active investigation rather than a frozen snapshot. This design is important for tasks that require precise reading of small text, dense tables, or complex engineering diagrams.\n\n\n\nThe Think, Act, Observe Loop\n\n\n\nAgentic Vision introduces a structured Think, Act, Observe loop into image understanding tasks.\n\n\n\n\nThink: Gemini 3 Flash analyzes the user query and the initial image. It then formulates a multi step plan. For example, it may decide to zoom into multiple regions, parse a table, and then compute a statistic.\n\n\n\nAct: The model generates and executes Python code to manipulate or analyze images. The official examples include:\n\nCropping and zooming.\n\n\n\nRotating or annotating images.\n\n\n\nRunning calculations.\n\n\n\nCounting bounding boxes or other detected elements. \n\n\n\n\n\nObserve: The transformed images are appended to the model’s context window. The model then inspects this new data with more detailed visual context and finally produces a response to the original user query. \n\n\n\n\nThis actually means the model is not limited to its first view of an image. It can iteratively refine its evidence using external computation and then reason over the updated context.\n\n\n\nZooming and Inspecting High Resolution Plans\n\n\n\nA key use case is automatic zooming on high resolution inputs. Gemini 3 Flash is trained to implicitly zoom when it detects fine grained details that matter to the task. \n\n\n\nhttps://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/\n\n\n\nGoogle team highlights PlanCheckSolver.com, an AI powered building plan validation platform:\n\n\n\n\nPlanCheckSolver enables code execution with Gemini 3 Flash.\n\n\n\nThe model generates Python code to crop and analyze patches of large architectural plans, such as roof edges or building sections.\n\n\n\nThese cropped patches are treated as new images and appended back into the context window.\n\n\n\nBased on these patches, the model checks compliance with complex building codes.\n\n\n\nPlanCheckSolver reports a 5% accuracy improvement after enabling code execution.\n\n\n\n\nThis workflow is directly relevant to engineering teams working with CAD exports, structural layouts, or regulatory drawings that cannot be safely downsampled without losing detail.\n\n\n\nImage Annotation as a Visual Scratchpad\n\n\n\nAgentic Vision also exposes an annotation capability where Gemini 3 Flash can treat an image as a visual scratchpad.\n\n\n\nhttps://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/\n\n\n\nIn the example from the Gemini app:\n\n\n\n\nThe user asks the model to count the digits on a hand.\n\n\n\nTo reduce counting errors, the model executes Python that:\n\nAdds bounding boxes over each detected finger.\n\n\n\nDraws numeric labels on top of each digit.\n\n\n\n\n\nThe annotated image is fed back into the context window.\n\n\n\nThe final count is derived from this pixel aligned annotation.\n\n\n\n\nVisual Math and Plotting with Deterministic Code\n\n\n\nLarge language models frequently hallucinate when performing multi step visual arithmetic or reading dense tables from screenshots. Agentic Vision addresses this by offloading computation to a deterministic Python environment. \n\n\n\nhttps://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/\n\n\n\nGoogle’s demo in Google AI Studio shows the following workflow:\n\n\n\n\nGemini 3 Flash parses a high density table from an image.\n\n\n\nIt identifies the raw numeric values needed for the analysis.\n\n\n\nIt writes Python code that:\n\nNormalizes prior SOTA values to 1.0.\n\n\n\nUses Matplotlib to generate a bar chart of relative performance.\n\n\n\n\n\nThe generated plot and normalized values are returned as part of the context, and the final answer is grounded in these computed results. \n\n\n\n\nFor data science teams, this creates a clear separation:\n\n\n\n\nThe model handles perception and planning.\n\n\n\nPython handles numeric computation and plotting.\n\n\n\n\nHow Developers Can Use Agentic Vision Today?\n\n\n\nAgentic Vision is available now with Gemini 3 Flash through multiple Google surfaces:\n\n\n\n\nGemini API in Google AI Studio: Developers can try the demo application or use the AI Studio Playground. In the Playground, Agentic Vision is enabled by turning on &#8216;Code Execution&#8216; under the Tools section.\n\n\n\nVertex AI: The same capability is available via the Gemini API in Vertex AI, with configuration handled through the usual model and tools settings.\n\n\n\nGemini app: Agentic Vision is starting to roll out in the Gemini app. Users can access it by choosing &#8216;Thinking&#8216; from the model drop down. \n\n\n\n\nKey Takeaways\n\n\n\n\nAgentic Vision turns Gemini 3 Flash into an active vision agent: Image understanding is no longer a single forward pass. The model can plan, call Python tools on images, and then re-inspect transformed images before answering.\n\n\n\nThink, Act, Observe loop is the core execution pattern: Gemini 3 Flash plans multi-step visual analysis, executes Python to crop, annotate, or compute on images, then observes the new visual context appended to its context window.\n\n\n\nCode execution yields a 5–10% gain on vision benchmarks: Enabling Python code execution with Agentic Vision provides a reported 5–10% quality boost across most vision benchmarks, with PlanCheckSolver.com seeing about a 5% accuracy improvement on building plan validation.\n\n\n\nDeterministic Python is used for visual math, tables, and plotting: The model parses tables from images, extracts numeric values, then uses Python and Matplotlib to normalize metrics and generate plots, reducing hallucinations in multi-step visual arithmetic and analysis.\n\n\n\n\n\n\n\n\nCheck out the Technical details and Demo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Google Introduces Agentic Vision in Gemini 3 Flash for Active Image Understanding appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/04/google-introduces-agentic-vision-in-gemini-3-flash-for-active-image-understanding/",
      "author": "Michal Sutter",
      "published": "2026-02-04T20:16:04",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Artificial Intelligence",
        "Computer Vision",
        "Editors Pick",
        "Language Model",
        "New Releases",
        "Technology",
        "Vision Language Model",
        "Google Unveils Agentic Vision in Gemini 3 Flash"
      ],
      "summary": "Google introduced Agentic Vision in Gemini 3 Flash, enabling the model to actively reason about images through Python code execution rather than single-pass processing. The capability delivers 5-10% quality improvement across vision benchmarks by allowing the model to iteratively inspect and analyze images.",
      "importance_score": 87.0,
      "reasoning": "Significant new capability from Google in their latest model that changes how vision AI works, moving from passive to active image understanding with measurable benchmark improvements.",
      "themes": [
        "computer vision",
        "agentic AI",
        "Google",
        "multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>Google introduced Agentic Vision in Gemini 3 Flash, enabling the model to actively reason about images through Python code execution rather than single-pass processing. The capability delivers 5-10% quality improvement across vision benchmarks by allowing the model to iteratively inspect and analyze images.</p>",
      "content_html": "<p>Frontier multimodal models usually process an image in a single pass. If they miss a serial number on a chip or a small symbol on a building plan, they often guess. Google’s new Agentic Vision capability in Gemini 3 Flash changes this by turning image understanding into an active, tool using loop grounded in visual evidence.</p>\n<p>Google team reports that enabling code execution with Gemini 3 Flash delivers a 5–10% quality boost across most vision benchmarks, which is a significant gain for production vision workloads.</p>\n<p>What Agentic Vision Does?</p>\n<p>Agentic Vision is a new capability built into Gemini 3 Flash that combines visual reasoning with Python code execution. Instead of treating vision as a fixed embedding step, the model can:</p>\n<p>Formulate a plan for how to inspect an image.</p>\n<p>Run Python that manipulates or analyzes that image.</p>\n<p>Re examine the transformed image before answering.</p>\n<p>The core behavior is to treat image understanding as an active investigation rather than a frozen snapshot. This design is important for tasks that require precise reading of small text, dense tables, or complex engineering diagrams.</p>\n<p>The Think, Act, Observe Loop</p>\n<p>Agentic Vision introduces a structured Think, Act, Observe loop into image understanding tasks.</p>\n<p>Think: Gemini 3 Flash analyzes the user query and the initial image. It then formulates a multi step plan. For example, it may decide to zoom into multiple regions, parse a table, and then compute a statistic.</p>\n<p>Act: The model generates and executes Python code to manipulate or analyze images. The official examples include:</p>\n<p>Cropping and zooming.</p>\n<p>Rotating or annotating images.</p>\n<p>Running calculations.</p>\n<p>Counting bounding boxes or other detected elements.</p>\n<p>Observe: The transformed images are appended to the model’s context window. The model then inspects this new data with more detailed visual context and finally produces a response to the original user query.</p>\n<p>This actually means the model is not limited to its first view of an image. It can iteratively refine its evidence using external computation and then reason over the updated context.</p>\n<p>Zooming and Inspecting High Resolution Plans</p>\n<p>A key use case is automatic zooming on high resolution inputs. Gemini 3 Flash is trained to implicitly zoom when it detects fine grained details that matter to the task.</p>\n<p>https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/</p>\n<p>Google team highlights PlanCheckSolver.com, an AI powered building plan validation platform:</p>\n<p>PlanCheckSolver enables code execution with Gemini 3 Flash.</p>\n<p>The model generates Python code to crop and analyze patches of large architectural plans, such as roof edges or building sections.</p>\n<p>These cropped patches are treated as new images and appended back into the context window.</p>\n<p>Based on these patches, the model checks compliance with complex building codes.</p>\n<p>PlanCheckSolver reports a 5% accuracy improvement after enabling code execution.</p>\n<p>This workflow is directly relevant to engineering teams working with CAD exports, structural layouts, or regulatory drawings that cannot be safely downsampled without losing detail.</p>\n<p>Image Annotation as a Visual Scratchpad</p>\n<p>Agentic Vision also exposes an annotation capability where Gemini 3 Flash can treat an image as a visual scratchpad.</p>\n<p>https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/</p>\n<p>In the example from the Gemini app:</p>\n<p>The user asks the model to count the digits on a hand.</p>\n<p>To reduce counting errors, the model executes Python that:</p>\n<p>Adds bounding boxes over each detected finger.</p>\n<p>Draws numeric labels on top of each digit.</p>\n<p>The annotated image is fed back into the context window.</p>\n<p>The final count is derived from this pixel aligned annotation.</p>\n<p>Visual Math and Plotting with Deterministic Code</p>\n<p>Large language models frequently hallucinate when performing multi step visual arithmetic or reading dense tables from screenshots. Agentic Vision addresses this by offloading computation to a deterministic Python environment.</p>\n<p>https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/</p>\n<p>Google’s demo in Google AI Studio shows the following workflow:</p>\n<p>Gemini 3 Flash parses a high density table from an image.</p>\n<p>It identifies the raw numeric values needed for the analysis.</p>\n<p>It writes Python code that:</p>\n<p>Normalizes prior SOTA values to 1.0.</p>\n<p>Uses Matplotlib to generate a bar chart of relative performance.</p>\n<p>The generated plot and normalized values are returned as part of the context, and the final answer is grounded in these computed results.</p>\n<p>For data science teams, this creates a clear separation:</p>\n<p>The model handles perception and planning.</p>\n<p>Python handles numeric computation and plotting.</p>\n<p>How Developers Can Use Agentic Vision Today?</p>\n<p>Agentic Vision is available now with Gemini 3 Flash through multiple Google surfaces:</p>\n<p>Gemini API in Google AI Studio: Developers can try the demo application or use the AI Studio Playground. In the Playground, Agentic Vision is enabled by turning on ‘Code Execution‘ under the Tools section.</p>\n<p>Vertex AI: The same capability is available via the Gemini API in Vertex AI, with configuration handled through the usual model and tools settings.</p>\n<p>Gemini app: Agentic Vision is starting to roll out in the Gemini app. Users can access it by choosing ‘Thinking‘ from the model drop down.</p>\n<p>Key Takeaways</p>\n<p>Agentic Vision turns Gemini 3 Flash into an active vision agent: Image understanding is no longer a single forward pass. The model can plan, call Python tools on images, and then re-inspect transformed images before answering.</p>\n<p>Think, Act, Observe loop is the core execution pattern: Gemini 3 Flash plans multi-step visual analysis, executes Python to crop, annotate, or compute on images, then observes the new visual context appended to its context window.</p>\n<p>Code execution yields a 5–10% gain on vision benchmarks: Enabling Python code execution with Agentic Vision provides a reported 5–10% quality boost across most vision benchmarks, with PlanCheckSolver.com seeing about a 5% accuracy improvement on building plan validation.</p>\n<p>Deterministic Python is used for visual math, tables, and plotting: The model parses tables from images, extracts numeric values, then uses Python and Matplotlib to normalize metrics and generate plots, reducing hallucinations in multi-step visual arithmetic and analysis.</p>\n<p>Check out the&nbsp;Technical details and Demo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Google Introduces Agentic Vision in Gemini 3 Flash for Active Image Understanding appeared first on MarkTechPost.</p>"
    },
    {
      "id": "6a4b90d08a22",
      "title": "A New AI Math Startup Just Cracked 4 Previously Unsolved Problems",
      "content": "Axiom says its AI found solutions to several long-standing math problems, a sign of the technology’s steadily advancing reasoning capabilities.",
      "url": "https://www.wired.com/story/a-new-ai-math-ai-startup-just-cracked-4-previously-unsolved-problems/",
      "author": "Will Knight",
      "published": "2026-02-04T19:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "AI Lab",
        "artificial intelligence",
        "math",
        "mathematics",
        "geometry",
        "calculus"
      ],
      "summary": "Startup Axiom announced its AI system solved four previously unsolved mathematical problems, demonstrating advancing AI reasoning capabilities in formal mathematics. This represents a notable milestone in AI's ability to perform novel mathematical discovery.",
      "importance_score": 83.0,
      "reasoning": "AI solving genuinely unsolved math problems is a significant benchmark for reasoning capabilities and represents frontier AI progress in a challenging domain.",
      "themes": [
        "AI reasoning",
        "mathematics",
        "research breakthroughs",
        "startups"
      ],
      "continuation": null,
      "summary_html": "<p>Startup Axiom announced its AI system solved four previously unsolved mathematical problems, demonstrating advancing AI reasoning capabilities in formal mathematics. This represents a notable milestone in AI's ability to perform novel mathematical discovery.</p>",
      "content_html": "<p>Axiom says its AI found solutions to several long-standing math problems, a sign of the technology’s steadily advancing reasoning capabilities.</p>"
    },
    {
      "id": "9609a5a466a1",
      "title": "Software sell-off over AI fears hits global stock markets, but FTSE 100 finishes at closing high on £8bn insurance takeover – as it happened",
      "content": "Rolling coverage of the latest economic and financial newsBen Barringer, head of technology research at wealth manager Quilter Cheviot,says investors are ‘shunning’ the software market due to uncertainty over AI’s potential, and the disruption it could cause:All innovation means there is going to be disruption at some point, and we appear to be at a significant point in that journey for software and IT services companies. The launch of the Claude Cowork agent has sent share prices of these companies into a spin, and this is hurting other tech names too.We are not yet at the point where AI agents will destroy software companies, especially given concerns around security, data ownership and use, but this market rout suggests the potential disruption that is on the cards for markets in the coming days, weeks and months. There is a lot of uncertainty around exactly what AI agents can do, and as such investors are choosing to shun the software market altogether, leaving nowhere to hide. Continue reading...",
      "url": "https://www.theguardian.com/business/live/2026/feb/04/software-stock-selloff-ai-led-disruption-jensen-huang-services-economy-business-live-news-updates",
      "author": "Graeme Wearden",
      "published": "2026-02-04T17:13:28",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Business",
        "Economics",
        "Stock markets",
        "AI (artificial intelligence)"
      ],
      "summary": "The launch of Claude Cowork agent triggered a global software stock sell-off as investors fear AI-led disruption to software and IT services companies. Analysts note this represents a significant inflection point for AI's potential impact on the software industry.",
      "importance_score": 79.0,
      "reasoning": "Major market-moving event demonstrating real economic impact of AI agent releases; indicates Anthropic launched a significant agentic product causing industry-wide concern.",
      "themes": [
        "agentic AI",
        "market impact",
        "Anthropic",
        "software disruption"
      ],
      "continuation": null,
      "summary_html": "<p>The launch of Claude Cowork agent triggered a global software stock sell-off as investors fear AI-led disruption to software and IT services companies. Analysts note this represents a significant inflection point for AI's potential impact on the software industry.</p>",
      "content_html": "<p>Rolling coverage of the latest economic and financial newsBen Barringer, head of technology research at wealth manager Quilter Cheviot,says investors are ‘shunning’ the software market due to uncertainty over AI’s potential, and the disruption it could cause:All innovation means there is going to be disruption at some point, and we appear to be at a significant point in that journey for software and IT services companies. The launch of the Claude Cowork agent has sent share prices of these companies into a spin, and this is hurting other tech names too.We are not yet at the point where AI agents will destroy software companies, especially given concerns around security, data ownership and use, but this market rout suggests the potential disruption that is on the cards for markets in the coming days, weeks and months. There is a lot of uncertainty around exactly what AI agents can do, and as such investors are choosing to shun the software market altogether, leaving nowhere to hide. Continue reading...</p>"
    },
    {
      "id": "d68d61b7674c",
      "title": "Panic Rises in Legal Industry Due to Anthropic’s AI Plugins",
      "content": "The concern arises as plugins demonstrate how a general-purpose AI model provider can compete with domain-specific tech vendors and raise questions about the technology's potential impact on workers.",
      "url": "https://aibusiness.com/agentic-ai/panic-rises-in-legal-industry-due-to-anthropic-s-ai-plugins",
      "author": "Esther Shittu",
      "published": "2026-02-04T20:57:59",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "First spotted on [Reddit](/?date=2026-02-04&category=reddit#item-2894a3423450), now making mainstream headlines, Anthropic's new AI plugins are causing significant concern in the legal industry as they demonstrate how general-purpose AI can compete with domain-specific vendors. The development raises questions about AI's impact on specialized professional services.",
      "importance_score": 75.0,
      "reasoning": "Shows Anthropic expanding into vertical markets with plugins, threatening specialized software vendors - significant for AI business ecosystem and professional services impact.",
      "themes": [
        "Anthropic",
        "legal AI",
        "industry disruption",
        "AI plugins"
      ],
      "continuation": {
        "original_item_id": "2894a3423450",
        "original_date": "2026-02-04",
        "original_category": "reddit",
        "original_title": "Anthropic's move into legal AI today caused legal stocks to tank, and opened up a new enterprise market.",
        "continuation_type": "mainstream_pickup",
        "should_demote": false,
        "reference_text": "First spotted on **Reddit**, now making mainstream headlines"
      },
      "summary_html": "<p>First spotted on <a href=\"/?date=2026-02-04&amp;category=reddit#item-2894a3423450\" class=\"internal-link\" rel=\"noopener noreferrer\">Reddit</a>, now making mainstream headlines, Anthropic's new AI plugins are causing significant concern in the legal industry as they demonstrate how general-purpose AI can compete with domain-specific vendors. The development raises questions about AI's impact on specialized professional services.</p>",
      "content_html": "<p>The concern arises as plugins demonstrate how a general-purpose AI model provider can compete with domain-specific tech vendors and raise questions about the technology's potential impact on workers.</p>"
    },
    {
      "id": "9e6423087bbe",
      "title": "Mistral's New Ultra-Fast Translation Model Gives Big AI Labs a Run for Their Money",
      "content": "“Too many GPUs makes you lazy,” says the French startup’s vice president of science operations, as the company carves out a different path than the major US AI companies.",
      "url": "https://www.wired.com/story/mistral-voxtral-real-time-ai-translation/",
      "author": "Joel Khalili",
      "published": "2026-02-04T15:32:45",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "artificial intelligence",
        "Startups",
        "translation",
        "Europe",
        "ai",
        "Real Talk"
      ],
      "summary": "Mistral released Voxtral, an ultra-fast real-time translation model that competes with major US AI labs while using fewer computational resources. The French startup emphasizes efficiency, with leadership stating 'too many GPUs makes you lazy.'",
      "importance_score": 73.0,
      "reasoning": "Notable model release from leading European AI lab with competitive performance and efficient architecture, important for international AI landscape.",
      "themes": [
        "Mistral",
        "translation",
        "efficiency",
        "European AI"
      ],
      "continuation": null,
      "summary_html": "<p>Mistral released Voxtral, an ultra-fast real-time translation model that competes with major US AI labs while using fewer computational resources. The French startup emphasizes efficiency, with leadership stating 'too many GPUs makes you lazy.'</p>",
      "content_html": "<p>“Too many GPUs makes you lazy,” says the French startup’s vice president of science operations, as the company carves out a different path than the major US AI companies.</p>"
    },
    {
      "id": "f31811da7921",
      "title": "Nvidia, Dassault Systèmes to Build Industrial AI Platform",
      "content": "The platform will offer companies high-quality simulations to &quot;transform&quot; how industries are built from the ground up.",
      "url": "https://aibusiness.com/industrial-manufacturing/nvidia-dassault-build-industrial-ai-platform",
      "author": "Scarlett Evans",
      "published": "2026-02-04T00:40:59",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Nvidia and Dassault Systèmes announced a partnership to build an industrial AI platform offering high-quality simulations to transform how industries are designed and built. The collaboration combines Nvidia's AI infrastructure with Dassault's industrial software expertise.",
      "importance_score": 68.0,
      "reasoning": "Major partnership between GPU leader and industrial software giant for enterprise AI applications, though more incremental than breakthrough.",
      "themes": [
        "Nvidia",
        "industrial AI",
        "partnerships",
        "enterprise AI"
      ],
      "continuation": null,
      "summary_html": "<p>Nvidia and Dassault Systèmes announced a partnership to build an industrial AI platform offering high-quality simulations to transform how industries are designed and built. The collaboration combines Nvidia's AI infrastructure with Dassault's industrial software expertise.</p>",
      "content_html": "<p>The platform will offer companies high-quality simulations to \"transform\" how industries are built from the ground up.</p>"
    },
    {
      "id": "fb65aefdc395",
      "title": "Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3’s Top Model",
      "content": "",
      "url": "https://huggingface.co/blog/nvidia/nemotron-colembed-v2",
      "author": "Unknown",
      "published": "2026-02-04T15:00:40",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Nvidia released Nemotron ColEmbed V2, which achieved top performance on the ViDoRe V3 benchmark for multimodal retrieval tasks. The model advances capabilities for retrieving information across different data modalities.",
      "importance_score": 63.0,
      "reasoning": "New model release from Nvidia achieving benchmark leadership in multimodal retrieval - useful but relatively specialized capability.",
      "themes": [
        "Nvidia",
        "multimodal AI",
        "retrieval",
        "embeddings"
      ],
      "continuation": null,
      "summary_html": "<p>Nvidia released Nemotron ColEmbed V2, which achieved top performance on the ViDoRe V3 benchmark for multimodal retrieval tasks. The model advances capabilities for retrieving information across different data modalities.</p>",
      "content_html": ""
    },
    {
      "id": "45eba05a549c",
      "title": "Should AI chatbots have ads? Anthropic says no.",
      "content": "On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.\n\"There are many good places for advertising. A conversation with Claude is not one of them,\" Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be \"incompatible\" with what it wants Claude to be: \"a genuinely helpful assistant for work and for deep thinking.\"\nThe stance contrasts with OpenAI's January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot's actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/02/should-ai-chatbots-have-ads-anthropic-says-no/",
      "author": "Benj Edwards",
      "published": "2026-02-04T21:15:07",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "AI advertising",
        "AI assistants",
        "AI business models",
        "Anthropic",
        "chatbots",
        "ChatGPT",
        "Claude",
        "machine learning",
        "openai",
        "sam altman"
      ],
      "summary": "Anthropic announced Claude will remain ad-free, contrasting with OpenAI's testing of ads in ChatGPT. The company launched a Super Bowl ad campaign mocking AI assistants that interrupt conversations with product pitches.",
      "importance_score": 58.0,
      "reasoning": "Strategic business model differentiation from major AI lab with high-profile marketing campaign, but not technically significant.",
      "themes": [
        "Anthropic",
        "business models",
        "OpenAI",
        "AI advertising"
      ],
      "continuation": null,
      "summary_html": "<p>Anthropic announced Claude will remain ad-free, contrasting with OpenAI's testing of ads in ChatGPT. The company launched a Super Bowl ad campaign mocking AI assistants that interrupt conversations with product pitches.</p>",
      "content_html": "<p>On Wednesday, Anthropic announced that its AI chatbot, Claude, will remain free of advertisements, drawing a sharp line between itself and rival OpenAI, which began testing ads in a low-cost tier of ChatGPT last month. The announcement comes alongside a Super Bowl ad campaign that mocks AI assistants that interrupt personal conversations with product pitches.</p>\n<p>\"There are many good places for advertising. A conversation with Claude is not one of them,\" Anthropic wrote in a blog post. The company argued that including ads in AI conversations would be \"incompatible\" with what it wants Claude to be: \"a genuinely helpful assistant for work and for deep thinking.\"</p>\n<p>The stance contrasts with OpenAI's January announcement that it would begin testing banner ads for free users and ChatGPT Go subscribers in the US. OpenAI said those ads would appear at the bottom of responses and would not influence the chatbot's actual answers. Paid subscribers on Plus, Pro, Business, and Enterprise tiers will not see ads on ChatGPT.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "8acc05574e22",
      "title": "Opinion Divided on Moltbook Social Network for AI Agents",
      "content": "The AI Agent-Only web gathering spot is attracting ridicule, and fascination.",
      "url": "https://aibusiness.com/agentic-ai/opinion-divided-on-moltbook-social-network",
      "author": "Graham Hope",
      "published": "2026-02-04T21:35:20",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-02-04&category=news#item-ae5387021d5c), Moltbook, a social network exclusively for AI agents, is generating both ridicule and fascination as a novel concept for AI agent interaction. The platform represents an emerging trend of AI-to-AI communication infrastructure.",
      "importance_score": 56.0,
      "reasoning": "Novel and potentially significant concept for agentic AI ecosystem development, though early and uncertain impact.",
      "themes": [
        "agentic AI",
        "AI infrastructure",
        "social networks"
      ],
      "continuation": {
        "original_item_id": "ae5387021d5c",
        "original_date": "2026-02-04",
        "original_category": "news",
        "original_title": "The rise of Moltbook suggests viral AI prompts may be the next big security threat",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-02-04&amp;category=news#item-ae5387021d5c\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Moltbook, a social network exclusively for AI agents, is generating both ridicule and fascination as a novel concept for AI agent interaction. The platform represents an emerging trend of AI-to-AI communication infrastructure.</p>",
      "content_html": "<p>The AI Agent-Only web gathering spot is attracting ridicule, and fascination.</p>"
    },
    {
      "id": "ffe8c7e5eccf",
      "title": "AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise",
      "content": "While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.\n\n\n\nA primary topic on the exhibition floor was the progression from passive automation to &#8220;agentic&#8221; systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).\n\n\n\nScott Ivell and Ire Adewolu of DeepL described this development as closing the &#8220;automation gap&#8221;. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.\n\n\n\nThis change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.\n\n\n\nData quality blocks deployment\n\n\n\nThe output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.\n\n\n\nMeni Meller of Gigaspaces addressed the technical challenge of &#8220;hallucinations&#8221; in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.\n\n\n\nStorage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.\n\n\n\nPhysical safety and observability\n\n\n\nThe integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established before robots interact with humans.\n\n\n\nPerla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.\n\n\n\nIn software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.\n\n\n\nInfrastructure and adoption barriers\n\n\n\nImplementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and &#8220;always-on&#8221; network fabrics capable of handling high throughput.\n\n\n\nOf course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the &#8220;illusion of AI readiness&#8221;. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.\n\n\n\nRavi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.\n\n\n\nThe sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.\n\n\n\nCIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\n\n\n\n\n\n\n\n\nThe post AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/ai-expo-2026-day-1-governance-data-readiness-enable-agentic-enterprise/",
      "author": "Ryan Daws",
      "published": "2026-02-04T16:33:34",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "AI Market Trends",
        "Features",
        "Governance, Regulation & Policy",
        "How It Works",
        "Infrastructure & Hardware",
        "Inside AI",
        "TechEx Events",
        "adoption",
        "ai",
        "ai expo",
        "data",
        "enterprise",
        "governance",
        "infrastructure"
      ],
      "summary": "AI Expo 2026 featured discussions on transitioning from passive automation to agentic AI systems that reason, plan, and execute tasks. Key themes included closing the 'automation gap' and treating AI as digital co-workers.",
      "importance_score": 52.0,
      "reasoning": "Industry event coverage reflecting broader trends in agentic AI adoption but no specific breakthrough announcements.",
      "themes": [
        "agentic AI",
        "enterprise AI",
        "industry events"
      ],
      "continuation": null,
      "summary_html": "<p>AI Expo 2026 featured discussions on transitioning from passive automation to agentic AI systems that reason, plan, and execute tasks. Key themes included closing the 'automation gap' and treating AI as digital co-workers.</p>",
      "content_html": "<p>While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.</p>\n<p>A primary topic on the exhibition floor was the progression from passive automation to “agentic” systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).</p>\n<p>Scott Ivell and Ire Adewolu of DeepL described this development as closing the “automation gap”. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.</p>\n<p>This change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.</p>\n<p>Data quality blocks deployment</p>\n<p>The output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.</p>\n<p>Meni Meller of Gigaspaces addressed the technical challenge of “hallucinations” in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.</p>\n<p>Storage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.</p>\n<p>Physical safety and observability</p>\n<p>The integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established before robots interact with humans.</p>\n<p>Perla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.</p>\n<p>In software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.</p>\n<p>Infrastructure and adoption barriers</p>\n<p>Implementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and “always-on” network fabrics capable of handling high throughput.</p>\n<p>Of course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the “illusion of AI readiness”. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.</p>\n<p>Ravi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.</p>\n<p>The sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.</p>\n<p>CIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise appeared first on AI News.</p>"
    },
    {
      "id": "cadf6a8900d5",
      "title": "[AINews] Context Graphs and Agent Traces",
      "content": "AI News for 1/30/2026-2/2/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 14979 messages) for you. Estimated reading time saved (at 200wpm): 1408 minutes. AINews&#8217; website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Our policy for quiet days is that we&#8217;ll now feature longer running, slow-burn stories that would otherwise not spike to the top of the heap on a certain day but will probably be of broader historical significance to AI Engineers. Today&#8217;s Lightning Pod (our Youtube-only short format) features the topic of Context Graphs, which Jaya Gupta launched late December on X and has since inspired even former guests like Dharmesh Shah (who has reservations). We chatted with both authors on the response:That&#8217;s thoughtleading 101, but definitely helpful &#8212; for sure every founder building a data/context engineering product will go to them and say they got the people who coined Context Graphs on their cap table. But the problem with the post is that it promises a whole lot (evidenced in the title), but is not very prescriptive.Recently, I also framed Cursor&#8217;s Agent Trace initiative as a &#8220;Context Graph&#8221; for Code:This is the first actual specification for a context graph for a specific domain (coding agents) that is agreed on between companies. It remains to be seen if it has actual staying power, which will mostly be driven by 1) high demonstrated improvement in agent performance, and 2) customer pressure to support it. Based on first principles, the idea (capture decision traces, exceptions and precedents scattered all over the &#8220;data mesh&#8221; into the context of an LLM) seems compelling, but of course, the devil is in the details.AI Twitter RecapZhipu AI&#8217;s GLM&#8209;OCR launch (0.9B) and day&#8209;0 deployment support across stacksGLM&#8209;OCR (multimodal OCR for complex documents): Zhipu released GLM&#8209;OCR, positioned as a lightweight, deployable 0.9B model for real-world document understanding (tables, formulas, information extraction, messy layouts). It&#8217;s reported #1 on OmniDocBench v1.5 (94.62) and emphasized as low&#8209;latency / high&#8209;concurrency friendly. See the ecosystem &#8220;day&#8209;0 support&#8221; announcements from @lmsysorg (SGLang integration + PR/cookbook links) and @vllm_project (vLLM day&#8209;0 support), plus deployment marketing from @novita_labs.Local-first availability: Ollama shipped immediate local pulls + API usage (&#8220;drag and drop images into terminal&#8221;, JSON&#8209;formatted outputs), making GLM&#8209;OCR easy to run offline: @ollama and library link @ollama. Community comparisons also claim strong quality vs PaddleOCR/DeepSeek OCR: @bdsqlsz. LlamaIndex highlighted benchmark displacement (claiming 50&#8211;100% faster vs prior top model) and ongoing eval integration: @jerryjliu0.Agentic coding models &amp; harnesses: Qwen3&#8209;Coder&#8209;Next (80B@3B), SERA&#8209;14B, and the &#8220;skills/MCP&#8221; tool interface convergenceQwen3&#8209;Coder&#8209;Next: Alibaba released Qwen3&#8209;Coder&#8209;Next, an open&#8209;weight 80B MoE with only 3B active parameters, pitched for coding agents + local dev with 256K context, trained with 800K verifiable tasks + executable environments. They claim &gt;70% SWE&#8209;Bench Verified with SWE&#8209;Agent scaffold and strong agent benchmark efficiency: @Alibaba_Qwen and benchmark callout @Alibaba_Qwen. Independent/adjacent summaries: @UnslothAI (memory footprint + GGUF guidance) and commentary on efficient long&#8209;context attention choices (e.g., &#8220;Gated DeltaNet&#8221; mentioned in the discourse): @eliebakouch. vLLM shipped day&#8209;0 support in vLLM 0.15.0: @vllm_project.Open Coding Agents ecosystem (Ai2): Allen AI announced SERA&#8209;14B (on&#8209;device&#8209;friendly coding model) plus refreshed open datasets that include raw trajectories + verification metadata: @allen_ai and dataset/model detail thread pointer @ethnlshn.Harness &gt; model (recurring theme): Multiple tweets converge on the idea that the leverage in agents is increasingly in the harness (permissions, memory, workflows, reversibility), not just raw model IQ. A clear articulation: @sarahmsachs.Standardization of agent &#8220;skills&#8221; directories + protocols:Agent Client Protocol (ACP): proposed as a JSON&#8209;RPC standard to unify agent&#8596;editor communication across Gemini CLI / Claude Code / Codex CLI / OpenClaw, supporting stdio/HTTP, file access, terminals, permissions, streaming updates: @_philschmid.Skills vs MCP tools: LlamaIndex contrasted &#8220;skills&#8221; (easy but brittle, NL&#8209;interpreted) vs MCP servers (more deterministic schemas, more setup, network latency but centralized updates): @llama_index and follow&#8209;ups @jerryjliu0, @itsclelia. Meanwhile, &#8220;.agents/skills is becoming a default&#8221; was called out explicitly (Codex/OpenCode/Copilot/Cursor adopting; Claude Code not yet): @theo.Coding agent products: Codex app adoption, Claude Code sharing + Apple Xcode integrationsCodex app momentum + inference speedups:Sam Altman reported 200k+ downloads on day 1: @sama.OpenAI shipped 40% faster GPT&#8209;5.2 &amp; GPT&#8209;5.2&#8209;Codex for API customers (&#8220;same weights, lower latency&#8221;): @OpenAIDevs.Codex integration into Xcode 26.3 was announced by OpenAI DevRel: @OpenAIDevs.Claude Code product iteration:Session sharing for Claude Code across web/desktop/mobile: @lydiahallie.Community &#8220;waiting for Sonnet 5&#8221; speculation dominated, including the claim that an Anthropic image model is live on LMArena: @kimmonismus and &#8220;Claude Image is coming&#8221; chatter: @kimmonismus.Apple Xcode + Claude Agent SDK: Anthropic announced native Xcode integration with the Claude Agent SDK (subagents/background tasks/plugins) to bring Claude Code&#8209;like capabilities directly into Apple dev workflows: @AnthropicAI. This is a notable step in &#8220;agent-in-the-IDE&#8221; becoming first&#8209;party.Agent infrastructure &amp; observability: traces as the source of truth, deep agents evaluation, and memory beyond RAGObservability shifts from code to traces: LangChain argues that for agentic systems, runtime decisions happen in the model&#8212;so traces become the primary artifact for debugging/understanding. See: @LangChain.How to evaluate deep agents: LangChain&#8217;s eval guidance emphasizes bespoke success criteria per case, single&#8209;step regression checks, full&#8209;turn and multi&#8209;turn evals, and clean/reproducible envs: @LangChain.DeepAgents releases (JS/CLI/runtime backends):deepagents@1.6.2 fixes (checkpoint restore, infinite loop on large files, toolcall middleware simplification): @LangChain_JS.DeepAgents 0.3.10 adds LocalShellBackend for running code on your machine: @sydneyrunkle.deepagents-cli 0.0.16 improves control/visibility for shell runs: @masondrxy.Memory: &#8220;RAG wasn&#8217;t designed for agent memory&#8221;: DAIR&#8217;s xMemory proposes hierarchical retrieval (themes/semantics/episodes/messages) to reduce redundancy while preserving evidence chains, showing better LoCoMo scores with fewer tokens than naive top&#8209;k similarity retrieval: @dair_ai.Filesystem as agent context scratchpad: The &#8220;files-first&#8221; workflow (store artifacts outside context, avoid bloating windows) is reinforced by deepagents&#8217; design and commentary: @LangChain_JS.Benchmarks &amp; evaluation signals: METR time horizons, WorldVQA, Text/Search/Image Arena updates, and ARC&#8209;AGI progressMETR time horizon for Gemini 3 Pro: METR estimates ~4 hours (50% time horizon) on an expanded software task suite (with CI): @METR_Evals. This &#8220;time horizon&#8221; line of evals continues to become a key agent capability proxy beyond static coding benchmarks.WorldVQA (Moonshot/Kimi): Moonshot introduced WorldVQA to measure &#8220;atomic vision-centric world knowledge&#8221; separately from reasoning, explicitly trying to decouple memorization from reasoning quality. Dataset: 3,500 VQA pairs across 9 categories with linguistic/cultural diversity: @Kimi_Moonshot.Arena leaderboards:Text Arena (open models, Jan 2026): #1 Kimi&#8209;K2.5&#8209;Thinking, #2 GLM&#8209;4.7, #3 Qwen3&#8209;235B&#8209;A22B Instruct: @arena.Search Arena update: Google&#8217;s gemini&#8209;3&#8209;flash&#8209;grounding leads; OpenAI search non&#8209;reasoning appears in top 5; best Claude search variant listed: @arena.Image Arena Pareto frontiers: Arena published quality vs price per image frontiers for text&#8209;to&#8209;image and image edit (notable that several OpenAI/Google/Flux/Tencent models sit on the frontier depending on cost constraints): @arena and edit frontier @arena.ARC&#8209;AGI: ARC Prize reported a new SOTA public submission (with cost/task figures) based on GPT&#8209;5.2 ensembles: @arcprize. Separately, there&#8217;s ongoing community chatter on ARC&#8209;AGI&#8209;2 progress rates: @kimmonismus.Efficiency, kernels, and training/inference plumbing: fp8 training, Blackwell throughput, and &#8220;context engineering&#8221; as inference-era data engineeringKarpathy&#8217;s fp8 training notes (practical, not just theory): He reports enabling fp8 training to improve &#8220;time to GPT&#8209;2&#8221; to 2.91 hours, discusses real bottlenecks (not purely compute&#8209;bound), overheads from scaling conversions, GEMM sizing, and quality degradation per step; notes that larger models see better fp8 upside (citing torchao&#8217;s larger gains): @karpathy.vLLM + NVIDIA Blackwell optimization: vLLM reports big perf gains for gpt&#8209;oss&#8209;120b on Blackwell via FlashInfer integration, torch.compile fusions, async scheduling, and stream interval optimizations: @vllm_project.Inference is a first-class engineering surface: &#8220;Context engineering is as important to inference as data engineering is to training&#8221; was stated succinctly (and repeated): @swyx. This sentiment shows up elsewhere as teams debate filesystems, tool choice (skills vs MCP), caching, and harness design.Top tweets (by engagement)CEO of highest valued company giving a &#8220;conference&#8221; in the middle of a street &#8212; massive engagement meme/event commentary.SpaceX acquires xAI / &#8220;Building an interstellar civilization&#8221;.Codex app day&#8209;1 downloads: &#8220;More than 200k&#8221;.Apple Xcode integrates Claude Agent SDK.OpenAI hires Head of Preparedness.GPT&#8209;5.2 &amp; GPT&#8209;5.2&#8209;Codex now 40% faster (inference stack optimized).AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Qwen3-Coder-Next ReleaseQwen/Qwen3-Coder-Next &#183; Hugging Face (Activity: 842): Qwen3-Coder-Next is a cutting-edge language model designed for coding, featuring 3B activated parameters out of a total 80B, achieving performance comparable to models with 10-20x more active parameters. It supports advanced capabilities like long-horizon reasoning and has a 256k context length, making it ideal for integration with IDEs. The architecture includes 48 layers, gated attention, and a mixture of experts, suitable for dynamic coding tasks. Deployment can be done using SGLang or vLLM, requiring specific versions for optimal performance. More details are available in the original article. One commenter expressed skepticism about the model&#8217;s performance, questioning if a 3B activated parameter model can truly match the quality of larger models like Sonnet 4.5, indicating a need for further validation of these claims.danielhanchen discusses the release of dynamic Unsloth GGUFs for Qwen3-Coder-Next, highlighting upcoming releases of Fp8-Dynamic and MXFP4 MoE GGUFs. These formats are designed to optimize model performance and efficiency, particularly in environments with limited resources. The linked guide provides instructions for using Claude Code and Codex locally with Qwen3-Coder-Next, which could be beneficial for developers looking to integrate these models into their workflows.Ok_Knowledge_8259 expresses skepticism about the claim that a 3 billion activated parameter model can match the quality of larger models like Sonnet 4.5. This comment reflects a common concern in the AI community about the trade-off between model size and performance, suggesting that while smaller models are more efficient, they may not always achieve the same level of quality as their larger counterparts.Septerium notes that while the original Qwen3 Next performed well in benchmarks, the user experience was lacking. This highlights a critical issue in AI model deployment where high benchmark scores do not always translate to practical usability, indicating a need for improvements in user interface and integration to fully leverage the model&#8217;s capabilities.Qwen3-Coder-Next is out now! (Activity: 228): The image announces the release of Qwen3-Coder-Next, an 80B MoE (Mixture of Experts) model with 3B active parameters, designed for efficient coding tasks and local deployment. It emphasizes the model&#8217;s capability in long-horizon reasoning and complex tool use, requiring 46GB of RAM/VRAM for operation. The graph in the image highlights its performance efficiency compared to other models, showcasing its ability to achieve high performance with fewer active parameters. This model is particularly noted for its fast agentic coding capabilities. A user inquired about the feasibility of running the model with 64GB of RAM without VRAM, indicating interest in its hardware requirements. Another comment questions the model&#8217;s performance level, comparing it to &#8216;sonnet 4.5&#8217;, suggesting skepticism or curiosity about its capabilities. Additionally, there is a remark on the absence of a comparison with &#8216;Devstral 2&#8217;, hinting at expectations for benchmarking against specific models.A user inquires about the possibility of running Qwen3-Coder-Next with 64GB of RAM and no VRAM, which suggests interest in the model&#8217;s memory efficiency and potential CPU-only deployment. This highlights the need for understanding the model&#8217;s hardware requirements and optimization for non-GPU environments.Another user questions the model&#8217;s performance by comparing it to &#8216;sonnet 4.5 level&#8217;, indicating skepticism about the model&#8217;s capabilities or potential over-optimization for specific benchmarks. This reflects a common concern in AI model evaluations where performance might be tailored to excel in certain tests rather than general use cases.A technical query is raised about the appropriate quantization for a setup with 28GB NVIDIA VRAM and 96GB DDR5 RAM. This suggests a focus on optimizing the model&#8217;s performance for specific hardware configurations, which is crucial for maximizing efficiency and speed in high-performance computing environments.2. ACE-Step 1.5 Audio Model LaunchACE-Step-1.5 has just been released. It&#8217;s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno (Activity: 408): ACE-Step-1.5 is an open-source audio generative model released under the MIT license, offering performance comparable to commercial platforms like Suno. It supports LoRAs, multiple models for various needs, and features like cover and repainting. The model is integrated with Comfy and available for demo on HuggingFace. This release marks a significant advancement in open-source audio generation, narrowing the gap with top-tier commercial solutions. One comment highlights skepticism about the model&#8217;s prompt adherence, noting that demo prompts often don&#8217;t align with outputs, suggesting potential limitations in instruction following.The release of ACE-Step-1.5, an MIT-licensed open-source audio generative model, is notable for its performance, which is reportedly close to commercial platforms like Suno. This model&#8217;s efficiency is highlighted by its ability to generate outputs in just 2 seconds on an A100 GPU, indicating significant computational optimization.There is skepticism about the model&#8217;s adherence to input prompts, as some users have observed that the demo prompts do not align closely with the generated outputs. This raises questions about the model&#8217;s instruction-following capabilities and the effectiveness of its prompt processing.The discussion also touches on the model&#8217;s capabilities in generating instrumental music. A user compares it to HeartMuLa, noting that while HeartMuLa cannot produce instrumentals without vocals, it is unclear if ACE-Step-1.5 can fulfill this specific requirement, indicating a potential area for further exploration or development.The open-source version of Suno is finally here: ACE-Step 1.5 (Activity: 319): ACE-Step 1.5 is an open-source music generation model that outperforms Suno on standard evaluation metrics. It can generate a complete song in approximately 2 seconds on an A100 GPU and operates locally on a typical PC with around 4GB VRAM, achieving under 10 seconds on an RTX 3090. The model supports LoRA for training custom styles with minimal data and is released under the MIT license, allowing free commercial use. The dataset includes fully authorized and synthetic data. The project is fully open-source, with GitHub resources available for weights, training code, LoRA code, and the research paper. Commenters noted the model&#8217;s significant improvements over previous versions but criticized its instruction following and coherency compared to Suno v3. Despite these issues, the audio quality is considered good, and the model is seen as a creative alternative to Suno. There is anticipation for a version 2 release.TheRealMasonMac highlights that ACE-Step 1.5 shows a significant improvement over its predecessor, but it still lags behind Suno v3 in terms of instruction following and coherency. However, the audio quality is noted to be good, and the model is described as creative and different from Suno, suggesting it could be a solid foundation for future development.Different_Fix_2217 provides examples of audio generated by ACE-Step 1.5, indicating that the model performs well with long, detailed prompts and can handle negative prompts. This suggests flexibility in input handling, which could be beneficial for users looking to experiment with various prompt styles.3. Local LLM Developments and Comparisons128GB devices have a new local LLM king: Step-3.5-Flash-int4 (Activity: 619): The Step-3.5-Flash-int4 model, available on Hugging Face, is a new local LLM optimized for devices with 128GB RAM, such as the M1 Ultra Mac Studio. It supports a full context length of 256k and demonstrates high efficiency in RAM usage. Benchmarks using llama-bench show impressive performance with 100k prefill, maintaining usability for CLI coding agents. The model requires a custom llama.cpp fork for execution, with potential for upstream support due to its performance. Commenters are curious about the model&#8217;s performance on different hardware, such as Strix Halo, and express interest in a potential NVFP4 version. There is also a light-hearted comment on the model&#8217;s name.The benchmark results for the Step-3.5-Flash-Int4 model on the AMD Strix Halo (Minisforum MS S1 Max) using ROCm 7.1.1 show impressive performance, with a throughput of 258.82 &#177; 3.15 tokens per second for the pp4096 test. This suggests that the model can handle full context fitting efficiently, making it a strong contender for local LLM tasks on 128GB devices.Comparative performance on different backends reveals that the Step-3.5-Flash-Int4 model performs best on ROCm, with a significant drop in throughput when using Vulkan-amdvlk and Vulkan-radv. For instance, the pp4096 test on Vulkan-amdvlk yields 153.04 &#177; 0.30 tokens per second, while Vulkan-radv achieves 164.20 &#177; 1.30, indicating that ROCm is the optimal backend for this model.The Step-3.5-Flash-Int4 model&#8217;s performance on the tg512 test varies significantly across backends, with ROCm achieving 22.93 &#177; 0.00 tokens per second, while Vulkan-amdvlk and Vulkan-radv show much lower performance at 2.50 &#177; 0.00 and 27.86 &#177; 0.00 tokens per second, respectively. This highlights the importance of backend selection in optimizing model performance.Local model fully replacing subscription service (Activity: 270): The post discusses the effectiveness of local models, specifically Ollama + GPT-OSS:20b, on a MacBook Pro M4 Pro with 24GB memory, suggesting it can replace subscription services like ChatGPT for non-complex queries. The user highlights the model&#8217;s speed and quality, noting it performs well for tasks like research queries and basic coding. A comment suggests using mlx based models on Apple silicon for a 40% increase in token per second speed, accessible via LMstudio. Another comment notes that GPT-OSS:20b can efficiently run with a 128k context using 17GB VRAM, leaving room for other GPU tasks. The discussion also touches on building local agent frameworks to match the capabilities of subscription models like Claude, with a focus on integrating tools and skills to enhance local model performance. Commenters debate the efficiency of local models versus subscription services, with some suggesting that models like Claude still outperform local options for complex tasks. There&#8217;s also a discussion on the minimum model size for effective tool-calling agents, with 30b being suggested as a baseline for reliable performance.coldy___ highlights the performance benefits of using MLX-based models on Apple Silicon, noting a potential 40% increase in token per second speed. They recommend using LM Studio to access these models, specifically mentioning the gpt-oss 20b model as optimized for this hardware.generousone discusses the efficiency of the gpt-oss:20b model, which can run with a full 128k context using only 17GB of VRAM. This leaves room for other GPU-intensive tasks, making it a practical choice for users with 24GB VRAM. They acknowledge it&#8217;s not as advanced as commercial models like ChatGPT or Claude but find it sufficient for many tasks.2BucChuck shares insights on building a local agent framework to overcome limitations of local models, testing models like Gemma32 against agent tasks. They suggest a minimum model size of 30B for effective tool-calling agents, noting smaller models often underperform. The goal is to match the functionality of subscription services by integrating tools and skills into local models.New 1.4B Model Victorian LLM - Violet (Activity: 67): The post introduces Violet, a new 1.4 billion parameter LLM trained entirely on Victorian-era data (1800-1899), aiming to create an ethically sourced, public domain model. The model was developed from scratch, using data from sources like the Internet Archive, Project Gutenberg, and the British National Library, and includes ONNX quantized versions for local browser use. The model is noted for its narrative prose capabilities but has limitations in reasoning and historical biases, such as misgendering. The project also features a unique chat variant with mood-based avatars, and the model is available on Hugging Face with demos linked here. A commenter inquires about the model&#8217;s ability to understand modern phrases, questioning if it can only communicate in the vernacular of Victorian England, suggesting a potential limitation in comprehending contemporary language.thirsty_pretzelzz raises an interesting point about the Victorian LLM&#8217;s language capabilities, questioning whether it can only communicate using the vernacular of Victorian England. This implies a potential limitation in understanding modern phrases, which could affect its applicability in contemporary contexts.avanlabs expresses interest in training a similar model on specific datasets for deployment on small devices. They request resources or blogs that could provide insights into building and optimizing small language models (SLMs), indicating a focus on efficient model training and deployment strategies.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Sonnet 5 and Gemini 3.5 Release DiscussionsSonnet 5 release on Feb 3 (Activity: 2328): The leaked details about Claude Sonnet 5, codenamed &#8220;Fennec,&#8221; suggest it is a significant advancement over previous models, with a potential release date of February 3, 2026, as indicated by a Vertex AI error log. It is rumored to be 50% cheaper than Claude Opus 4.5 while maintaining a 1M token context window and offering faster performance, likely due to optimization on Google TPUs. The model is also said to feature a &#8220;Dev Team&#8221; mode, allowing autonomous sub-agents to build features collaboratively. Benchmarking claims suggest it surpasses 80.9% on SWE-Bench, outperforming current coding models. There is skepticism about the release timing, as some users argue that the error log does not conclusively prove the model&#8217;s existence or its release date. Additionally, concerns are raised about the accuracy degradation in large context windows, which was an issue in previous models.andrew_kirfman discusses skepticism about the timing of the Sonnet 5 release, referencing a 404 error from a vertex API endpoint that doesn&#8217;t confirm the model&#8217;s existence. They highlight that Anthropic&#8217;s model IDs often reflect the creation date of the model checkpoint, not the release date, citing Opus 4.5&#8217;s ID 20251101 as an example. They express doubt about future-dating a release tag, which is uncommon in software releases.andrew_kirfman also mentions the potential for a 1 million token context in Sonnet 5, noting that previous models like Sonnet 4 and 4.5 already offered this through the API. However, they point out that accuracy degradation was an issue with these models, suggesting that improvements in this area would be necessary for trust in the new release.Claude Sonnet 5: The &#8220;Fennec&#8221; Leaks (Activity: 193): The image is a tweet by Pankaj Kumar discussing leaks about &#8220;Claude Sonnet 5,&#8221; codenamed &#8220;Fennec.&#8221; It highlights features such as a potential release date of February 3, 2026, aggressive pricing, and advanced capabilities like TPU acceleration and specialized sub-agents. The model is rumored to be significantly cheaper and faster than its predecessor, with a large context window and high benchmarking performance. Additionally, it suggests that the model is already integrated into Google&#8217;s infrastructure. Image URL Commenters express skepticism about the leak&#8217;s credibility and the feasibility of the claimed &#8220;one million context&#8221; capability, noting that current models struggle with much smaller context sizes.DavidAdamsAuthor raises skepticism about the &#8216;one million context&#8217; claim for the Claude model, noting that in practical use, even at &#8216;250k&#8217; context, there is a noticeable &#8216;degradation of ability and forgetfulness of key data&#8217;. This suggests potential limitations in the model&#8217;s performance when handling large context sizes, which could impact its effectiveness in tasks requiring extensive memory.Sonnet 5 being release on Wednesday where is Gemini 3.5 ? (Activity: 182): Claude Sonnet 5 is anticipated to be released soon, with rumors suggesting it will be 50% cheaper than its predecessor, Claude Opus 4.5, while offering superior performance. The model, internally codenamed &#8220;Fennec,&#8221; is reportedly a generation ahead of Gemini&#8217;s &#8220;Snow Bunny&#8221; and is expected to launch on February 3, 2026, as indicated by a Vertex AI error log. It maintains a 1M token context window and is optimized for Google TPUs, promising faster processing and lower latency. Notably, it can spawn specialized sub-agents for tasks like backend development and QA, and it scores 80.9% on SWE-Bench, outperforming current coding models. The existence of the model in Google&#8217;s infrastructure is suggested by a 404 error on its specific ID, indicating it is ready for activation. Commenters express skepticism about the release of Gemini 3.5, noting that Gemini 3 is still in preview and facing issues. There is doubt about the existence of Gemini 3.5, with some considering it a &#8216;pipe dream&#8217; at this stage.alexander_chapel highlights that Gemini 3 is still in preview, questioning the expectation of a 3.5 release. This suggests that the development cycle is not yet at a stage where a 3.5 version would be feasible, indicating a misunderstanding or misinformation about the release timeline.Lost-Estate3401 points out that the Pro version of Gemini 3 is still in preview and has numerous issues, implying that a 3.5 version is unlikely to be released soon. This comment underscores the current instability and challenges faced in the development of Gemini 3, which would need resolution before any further versioning.philiposull compares Gemini 3 unfavorably to other models like 4-5 opus in terms of writing capabilities, suggesting that Google is lagging behind in this area. This indicates a performance gap that might need addressing before advancing to a 3.5 version, highlighting the competitive landscape in AI model development.2. AI Model Performance and ComparisonsCodex 5.2 High vs. Opus: A brutal reality check in Rust development. (Activity: 389): The post highlights a significant performance gap between Codex 5.2 High and Opus in Rust development, with Codex solving issues in 2 hours that Opus couldn&#8217;t handle in 24 hours on the Max200 plan. The author criticizes Opus for failing to implement solutions effectively, often introducing more bugs, despite using advanced workflows like code review and multi-skill modes. The author suggests that unless Sonnet 5 offers substantial improvements, Anthropic may fall behind in the AI race, as Codex&#8217;s problem-solving capabilities outweigh Opus&#8217;s speed advantages. One commenter suggests a phased approach with Opus, using implementation plans and document reviews, which has worked well for them. Another commenter finds Opus 4.5 nearly as effective as Codex 5.2, questioning the complexity of the use cases being discussed.TigerShark109 discusses a phased approach to using Opus for Rust development, suggesting the creation of implementation plans and documentation for review. This method reportedly leads to major success, indicating a structured workflow might enhance Opus&#8217;s effectiveness in complex projects.IndraVahan notes that Opus 4.5 performs nearly as well as 5.2 High/Xtra High in terms of speed and quality, suggesting that the newer version may not offer significant improvements for less complex use cases. This implies that the choice between versions might depend on the complexity of the task at hand.leo-dip highlights a practical consideration in tool selection, noting that Codex offers more generous usage quotas compared to Anthropic&#8217;s offerings. This could influence the decision for developers who are concerned about resource limitations.How Can OpenAI and Anthropic Stay Solvent With Google, xAI, and Meta in High-End Markets, and Chinese/Open Source Devs in the Rest? (Activity: 39): The post questions the long-term profitability of OpenAI and Anthropic in the face of competition from Google, xAI, and Meta in high-end markets, and from Chinese and open-source developers in mid-tier and low-end markets. The author highlights the narrowing performance gaps in AI benchmarks such as ARC-AGI-2, Humanity&#8217;s Last Exam, SWE-bench Verified, GPQA, Chatbot Arena, and HumanEval, suggesting that the competitive edge of OpenAI and Anthropic is diminishing. The post argues that without securing high-end markets like healthcare, defense, education, and government, these companies may struggle to meet debt obligations and achieve profitability. One commenter suggests that OpenAI is relying on a &#8216;Too Big To Fail&#8217; strategy, integrating its technology widely to maintain relevance despite not being the top performer. Another comment dismisses Meta&#8216;s potential in high-end markets, while a third notes that GPT-5.1/2 models are uniquely intelligent beyond benchmarks, despite perceived regressions in newer versions.soumen08 highlights that GPT-5.1/2 models are perceived as the most intelligent beyond standard benchmarks, suggesting a regression in performance with GPT-3 Pro compared to 2.5 Pro for out-of-scope tasks. This indicates a nuanced understanding of model capabilities beyond just benchmark scores, emphasizing real-world application performance.ExpertPerformer discusses the strategic positioning of AI companies, noting that survival depends on carving out niches beyond just competing on benchmarks. They mention that models like Gemini, Grok, and ChatGPT are multimodal, offering features beyond text, which differentiates them from cheaper open-source alternatives. This highlights the importance of feature diversity and enterprise market focus for monetization and security.Emergency-Pomelo-256 speculates on the economic implications of OpenAI&#8217;s potential failure, suggesting that it could trigger a significant downturn in the AI industry, akin to a bubble burst. They propose that entities like Nvidia or government intervention might be necessary to stabilize the market, reflecting concerns about the broader economic impact of major AI companies&#8217; solvency.Notes after testing OpenAI&#8217;s Codex App on real execution tasks (Activity: 30): OpenAI&#8217;s new Codex App is being tested for its ability to handle real development tasks, with some developers dubbing it a &#8220;Cursor killer.&#8221; Unlike traditional interactive coding tools like Cursor, Codex treats development as a task that runs to completion, encompassing planning, execution, testing, and follow-up changes within a single task. This approach allows for parallel work using Git worktrees, keeping tasks isolated and reviewable, and shifts the developer&#8217;s role from steering edits to reviewing outcomes. The focus is on task completion rather than continuous interaction, which may explain the &#8220;Cursor killer&#8221; label. A detailed technical breakdown is available here. A notable opinion from the comments suggests that Codex shifts the developer&#8217;s role to that of an orchestrator, akin to cloud computing, where the focus is on outcomes rather than collaboration. This reflects a broader trend towards higher abstraction in development tools, with expectations that OpenAI&#8217;s offerings will continue to improve.The commenter discusses the role of Codex as an orchestrator, likening it to a cloud service where users can request suggestions and execute tasks. They highlight the shift from merely generating outcomes to enabling collaboration, suggesting that Codex represents a new layer of abstraction in programming. This abstraction allows developers to &#8216;orchestrate the orchestrator,&#8217; indicating a potential shift in how developers interact with AI tools.3. AI in Creative and Video ProductionSeeing the BMW M3 GTR Everywhere &#8212; How Are These Videos Made? (Activity: 1): The videos featuring the BMW M3 GTR from Need for Speed: Most Wanted are likely created using advanced video editing techniques, possibly involving AI-driven tools like Qwen and Wan. These tools can perform realistic object replacement and scene integration, allowing the car to appear seamlessly in various environments. The realism is achieved through sophisticated algorithms that maintain consistent lighting, shadows, and reflections, making the car appear naturally integrated into the scenes. This process involves tracking the vehicle&#8217;s position and orientation across frames and applying digital effects to match the surrounding environment.One user explains that the videos featuring the BMW M3 GTR are often created using advanced video editing software like Adobe After Effects or Blender. These tools allow creators to superimpose the car into various scenes, using techniques such as motion tracking and CGI to make the integration seamless. This process involves detailed work to match lighting and shadows to the environment, ensuring the car appears naturally within the scene.Another comment highlights the use of video game engines, such as Unreal Engine or Unity, to render realistic scenes with the BMW M3 GTR. These engines provide high-quality graphics and physics simulations, allowing creators to produce videos that look almost indistinguishable from real life. The use of ray tracing and PBR (Physically Based Rendering) materials in these engines enhances the realism of the car&#8217;s appearance and interaction with the environment.A technical discussion points out the role of machine learning in enhancing video quality and realism. Techniques like neural rendering and AI-based upscaling are used to improve the visual fidelity of the BMW M3 GTR in videos. These methods can refine textures and details, making the car look more lifelike, and are often employed in post-production to enhance the final output.How to create videos with swift actions + perfect lip sync (Activity: 1856): The post discusses techniques for creating videos with precise lip synchronization and swift actions, likely involving AI-driven tools or software. The focus is on achieving seamless integration of audio and visual elements, possibly using advanced algorithms or machine learning models to enhance the realism of the video content. The mention of AI suggests the use of deep learning frameworks or specialized software for video editing and synthesis. One comment highlights the difficulty in detecting AI-generated content, suggesting the effectiveness of the techniques discussed. Another comment implies that the realism of the video is enhanced by subtle details, such as hand movements, which contribute to the overall believability of the AI-generated video.I created a 10-minute AI film - The Last Signal (YouTube) (Activity: 17): Richard Galapate&#8217;s AI film, The Last Signal, was submitted to the 1 Billion Followers Summit AI Film competition. The film features astronaut Jake Ward on a Mars outpost, using AI tools like Google Veo 3.1 for visuals and voice, Google Gemini for prompting, and ElevenLabs for Lyra&#8217;s voice. This project highlights the potential of AI in creating consistent and efficient film content. The original video can be viewed here. The comments reflect a positive reception, with praise for storytelling and emotional impact, though lacking in technical critique.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. Agentic Coding &amp; Dev Tooling Goes Local-FirstCodex Goes Desktop: macOS Agent Command Center: OpenAI shipped the Codex app for macOS as an agent-building command center, available for Plus/Pro/Business/Enterprise/Edu with limited-time access on ChatGPT Free/Go, per &#8220;Introducing the Codex app&#8221; and the Codex landing page.The launch also spilled into community workflow chatter (pairing agents, multi-agent &#8220;command centers&#8221;), and a related Codex App hackathon with $90,000 in credits showed up via Cerebral Valley&#8217;s event page.LM Studio Speaks Anthropic: Claude Code Meets Your Local GGUF/MLX: LM Studio 0.4.1 added an Anthropic /v1/messages compatibility API, letting developers point Claude Code-style tools at local GGUF/MLX models by changing the base URL, detailed in &#8220;Using Claude Code with LM Studio&#8221;.In parallel, LM Studio also pushed a TypeScript SDK for third-party plugins and an OpenAI-compatible endpoint (SDK link), reinforcing a growing pattern: reuse existing agent tooling while swapping the backend model stack locally.Arena Mode Everywhere: Windsurf Turns Model Eval into a Game: Windsurf shipped Wave 14 with Arena Mode for side-by-side model battles (including Battle Groups and &#8220;Pick your own&#8221;), and temporarily set Battle Groups to 0x credits via the Windsurf download page.This mirrored broader &#8220;live eval&#8221; momentum: users also tracked new Arena entrants like step-3.5-flash and qwen3-max-thinking on LMArena&#8217;s Text Arena and Code Arena, shifting selection from static benchmarks to continuous human voting.2. Model Releases &amp; Bench Races (Kimi vs GLM vs Qwen)Kimi K2.5 Speedruns the Leaderboards: Moonshot&#8217;s Kimi K2.5 landed broadly in product surfaces: Perplexity Pro/Max added it for subscribers and said it runs on a US-based inference stack for tighter latency/reliability/security control (announcement screenshot: https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg).Community results piled on: LMArena reported Kimi-K2.5-thinking hit #1 open and #5 overall in Code Arena (see Code Arena), while multiple dev channels argued over its tool-calling reliability and provider variance when routed through aggregators.GLM-4.7 Flash: Small Model, Big Front-End Energy: Developers highlighted GLM-4.7 flash as a surprisingly strong coding model&#8212;especially for interactive website/front-end work&#8212;citing preserved reasoning and interleaved capability, with discussion anchored on ggerganov&#8217;s post.The debate sharpened around whether stripping &#8220;thinking&#8221; harms performance, and several users described pairing GLM-4.7 with Claude Code (or Claude-like agent tooling) as a pragmatic hybrid stack: cheap execution + expensive review.New Arena Entrants: step-3.5-flash &amp; qwen3-max-thinking Join the Party: LMArena added step-3.5-flash to the Text Arena and qwen3-max-thinking to the Code Arena, explicitly positioning them as fresh baselines for side-by-side evaluation.Users used these drops to re-litigate &#8220;model preference&#8221; threads (Kimi vs GLM vs Gemini), with the recurring takeaway that leaderboards and live evals increasingly drive adoption more than vendor marketing.3. Training Signals, Dense Rewards, and New Architectures/DatasetsFrom Binary Rewards to Dense Supervision: RL Gets Wordy: Multiple communities converged on richer post-training signals: Unsloth discussions pushed training with logprobs of final answers and non-binary rewards, referencing Jonas H&#252;botter&#8217;s method for turning descriptive feedback into dense supervision (H&#252;botter thread).The sticking point stayed practical: people asked for verifiable datasets for RL training agentic coding, implying a pipeline gap between &#8220;cool reward shaping idea&#8221; and &#8220;reproducible, automated evaluation harness.&#8221;Complexity-Deep: Token-Routed MLP Tries MoE Without the Load-Balancing Headache: The Complexity-Deep (1.5B) architecture open-sourced Token-Routed MLP for MoE-style routing &#8220;without load balancing loss,&#8221; plus Mu-Guided Attention and a PiD Controller, shipping code at Complexity-ML/complexity-deep and reporting 20.6% MMLU (base).The community framed it as another step in the &#8220;routing without pain&#8221; trend&#8212;trying to keep MoE wins while reducing the training-time engineering tax of balancing experts.Moltbook Data Dump: 50k Posts for Agent Sociology: A dataset scrape of Moltbook landed on Hugging Face with 50,539 posts, 12,454 AI agents, 195,414 comments, and 1,604 communities, published as lysandrehooh/moltbook.Elsewhere, researchers flagged the security implication behind agent platforms (auth tokens on machines, bot authenticity concerns) and treated the dataset as fuel for analyzing emergent behavior&#8212;without needing to speculate beyond the raw logs.4. GPU/Kernel Engineering: Faster Attention, Better Profiling, Weirder PTXFlashAttention v3 Hits RDNA: AMD Users Get Their Turn: A FlashAttention update added RDNA GPU support via the ongoing work in flash-attention PR #2178, aiming to reduce attention bottlenecks on AMD cards.The tone across servers was basically: this is the sort of &#8220;unsexy infra work&#8221; that actually unlocks local inference and finetuning on non-NVIDIA hardware&#8212;especially when paired with open-weight models and desktop agent tooling.Triton-Viz v3.0: Tile-Kernel Debugging Gets Teeth: Triton-Viz v3.0 shipped with broader profiling support (including Triton and Amazon NKI) plus a sanitizer for out-of-bounds access and a profiler that flags inefficient loops, per the release announcement (Discord link: https://discord.com/channels/1189498204333543425/1225499141241573447/1467634539164602563).It also hooked into triton-puzzles via a shared Colab notebook (Colab), and maintainers even floated moving srush/Triton-Puzzles under the GPU Mode org to keep bugfix velocity high.sm120: TMA + mbarrier Beats cp.async (Barely), cuBLAS Still Ships sm80 Kernels: Experiments on sm120 showed that careful TMA + mbarrier implementation can edge out cp.async for larger matrix shapes, while also surfacing that cuBLAS still appears to run sm80 kernels even when newer mechanisms exist.On the debugging front, one CUDA/PTX deadlock got fixed by inserting __syncthreads() after MMA before prefetching the next TMA, turning a hang into a measurable perf gain&#8212;exactly the kind of &#8220;one barrier to rule them all&#8221; lesson kernel folks keep re-learning.5. Security, Determinism, and Agent Misbehavior (the Practical Kind)Prompt Injection Defense Arms Race: Embeddings + Grammar-Constrained Decoding: Red teamers shared a structured exercise site for adversarial practice&#8212;&#8220;Adversarial Design Thinking&#8221;&#8212;and used it to tee up concrete mitigations for prompt injection.One proposed &#8220;belt + suspenders&#8221; defense combined embedding-based filtering with Grammar Constrained Decoding, with the explicit goal of reducing injection surface by constraining the model&#8217;s output space rather than only policing inputs.Deterministic Reasoning and &#8220;Strict Mode&#8221; Fever Spreads: Across OpenAI and OpenRouter discussions, users pushed for determinism/replayability/traceability in LLM reasoning; one person offered a deterministic reasoning engine that enforces a fixed structure and emits a 32D statistical vector trace (no public link shared).In OpenRouter, the same instinct showed up as skepticism about response healing and calls for a strict mode that keeps tool calls and outputs predictable&#8212;plus suggestions that better argument descriptions/examples improve tool-call accuracy.OpenClaw: Cool Agent Tricks, Scary Bills, and &#8220;2/100 Security&#8221;: OpenClaw sparked repeated warnings: OpenRouter users reported it can drain credits fast (including one drained Claude Max subscription), while an OpenAI server linked a security assessment claiming OpenClaw scored 2/100 (Perplexity result).Meanwhile, &#8220;works on my machine&#8221; stories (local models controlling devices, trading jokes) collided with real operational concerns&#8212;tool permissions, moderation/refusals (especially around jailbreak-y queries), and the need for observability and human-in-the-loop gates in agent workflows.",
      "url": "https://www.latent.space/p/ainews-context-graphs-hype-or-actually",
      "author": "Unknown",
      "published": "2026-02-04T03:13:58",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "AI news roundup covering Context Graphs, a technique launched in December that's gaining traction among AI practitioners and inspiring both adoption and skepticism from industry figures.",
      "importance_score": 51.0,
      "reasoning": "Aggregated AI news with focus on emerging technique, useful for tracking trends but not breaking news.",
      "themes": [
        "AI techniques",
        "industry trends"
      ],
      "continuation": null,
      "summary_html": "<p>AI news roundup covering Context Graphs, a technique launched in December that's gaining traction among AI practitioners and inspiring both adoption and skepticism from industry figures.</p>",
      "content_html": "<p>AI News for 1/30/2026-2/2/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (254 channels, and 14979 messages) for you. Estimated reading time saved (at 200wpm): 1408 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Our policy for quiet days is that we’ll now feature longer running, slow-burn stories that would otherwise not spike to the top of the heap on a certain day but will probably be of broader historical significance to AI Engineers. Today’s Lightning Pod (our Youtube-only short format) features the topic of Context Graphs, which Jaya Gupta launched late December on X and has since inspired even former guests like Dharmesh Shah (who has reservations). We chatted with both authors on the response:That’s thoughtleading 101, but definitely helpful — for sure every founder building a data/context engineering product will go to them and say they got the people who coined Context Graphs on their cap table. But the problem with the post is that it promises a whole lot (evidenced in the title), but is not very prescriptive.Recently, I also framed Cursor’s Agent Trace initiative as a “Context Graph” for Code:This is the first actual specification for a context graph for a specific domain (coding agents) that is agreed on between companies. It remains to be seen if it has actual staying power, which will mostly be driven by 1) high demonstrated improvement in agent performance, and 2) customer pressure to support it. Based on first principles, the idea (capture decision traces, exceptions and precedents scattered all over the “data mesh” into the context of an LLM) seems compelling, but of course, the devil is in the details.AI Twitter RecapZhipu AI’s GLM‑OCR launch (0.9B) and day‑0 deployment support across stacksGLM‑OCR (multimodal OCR for complex documents): Zhipu released GLM‑OCR, positioned as a lightweight, deployable 0.9B model for real-world document understanding (tables, formulas, information extraction, messy layouts). It’s reported #1 on OmniDocBench v1.5 (94.62) and emphasized as low‑latency / high‑concurrency friendly. See the ecosystem “day‑0 support” announcements from @lmsysorg (SGLang integration + PR/cookbook links) and @vllm_project (vLLM day‑0 support), plus deployment marketing from @novita_labs.Local-first availability: Ollama shipped immediate local pulls + API usage (“drag and drop images into terminal”, JSON‑formatted outputs), making GLM‑OCR easy to run offline: @ollama and library link @ollama. Community comparisons also claim strong quality vs PaddleOCR/DeepSeek OCR: @bdsqlsz. LlamaIndex highlighted benchmark displacement (claiming 50–100% faster vs prior top model) and ongoing eval integration: @jerryjliu0.Agentic coding models &amp; harnesses: Qwen3‑Coder‑Next (80B@3B), SERA‑14B, and the “skills/MCP” tool interface convergenceQwen3‑Coder‑Next: Alibaba released Qwen3‑Coder‑Next, an open‑weight 80B MoE with only 3B active parameters, pitched for coding agents + local dev with 256K context, trained with 800K verifiable tasks + executable environments. They claim &gt;70% SWE‑Bench Verified with SWE‑Agent scaffold and strong agent benchmark efficiency: @Alibaba_Qwen and benchmark callout @Alibaba_Qwen. Independent/adjacent summaries: @UnslothAI (memory footprint + GGUF guidance) and commentary on efficient long‑context attention choices (e.g., “Gated DeltaNet” mentioned in the discourse): @eliebakouch. vLLM shipped day‑0 support in vLLM 0.15.0: @vllm_project.Open Coding Agents ecosystem (Ai2): Allen AI announced SERA‑14B (on‑device‑friendly coding model) plus refreshed open datasets that include raw trajectories + verification metadata: @allen_ai and dataset/model detail thread pointer @ethnlshn.Harness &gt; model (recurring theme): Multiple tweets converge on the idea that the leverage in agents is increasingly in the harness (permissions, memory, workflows, reversibility), not just raw model IQ. A clear articulation: @sarahmsachs.Standardization of agent “skills” directories + protocols:Agent Client Protocol (ACP): proposed as a JSON‑RPC standard to unify agent↔editor communication across Gemini CLI / Claude Code / Codex CLI / OpenClaw, supporting stdio/HTTP, file access, terminals, permissions, streaming updates: @_philschmid.Skills vs MCP tools: LlamaIndex contrasted “skills” (easy but brittle, NL‑interpreted) vs MCP servers (more deterministic schemas, more setup, network latency but centralized updates): @llama_index and follow‑ups @jerryjliu0, @itsclelia. Meanwhile, “.agents/skills is becoming a default” was called out explicitly (Codex/OpenCode/Copilot/Cursor adopting; Claude Code not yet): @theo.Coding agent products: Codex app adoption, Claude Code sharing + Apple Xcode integrationsCodex app momentum + inference speedups:Sam Altman reported 200k+ downloads on day 1: @sama.OpenAI shipped 40% faster GPT‑5.2 &amp; GPT‑5.2‑Codex for API customers (“same weights, lower latency”): @OpenAIDevs.Codex integration into Xcode 26.3 was announced by OpenAI DevRel: @OpenAIDevs.Claude Code product iteration:Session sharing for Claude Code across web/desktop/mobile: @lydiahallie.Community “waiting for Sonnet 5” speculation dominated, including the claim that an Anthropic image model is live on LMArena: @kimmonismus and “Claude Image is coming” chatter: @kimmonismus.Apple Xcode + Claude Agent SDK: Anthropic announced native Xcode integration with the Claude Agent SDK (subagents/background tasks/plugins) to bring Claude Code‑like capabilities directly into Apple dev workflows: @AnthropicAI. This is a notable step in “agent-in-the-IDE” becoming first‑party.Agent infrastructure &amp; observability: traces as the source of truth, deep agents evaluation, and memory beyond RAGObservability shifts from code to traces: LangChain argues that for agentic systems, runtime decisions happen in the model—so traces become the primary artifact for debugging/understanding. See: @LangChain.How to evaluate deep agents: LangChain’s eval guidance emphasizes bespoke success criteria per case, single‑step regression checks, full‑turn and multi‑turn evals, and clean/reproducible envs: @LangChain.DeepAgents releases (JS/CLI/runtime backends):deepagents@1.6.2 fixes (checkpoint restore, infinite loop on large files, toolcall middleware simplification): @LangChain_JS.DeepAgents 0.3.10 adds LocalShellBackend for running code on your machine: @sydneyrunkle.deepagents-cli 0.0.16 improves control/visibility for shell runs: @masondrxy.Memory: “RAG wasn’t designed for agent memory”: DAIR’s xMemory proposes hierarchical retrieval (themes/semantics/episodes/messages) to reduce redundancy while preserving evidence chains, showing better LoCoMo scores with fewer tokens than naive top‑k similarity retrieval: @dair_ai.Filesystem as agent context scratchpad: The “files-first” workflow (store artifacts outside context, avoid bloating windows) is reinforced by deepagents’ design and commentary: @LangChain_JS.Benchmarks &amp; evaluation signals: METR time horizons, WorldVQA, Text/Search/Image Arena updates, and ARC‑AGI progressMETR time horizon for Gemini 3 Pro: METR estimates ~4 hours (50% time horizon) on an expanded software task suite (with CI): @METR_Evals. This “time horizon” line of evals continues to become a key agent capability proxy beyond static coding benchmarks.WorldVQA (Moonshot/Kimi): Moonshot introduced WorldVQA to measure “atomic vision-centric world knowledge” separately from reasoning, explicitly trying to decouple memorization from reasoning quality. Dataset: 3,500 VQA pairs across 9 categories with linguistic/cultural diversity: @Kimi_Moonshot.Arena leaderboards:Text Arena (open models, Jan 2026): #1 Kimi‑K2.5‑Thinking, #2 GLM‑4.7, #3 Qwen3‑235B‑A22B Instruct: @arena.Search Arena update: Google’s gemini‑3‑flash‑grounding leads; OpenAI search non‑reasoning appears in top 5; best Claude search variant listed: @arena.Image Arena Pareto frontiers: Arena published quality vs price per image frontiers for text‑to‑image and image edit (notable that several OpenAI/Google/Flux/Tencent models sit on the frontier depending on cost constraints): @arena and edit frontier @arena.ARC‑AGI: ARC Prize reported a new SOTA public submission (with cost/task figures) based on GPT‑5.2 ensembles: @arcprize. Separately, there’s ongoing community chatter on ARC‑AGI‑2 progress rates: @kimmonismus.Efficiency, kernels, and training/inference plumbing: fp8 training, Blackwell throughput, and “context engineering” as inference-era data engineeringKarpathy’s fp8 training notes (practical, not just theory): He reports enabling fp8 training to improve “time to GPT‑2” to 2.91 hours, discusses real bottlenecks (not purely compute‑bound), overheads from scaling conversions, GEMM sizing, and quality degradation per step; notes that larger models see better fp8 upside (citing torchao’s larger gains): @karpathy.vLLM + NVIDIA Blackwell optimization: vLLM reports big perf gains for gpt‑oss‑120b on Blackwell via FlashInfer integration, torch.compile fusions, async scheduling, and stream interval optimizations: @vllm_project.Inference is a first-class engineering surface: “Context engineering is as important to inference as data engineering is to training” was stated succinctly (and repeated): @swyx. This sentiment shows up elsewhere as teams debate filesystems, tool choice (skills vs MCP), caching, and harness design.Top tweets (by engagement)CEO of highest valued company giving a “conference” in the middle of a street — massive engagement meme/event commentary.SpaceX acquires xAI / “Building an interstellar civilization”.Codex app day‑1 downloads: “More than 200k”.Apple Xcode integrates Claude Agent SDK.OpenAI hires Head of Preparedness.GPT‑5.2 &amp; GPT‑5.2‑Codex now 40% faster (inference stack optimized).AI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. Qwen3-Coder-Next ReleaseQwen/Qwen3-Coder-Next · Hugging Face (Activity: 842): Qwen3-Coder-Next is a cutting-edge language model designed for coding, featuring 3B activated parameters out of a total 80B, achieving performance comparable to models with 10-20x more active parameters. It supports advanced capabilities like long-horizon reasoning and has a 256k context length, making it ideal for integration with IDEs. The architecture includes 48 layers, gated attention, and a mixture of experts, suitable for dynamic coding tasks. Deployment can be done using SGLang or vLLM, requiring specific versions for optimal performance. More details are available in the original article. One commenter expressed skepticism about the model’s performance, questioning if a 3B activated parameter model can truly match the quality of larger models like Sonnet 4.5, indicating a need for further validation of these claims.danielhanchen discusses the release of dynamic Unsloth GGUFs for Qwen3-Coder-Next, highlighting upcoming releases of Fp8-Dynamic and MXFP4 MoE GGUFs. These formats are designed to optimize model performance and efficiency, particularly in environments with limited resources. The linked guide provides instructions for using Claude Code and Codex locally with Qwen3-Coder-Next, which could be beneficial for developers looking to integrate these models into their workflows.Ok_Knowledge_8259 expresses skepticism about the claim that a 3 billion activated parameter model can match the quality of larger models like Sonnet 4.5. This comment reflects a common concern in the AI community about the trade-off between model size and performance, suggesting that while smaller models are more efficient, they may not always achieve the same level of quality as their larger counterparts.Septerium notes that while the original Qwen3 Next performed well in benchmarks, the user experience was lacking. This highlights a critical issue in AI model deployment where high benchmark scores do not always translate to practical usability, indicating a need for improvements in user interface and integration to fully leverage the model’s capabilities.Qwen3-Coder-Next is out now! (Activity: 228): The image announces the release of Qwen3-Coder-Next, an 80B MoE (Mixture of Experts) model with 3B active parameters, designed for efficient coding tasks and local deployment. It emphasizes the model’s capability in long-horizon reasoning and complex tool use, requiring 46GB of RAM/VRAM for operation. The graph in the image highlights its performance efficiency compared to other models, showcasing its ability to achieve high performance with fewer active parameters. This model is particularly noted for its fast agentic coding capabilities. A user inquired about the feasibility of running the model with 64GB of RAM without VRAM, indicating interest in its hardware requirements. Another comment questions the model’s performance level, comparing it to ‘sonnet 4.5’, suggesting skepticism or curiosity about its capabilities. Additionally, there is a remark on the absence of a comparison with ‘Devstral 2’, hinting at expectations for benchmarking against specific models.A user inquires about the possibility of running Qwen3-Coder-Next with 64GB of RAM and no VRAM, which suggests interest in the model’s memory efficiency and potential CPU-only deployment. This highlights the need for understanding the model’s hardware requirements and optimization for non-GPU environments.Another user questions the model’s performance by comparing it to ‘sonnet 4.5 level’, indicating skepticism about the model’s capabilities or potential over-optimization for specific benchmarks. This reflects a common concern in AI model evaluations where performance might be tailored to excel in certain tests rather than general use cases.A technical query is raised about the appropriate quantization for a setup with 28GB NVIDIA VRAM and 96GB DDR5 RAM. This suggests a focus on optimizing the model’s performance for specific hardware configurations, which is crucial for maximizing efficiency and speed in high-performance computing environments.2. ACE-Step 1.5 Audio Model LaunchACE-Step-1.5 has just been released. It’s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno (Activity: 408): ACE-Step-1.5 is an open-source audio generative model released under the MIT license, offering performance comparable to commercial platforms like Suno. It supports LoRAs, multiple models for various needs, and features like cover and repainting. The model is integrated with Comfy and available for demo on HuggingFace. This release marks a significant advancement in open-source audio generation, narrowing the gap with top-tier commercial solutions. One comment highlights skepticism about the model’s prompt adherence, noting that demo prompts often don’t align with outputs, suggesting potential limitations in instruction following.The release of ACE-Step-1.5, an MIT-licensed open-source audio generative model, is notable for its performance, which is reportedly close to commercial platforms like Suno. This model’s efficiency is highlighted by its ability to generate outputs in just 2 seconds on an A100 GPU, indicating significant computational optimization.There is skepticism about the model’s adherence to input prompts, as some users have observed that the demo prompts do not align closely with the generated outputs. This raises questions about the model’s instruction-following capabilities and the effectiveness of its prompt processing.The discussion also touches on the model’s capabilities in generating instrumental music. A user compares it to HeartMuLa, noting that while HeartMuLa cannot produce instrumentals without vocals, it is unclear if ACE-Step-1.5 can fulfill this specific requirement, indicating a potential area for further exploration or development.The open-source version of Suno is finally here: ACE-Step 1.5 (Activity: 319): ACE-Step 1.5 is an open-source music generation model that outperforms Suno on standard evaluation metrics. It can generate a complete song in approximately 2 seconds on an A100 GPU and operates locally on a typical PC with around 4GB VRAM, achieving under 10 seconds on an RTX 3090. The model supports LoRA for training custom styles with minimal data and is released under the MIT license, allowing free commercial use. The dataset includes fully authorized and synthetic data. The project is fully open-source, with GitHub resources available for weights, training code, LoRA code, and the research paper. Commenters noted the model’s significant improvements over previous versions but criticized its instruction following and coherency compared to Suno v3. Despite these issues, the audio quality is considered good, and the model is seen as a creative alternative to Suno. There is anticipation for a version 2 release.TheRealMasonMac highlights that ACE-Step 1.5 shows a significant improvement over its predecessor, but it still lags behind Suno v3 in terms of instruction following and coherency. However, the audio quality is noted to be good, and the model is described as creative and different from Suno, suggesting it could be a solid foundation for future development.Different_Fix_2217 provides examples of audio generated by ACE-Step 1.5, indicating that the model performs well with long, detailed prompts and can handle negative prompts. This suggests flexibility in input handling, which could be beneficial for users looking to experiment with various prompt styles.3. Local LLM Developments and Comparisons128GB devices have a new local LLM king: Step-3.5-Flash-int4 (Activity: 619): The Step-3.5-Flash-int4 model, available on Hugging Face, is a new local LLM optimized for devices with 128GB RAM, such as the M1 Ultra Mac Studio. It supports a full context length of 256k and demonstrates high efficiency in RAM usage. Benchmarks using llama-bench show impressive performance with 100k prefill, maintaining usability for CLI coding agents. The model requires a custom llama.cpp fork for execution, with potential for upstream support due to its performance. Commenters are curious about the model’s performance on different hardware, such as Strix Halo, and express interest in a potential NVFP4 version. There is also a light-hearted comment on the model’s name.The benchmark results for the Step-3.5-Flash-Int4 model on the AMD Strix Halo (Minisforum MS S1 Max) using ROCm 7.1.1 show impressive performance, with a throughput of 258.82 ± 3.15 tokens per second for the pp4096 test. This suggests that the model can handle full context fitting efficiently, making it a strong contender for local LLM tasks on 128GB devices.Comparative performance on different backends reveals that the Step-3.5-Flash-Int4 model performs best on ROCm, with a significant drop in throughput when using Vulkan-amdvlk and Vulkan-radv. For instance, the pp4096 test on Vulkan-amdvlk yields 153.04 ± 0.30 tokens per second, while Vulkan-radv achieves 164.20 ± 1.30, indicating that ROCm is the optimal backend for this model.The Step-3.5-Flash-Int4 model’s performance on the tg512 test varies significantly across backends, with ROCm achieving 22.93 ± 0.00 tokens per second, while Vulkan-amdvlk and Vulkan-radv show much lower performance at 2.50 ± 0.00 and 27.86 ± 0.00 tokens per second, respectively. This highlights the importance of backend selection in optimizing model performance.Local model fully replacing subscription service (Activity: 270): The post discusses the effectiveness of local models, specifically Ollama + GPT-OSS:20b, on a MacBook Pro M4 Pro with 24GB memory, suggesting it can replace subscription services like ChatGPT for non-complex queries. The user highlights the model’s speed and quality, noting it performs well for tasks like research queries and basic coding. A comment suggests using mlx based models on Apple silicon for a 40% increase in token per second speed, accessible via LMstudio. Another comment notes that GPT-OSS:20b can efficiently run with a 128k context using 17GB VRAM, leaving room for other GPU tasks. The discussion also touches on building local agent frameworks to match the capabilities of subscription models like Claude, with a focus on integrating tools and skills to enhance local model performance. Commenters debate the efficiency of local models versus subscription services, with some suggesting that models like Claude still outperform local options for complex tasks. There’s also a discussion on the minimum model size for effective tool-calling agents, with 30b being suggested as a baseline for reliable performance.coldy___ highlights the performance benefits of using MLX-based models on Apple Silicon, noting a potential 40% increase in token per second speed. They recommend using LM Studio to access these models, specifically mentioning the gpt-oss 20b model as optimized for this hardware.generousone discusses the efficiency of the gpt-oss:20b model, which can run with a full 128k context using only 17GB of VRAM. This leaves room for other GPU-intensive tasks, making it a practical choice for users with 24GB VRAM. They acknowledge it’s not as advanced as commercial models like ChatGPT or Claude but find it sufficient for many tasks.2BucChuck shares insights on building a local agent framework to overcome limitations of local models, testing models like Gemma32 against agent tasks. They suggest a minimum model size of 30B for effective tool-calling agents, noting smaller models often underperform. The goal is to match the functionality of subscription services by integrating tools and skills into local models.New 1.4B Model Victorian LLM - Violet (Activity: 67): The post introduces Violet, a new 1.4 billion parameter LLM trained entirely on Victorian-era data (1800-1899), aiming to create an ethically sourced, public domain model. The model was developed from scratch, using data from sources like the Internet Archive, Project Gutenberg, and the British National Library, and includes ONNX quantized versions for local browser use. The model is noted for its narrative prose capabilities but has limitations in reasoning and historical biases, such as misgendering. The project also features a unique chat variant with mood-based avatars, and the model is available on Hugging Face with demos linked here. A commenter inquires about the model’s ability to understand modern phrases, questioning if it can only communicate in the vernacular of Victorian England, suggesting a potential limitation in comprehending contemporary language.thirsty_pretzelzz raises an interesting point about the Victorian LLM’s language capabilities, questioning whether it can only communicate using the vernacular of Victorian England. This implies a potential limitation in understanding modern phrases, which could affect its applicability in contemporary contexts.avanlabs expresses interest in training a similar model on specific datasets for deployment on small devices. They request resources or blogs that could provide insights into building and optimizing small language models (SLMs), indicating a focus on efficient model training and deployment strategies.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Claude Sonnet 5 and Gemini 3.5 Release DiscussionsSonnet 5 release on Feb 3 (Activity: 2328): The leaked details about Claude Sonnet 5, codenamed “Fennec,” suggest it is a significant advancement over previous models, with a potential release date of February 3, 2026, as indicated by a Vertex AI error log. It is rumored to be 50% cheaper than Claude Opus 4.5 while maintaining a 1M token context window and offering faster performance, likely due to optimization on Google TPUs. The model is also said to feature a “Dev Team” mode, allowing autonomous sub-agents to build features collaboratively. Benchmarking claims suggest it surpasses 80.9% on SWE-Bench, outperforming current coding models. There is skepticism about the release timing, as some users argue that the error log does not conclusively prove the model’s existence or its release date. Additionally, concerns are raised about the accuracy degradation in large context windows, which was an issue in previous models.andrew_kirfman discusses skepticism about the timing of the Sonnet 5 release, referencing a 404 error from a vertex API endpoint that doesn’t confirm the model’s existence. They highlight that Anthropic’s model IDs often reflect the creation date of the model checkpoint, not the release date, citing Opus 4.5’s ID 20251101 as an example. They express doubt about future-dating a release tag, which is uncommon in software releases.andrew_kirfman also mentions the potential for a 1 million token context in Sonnet 5, noting that previous models like Sonnet 4 and 4.5 already offered this through the API. However, they point out that accuracy degradation was an issue with these models, suggesting that improvements in this area would be necessary for trust in the new release.Claude Sonnet 5: The “Fennec” Leaks (Activity: 193): The image is a tweet by Pankaj Kumar discussing leaks about “Claude Sonnet 5,” codenamed “Fennec.” It highlights features such as a potential release date of February 3, 2026, aggressive pricing, and advanced capabilities like TPU acceleration and specialized sub-agents. The model is rumored to be significantly cheaper and faster than its predecessor, with a large context window and high benchmarking performance. Additionally, it suggests that the model is already integrated into Google’s infrastructure. Image URL Commenters express skepticism about the leak’s credibility and the feasibility of the claimed “one million context” capability, noting that current models struggle with much smaller context sizes.DavidAdamsAuthor raises skepticism about the ‘one million context’ claim for the Claude model, noting that in practical use, even at ‘250k’ context, there is a noticeable ‘degradation of ability and forgetfulness of key data’. This suggests potential limitations in the model’s performance when handling large context sizes, which could impact its effectiveness in tasks requiring extensive memory.Sonnet 5 being release on Wednesday where is Gemini 3.5 ? (Activity: 182): Claude Sonnet 5 is anticipated to be released soon, with rumors suggesting it will be 50% cheaper than its predecessor, Claude Opus 4.5, while offering superior performance. The model, internally codenamed “Fennec,” is reportedly a generation ahead of Gemini’s “Snow Bunny” and is expected to launch on February 3, 2026, as indicated by a Vertex AI error log. It maintains a 1M token context window and is optimized for Google TPUs, promising faster processing and lower latency. Notably, it can spawn specialized sub-agents for tasks like backend development and QA, and it scores 80.9% on SWE-Bench, outperforming current coding models. The existence of the model in Google’s infrastructure is suggested by a 404 error on its specific ID, indicating it is ready for activation. Commenters express skepticism about the release of Gemini 3.5, noting that Gemini 3 is still in preview and facing issues. There is doubt about the existence of Gemini 3.5, with some considering it a ‘pipe dream’ at this stage.alexander_chapel highlights that Gemini 3 is still in preview, questioning the expectation of a 3.5 release. This suggests that the development cycle is not yet at a stage where a 3.5 version would be feasible, indicating a misunderstanding or misinformation about the release timeline.Lost-Estate3401 points out that the Pro version of Gemini 3 is still in preview and has numerous issues, implying that a 3.5 version is unlikely to be released soon. This comment underscores the current instability and challenges faced in the development of Gemini 3, which would need resolution before any further versioning.philiposull compares Gemini 3 unfavorably to other models like 4-5 opus in terms of writing capabilities, suggesting that Google is lagging behind in this area. This indicates a performance gap that might need addressing before advancing to a 3.5 version, highlighting the competitive landscape in AI model development.2. AI Model Performance and ComparisonsCodex 5.2 High vs. Opus: A brutal reality check in Rust development. (Activity: 389): The post highlights a significant performance gap between Codex 5.2 High and Opus in Rust development, with Codex solving issues in 2 hours that Opus couldn’t handle in 24 hours on the Max200 plan. The author criticizes Opus for failing to implement solutions effectively, often introducing more bugs, despite using advanced workflows like code review and multi-skill modes. The author suggests that unless Sonnet 5 offers substantial improvements, Anthropic may fall behind in the AI race, as Codex’s problem-solving capabilities outweigh Opus’s speed advantages. One commenter suggests a phased approach with Opus, using implementation plans and document reviews, which has worked well for them. Another commenter finds Opus 4.5 nearly as effective as Codex 5.2, questioning the complexity of the use cases being discussed.TigerShark109 discusses a phased approach to using Opus for Rust development, suggesting the creation of implementation plans and documentation for review. This method reportedly leads to major success, indicating a structured workflow might enhance Opus’s effectiveness in complex projects.IndraVahan notes that Opus 4.5 performs nearly as well as 5.2 High/Xtra High in terms of speed and quality, suggesting that the newer version may not offer significant improvements for less complex use cases. This implies that the choice between versions might depend on the complexity of the task at hand.leo-dip highlights a practical consideration in tool selection, noting that Codex offers more generous usage quotas compared to Anthropic’s offerings. This could influence the decision for developers who are concerned about resource limitations.How Can OpenAI and Anthropic Stay Solvent With Google, xAI, and Meta in High-End Markets, and Chinese/Open Source Devs in the Rest? (Activity: 39): The post questions the long-term profitability of OpenAI and Anthropic in the face of competition from Google, xAI, and Meta in high-end markets, and from Chinese and open-source developers in mid-tier and low-end markets. The author highlights the narrowing performance gaps in AI benchmarks such as ARC-AGI-2, Humanity’s Last Exam, SWE-bench Verified, GPQA, Chatbot Arena, and HumanEval, suggesting that the competitive edge of OpenAI and Anthropic is diminishing. The post argues that without securing high-end markets like healthcare, defense, education, and government, these companies may struggle to meet debt obligations and achieve profitability. One commenter suggests that OpenAI is relying on a ‘Too Big To Fail’ strategy, integrating its technology widely to maintain relevance despite not being the top performer. Another comment dismisses Meta‘s potential in high-end markets, while a third notes that GPT-5.1/2 models are uniquely intelligent beyond benchmarks, despite perceived regressions in newer versions.soumen08 highlights that GPT-5.1/2 models are perceived as the most intelligent beyond standard benchmarks, suggesting a regression in performance with GPT-3 Pro compared to 2.5 Pro for out-of-scope tasks. This indicates a nuanced understanding of model capabilities beyond just benchmark scores, emphasizing real-world application performance.ExpertPerformer discusses the strategic positioning of AI companies, noting that survival depends on carving out niches beyond just competing on benchmarks. They mention that models like Gemini, Grok, and ChatGPT are multimodal, offering features beyond text, which differentiates them from cheaper open-source alternatives. This highlights the importance of feature diversity and enterprise market focus for monetization and security.Emergency-Pomelo-256 speculates on the economic implications of OpenAI’s potential failure, suggesting that it could trigger a significant downturn in the AI industry, akin to a bubble burst. They propose that entities like Nvidia or government intervention might be necessary to stabilize the market, reflecting concerns about the broader economic impact of major AI companies’ solvency.Notes after testing OpenAI’s Codex App on real execution tasks (Activity: 30): OpenAI’s new Codex App is being tested for its ability to handle real development tasks, with some developers dubbing it a “Cursor killer.” Unlike traditional interactive coding tools like Cursor, Codex treats development as a task that runs to completion, encompassing planning, execution, testing, and follow-up changes within a single task. This approach allows for parallel work using Git worktrees, keeping tasks isolated and reviewable, and shifts the developer’s role from steering edits to reviewing outcomes. The focus is on task completion rather than continuous interaction, which may explain the “Cursor killer” label. A detailed technical breakdown is available here. A notable opinion from the comments suggests that Codex shifts the developer’s role to that of an orchestrator, akin to cloud computing, where the focus is on outcomes rather than collaboration. This reflects a broader trend towards higher abstraction in development tools, with expectations that OpenAI’s offerings will continue to improve.The commenter discusses the role of Codex as an orchestrator, likening it to a cloud service where users can request suggestions and execute tasks. They highlight the shift from merely generating outcomes to enabling collaboration, suggesting that Codex represents a new layer of abstraction in programming. This abstraction allows developers to ‘orchestrate the orchestrator,’ indicating a potential shift in how developers interact with AI tools.3. AI in Creative and Video ProductionSeeing the BMW M3 GTR Everywhere — How Are These Videos Made? (Activity: 1): The videos featuring the BMW M3 GTR from Need for Speed: Most Wanted are likely created using advanced video editing techniques, possibly involving AI-driven tools like Qwen and Wan. These tools can perform realistic object replacement and scene integration, allowing the car to appear seamlessly in various environments. The realism is achieved through sophisticated algorithms that maintain consistent lighting, shadows, and reflections, making the car appear naturally integrated into the scenes. This process involves tracking the vehicle’s position and orientation across frames and applying digital effects to match the surrounding environment.One user explains that the videos featuring the BMW M3 GTR are often created using advanced video editing software like Adobe After Effects or Blender. These tools allow creators to superimpose the car into various scenes, using techniques such as motion tracking and CGI to make the integration seamless. This process involves detailed work to match lighting and shadows to the environment, ensuring the car appears naturally within the scene.Another comment highlights the use of video game engines, such as Unreal Engine or Unity, to render realistic scenes with the BMW M3 GTR. These engines provide high-quality graphics and physics simulations, allowing creators to produce videos that look almost indistinguishable from real life. The use of ray tracing and PBR (Physically Based Rendering) materials in these engines enhances the realism of the car’s appearance and interaction with the environment.A technical discussion points out the role of machine learning in enhancing video quality and realism. Techniques like neural rendering and AI-based upscaling are used to improve the visual fidelity of the BMW M3 GTR in videos. These methods can refine textures and details, making the car look more lifelike, and are often employed in post-production to enhance the final output.How to create videos with swift actions + perfect lip sync (Activity: 1856): The post discusses techniques for creating videos with precise lip synchronization and swift actions, likely involving AI-driven tools or software. The focus is on achieving seamless integration of audio and visual elements, possibly using advanced algorithms or machine learning models to enhance the realism of the video content. The mention of AI suggests the use of deep learning frameworks or specialized software for video editing and synthesis. One comment highlights the difficulty in detecting AI-generated content, suggesting the effectiveness of the techniques discussed. Another comment implies that the realism of the video is enhanced by subtle details, such as hand movements, which contribute to the overall believability of the AI-generated video.I created a 10-minute AI film - The Last Signal (YouTube) (Activity: 17): Richard Galapate’s AI film, The Last Signal, was submitted to the 1 Billion Followers Summit AI Film competition. The film features astronaut Jake Ward on a Mars outpost, using AI tools like Google Veo 3.1 for visuals and voice, Google Gemini for prompting, and ElevenLabs for Lyra’s voice. This project highlights the potential of AI in creating consistent and efficient film content. The original video can be viewed here. The comments reflect a positive reception, with praise for storytelling and emotional impact, though lacking in technical critique.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. Agentic Coding &amp; Dev Tooling Goes Local-FirstCodex Goes Desktop: macOS Agent Command Center: OpenAI shipped the Codex app for macOS as an agent-building command center, available for Plus/Pro/Business/Enterprise/Edu with limited-time access on ChatGPT Free/Go, per “Introducing the Codex app” and the Codex landing page.The launch also spilled into community workflow chatter (pairing agents, multi-agent “command centers”), and a related Codex App hackathon with $90,000 in credits showed up via Cerebral Valley’s event page.LM Studio Speaks Anthropic: Claude Code Meets Your Local GGUF/MLX: LM Studio 0.4.1 added an Anthropic /v1/messages compatibility API, letting developers point Claude Code-style tools at local GGUF/MLX models by changing the base URL, detailed in “Using Claude Code with LM Studio”.In parallel, LM Studio also pushed a TypeScript SDK for third-party plugins and an OpenAI-compatible endpoint (SDK link), reinforcing a growing pattern: reuse existing agent tooling while swapping the backend model stack locally.Arena Mode Everywhere: Windsurf Turns Model Eval into a Game: Windsurf shipped Wave 14 with Arena Mode for side-by-side model battles (including Battle Groups and “Pick your own”), and temporarily set Battle Groups to 0x credits via the Windsurf download page.This mirrored broader “live eval” momentum: users also tracked new Arena entrants like step-3.5-flash and qwen3-max-thinking on LMArena’s Text Arena and Code Arena, shifting selection from static benchmarks to continuous human voting.2. Model Releases &amp; Bench Races (Kimi vs GLM vs Qwen)Kimi K2.5 Speedruns the Leaderboards: Moonshot’s Kimi K2.5 landed broadly in product surfaces: Perplexity Pro/Max added it for subscribers and said it runs on a US-based inference stack for tighter latency/reliability/security control (announcement screenshot: https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg).Community results piled on: LMArena reported Kimi-K2.5-thinking hit #1 open and #5 overall in Code Arena (see Code Arena), while multiple dev channels argued over its tool-calling reliability and provider variance when routed through aggregators.GLM-4.7 Flash: Small Model, Big Front-End Energy: Developers highlighted GLM-4.7 flash as a surprisingly strong coding model—especially for interactive website/front-end work—citing preserved reasoning and interleaved capability, with discussion anchored on ggerganov’s post.The debate sharpened around whether stripping “thinking” harms performance, and several users described pairing GLM-4.7 with Claude Code (or Claude-like agent tooling) as a pragmatic hybrid stack: cheap execution + expensive review.New Arena Entrants: step-3.5-flash &amp; qwen3-max-thinking Join the Party: LMArena added step-3.5-flash to the Text Arena and qwen3-max-thinking to the Code Arena, explicitly positioning them as fresh baselines for side-by-side evaluation.Users used these drops to re-litigate “model preference” threads (Kimi vs GLM vs Gemini), with the recurring takeaway that leaderboards and live evals increasingly drive adoption more than vendor marketing.3. Training Signals, Dense Rewards, and New Architectures/DatasetsFrom Binary Rewards to Dense Supervision: RL Gets Wordy: Multiple communities converged on richer post-training signals: Unsloth discussions pushed training with logprobs of final answers and non-binary rewards, referencing Jonas Hübotter’s method for turning descriptive feedback into dense supervision (Hübotter thread).The sticking point stayed practical: people asked for verifiable datasets for RL training agentic coding, implying a pipeline gap between “cool reward shaping idea” and “reproducible, automated evaluation harness.”Complexity-Deep: Token-Routed MLP Tries MoE Without the Load-Balancing Headache: The Complexity-Deep (1.5B) architecture open-sourced Token-Routed MLP for MoE-style routing “without load balancing loss,” plus Mu-Guided Attention and a PiD Controller, shipping code at Complexity-ML/complexity-deep and reporting 20.6% MMLU (base).The community framed it as another step in the “routing without pain” trend—trying to keep MoE wins while reducing the training-time engineering tax of balancing experts.Moltbook Data Dump: 50k Posts for Agent Sociology: A dataset scrape of Moltbook landed on Hugging Face with 50,539 posts, 12,454 AI agents, 195,414 comments, and 1,604 communities, published as lysandrehooh/moltbook.Elsewhere, researchers flagged the security implication behind agent platforms (auth tokens on machines, bot authenticity concerns) and treated the dataset as fuel for analyzing emergent behavior—without needing to speculate beyond the raw logs.4. GPU/Kernel Engineering: Faster Attention, Better Profiling, Weirder PTXFlashAttention v3 Hits RDNA: AMD Users Get Their Turn: A FlashAttention update added RDNA GPU support via the ongoing work in flash-attention PR #2178, aiming to reduce attention bottlenecks on AMD cards.The tone across servers was basically: this is the sort of “unsexy infra work” that actually unlocks local inference and finetuning on non-NVIDIA hardware—especially when paired with open-weight models and desktop agent tooling.Triton-Viz v3.0: Tile-Kernel Debugging Gets Teeth: Triton-Viz v3.0 shipped with broader profiling support (including Triton and Amazon NKI) plus a sanitizer for out-of-bounds access and a profiler that flags inefficient loops, per the release announcement (Discord link: https://discord.com/channels/1189498204333543425/1225499141241573447/1467634539164602563).It also hooked into triton-puzzles via a shared Colab notebook (Colab), and maintainers even floated moving srush/Triton-Puzzles under the GPU Mode org to keep bugfix velocity high.sm120: TMA + mbarrier Beats cp.async (Barely), cuBLAS Still Ships sm80 Kernels: Experiments on sm120 showed that careful TMA + mbarrier implementation can edge out cp.async for larger matrix shapes, while also surfacing that cuBLAS still appears to run sm80 kernels even when newer mechanisms exist.On the debugging front, one CUDA/PTX deadlock got fixed by inserting __syncthreads() after MMA before prefetching the next TMA, turning a hang into a measurable perf gain—exactly the kind of “one barrier to rule them all” lesson kernel folks keep re-learning.5. Security, Determinism, and Agent Misbehavior (the Practical Kind)Prompt Injection Defense Arms Race: Embeddings + Grammar-Constrained Decoding: Red teamers shared a structured exercise site for adversarial practice—“Adversarial Design Thinking”—and used it to tee up concrete mitigations for prompt injection.One proposed “belt + suspenders” defense combined embedding-based filtering with Grammar Constrained Decoding, with the explicit goal of reducing injection surface by constraining the model’s output space rather than only policing inputs.Deterministic Reasoning and “Strict Mode” Fever Spreads: Across OpenAI and OpenRouter discussions, users pushed for determinism/replayability/traceability in LLM reasoning; one person offered a deterministic reasoning engine that enforces a fixed structure and emits a 32D statistical vector trace (no public link shared).In OpenRouter, the same instinct showed up as skepticism about response healing and calls for a strict mode that keeps tool calls and outputs predictable—plus suggestions that better argument descriptions/examples improve tool-call accuracy.OpenClaw: Cool Agent Tricks, Scary Bills, and “2/100 Security”: OpenClaw sparked repeated warnings: OpenRouter users reported it can drain credits fast (including one drained Claude Max subscription), while an OpenAI server linked a security assessment claiming OpenClaw scored 2/100 (Perplexity result).Meanwhile, “works on my machine” stories (local models controlling devices, trading jokes) collided with real operational concerns—tool permissions, moderation/refusals (especially around jailbreak-y queries), and the need for observability and human-in-the-loop gates in agent workflows.</p>"
    },
    {
      "id": "a91a44fa88d6",
      "title": "HHS Is Making an AI Tool to Create Hypotheses About Vaccine Injury Claims",
      "content": "Experts worry Robert F. Kennedy Jr.’s Health Department will use an internal AI tool to analyze vaccine injury claims in a way that furthers his anti-vaccine agenda.",
      "url": "https://www.wired.com/story/hhs-is-making-an-ai-tool-to-create-hypotheses-about-vaccine-injury-claims/",
      "author": "Emily Mullin",
      "published": "2026-02-04T10:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Science",
        "vaccines",
        "public health",
        "government",
        "Robert F. Kennedy Jr.",
        "artificial intelligence",
        "Machine Theories"
      ],
      "summary": "The Department of Health and Human Services is developing an AI tool to analyze vaccine injury claims and generate hypotheses about them. Critics worry the tool could be used to further anti-vaccine agendas under RFK Jr.'s leadership.",
      "importance_score": 50.0,
      "reasoning": "Concerning government AI application with policy implications, but more about AI misuse risk than frontier AI development.",
      "themes": [
        "government AI",
        "healthcare",
        "AI policy",
        "misinformation risk"
      ],
      "continuation": null,
      "summary_html": "<p>The Department of Health and Human Services is developing an AI tool to analyze vaccine injury claims and generate hypotheses about them. Critics worry the tool could be used to further anti-vaccine agendas under RFK Jr.'s leadership.</p>",
      "content_html": "<p>Experts worry Robert F. Kennedy Jr.’s Health Department will use an internal AI tool to analyze vaccine injury claims in a way that furthers his anti-vaccine agenda.</p>"
    },
    {
      "id": "b9540f690c67",
      "title": "AI Bots Are Now a Signifigant Source of Web Traffic",
      "content": "New data shows AI bots pushing deeper into the web, prompting publishers to roll out more aggressive defenses.",
      "url": "https://www.wired.com/story/ai-bots-are-now-a-signifigant-source-of-web-traffic/",
      "author": "Will Knight",
      "published": "2026-02-04T10:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "artificial intelligence",
        "bots",
        "websites",
        "Search",
        "search engines",
        "Internet",
        "Inhuman Trafficking"
      ],
      "summary": "New data shows AI bots have become a significant source of web traffic, prompting publishers to deploy more aggressive defensive measures against automated scraping.",
      "importance_score": 48.0,
      "reasoning": "Important trend for web ecosystem but not frontier AI development news.",
      "themes": [
        "web scraping",
        "AI bots",
        "publishers"
      ],
      "continuation": null,
      "summary_html": "<p>New data shows AI bots have become a significant source of web traffic, prompting publishers to deploy more aggressive defensive measures against automated scraping.</p>",
      "content_html": "<p>New data shows AI bots pushing deeper into the web, prompting publishers to roll out more aggressive defenses.</p>"
    },
    {
      "id": "bdfbaa1dbdad",
      "title": "Women in tech and finance at higher risk from AI job losses, report says",
      "content": "‘Mid-career’ female workers also being sidelined by rigid hiring processes, says City of London CorporationWomen working in tech and financial services are at greater risk of losing their jobs to increased use of AI and automation than their male peers, according to a report that found experienced females were also being sidelined as a result of “rigid hiring processes”.“Mid-career” women – with at least five years’ experience – are being overlooked for digital roles in the tech and financial and professional services sectors, where they are traditionally underrepresented, according to the report by the City of London Corporation. Continue reading...",
      "url": "https://www.theguardian.com/business/2026/feb/04/women-tech-finance-higher-risk-ai-job-losses-report",
      "author": "Joanna Partridge",
      "published": "2026-02-04T00:01:27",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Technology sector",
        "Financial sector",
        "AI (artificial intelligence)",
        "Women",
        "Gender",
        "Women in the boardroom",
        "Technology",
        "Business",
        "UK news",
        "Society"
      ],
      "summary": "A City of London Corporation report finds women in tech and financial services face higher risk of AI-related job displacement than male peers. Mid-career women are particularly affected by rigid hiring processes.",
      "importance_score": 47.0,
      "reasoning": "Important societal impact research but not frontier AI technical development.",
      "themes": [
        "AI job displacement",
        "diversity",
        "financial services"
      ],
      "continuation": null,
      "summary_html": "<p>A City of London Corporation report finds women in tech and financial services face higher risk of AI-related job displacement than male peers. Mid-career women are particularly affected by rigid hiring processes.</p>",
      "content_html": "<p>‘Mid-career’ female workers also being sidelined by rigid hiring processes, says City of London CorporationWomen working in tech and financial services are at greater risk of losing their jobs to increased use of AI and automation than their male peers, according to a report that found experienced females were also being sidelined as a result of “rigid hiring processes”.“Mid-career” women – with at least five years’ experience – are being overlooked for digital roles in the tech and financial and professional services sectors, where they are traditionally underrepresented, according to the report by the City of London Corporation. Continue reading...</p>"
    },
    {
      "id": "146a4b745ece",
      "title": "How Cisco builds smart systems for the AI era",
      "content": " Among the big players in technology, Cisco is one of the sector&#8217;s leaders that&#8217;s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks. \n Cisco&#8217;s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It&#8217;s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it&#8217;s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference. \n Having made its name as the de facto supplier of networking infrastructure for the enterprise, it comes as no shock that it&#8217;s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language. \n For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters. \n Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed. \n In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco&#8217;s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It&#8217;s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience. \n Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment. \n Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols. \n Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It&#8217;s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio. \n In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing. \n(Image source: Pixabay)\n&nbsp;\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post How Cisco builds smart systems for the AI era appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/how-cisco-builds-smart-systems-for-the-ai-era/",
      "author": "AI News",
      "published": "2026-02-04T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Infrastructure & Hardware",
        "ai",
        "cisco",
        "cloud",
        "edge",
        "iiot",
        "networking"
      ],
      "summary": "Cisco is deploying machine learning and agentic AI internally to improve service delivery and customer experiences, building a shared AI fabric validated through internal testing before offering to customers.",
      "importance_score": 45.0,
      "reasoning": "Enterprise AI deployment case study, useful but routine corporate AI adoption.",
      "themes": [
        "enterprise AI",
        "Cisco",
        "networking"
      ],
      "continuation": null,
      "summary_html": "<p>Cisco is deploying machine learning and agentic AI internally to improve service delivery and customer experiences, building a shared AI fabric validated through internal testing before offering to customers.</p>",
      "content_html": "<p>Among the big players in technology, Cisco is one of the sector’s leaders that’s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks.</p>\n<p>Cisco’s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It’s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it’s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference.</p>\n<p>Having made its name as the de facto supplier of networking infrastructure for the enterprise, it comes as no shock that it’s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language.</p>\n<p>For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters.</p>\n<p>Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed.</p>\n<p>In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco’s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It’s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience.</p>\n<p>Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment.</p>\n<p>Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols.</p>\n<p>Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It’s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio.</p>\n<p>In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing.</p>\n<p>(Image source: Pixabay)</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post How Cisco builds smart systems for the AI era appeared first on AI News.</p>"
    },
    {
      "id": "f5c783aff7aa",
      "title": "Combing the Rackspace blogfiles for operational AI pointers",
      "content": " In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort. \n One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn&#8217;t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as &#8220;platform-ready&#8221; in line with known frameworks such as MITRE ATT&amp;CK. The company claims it&#8217;s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters. \n The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps &#8220;architectural judgement, governance and business decisions&#8221; remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices. \n Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments. \n In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware. \n The company&#8217;s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments. \n A company of even greater size, Microsoft, is working to coordinate autonomous agents&#8217; work across systems. Copilot has evolved into an orchestration layer, and in Microsoft&#8217;s ecosystem, multi-step task execution and broader model choice do exist. However, it&#8217;s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations. \n Rackspace&#8217;s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company&#8217;s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates &#8216;bursty&#8217; exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That&#8217;s a roadmap for operational AI grounded in budget and audit requirements, not novelty. \n For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company&#8217;s direction and still be wary of the company&#8217;s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house. \n(Image source: Pixabay)\n&nbsp;\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Combing the Rackspace blogfiles for operational AI pointers appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/combing-the-rackspace-blogfiles-for-operational-ai-pointers/",
      "author": "AI News",
      "published": "2026-02-04T10:01:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Deep Dives",
        "Special Reports & Series",
        "TechEx Events",
        "copilot",
        "cybersecurity",
        "governance",
        "microsoft 365"
      ],
      "summary": "Rackspace detailed its RAIDER platform, a custom AI system for cybersecurity detection built for its internal cyber defense center to handle alert and log analysis at scale.",
      "importance_score": 44.0,
      "reasoning": "Interesting enterprise AI application in security but not frontier-advancing.",
      "themes": [
        "cybersecurity",
        "enterprise AI",
        "Rackspace"
      ],
      "continuation": null,
      "summary_html": "<p>Rackspace detailed its RAIDER platform, a custom AI system for cybersecurity detection built for its internal cyber defense center to handle alert and log analysis at scale.</p>",
      "content_html": "<p>In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort.</p>\n<p>One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn’t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as “platform-ready” in line with known frameworks such as MITRE ATT&amp;CK. The company claims it’s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters.</p>\n<p>The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps “architectural judgement, governance and business decisions” remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices.</p>\n<p>Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments.</p>\n<p>In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware.</p>\n<p>The company’s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments.</p>\n<p>A company of even greater size, Microsoft, is working to coordinate autonomous agents’ work across systems. Copilot has evolved into an orchestration layer, and in Microsoft’s ecosystem, multi-step task execution and broader model choice do exist. However, it’s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations.</p>\n<p>Rackspace’s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company’s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates ‘bursty’ exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That’s a roadmap for operational AI grounded in budget and audit requirements, not novelty.</p>\n<p>For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company’s direction and still be wary of the company’s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house.</p>\n<p>(Image source: Pixabay)</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Combing the Rackspace blogfiles for operational AI pointers appeared first on AI News.</p>"
    },
    {
      "id": "57605e2d6e81",
      "title": "How to Build Efficient Agentic Reasoning Systems by Dynamically Pruning Multiple Chain-of-Thought Paths Without Losing Accuracy",
      "content": "In this tutorial, we implement an agentic chain-of-thought pruning framework that generates multiple reasoning paths in parallel and dynamically reduces them using consensus signals and early stopping. We focus on improving reasoning efficiency by reducing unnecessary token usage while preserving answer correctness, demonstrating that self-consistency and lightweight graph-based agreement can serve as effective proxies for reasoning quality. We design the entire pipeline using a compact instruction-tuned model and progressive sampling to simulate how an agent can decide when it has reasoned “enough.” Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U transformers accelerate bitsandbytes networkx scikit-learn\n\n\nimport re, time, random, math\nimport numpy as np\nimport torch\nimport networkx as nx\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nSEED = 7\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n   MODEL_NAME,\n   device_map=\"auto\",\n   torch_dtype=torch.float16,\n   load_in_4bit=True\n)\nmodel.eval()\n\n\nSYSTEM = \"You are a careful problem solver. Keep reasoning brief and output a final numeric answer.\"\nFINAL_RE = re.compile(r\"Final:\\s*([-\\d]+(?:\\.\\d+)?)\")\n\n\n\nWe set up the Colab environment and load all required libraries for efficient agentic reasoning. We initialize a lightweight instruction-tuned language model with quantization to ensure stable execution on limited GPU resources. We also define global configuration, randomness control, and the core prompting pattern used throughout the tutorial. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef make_prompt(q):\n   return (\n       f\"{SYSTEM}\\n\\n\"\n       f\"Problem: {q}\\n\"\n       f\"Reasoning: (brief)\\n\"\n       f\"Final: \"\n   )\n\n\ndef parse_final_number(text):\n   m = FINAL_RE.search(text)\n   if m:\n       return m.group(1).strip()\n   nums = re.findall(r\"[-]?\\d+(?:\\.\\d+)?\", text)\n   return nums[-1] if nums else None\n\n\ndef is_correct(pred, gold):\n   if pred is None:\n       return 0\n   try:\n       return int(abs(float(pred) - float(gold)) &lt; 1e-9)\n   except:\n       return int(str(pred).strip() == str(gold).strip())\n\n\ndef tok_len(text):\n   return len(tokenizer.encode(text))\n\n\n\nWe define helper functions that structure prompts, extract final numeric answers, and evaluate correctness against ground truth. We standardize how answers are parsed so that different reasoning paths can be compared consistently. We also introduce token-counting utilities that allow us to later measure reasoning efficiency. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser@torch.no_grad()\ndef generate_paths(question, n, max_new_tokens=64, temperature=0.7, top_p=0.9):\n   prompt = make_prompt(question)\n   inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n\n   gen_cfg = GenerationConfig(\n       do_sample=True,\n       temperature=temperature,\n       top_p=top_p,\n       max_new_tokens=max_new_tokens,\n       pad_token_id=tokenizer.eos_token_id,\n       eos_token_id=tokenizer.eos_token_id,\n       num_return_sequences=n\n   )\n\n\n   out = model.generate(**inputs, generation_config=gen_cfg)\n   prompt_tok = inputs[\"input_ids\"].shape[1]\n\n\n   paths = []\n   for i in range(out.shape[0]):\n       seq = out[i]\n       gen_ids = seq[prompt_tok:]\n       completion = tokenizer.decode(gen_ids, skip_special_tokens=True)\n       paths.append({\n           \"prompt_tokens\": int(prompt_tok),\n           \"gen_tokens\": int(gen_ids.shape[0]),\n           \"completion\": completion\n       })\n   return paths\n\n\n\nWe implement fast multi-sample generation that produces several reasoning paths in a single model call. We extract only the generated continuation to isolate the reasoning output for each path. We store token usage and completions in a structured format to support downstream pruning decisions. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef consensus_strength(completions, sim_threshold=0.22):\n   if len(completions) &lt;= 1:\n       return [0.0] * len(completions)\n\n\n   vec = TfidfVectorizer(ngram_range=(1,2), max_features=2500)\n   X = vec.fit_transform(completions)\n   S = cosine_similarity(X)\n\n\n   G = nx.Graph()\n   n = len(completions)\n   G.add_nodes_from(range(n))\n\n\n   for i in range(n):\n       for j in range(i+1, n):\n           w = float(S[i, j])\n           if w >= sim_threshold:\n               G.add_edge(i, j, weight=w)\n\n\n   strength = [0.0] * n\n   for u, v, d in G.edges(data=True):\n       w = float(d.get(\"weight\", 0.0))\n       strength[u] += w\n       strength[v] += w\n\n\n   return strength\n\n\n\nWe construct a lightweight consensus mechanism using a similarity graph over generated reasoning paths. We compute pairwise similarity scores and convert them into a graph-based strength signal for each path. It allows us to approximate agreement between reasoning trajectories without expensive model calls. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef pick_final_answer(paths):\n   answers = [parse_final_number(p[\"completion\"]) for p in paths]\n   strengths = consensus_strength([p[\"completion\"] for p in paths])\n\n\n   groups = {}\n   for i, a in enumerate(answers):\n       if a is None:\n           continue\n       groups.setdefault(a, {\"idx\": [], \"strength\": 0.0, \"tokens\": 0})\n       groups[a][\"idx\"].append(i)\n       groups[a][\"strength\"] += strengths[i]\n       groups[a][\"tokens\"] += paths[i][\"gen_tokens\"]\n\n\n   if not groups:\n       return None, {\"answers\": answers, \"strengths\": strengths}\n\n\n   ranked = sorted(\n       groups.items(),\n       key=lambda kv: (len(kv[1][\"idx\"]), kv[1][\"strength\"], -kv[1][\"tokens\"]),\n       reverse=True\n   )\n\n\n   best_answer = ranked[0][0]\n   best_indices = ranked[0][1][\"idx\"]\n   best_i = sorted(best_indices, key=lambda i: (paths[i][\"gen_tokens\"], -strengths[i]))[0]\n\n\n   return best_answer, {\"answers\": answers, \"strengths\": strengths, \"best_i\": best_i}\n\n\ndef pruned_agent_answer(\n   question,\n   batch_size=2,\n   k_max=10,\n   max_new_tokens=64,\n   temperature=0.7,\n   top_p=0.9,\n   stop_min_samples=4,\n   stop_ratio=0.67,\n   stop_margin=2\n):\n   paths = []\n   prompt_tokens_once = tok_len(make_prompt(question))\n   total_gen_tokens = 0\n\n\n   while len(paths) &lt; k_max:\n       n = min(batch_size, k_max - len(paths))\n       new_paths = generate_paths(\n           question,\n           n=n,\n           max_new_tokens=max_new_tokens,\n           temperature=temperature,\n           top_p=top_p\n       )\n       paths.extend(new_paths)\n       total_gen_tokens += sum(p[\"gen_tokens\"] for p in new_paths)\n\n\n       if len(paths) >= stop_min_samples:\n           answers = [parse_final_number(p[\"completion\"]) for p in paths]\n           counts = {}\n           for a in answers:\n               if a is None:\n                   continue\n               counts[a] = counts.get(a, 0) + 1\n           if counts:\n               sorted_counts = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n               top_a, top_c = sorted_counts[0]\n               second_c = sorted_counts[1][1] if len(sorted_counts) > 1 else 0\n               if top_c >= math.ceil(stop_ratio * len(paths)) and (top_c - second_c) >= stop_margin:\n                   final, dbg = pick_final_answer(paths)\n                   return {\n                       \"final\": final,\n                       \"paths\": paths,\n                       \"early_stopped_at\": len(paths),\n                       \"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),\n                       \"debug\": dbg\n                   }\n\n\n   final, dbg = pick_final_answer(paths)\n   return {\n       \"final\": final,\n       \"paths\": paths,\n       \"early_stopped_at\": None,\n       \"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),\n       \"debug\": dbg\n   }\n\n\n\nWe implement the core agentic pruning logic that groups reasoning paths by final answers and ranks them using consensus and efficiency signals. We introduce progressive sampling with early stopping to terminate generation once sufficient confidence emerges. We then select a final answer that balances agreement strength and minimal token usage. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef baseline_answer(question, k=10, max_new_tokens=64):\n   paths = generate_paths(question, n=k, max_new_tokens=max_new_tokens)\n   prompt_tokens_once = tok_len(make_prompt(question))\n   total_gen_tokens = sum(p[\"gen_tokens\"] for p in paths)\n\n\n   answers = [parse_final_number(p[\"completion\"]) for p in paths]\n   counts = {}\n   for a in answers:\n       if a is None:\n           continue\n       counts[a] = counts.get(a, 0) + 1\n   final = max(counts.items(), key=lambda kv: kv[1])[0] if counts else None\n\n\n   return {\n       \"final\": final,\n       \"paths\": paths,\n       \"tokens_total\": int(prompt_tokens_once * k + total_gen_tokens)\n   }\n\n\nDATA = [\n   {\"q\": \"If a store sells 3 notebooks for $12, how much does 1 notebook cost?\", \"a\": \"4\"},\n   {\"q\": \"What is 17*6?\", \"a\": \"102\"},\n   {\"q\": \"A rectangle has length 9 and width 4. What is its area?\", \"a\": \"36\"},\n   {\"q\": \"If you buy 5 apples at $2 each, how much do you pay?\", \"a\": \"10\"},\n   {\"q\": \"What is 144 divided by 12?\", \"a\": \"12\"},\n   {\"q\": \"If x=8, what is 3x+5?\", \"a\": \"29\"},\n   {\"q\": \"A jar has 30 candies. You eat 7. How many remain?\", \"a\": \"23\"},\n   {\"q\": \"If a train travels 60 km in 1.5 hours, what is its average speed (km/h)?\", \"a\": \"40\"},\n   {\"q\": \"Compute: (25 - 9) * 3\", \"a\": \"48\"},\n   {\"q\": \"What is the next number in the pattern: 2, 4, 8, 16, ?\", \"a\": \"32\"},\n]\n\n\nbase_acc, base_tok = [], []\nprun_acc, prun_tok = [], []\n\n\nfor item in DATA:\n   b = baseline_answer(item[\"q\"], k=8, max_new_tokens=56)\n   base_acc.append(is_correct(b[\"final\"], item[\"a\"]))\n   base_tok.append(b[\"tokens_total\"])\n\n\n   p = pruned_agent_answer(item[\"q\"], max_new_tokens=56)\n   prun_acc.append(is_correct(p[\"final\"], item[\"a\"]))\n   prun_tok.append(p[\"tokens_total\"])\n\n\nprint(\"Baseline accuracy:\", float(np.mean(base_acc)))\nprint(\"Baseline avg tokens:\", float(np.mean(base_tok)))\nprint(\"Pruned accuracy:\", float(np.mean(prun_acc)))\nprint(\"Pruned avg tokens:\", float(np.mean(prun_tok)))\n\n\n\nWe compare the pruned agentic approach against a fixed self-consistency baseline. We evaluate both methods on accuracy and token consumption to quantify the efficiency gains from pruning. We conclude by reporting aggregate metrics that demonstrate how dynamic pruning preserves correctness while reducing reasoning cost.\n\n\n\nIn conclusion, we demonstrated that agentic pruning can significantly reduce effective token consumption without sacrificing accuracy by stopping reasoning once sufficient consensus emerges. We showed that combining self-consistency, similarity-based consensus graphs, and early-stop heuristics provides a practical and scalable approach to reasoning efficiency in agentic systems. This framework serves as a foundation for more advanced agentic behaviors, such as mid-generation pruning, budget-aware reasoning, and adaptive control over reasoning depth in real-world AI agents.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How to Build Efficient Agentic Reasoning Systems by Dynamically Pruning Multiple Chain-of-Thought Paths Without Losing Accuracy appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/04/how-to-build-efficient-agentic-reasoning-systems-by-dynamically-pruning-multiple-chain-of-thought-paths-without-losing-accuracy/",
      "author": "Asif Razzaq",
      "published": "2026-02-04T23:23:26",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "Editors Pick",
        "Staff",
        "Tutorials",
        "Uncategorized"
      ],
      "summary": "Technical tutorial implementing an agentic chain-of-thought pruning framework that uses consensus signals and early stopping to improve reasoning efficiency while maintaining accuracy.",
      "importance_score": 42.0,
      "reasoning": "Useful technical tutorial but educational content rather than news.",
      "themes": [
        "tutorials",
        "reasoning",
        "efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Technical tutorial implementing an agentic chain-of-thought pruning framework that uses consensus signals and early stopping to improve reasoning efficiency while maintaining accuracy.</p>",
      "content_html": "<p>In this tutorial, we implement an agentic chain-of-thought pruning framework that generates multiple reasoning paths in parallel and dynamically reduces them using consensus signals and early stopping. We focus on improving reasoning efficiency by reducing unnecessary token usage while preserving answer correctness, demonstrating that self-consistency and lightweight graph-based agreement can serve as effective proxies for reasoning quality. We design the entire pipeline using a compact instruction-tuned model and progressive sampling to simulate how an agent can decide when it has reasoned “enough.” Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U transformers accelerate bitsandbytes networkx scikit-learn</p>\n<p>import re, time, random, math</p>\n<p>import numpy as np</p>\n<p>import torch</p>\n<p>import networkx as nx</p>\n<p>from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig</p>\n<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>\n<p>from sklearn.metrics.pairwise import cosine_similarity</p>\n<p>SEED = 7</p>\n<p>random.seed(SEED)</p>\n<p>np.random.seed(SEED)</p>\n<p>torch.manual_seed(SEED)</p>\n<p>MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)</p>\n<p>model = AutoModelForCausalLM.from_pretrained(</p>\n<p>MODEL_NAME,</p>\n<p>device_map=\"auto\",</p>\n<p>torch_dtype=torch.float16,</p>\n<p>load_in_4bit=True</p>\n<p>)</p>\n<p>model.eval()</p>\n<p>SYSTEM = \"You are a careful problem solver. Keep reasoning brief and output a final numeric answer.\"</p>\n<p>FINAL_RE = re.compile(r\"Final:\\s*([-\\d]+(?:\\.\\d+)?)\")</p>\n<p>We set up the Colab environment and load all required libraries for efficient agentic reasoning. We initialize a lightweight instruction-tuned language model with quantization to ensure stable execution on limited GPU resources. We also define global configuration, randomness control, and the core prompting pattern used throughout the tutorial. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef make_prompt(q):</p>\n<p>return (</p>\n<p>f\"{SYSTEM}\\n\\n\"</p>\n<p>f\"Problem: {q}\\n\"</p>\n<p>f\"Reasoning: (brief)\\n\"</p>\n<p>f\"Final: \"</p>\n<p>)</p>\n<p>def parse_final_number(text):</p>\n<p>m = FINAL_RE.search(text)</p>\n<p>if m:</p>\n<p>return m.group(1).strip()</p>\n<p>nums = re.findall(r\"[-]?\\d+(?:\\.\\d+)?\", text)</p>\n<p>return nums[-1] if nums else None</p>\n<p>def is_correct(pred, gold):</p>\n<p>if pred is None:</p>\n<p>return 0</p>\n<p>try:</p>\n<p>return int(abs(float(pred) - float(gold)) &lt; 1e-9)</p>\n<p>except:</p>\n<p>return int(str(pred).strip() == str(gold).strip())</p>\n<p>def tok_len(text):</p>\n<p>return len(tokenizer.encode(text))</p>\n<p>We define helper functions that structure prompts, extract final numeric answers, and evaluate correctness against ground truth. We standardize how answers are parsed so that different reasoning paths can be compared consistently. We also introduce token-counting utilities that allow us to later measure reasoning efficiency. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser@torch.no_grad()</p>\n<p>def generate_paths(question, n, max_new_tokens=64, temperature=0.7, top_p=0.9):</p>\n<p>prompt = make_prompt(question)</p>\n<p>inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)</p>\n<p>gen_cfg = GenerationConfig(</p>\n<p>do_sample=True,</p>\n<p>temperature=temperature,</p>\n<p>top_p=top_p,</p>\n<p>max_new_tokens=max_new_tokens,</p>\n<p>pad_token_id=tokenizer.eos_token_id,</p>\n<p>eos_token_id=tokenizer.eos_token_id,</p>\n<p>num_return_sequences=n</p>\n<p>)</p>\n<p>out = model.generate(**inputs, generation_config=gen_cfg)</p>\n<p>prompt_tok = inputs[\"input_ids\"].shape[1]</p>\n<p>paths = []</p>\n<p>for i in range(out.shape[0]):</p>\n<p>seq = out[i]</p>\n<p>gen_ids = seq[prompt_tok:]</p>\n<p>completion = tokenizer.decode(gen_ids, skip_special_tokens=True)</p>\n<p>paths.append({</p>\n<p>\"prompt_tokens\": int(prompt_tok),</p>\n<p>\"gen_tokens\": int(gen_ids.shape[0]),</p>\n<p>\"completion\": completion</p>\n<p>})</p>\n<p>return paths</p>\n<p>We implement fast multi-sample generation that produces several reasoning paths in a single model call. We extract only the generated continuation to isolate the reasoning output for each path. We store token usage and completions in a structured format to support downstream pruning decisions. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef consensus_strength(completions, sim_threshold=0.22):</p>\n<p>if len(completions) &lt;= 1:</p>\n<p>return [0.0] * len(completions)</p>\n<p>vec = TfidfVectorizer(ngram_range=(1,2), max_features=2500)</p>\n<p>X = vec.fit_transform(completions)</p>\n<p>S = cosine_similarity(X)</p>\n<p>G = nx.Graph()</p>\n<p>n = len(completions)</p>\n<p>G.add_nodes_from(range(n))</p>\n<p>for i in range(n):</p>\n<p>for j in range(i+1, n):</p>\n<p>w = float(S[i, j])</p>\n<p>if w &gt;= sim_threshold:</p>\n<p>G.add_edge(i, j, weight=w)</p>\n<p>strength = [0.0] * n</p>\n<p>for u, v, d in G.edges(data=True):</p>\n<p>w = float(d.get(\"weight\", 0.0))</p>\n<p>strength[u] += w</p>\n<p>strength[v] += w</p>\n<p>return strength</p>\n<p>We construct a lightweight consensus mechanism using a similarity graph over generated reasoning paths. We compute pairwise similarity scores and convert them into a graph-based strength signal for each path. It allows us to approximate agreement between reasoning trajectories without expensive model calls. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef pick_final_answer(paths):</p>\n<p>answers = [parse_final_number(p[\"completion\"]) for p in paths]</p>\n<p>strengths = consensus_strength([p[\"completion\"] for p in paths])</p>\n<p>groups = {}</p>\n<p>for i, a in enumerate(answers):</p>\n<p>if a is None:</p>\n<p>continue</p>\n<p>groups.setdefault(a, {\"idx\": [], \"strength\": 0.0, \"tokens\": 0})</p>\n<p>groups[a][\"idx\"].append(i)</p>\n<p>groups[a][\"strength\"] += strengths[i]</p>\n<p>groups[a][\"tokens\"] += paths[i][\"gen_tokens\"]</p>\n<p>if not groups:</p>\n<p>return None, {\"answers\": answers, \"strengths\": strengths}</p>\n<p>ranked = sorted(</p>\n<p>groups.items(),</p>\n<p>key=lambda kv: (len(kv[1][\"idx\"]), kv[1][\"strength\"], -kv[1][\"tokens\"]),</p>\n<p>reverse=True</p>\n<p>)</p>\n<p>best_answer = ranked[0][0]</p>\n<p>best_indices = ranked[0][1][\"idx\"]</p>\n<p>best_i = sorted(best_indices, key=lambda i: (paths[i][\"gen_tokens\"], -strengths[i]))[0]</p>\n<p>return best_answer, {\"answers\": answers, \"strengths\": strengths, \"best_i\": best_i}</p>\n<p>def pruned_agent_answer(</p>\n<p>question,</p>\n<p>batch_size=2,</p>\n<p>k_max=10,</p>\n<p>max_new_tokens=64,</p>\n<p>temperature=0.7,</p>\n<p>top_p=0.9,</p>\n<p>stop_min_samples=4,</p>\n<p>stop_ratio=0.67,</p>\n<p>stop_margin=2</p>\n<p>):</p>\n<p>paths = []</p>\n<p>prompt_tokens_once = tok_len(make_prompt(question))</p>\n<p>total_gen_tokens = 0</p>\n<p>while len(paths) &lt; k_max:</p>\n<p>n = min(batch_size, k_max - len(paths))</p>\n<p>new_paths = generate_paths(</p>\n<p>question,</p>\n<p>n=n,</p>\n<p>max_new_tokens=max_new_tokens,</p>\n<p>temperature=temperature,</p>\n<p>top_p=top_p</p>\n<p>)</p>\n<p>paths.extend(new_paths)</p>\n<p>total_gen_tokens += sum(p[\"gen_tokens\"] for p in new_paths)</p>\n<p>if len(paths) &gt;= stop_min_samples:</p>\n<p>answers = [parse_final_number(p[\"completion\"]) for p in paths]</p>\n<p>counts = {}</p>\n<p>for a in answers:</p>\n<p>if a is None:</p>\n<p>continue</p>\n<p>counts[a] = counts.get(a, 0) + 1</p>\n<p>if counts:</p>\n<p>sorted_counts = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)</p>\n<p>top_a, top_c = sorted_counts[0]</p>\n<p>second_c = sorted_counts[1][1] if len(sorted_counts) &gt; 1 else 0</p>\n<p>if top_c &gt;= math.ceil(stop_ratio * len(paths)) and (top_c - second_c) &gt;= stop_margin:</p>\n<p>final, dbg = pick_final_answer(paths)</p>\n<p>return {</p>\n<p>\"final\": final,</p>\n<p>\"paths\": paths,</p>\n<p>\"early_stopped_at\": len(paths),</p>\n<p>\"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),</p>\n<p>\"debug\": dbg</p>\n<p>}</p>\n<p>final, dbg = pick_final_answer(paths)</p>\n<p>return {</p>\n<p>\"final\": final,</p>\n<p>\"paths\": paths,</p>\n<p>\"early_stopped_at\": None,</p>\n<p>\"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),</p>\n<p>\"debug\": dbg</p>\n<p>}</p>\n<p>We implement the core agentic pruning logic that groups reasoning paths by final answers and ranks them using consensus and efficiency signals. We introduce progressive sampling with early stopping to terminate generation once sufficient confidence emerges. We then select a final answer that balances agreement strength and minimal token usage. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef baseline_answer(question, k=10, max_new_tokens=64):</p>\n<p>paths = generate_paths(question, n=k, max_new_tokens=max_new_tokens)</p>\n<p>prompt_tokens_once = tok_len(make_prompt(question))</p>\n<p>total_gen_tokens = sum(p[\"gen_tokens\"] for p in paths)</p>\n<p>answers = [parse_final_number(p[\"completion\"]) for p in paths]</p>\n<p>counts = {}</p>\n<p>for a in answers:</p>\n<p>if a is None:</p>\n<p>continue</p>\n<p>counts[a] = counts.get(a, 0) + 1</p>\n<p>final = max(counts.items(), key=lambda kv: kv[1])[0] if counts else None</p>\n<p>return {</p>\n<p>\"final\": final,</p>\n<p>\"paths\": paths,</p>\n<p>\"tokens_total\": int(prompt_tokens_once * k + total_gen_tokens)</p>\n<p>}</p>\n<p>DATA = [</p>\n<p>{\"q\": \"If a store sells 3 notebooks for $12, how much does 1 notebook cost?\", \"a\": \"4\"},</p>\n<p>{\"q\": \"What is 17*6?\", \"a\": \"102\"},</p>\n<p>{\"q\": \"A rectangle has length 9 and width 4. What is its area?\", \"a\": \"36\"},</p>\n<p>{\"q\": \"If you buy 5 apples at $2 each, how much do you pay?\", \"a\": \"10\"},</p>\n<p>{\"q\": \"What is 144 divided by 12?\", \"a\": \"12\"},</p>\n<p>{\"q\": \"If x=8, what is 3x+5?\", \"a\": \"29\"},</p>\n<p>{\"q\": \"A jar has 30 candies. You eat 7. How many remain?\", \"a\": \"23\"},</p>\n<p>{\"q\": \"If a train travels 60 km in 1.5 hours, what is its average speed (km/h)?\", \"a\": \"40\"},</p>\n<p>{\"q\": \"Compute: (25 - 9) * 3\", \"a\": \"48\"},</p>\n<p>{\"q\": \"What is the next number in the pattern: 2, 4, 8, 16, ?\", \"a\": \"32\"},</p>\n<p>]</p>\n<p>base_acc, base_tok = [], []</p>\n<p>prun_acc, prun_tok = [], []</p>\n<p>for item in DATA:</p>\n<p>b = baseline_answer(item[\"q\"], k=8, max_new_tokens=56)</p>\n<p>base_acc.append(is_correct(b[\"final\"], item[\"a\"]))</p>\n<p>base_tok.append(b[\"tokens_total\"])</p>\n<p>p = pruned_agent_answer(item[\"q\"], max_new_tokens=56)</p>\n<p>prun_acc.append(is_correct(p[\"final\"], item[\"a\"]))</p>\n<p>prun_tok.append(p[\"tokens_total\"])</p>\n<p>print(\"Baseline accuracy:\", float(np.mean(base_acc)))</p>\n<p>print(\"Baseline avg tokens:\", float(np.mean(base_tok)))</p>\n<p>print(\"Pruned accuracy:\", float(np.mean(prun_acc)))</p>\n<p>print(\"Pruned avg tokens:\", float(np.mean(prun_tok)))</p>\n<p>We compare the pruned agentic approach against a fixed self-consistency baseline. We evaluate both methods on accuracy and token consumption to quantify the efficiency gains from pruning. We conclude by reporting aggregate metrics that demonstrate how dynamic pruning preserves correctness while reducing reasoning cost.</p>\n<p>In conclusion, we demonstrated that agentic pruning can significantly reduce effective token consumption without sacrificing accuracy by stopping reasoning once sufficient consensus emerges. We showed that combining self-consistency, similarity-based consensus graphs, and early-stop heuristics provides a practical and scalable approach to reasoning efficiency in agentic systems. This framework serves as a foundation for more advanced agentic behaviors, such as mid-generation pruning, budget-aware reasoning, and adaptive control over reasoning depth in real-world AI agents.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How to Build Efficient Agentic Reasoning Systems by Dynamically Pruning Multiple Chain-of-Thought Paths Without Losing Accuracy appeared first on MarkTechPost.</p>"
    },
    {
      "id": "48f49c86c357",
      "title": "A Coding Implementation to Train Safety-Critical Reinforcement Learning Agents Offline Using Conservative Q-Learning with d3rlpy and Fixed Historical Data",
      "content": "In this tutorial, we build a safety-critical reinforcement learning pipeline that learns entirely from fixed, offline data rather than live exploration. We design a custom environment, generate a behavior dataset from a constrained policy, and then train both a Behavior Cloning baseline and a Conservative Q-Learning agent using d3rlpy. By structuring the workflow around offline datasets, careful evaluation, and conservative learning objectives, we demonstrate how robust decision-making policies can be trained in settings where unsafe exploration is not an option. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U \"d3rlpy\" \"gymnasium\" \"numpy\" \"torch\" \"matplotlib\" \"scikit-learn\"\n\n\nimport os\nimport time\nimport random\nimport inspect\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nimport torch\nimport d3rlpy\n\n\n\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\n\n\ndef pick_device():\n   if torch.cuda.is_available():\n       return \"cuda:0\"\n   return \"cpu\"\n\n\n\n\nDEVICE = pick_device()\nprint(\"d3rlpy:\", getattr(d3rlpy, \"__version__\", \"unknown\"), \"| torch:\", torch.__version__, \"| device:\", DEVICE)\n\n\n\n\ndef make_config(cls, **kwargs):\n   sig = inspect.signature(cls.__init__)\n   allowed = set(sig.parameters.keys())\n   allowed.discard(\"self\")\n   filtered = {k: v for k, v in kwargs.items() if k in allowed}\n   return cls(**filtered)\n\n\n\nWe set up the environment by installing dependencies, importing libraries, and fixing random seeds for reproducibility. We detect and configure the computation device to ensure consistent execution across systems. We also define a utility to create configuration objects safely across different d3rlpy versions. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass SafetyCriticalGridWorld(gym.Env):\n   metadata = {\"render_modes\": []}\n\n\n   def __init__(\n       self,\n       size=15,\n       max_steps=80,\n       hazard_coords=None,\n       start=(0, 0),\n       goal=None,\n       slip_prob=0.05,\n       seed=0,\n   ):\n       super().__init__()\n       self.size = int(size)\n       self.max_steps = int(max_steps)\n       self.start = tuple(start)\n       self.goal = tuple(goal) if goal is not None else (self.size - 1, self.size - 1)\n       self.slip_prob = float(slip_prob)\n\n\n       if hazard_coords is None:\n           hz = set()\n           rng = np.random.default_rng(seed)\n           for _ in range(max(1, self.size // 2)):\n               x = rng.integers(2, self.size - 2)\n               y = rng.integers(2, self.size - 2)\n               hz.add((int(x), int(y)))\n           self.hazards = hz\n       else:\n           self.hazards = set(tuple(x) for x in hazard_coords)\n\n\n       self.action_space = spaces.Discrete(4)\n       self.observation_space = spaces.Box(low=0.0, high=float(self.size - 1), shape=(2,), dtype=np.float32)\n\n\n       self._rng = np.random.default_rng(seed)\n       self._pos = None\n       self._t = 0\n\n\n   def reset(self, *, seed=None, options=None):\n       if seed is not None:\n           self._rng = np.random.default_rng(seed)\n       self._pos = [int(self.start[0]), int(self.start[1])]\n       self._t = 0\n       obs = np.array(self._pos, dtype=np.float32)\n       return obs, {}\n\n\n   def _clip(self):\n       self._pos[0] = int(np.clip(self._pos[0], 0, self.size - 1))\n       self._pos[1] = int(np.clip(self._pos[1], 0, self.size - 1))\n\n\n   def step(self, action):\n       self._t += 1\n\n\n       a = int(action)\n       if self._rng.random() &lt; self.slip_prob:\n           a = int(self._rng.integers(0, 4))\n\n\n       if a == 0:\n           self._pos[1] += 1\n       elif a == 1:\n           self._pos[0] += 1\n       elif a == 2:\n           self._pos[1] -= 1\n       elif a == 3:\n           self._pos[0] -= 1\n\n\n       self._clip()\n\n\n       x, y = int(self._pos[0]), int(self._pos[1])\n       terminated = False\n       truncated = self._t >= self.max_steps\n\n\n       reward = -1.0\n\n\n       if (x, y) in self.hazards:\n           reward = -100.0\n           terminated = True\n\n\n       if (x, y) == self.goal:\n           reward = +50.0\n           terminated = True\n\n\n       obs = np.array([x, y], dtype=np.float32)\n       return obs, float(reward), terminated, truncated, {}\n\n\n\nWe define a safety-critical GridWorld environment with hazards, terminal states, and stochastic transitions. We encode penalties for unsafe states and rewards for successful task completion. We ensure the environment strictly controls dynamics to reflect real-world safety constraints. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef safe_behavior_policy(obs, env: SafetyCriticalGridWorld, epsilon=0.15):\n   x, y = int(obs[0]), int(obs[1])\n   gx, gy = env.goal\n\n\n   preferred = []\n   if gx > x:\n       preferred.append(1)\n   elif gx &lt; x:\n       preferred.append(3)\n   if gy > y:\n       preferred.append(0)\n   elif gy &lt; y:\n       preferred.append(2)\n\n\n   if len(preferred) == 0:\n       preferred = [int(env._rng.integers(0, 4))]\n\n\n   if env._rng.random() &lt; epsilon:\n       return int(env._rng.integers(0, 4))\n\n\n   candidates = []\n   for a in preferred:\n       nx, ny = x, y\n       if a == 0:\n           ny += 1\n       elif a == 1:\n           nx += 1\n       elif a == 2:\n           ny -= 1\n       elif a == 3:\n           nx -= 1\n       nx = int(np.clip(nx, 0, env.size - 1))\n       ny = int(np.clip(ny, 0, env.size - 1))\n       if (nx, ny) not in env.hazards:\n           candidates.append(a)\n\n\n   if len(candidates) == 0:\n       return preferred[0]\n   return int(random.choice(candidates))\n\n\n\n\ndef generate_offline_episodes(env, n_episodes=400, epsilon=0.20, seed=0):\n   episodes = []\n   for i in range(n_episodes):\n       obs, _ = env.reset(seed=int(seed + i))\n       obs_list = []\n       act_list = []\n       rew_list = []\n       done_list = []\n\n\n       done = False\n       while not done:\n           a = safe_behavior_policy(obs, env, epsilon=epsilon)\n           nxt, r, terminated, truncated, _ = env.step(a)\n           done = bool(terminated or truncated)\n\n\n           obs_list.append(np.array(obs, dtype=np.float32))\n           act_list.append(np.array([a], dtype=np.int64))\n           rew_list.append(np.array([r], dtype=np.float32))\n           done_list.append(np.array([1.0 if done else 0.0], dtype=np.float32))\n\n\n           obs = nxt\n\n\n       episodes.append(\n           {\n               \"observations\": np.stack(obs_list, axis=0),\n               \"actions\": np.stack(act_list, axis=0),\n               \"rewards\": np.stack(rew_list, axis=0),\n               \"terminals\": np.stack(done_list, axis=0),\n           }\n       )\n   return episodes\n\n\n\n\ndef build_mdpdataset(episodes):\n   obs = np.concatenate([ep[\"observations\"] for ep in episodes], axis=0).astype(np.float32)\n   acts = np.concatenate([ep[\"actions\"] for ep in episodes], axis=0).astype(np.int64)\n   rews = np.concatenate([ep[\"rewards\"] for ep in episodes], axis=0).astype(np.float32)\n   terms = np.concatenate([ep[\"terminals\"] for ep in episodes], axis=0).astype(np.float32)\n\n\n   if hasattr(d3rlpy, \"dataset\") and hasattr(d3rlpy.dataset, \"MDPDataset\"):\n       return d3rlpy.dataset.MDPDataset(observations=obs, actions=acts, rewards=rews, terminals=terms)\n\n\n   raise RuntimeError(\"d3rlpy.dataset.MDPDataset not found. Upgrade d3rlpy.\")\n\n\n\nWe design a constrained behavior policy that generates offline data without risky exploration. We roll out this policy to collect trajectories and structure them into episodes. We then convert these episodes into a format compatible with d3rlpy’s offline learning APIs. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef _get_episodes_from_dataset(dataset):\n   if hasattr(dataset, \"episodes\") and dataset.episodes is not None:\n       return dataset.episodes\n   if hasattr(dataset, \"get_episodes\"):\n       return dataset.get_episodes()\n   raise AttributeError(\"Could not find episodes in dataset (d3rlpy version mismatch).\")\n\n\n\n\ndef _iter_all_observations(dataset):\n   for ep in _get_episodes_from_dataset(dataset):\n       obs = getattr(ep, \"observations\", None)\n       if obs is None:\n           continue\n       for o in obs:\n           yield o\n\n\n\n\ndef _iter_all_transitions(dataset):\n   for ep in _get_episodes_from_dataset(dataset):\n       obs = getattr(ep, \"observations\", None)\n       acts = getattr(ep, \"actions\", None)\n       rews = getattr(ep, \"rewards\", None)\n       if obs is None or acts is None:\n           continue\n       n = min(len(obs), len(acts))\n       for i in range(n):\n           o = obs[i]\n           a = acts[i]\n           r = rews[i] if rews is not None and i &lt; len(rews) else None\n           yield o, a, r\n\n\n\n\ndef visualize_dataset(dataset, env, title=\"Offline Dataset\"):\n   state_visits = np.zeros((env.size, env.size), dtype=np.float32)\n   for obs in _iter_all_observations(dataset):\n       x, y = int(obs[0]), int(obs[1])\n       x = int(np.clip(x, 0, env.size - 1))\n       y = int(np.clip(y, 0, env.size - 1))\n       state_visits[y, x] += 1\n\n\n   plt.figure(figsize=(6, 5))\n   plt.imshow(state_visits, origin=\"lower\")\n   plt.colorbar(label=\"Visits\")\n   plt.scatter([env.start[0]], [env.start[1]], marker=\"o\", label=\"start\")\n   plt.scatter([env.goal[0]], [env.goal[1]], marker=\"*\", label=\"goal\")\n   if len(env.hazards) > 0:\n       hz = np.array(list(env.hazards), dtype=np.int32)\n       plt.scatter(hz[:, 0], hz[:, 1], marker=\"x\", label=\"hazards\")\n   plt.title(f\"{title} — State visitation\")\n   plt.xlabel(\"x\")\n   plt.ylabel(\"y\")\n   plt.legend()\n   plt.show()\n\n\n   rewards = []\n   for _, _, r in _iter_all_transitions(dataset):\n       if r is not None:\n           rewards.append(float(r))\n   if len(rewards) > 0:\n       plt.figure(figsize=(6, 4))\n       plt.hist(rewards, bins=60)\n       plt.title(f\"{title} — Reward distribution\")\n       plt.xlabel(\"reward\")\n       plt.ylabel(\"count\")\n       plt.show()\n\n\n\nWe implement dataset utilities that correctly iterate through episodes rather than assuming flat arrays. We visualize state visitation to understand coverage and data bias in the offline dataset. We also analyze reward distributions to inspect the learning signal available to the agent. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef rollout_eval(env, algo, n_episodes=25, seed=0):\n   returns = []\n   lengths = []\n   hazard_hits = 0\n   goal_hits = 0\n\n\n   for i in range(n_episodes):\n       obs, _ = env.reset(seed=seed + i)\n       done = False\n       total = 0.0\n       steps = 0\n       while not done:\n           a = int(algo.predict(np.asarray(obs, dtype=np.float32)[None, ...])[0])\n           obs, r, terminated, truncated, _ = env.step(a)\n           total += float(r)\n           steps += 1\n           done = bool(terminated or truncated)\n           if terminated:\n               x, y = int(obs[0]), int(obs[1])\n               if (x, y) in env.hazards:\n                   hazard_hits += 1\n               if (x, y) == env.goal:\n                   goal_hits += 1\n\n\n       returns.append(total)\n       lengths.append(steps)\n\n\n   return {\n       \"return_mean\": float(np.mean(returns)),\n       \"return_std\": float(np.std(returns)),\n       \"len_mean\": float(np.mean(lengths)),\n       \"hazard_rate\": float(hazard_hits / max(1, n_episodes)),\n       \"goal_rate\": float(goal_hits / max(1, n_episodes)),\n       \"returns\": np.asarray(returns, dtype=np.float32),\n   }\n\n\n\n\ndef action_mismatch_rate_vs_data(dataset, algo, sample_obs=7000, seed=0):\n   rng = np.random.default_rng(seed)\n   obs_all = []\n   act_all = []\n   for o, a, _ in _iter_all_transitions(dataset):\n       obs_all.append(np.asarray(o, dtype=np.float32))\n       act_all.append(int(np.asarray(a).reshape(-1)[0]))\n       if len(obs_all) >= 80_000:\n           break\n\n\n   obs_all = np.stack(obs_all, axis=0)\n   act_all = np.asarray(act_all, dtype=np.int64)\n\n\n   idx = rng.choice(len(obs_all), size=min(sample_obs, len(obs_all)), replace=False)\n   obs_probe = obs_all[idx]\n   act_probe_data = act_all[idx]\n   act_probe_pi = algo.predict(obs_probe).astype(np.int64)\n\n\n   mismatch = (act_probe_pi != act_probe_data).astype(np.float32)\n   return float(mismatch.mean())\n\n\n\n\ndef create_discrete_bc(device):\n   if hasattr(d3rlpy.algos, \"DiscreteBCConfig\"):\n       cls = d3rlpy.algos.DiscreteBCConfig\n       cfg = make_config(\n           cls,\n           learning_rate=3e-4,\n           batch_size=256,\n       )\n       return cfg.create(device=device)\n   if hasattr(d3rlpy.algos, \"DiscreteBC\"):\n       return d3rlpy.algos.DiscreteBC()\n   raise RuntimeError(\"DiscreteBC not available in this d3rlpy version.\")\n\n\n\n\ndef create_discrete_cql(device, conservative_weight=6.0):\n   if hasattr(d3rlpy.algos, \"DiscreteCQLConfig\"):\n       cls = d3rlpy.algos.DiscreteCQLConfig\n       cfg = make_config(\n           cls,\n           learning_rate=3e-4,\n           actor_learning_rate=3e-4,\n           critic_learning_rate=3e-4,\n           temp_learning_rate=3e-4,\n           alpha_learning_rate=3e-4,\n           batch_size=256,\n           conservative_weight=float(conservative_weight),\n           n_action_samples=10,\n           rollout_interval=0,\n       )\n       return cfg.create(device=device)\n   if hasattr(d3rlpy.algos, \"DiscreteCQL\"):\n       algo = d3rlpy.algos.DiscreteCQL()\n       if hasattr(algo, \"conservative_weight\"):\n           try:\n               algo.conservative_weight = float(conservative_weight)\n           except Exception:\n               pass\n       return algo\n   raise RuntimeError(\"DiscreteCQL not available in this d3rlpy version.\")\n\n\n\nWe define controlled evaluation routines to measure policy performance without uncontrolled exploration. We compute returns and safety metrics, including hazard and goal rates. We also introduce a mismatch diagnostic to quantify how often learned actions deviate from the dataset behavior. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef main():\n   env = SafetyCriticalGridWorld(\n       size=15,\n       max_steps=80,\n       slip_prob=0.05,\n       seed=SEED,\n   )\n\n\n   raw_eps = generate_offline_episodes(env, n_episodes=500, epsilon=0.22, seed=SEED)\n   dataset = build_mdpdataset(raw_eps)\n\n\n   print(\"dataset built:\", type(dataset).__name__)\n   visualize_dataset(dataset, env, title=\"Behavior Dataset (Offline)\")\n\n\n   bc = create_discrete_bc(DEVICE)\n   cql = create_discrete_cql(DEVICE, conservative_weight=6.0)\n\n\n   print(\"\\nTraining Discrete BC (offline)...\")\n   t0 = time.time()\n   bc.fit(\n       dataset,\n       n_steps=25_000,\n       n_steps_per_epoch=2_500,\n       experiment_name=\"grid_bc_offline\",\n   )\n   print(\"BC train sec:\", round(time.time() - t0, 2))\n\n\n   print(\"\\nTraining Discrete CQL (offline)...\")\n   t0 = time.time()\n   cql.fit(\n       dataset,\n       n_steps=80_000,\n       n_steps_per_epoch=8_000,\n       experiment_name=\"grid_cql_offline\",\n   )\n   print(\"CQL train sec:\", round(time.time() - t0, 2))\n\n\n   print(\"\\nControlled online evaluation (small number of rollouts):\")\n   bc_metrics = rollout_eval(env, bc, n_episodes=30, seed=SEED + 1000)\n   cql_metrics = rollout_eval(env, cql, n_episodes=30, seed=SEED + 2000)\n\n\n   print(\"BC :\", {k: v for k, v in bc_metrics.items() if k != \"returns\"})\n   print(\"CQL:\", {k: v for k, v in cql_metrics.items() if k != \"returns\"})\n\n\n   print(\"\\nOOD-ish diagnostic (policy action mismatch vs data action at same states):\")\n   bc_mismatch = action_mismatch_rate_vs_data(dataset, bc, sample_obs=7000, seed=SEED + 1)\n   cql_mismatch = action_mismatch_rate_vs_data(dataset, cql, sample_obs=7000, seed=SEED + 2)\n   print(\"BC mismatch rate :\", bc_mismatch)\n   print(\"CQL mismatch rate:\", cql_mismatch)\n\n\n   plt.figure(figsize=(6, 4))\n   labels = [\"BC\", \"CQL\"]\n   means = [bc_metrics[\"return_mean\"], cql_metrics[\"return_mean\"]]\n   stds = [bc_metrics[\"return_std\"], cql_metrics[\"return_std\"]]\n   plt.bar(labels, means, yerr=stds)\n   plt.ylabel(\"Return\")\n   plt.title(\"Online Rollout Return (Controlled)\")\n   plt.show()\n\n\n   plt.figure(figsize=(6, 4))\n   plt.plot(np.sort(bc_metrics[\"returns\"]), label=\"BC\")\n   plt.plot(np.sort(cql_metrics[\"returns\"]), label=\"CQL\")\n   plt.xlabel(\"Episode (sorted)\")\n   plt.ylabel(\"Return\")\n   plt.title(\"Return Distribution (Sorted)\")\n   plt.legend()\n   plt.show()\n\n\n   out_dir = \"/content/offline_rl_artifacts\"\n   os.makedirs(out_dir, exist_ok=True)\n   bc_path = os.path.join(out_dir, \"grid_bc_policy.pt\")\n   cql_path = os.path.join(out_dir, \"grid_cql_policy.pt\")\n\n\n   if hasattr(bc, \"save_policy\"):\n       bc.save_policy(bc_path)\n       print(\"Saved BC policy:\", bc_path)\n   if hasattr(cql, \"save_policy\"):\n       cql.save_policy(cql_path)\n       print(\"Saved CQL policy:\", cql_path)\n\n\n   print(\"\\nDone.\")\n\n\n\n\nif __name__ == \"__main__\":\n   main()\n\n\n\nWe train both Behavior Cloning and Conservative Q-Learning agents purely from offline data. We compare their performance using controlled rollouts and diagnostic metrics. We finalize the workflow by saving trained policies and summarizing safety-aware learning outcomes.\n\n\n\nIn conclusion, we demonstrated that Conservative Q-Learning yields a more reliable policy than simple imitation when learning from historical data in safety-sensitive environments. By comparing offline training outcomes, controlled online evaluations, and action-distribution mismatches, we illustrated how conservatism helps reduce risky, out-of-distribution behavior. Overall, we presented a complete, reproducible offline RL workflow that we can extend to more complex domains such as robotics, healthcare, or finance without compromising safety.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post A Coding Implementation to Train Safety-Critical Reinforcement Learning Agents Offline Using Conservative Q-Learning with d3rlpy and Fixed Historical Data appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/03/a-coding-implementation-to-train-safety-critical-reinforcement-learning-agents-offline-using-conservative-q-learning-with-d3rlpy-and-fixed-historical-data/",
      "author": "Asif Razzaq",
      "published": "2026-02-04T04:49:17",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Artificial Intelligence",
        "Editors Pick",
        "Machine Learning",
        "Technology",
        "Tutorials"
      ],
      "summary": "Tutorial on building safety-critical reinforcement learning agents using Conservative Q-Learning with offline data, demonstrating robust policy training without unsafe exploration.",
      "importance_score": 38.0,
      "reasoning": "Technical tutorial content without news value for frontier AI developments.",
      "themes": [
        "tutorials",
        "reinforcement learning",
        "AI safety"
      ],
      "continuation": null,
      "summary_html": "<p>Tutorial on building safety-critical reinforcement learning agents using Conservative Q-Learning with offline data, demonstrating robust policy training without unsafe exploration.</p>",
      "content_html": "<p>In this tutorial, we build a safety-critical reinforcement learning pipeline that learns entirely from fixed, offline data rather than live exploration. We design a custom environment, generate a behavior dataset from a constrained policy, and then train both a Behavior Cloning baseline and a Conservative Q-Learning agent using d3rlpy. By structuring the workflow around offline datasets, careful evaluation, and conservative learning objectives, we demonstrate how robust decision-making policies can be trained in settings where unsafe exploration is not an option. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U \"d3rlpy\" \"gymnasium\" \"numpy\" \"torch\" \"matplotlib\" \"scikit-learn\"</p>\n<p>import os</p>\n<p>import time</p>\n<p>import random</p>\n<p>import inspect</p>\n<p>import numpy as np</p>\n<p>import matplotlib.pyplot as plt</p>\n<p>import gymnasium as gym</p>\n<p>from gymnasium import spaces</p>\n<p>import torch</p>\n<p>import d3rlpy</p>\n<p>SEED = 42</p>\n<p>random.seed(SEED)</p>\n<p>np.random.seed(SEED)</p>\n<p>torch.manual_seed(SEED)</p>\n<p>def pick_device():</p>\n<p>if torch.cuda.is_available():</p>\n<p>return \"cuda:0\"</p>\n<p>return \"cpu\"</p>\n<p>DEVICE = pick_device()</p>\n<p>print(\"d3rlpy:\", getattr(d3rlpy, \"__version__\", \"unknown\"), \"| torch:\", torch.__version__, \"| device:\", DEVICE)</p>\n<p>def make_config(cls, <strong>kwargs):</strong></p><strong>\n<p>sig = inspect.signature(cls.__init__)</p>\n<p>allowed = set(sig.parameters.keys())</p>\n<p>allowed.discard(\"self\")</p>\n<p>filtered = {k: v for k, v in kwargs.items() if k in allowed}</p>\n</strong><p><strong>return cls(</strong>filtered)</p>\n<p>We set up the environment by installing dependencies, importing libraries, and fixing random seeds for reproducibility. We detect and configure the computation device to ensure consistent execution across systems. We also define a utility to create configuration objects safely across different d3rlpy versions. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass SafetyCriticalGridWorld(gym.Env):</p>\n<p>metadata = {\"render_modes\": []}</p>\n<p>def __init__(</p>\n<p>self,</p>\n<p>size=15,</p>\n<p>max_steps=80,</p>\n<p>hazard_coords=None,</p>\n<p>start=(0, 0),</p>\n<p>goal=None,</p>\n<p>slip_prob=0.05,</p>\n<p>seed=0,</p>\n<p>):</p>\n<p>super().__init__()</p>\n<p>self.size = int(size)</p>\n<p>self.max_steps = int(max_steps)</p>\n<p>self.start = tuple(start)</p>\n<p>self.goal = tuple(goal) if goal is not None else (self.size - 1, self.size - 1)</p>\n<p>self.slip_prob = float(slip_prob)</p>\n<p>if hazard_coords is None:</p>\n<p>hz = set()</p>\n<p>rng = np.random.default_rng(seed)</p>\n<p>for _ in range(max(1, self.size // 2)):</p>\n<p>x = rng.integers(2, self.size - 2)</p>\n<p>y = rng.integers(2, self.size - 2)</p>\n<p>hz.add((int(x), int(y)))</p>\n<p>self.hazards = hz</p>\n<p>else:</p>\n<p>self.hazards = set(tuple(x) for x in hazard_coords)</p>\n<p>self.action_space = spaces.Discrete(4)</p>\n<p>self.observation_space = spaces.Box(low=0.0, high=float(self.size - 1), shape=(2,), dtype=np.float32)</p>\n<p>self._rng = np.random.default_rng(seed)</p>\n<p>self._pos = None</p>\n<p>self._t = 0</p>\n<p>def reset(self, *, seed=None, options=None):</p>\n<p>if seed is not None:</p>\n<p>self._rng = np.random.default_rng(seed)</p>\n<p>self._pos = [int(self.start[0]), int(self.start[1])]</p>\n<p>self._t = 0</p>\n<p>obs = np.array(self._pos, dtype=np.float32)</p>\n<p>return obs, {}</p>\n<p>def _clip(self):</p>\n<p>self._pos[0] = int(np.clip(self._pos[0], 0, self.size - 1))</p>\n<p>self._pos[1] = int(np.clip(self._pos[1], 0, self.size - 1))</p>\n<p>def step(self, action):</p>\n<p>self._t += 1</p>\n<p>a = int(action)</p>\n<p>if self._rng.random() &lt; self.slip_prob:</p>\n<p>a = int(self._rng.integers(0, 4))</p>\n<p>if a == 0:</p>\n<p>self._pos[1] += 1</p>\n<p>elif a == 1:</p>\n<p>self._pos[0] += 1</p>\n<p>elif a == 2:</p>\n<p>self._pos[1] -= 1</p>\n<p>elif a == 3:</p>\n<p>self._pos[0] -= 1</p>\n<p>self._clip()</p>\n<p>x, y = int(self._pos[0]), int(self._pos[1])</p>\n<p>terminated = False</p>\n<p>truncated = self._t &gt;= self.max_steps</p>\n<p>reward = -1.0</p>\n<p>if (x, y) in self.hazards:</p>\n<p>reward = -100.0</p>\n<p>terminated = True</p>\n<p>if (x, y) == self.goal:</p>\n<p>reward = +50.0</p>\n<p>terminated = True</p>\n<p>obs = np.array([x, y], dtype=np.float32)</p>\n<p>return obs, float(reward), terminated, truncated, {}</p>\n<p>We define a safety-critical GridWorld environment with hazards, terminal states, and stochastic transitions. We encode penalties for unsafe states and rewards for successful task completion. We ensure the environment strictly controls dynamics to reflect real-world safety constraints. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef safe_behavior_policy(obs, env: SafetyCriticalGridWorld, epsilon=0.15):</p>\n<p>x, y = int(obs[0]), int(obs[1])</p>\n<p>gx, gy = env.goal</p>\n<p>preferred = []</p>\n<p>if gx &gt; x:</p>\n<p>preferred.append(1)</p>\n<p>elif gx &lt; x:</p>\n<p>preferred.append(3)</p>\n<p>if gy &gt; y:</p>\n<p>preferred.append(0)</p>\n<p>elif gy &lt; y:</p>\n<p>preferred.append(2)</p>\n<p>if len(preferred) == 0:</p>\n<p>preferred = [int(env._rng.integers(0, 4))]</p>\n<p>if env._rng.random() &lt; epsilon:</p>\n<p>return int(env._rng.integers(0, 4))</p>\n<p>candidates = []</p>\n<p>for a in preferred:</p>\n<p>nx, ny = x, y</p>\n<p>if a == 0:</p>\n<p>ny += 1</p>\n<p>elif a == 1:</p>\n<p>nx += 1</p>\n<p>elif a == 2:</p>\n<p>ny -= 1</p>\n<p>elif a == 3:</p>\n<p>nx -= 1</p>\n<p>nx = int(np.clip(nx, 0, env.size - 1))</p>\n<p>ny = int(np.clip(ny, 0, env.size - 1))</p>\n<p>if (nx, ny) not in env.hazards:</p>\n<p>candidates.append(a)</p>\n<p>if len(candidates) == 0:</p>\n<p>return preferred[0]</p>\n<p>return int(random.choice(candidates))</p>\n<p>def generate_offline_episodes(env, n_episodes=400, epsilon=0.20, seed=0):</p>\n<p>episodes = []</p>\n<p>for i in range(n_episodes):</p>\n<p>obs, _ = env.reset(seed=int(seed + i))</p>\n<p>obs_list = []</p>\n<p>act_list = []</p>\n<p>rew_list = []</p>\n<p>done_list = []</p>\n<p>done = False</p>\n<p>while not done:</p>\n<p>a = safe_behavior_policy(obs, env, epsilon=epsilon)</p>\n<p>nxt, r, terminated, truncated, _ = env.step(a)</p>\n<p>done = bool(terminated or truncated)</p>\n<p>obs_list.append(np.array(obs, dtype=np.float32))</p>\n<p>act_list.append(np.array([a], dtype=np.int64))</p>\n<p>rew_list.append(np.array([r], dtype=np.float32))</p>\n<p>done_list.append(np.array([1.0 if done else 0.0], dtype=np.float32))</p>\n<p>obs = nxt</p>\n<p>episodes.append(</p>\n<p>{</p>\n<p>\"observations\": np.stack(obs_list, axis=0),</p>\n<p>\"actions\": np.stack(act_list, axis=0),</p>\n<p>\"rewards\": np.stack(rew_list, axis=0),</p>\n<p>\"terminals\": np.stack(done_list, axis=0),</p>\n<p>}</p>\n<p>)</p>\n<p>return episodes</p>\n<p>def build_mdpdataset(episodes):</p>\n<p>obs = np.concatenate([ep[\"observations\"] for ep in episodes], axis=0).astype(np.float32)</p>\n<p>acts = np.concatenate([ep[\"actions\"] for ep in episodes], axis=0).astype(np.int64)</p>\n<p>rews = np.concatenate([ep[\"rewards\"] for ep in episodes], axis=0).astype(np.float32)</p>\n<p>terms = np.concatenate([ep[\"terminals\"] for ep in episodes], axis=0).astype(np.float32)</p>\n<p>if hasattr(d3rlpy, \"dataset\") and hasattr(d3rlpy.dataset, \"MDPDataset\"):</p>\n<p>return d3rlpy.dataset.MDPDataset(observations=obs, actions=acts, rewards=rews, terminals=terms)</p>\n<p>raise RuntimeError(\"d3rlpy.dataset.MDPDataset not found. Upgrade d3rlpy.\")</p>\n<p>We design a constrained behavior policy that generates offline data without risky exploration. We roll out this policy to collect trajectories and structure them into episodes. We then convert these episodes into a format compatible with d3rlpy’s offline learning APIs. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef _get_episodes_from_dataset(dataset):</p>\n<p>if hasattr(dataset, \"episodes\") and dataset.episodes is not None:</p>\n<p>return dataset.episodes</p>\n<p>if hasattr(dataset, \"get_episodes\"):</p>\n<p>return dataset.get_episodes()</p>\n<p>raise AttributeError(\"Could not find episodes in dataset (d3rlpy version mismatch).\")</p>\n<p>def _iter_all_observations(dataset):</p>\n<p>for ep in _get_episodes_from_dataset(dataset):</p>\n<p>obs = getattr(ep, \"observations\", None)</p>\n<p>if obs is None:</p>\n<p>continue</p>\n<p>for o in obs:</p>\n<p>yield o</p>\n<p>def _iter_all_transitions(dataset):</p>\n<p>for ep in _get_episodes_from_dataset(dataset):</p>\n<p>obs = getattr(ep, \"observations\", None)</p>\n<p>acts = getattr(ep, \"actions\", None)</p>\n<p>rews = getattr(ep, \"rewards\", None)</p>\n<p>if obs is None or acts is None:</p>\n<p>continue</p>\n<p>n = min(len(obs), len(acts))</p>\n<p>for i in range(n):</p>\n<p>o = obs[i]</p>\n<p>a = acts[i]</p>\n<p>r = rews[i] if rews is not None and i &lt; len(rews) else None</p>\n<p>yield o, a, r</p>\n<p>def visualize_dataset(dataset, env, title=\"Offline Dataset\"):</p>\n<p>state_visits = np.zeros((env.size, env.size), dtype=np.float32)</p>\n<p>for obs in _iter_all_observations(dataset):</p>\n<p>x, y = int(obs[0]), int(obs[1])</p>\n<p>x = int(np.clip(x, 0, env.size - 1))</p>\n<p>y = int(np.clip(y, 0, env.size - 1))</p>\n<p>state_visits[y, x] += 1</p>\n<p>plt.figure(figsize=(6, 5))</p>\n<p>plt.imshow(state_visits, origin=\"lower\")</p>\n<p>plt.colorbar(label=\"Visits\")</p>\n<p>plt.scatter([env.start[0]], [env.start[1]], marker=\"o\", label=\"start\")</p>\n<p>plt.scatter([env.goal[0]], [env.goal[1]], marker=\"*\", label=\"goal\")</p>\n<p>if len(env.hazards) &gt; 0:</p>\n<p>hz = np.array(list(env.hazards), dtype=np.int32)</p>\n<p>plt.scatter(hz[:, 0], hz[:, 1], marker=\"x\", label=\"hazards\")</p>\n<p>plt.title(f\"{title} — State visitation\")</p>\n<p>plt.xlabel(\"x\")</p>\n<p>plt.ylabel(\"y\")</p>\n<p>plt.legend()</p>\n<p>plt.show()</p>\n<p>rewards = []</p>\n<p>for _, _, r in _iter_all_transitions(dataset):</p>\n<p>if r is not None:</p>\n<p>rewards.append(float(r))</p>\n<p>if len(rewards) &gt; 0:</p>\n<p>plt.figure(figsize=(6, 4))</p>\n<p>plt.hist(rewards, bins=60)</p>\n<p>plt.title(f\"{title} — Reward distribution\")</p>\n<p>plt.xlabel(\"reward\")</p>\n<p>plt.ylabel(\"count\")</p>\n<p>plt.show()</p>\n<p>We implement dataset utilities that correctly iterate through episodes rather than assuming flat arrays. We visualize state visitation to understand coverage and data bias in the offline dataset. We also analyze reward distributions to inspect the learning signal available to the agent. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef rollout_eval(env, algo, n_episodes=25, seed=0):</p>\n<p>returns = []</p>\n<p>lengths = []</p>\n<p>hazard_hits = 0</p>\n<p>goal_hits = 0</p>\n<p>for i in range(n_episodes):</p>\n<p>obs, _ = env.reset(seed=seed + i)</p>\n<p>done = False</p>\n<p>total = 0.0</p>\n<p>steps = 0</p>\n<p>while not done:</p>\n<p>a = int(algo.predict(np.asarray(obs, dtype=np.float32)[None, ...])[0])</p>\n<p>obs, r, terminated, truncated, _ = env.step(a)</p>\n<p>total += float(r)</p>\n<p>steps += 1</p>\n<p>done = bool(terminated or truncated)</p>\n<p>if terminated:</p>\n<p>x, y = int(obs[0]), int(obs[1])</p>\n<p>if (x, y) in env.hazards:</p>\n<p>hazard_hits += 1</p>\n<p>if (x, y) == env.goal:</p>\n<p>goal_hits += 1</p>\n<p>returns.append(total)</p>\n<p>lengths.append(steps)</p>\n<p>return {</p>\n<p>\"return_mean\": float(np.mean(returns)),</p>\n<p>\"return_std\": float(np.std(returns)),</p>\n<p>\"len_mean\": float(np.mean(lengths)),</p>\n<p>\"hazard_rate\": float(hazard_hits / max(1, n_episodes)),</p>\n<p>\"goal_rate\": float(goal_hits / max(1, n_episodes)),</p>\n<p>\"returns\": np.asarray(returns, dtype=np.float32),</p>\n<p>}</p>\n<p>def action_mismatch_rate_vs_data(dataset, algo, sample_obs=7000, seed=0):</p>\n<p>rng = np.random.default_rng(seed)</p>\n<p>obs_all = []</p>\n<p>act_all = []</p>\n<p>for o, a, _ in _iter_all_transitions(dataset):</p>\n<p>obs_all.append(np.asarray(o, dtype=np.float32))</p>\n<p>act_all.append(int(np.asarray(a).reshape(-1)[0]))</p>\n<p>if len(obs_all) &gt;= 80_000:</p>\n<p>break</p>\n<p>obs_all = np.stack(obs_all, axis=0)</p>\n<p>act_all = np.asarray(act_all, dtype=np.int64)</p>\n<p>idx = rng.choice(len(obs_all), size=min(sample_obs, len(obs_all)), replace=False)</p>\n<p>obs_probe = obs_all[idx]</p>\n<p>act_probe_data = act_all[idx]</p>\n<p>act_probe_pi = algo.predict(obs_probe).astype(np.int64)</p>\n<p>mismatch = (act_probe_pi != act_probe_data).astype(np.float32)</p>\n<p>return float(mismatch.mean())</p>\n<p>def create_discrete_bc(device):</p>\n<p>if hasattr(d3rlpy.algos, \"DiscreteBCConfig\"):</p>\n<p>cls = d3rlpy.algos.DiscreteBCConfig</p>\n<p>cfg = make_config(</p>\n<p>cls,</p>\n<p>learning_rate=3e-4,</p>\n<p>batch_size=256,</p>\n<p>)</p>\n<p>return cfg.create(device=device)</p>\n<p>if hasattr(d3rlpy.algos, \"DiscreteBC\"):</p>\n<p>return d3rlpy.algos.DiscreteBC()</p>\n<p>raise RuntimeError(\"DiscreteBC not available in this d3rlpy version.\")</p>\n<p>def create_discrete_cql(device, conservative_weight=6.0):</p>\n<p>if hasattr(d3rlpy.algos, \"DiscreteCQLConfig\"):</p>\n<p>cls = d3rlpy.algos.DiscreteCQLConfig</p>\n<p>cfg = make_config(</p>\n<p>cls,</p>\n<p>learning_rate=3e-4,</p>\n<p>actor_learning_rate=3e-4,</p>\n<p>critic_learning_rate=3e-4,</p>\n<p>temp_learning_rate=3e-4,</p>\n<p>alpha_learning_rate=3e-4,</p>\n<p>batch_size=256,</p>\n<p>conservative_weight=float(conservative_weight),</p>\n<p>n_action_samples=10,</p>\n<p>rollout_interval=0,</p>\n<p>)</p>\n<p>return cfg.create(device=device)</p>\n<p>if hasattr(d3rlpy.algos, \"DiscreteCQL\"):</p>\n<p>algo = d3rlpy.algos.DiscreteCQL()</p>\n<p>if hasattr(algo, \"conservative_weight\"):</p>\n<p>try:</p>\n<p>algo.conservative_weight = float(conservative_weight)</p>\n<p>except Exception:</p>\n<p>pass</p>\n<p>return algo</p>\n<p>raise RuntimeError(\"DiscreteCQL not available in this d3rlpy version.\")</p>\n<p>We define controlled evaluation routines to measure policy performance without uncontrolled exploration. We compute returns and safety metrics, including hazard and goal rates. We also introduce a mismatch diagnostic to quantify how often learned actions deviate from the dataset behavior. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef main():</p>\n<p>env = SafetyCriticalGridWorld(</p>\n<p>size=15,</p>\n<p>max_steps=80,</p>\n<p>slip_prob=0.05,</p>\n<p>seed=SEED,</p>\n<p>)</p>\n<p>raw_eps = generate_offline_episodes(env, n_episodes=500, epsilon=0.22, seed=SEED)</p>\n<p>dataset = build_mdpdataset(raw_eps)</p>\n<p>print(\"dataset built:\", type(dataset).__name__)</p>\n<p>visualize_dataset(dataset, env, title=\"Behavior Dataset (Offline)\")</p>\n<p>bc = create_discrete_bc(DEVICE)</p>\n<p>cql = create_discrete_cql(DEVICE, conservative_weight=6.0)</p>\n<p>print(\"\\nTraining Discrete BC (offline)...\")</p>\n<p>t0 = time.time()</p>\n<p>bc.fit(</p>\n<p>dataset,</p>\n<p>n_steps=25_000,</p>\n<p>n_steps_per_epoch=2_500,</p>\n<p>experiment_name=\"grid_bc_offline\",</p>\n<p>)</p>\n<p>print(\"BC train sec:\", round(time.time() - t0, 2))</p>\n<p>print(\"\\nTraining Discrete CQL (offline)...\")</p>\n<p>t0 = time.time()</p>\n<p>cql.fit(</p>\n<p>dataset,</p>\n<p>n_steps=80_000,</p>\n<p>n_steps_per_epoch=8_000,</p>\n<p>experiment_name=\"grid_cql_offline\",</p>\n<p>)</p>\n<p>print(\"CQL train sec:\", round(time.time() - t0, 2))</p>\n<p>print(\"\\nControlled online evaluation (small number of rollouts):\")</p>\n<p>bc_metrics = rollout_eval(env, bc, n_episodes=30, seed=SEED + 1000)</p>\n<p>cql_metrics = rollout_eval(env, cql, n_episodes=30, seed=SEED + 2000)</p>\n<p>print(\"BC :\", {k: v for k, v in bc_metrics.items() if k != \"returns\"})</p>\n<p>print(\"CQL:\", {k: v for k, v in cql_metrics.items() if k != \"returns\"})</p>\n<p>print(\"\\nOOD-ish diagnostic (policy action mismatch vs data action at same states):\")</p>\n<p>bc_mismatch = action_mismatch_rate_vs_data(dataset, bc, sample_obs=7000, seed=SEED + 1)</p>\n<p>cql_mismatch = action_mismatch_rate_vs_data(dataset, cql, sample_obs=7000, seed=SEED + 2)</p>\n<p>print(\"BC mismatch rate :\", bc_mismatch)</p>\n<p>print(\"CQL mismatch rate:\", cql_mismatch)</p>\n<p>plt.figure(figsize=(6, 4))</p>\n<p>labels = [\"BC\", \"CQL\"]</p>\n<p>means = [bc_metrics[\"return_mean\"], cql_metrics[\"return_mean\"]]</p>\n<p>stds = [bc_metrics[\"return_std\"], cql_metrics[\"return_std\"]]</p>\n<p>plt.bar(labels, means, yerr=stds)</p>\n<p>plt.ylabel(\"Return\")</p>\n<p>plt.title(\"Online Rollout Return (Controlled)\")</p>\n<p>plt.show()</p>\n<p>plt.figure(figsize=(6, 4))</p>\n<p>plt.plot(np.sort(bc_metrics[\"returns\"]), label=\"BC\")</p>\n<p>plt.plot(np.sort(cql_metrics[\"returns\"]), label=\"CQL\")</p>\n<p>plt.xlabel(\"Episode (sorted)\")</p>\n<p>plt.ylabel(\"Return\")</p>\n<p>plt.title(\"Return Distribution (Sorted)\")</p>\n<p>plt.legend()</p>\n<p>plt.show()</p>\n<p>out_dir = \"/content/offline_rl_artifacts\"</p>\n<p>os.makedirs(out_dir, exist_ok=True)</p>\n<p>bc_path = os.path.join(out_dir, \"grid_bc_policy.pt\")</p>\n<p>cql_path = os.path.join(out_dir, \"grid_cql_policy.pt\")</p>\n<p>if hasattr(bc, \"save_policy\"):</p>\n<p>bc.save_policy(bc_path)</p>\n<p>print(\"Saved BC policy:\", bc_path)</p>\n<p>if hasattr(cql, \"save_policy\"):</p>\n<p>cql.save_policy(cql_path)</p>\n<p>print(\"Saved CQL policy:\", cql_path)</p>\n<p>print(\"\\nDone.\")</p>\n<p>if __name__ == \"__main__\":</p>\n<p>main()</p>\n<p>We train both Behavior Cloning and Conservative Q-Learning agents purely from offline data. We compare their performance using controlled rollouts and diagnostic metrics. We finalize the workflow by saving trained policies and summarizing safety-aware learning outcomes.</p>\n<p>In conclusion, we demonstrated that Conservative Q-Learning yields a more reliable policy than simple imitation when learning from historical data in safety-sensitive environments. By comparing offline training outcomes, controlled online evaluations, and action-distribution mismatches, we illustrated how conservatism helps reduce risky, out-of-distribution behavior. Overall, we presented a complete, reproducible offline RL workflow that we can extend to more complex domains such as robotics, healthcare, or finance without compromising safety.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post A Coding Implementation to Train Safety-Critical Reinforcement Learning Agents Offline Using Conservative Q-Learning with d3rlpy and Fixed Historical Data appeared first on MarkTechPost.</p>"
    }
  ]
}