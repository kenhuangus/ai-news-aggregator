{
  "category": "news",
  "date": "2026-01-21",
  "category_summary": "**OpenAI** [announced a three-year partnership](/?date=2026-01-21&category=news#item-08241bfbf383) with **ServiceNow** to embed AI models including **GPT-5.2** into enterprise workflows serving **80 billion** annual transactions. The **Wikimedia Foundation** [secured paid data licensing deals](/?date=2026-01-21&category=news#item-6924badf023f) with **Amazon**, **Meta**, and **Perplexity** for LLM training—a significant precedent for AI data access.\n\n**Model releases** dominated technical news:\n- **Microsoft Research** [launched **OptiMind**](/?date=2026-01-21&category=news#item-cacf74acbf5b), a 20B-parameter model converting natural language to optimization solvers\n- **Zhipu AI** released **GLM-4.7-Flash**, a 30B MoE model for efficient local deployment\n- **Overworld** [introduced **Waypoint-1**](/?date=2026-01-21&category=news#item-90264fc9057a) for real-time interactive video generation\n- **Microsoft** [published **Differential Transformer V2**](/?date=2026-01-21&category=news#item-64900f37913d) architecture improvements\n\n**OpenAI** is exploring a [cheaper **$8/month ChatGPT tier**](/?date=2026-01-21&category=news#item-25d0480d05b8) with advertising. On global AI development, **China's** [algorithm registry](/?date=2026-01-21&category=news#item-8c1ea14ae122) reveals thousands of AI companies, **UK MPs** [warned of AI risks](/?date=2026-01-21&category=news#item-208dd7adb3af) in financial services, and **India** [launched **IAIRO**](/?date=2026-01-21&category=news#item-74f29593b90c), a new national AI research institution with **₹300 crore** backing.",
  "category_summary_html": "<p><strong>OpenAI</strong> <a href=\"/?date=2026-01-21&category=news#item-08241bfbf383\" class=\"internal-link\" rel=\"noopener noreferrer\">announced a three-year partnership</a> with <strong>ServiceNow</strong> to embed AI models including <strong>GPT-5.2</strong> into enterprise workflows serving <strong>80 billion</strong> annual transactions. The <strong>Wikimedia Foundation</strong> <a href=\"/?date=2026-01-21&category=news#item-6924badf023f\" class=\"internal-link\" rel=\"noopener noreferrer\">secured paid data licensing deals</a> with <strong>Amazon</strong>, <strong>Meta</strong>, and <strong>Perplexity</strong> for LLM training—a significant precedent for AI data access.</p>\n<p><strong>Model releases</strong> dominated technical news:</p>\n<ul>\n<li><strong>Microsoft Research</strong> <a href=\"/?date=2026-01-21&category=news#item-cacf74acbf5b\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>OptiMind</strong></a>, a 20B-parameter model converting natural language to optimization solvers</li>\n<li><strong>Zhipu AI</strong> released <strong>GLM-4.7-Flash</strong>, a 30B MoE model for efficient local deployment</li>\n<li><strong>Overworld</strong> <a href=\"/?date=2026-01-21&category=news#item-90264fc9057a\" class=\"internal-link\" rel=\"noopener noreferrer\">introduced <strong>Waypoint-1</strong></a> for real-time interactive video generation</li>\n<li><strong>Microsoft</strong> <a href=\"/?date=2026-01-21&category=news#item-64900f37913d\" class=\"internal-link\" rel=\"noopener noreferrer\">published <strong>Differential Transformer V2</strong></a> architecture improvements</li>\n</ul>\n<p><strong>OpenAI</strong> is exploring a <a href=\"/?date=2026-01-21&category=news#item-25d0480d05b8\" class=\"internal-link\" rel=\"noopener noreferrer\">cheaper <strong>$8/month ChatGPT tier</strong></a> with advertising. On global AI development, <strong>China's</strong> <a href=\"/?date=2026-01-21&category=news#item-8c1ea14ae122\" class=\"internal-link\" rel=\"noopener noreferrer\">algorithm registry</a> reveals thousands of AI companies, <strong>UK MPs</strong> <a href=\"/?date=2026-01-21&category=news#item-208dd7adb3af\" class=\"internal-link\" rel=\"noopener noreferrer\">warned of AI risks</a> in financial services, and <strong>India</strong> <a href=\"/?date=2026-01-21&category=news#item-74f29593b90c\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>IAIRO</strong></a>, a new national AI research institution with <strong>₹300 crore</strong> backing.</p>",
  "themes": [
    {
      "name": "Model Releases & Research",
      "description": "New model announcements from major labs including Microsoft OptiMind, Zhipu GLM-4.7-Flash, and architecture improvements",
      "item_count": 4,
      "example_items": [],
      "importance": 70.0
    },
    {
      "name": "Enterprise AI & Partnerships",
      "description": "Strategic deals embedding AI into business workflows, including OpenAI-ServiceNow and Wikipedia data licensing",
      "item_count": 3,
      "example_items": [],
      "importance": 75.0
    },
    {
      "name": "AI Business Models",
      "description": "Evolution of consumer AI monetization including cheaper tiers and advertising",
      "item_count": 2,
      "example_items": [],
      "importance": 62.0
    },
    {
      "name": "Global AI Competition",
      "description": "National AI strategies and ecosystem insights from China, India, and regulatory responses in UK",
      "item_count": 4,
      "example_items": [],
      "importance": 56.0
    },
    {
      "name": "AI Policy & Regulation",
      "description": "Government approaches to AI oversight in financial services and broader regulatory landscape",
      "item_count": 2,
      "example_items": [],
      "importance": 52.0
    }
  ],
  "total_items": 18,
  "items": [
    {
      "id": "08241bfbf383",
      "title": "OpenAI, ServiceNow Partner to Build AI Agents for Business Workflows",
      "content": "OpenAI and ServiceNow have signed a three year partnership to embed OpenAI’s AI models into ServiceNow’s enterprise software, a move that deepens the push to place autonomous AI agents inside core business workflows.\n\n\n\nUnder the agreement, OpenAI will become a preferred intelligence capability for enterprises that collectively run more than 80 billion workflows each year on the ServiceNow platform.&nbsp;\n\n\n\nThe tie up expands customer access to OpenAI models such as GPT-5.2 and adds native voice and speech to speech capabilities inside ServiceNow’s products.\n\n\n\nThe deal is also tied to how many customers adopt OpenAI’s technology on the ServiceNow platform and includes a revenue commitment from ServiceNow to OpenAI.&nbsp;\n\n\n\nFinancial terms were not disclosed.\n\n\n\nThe partnership reflects a broader shift in enterprise software toward AI agents that can act on behalf of users. Rivals such as Salesforce, SAP and Workday are already promoting built in AI agents as a key way for companies to get value from AI.\n\n\n\nFor OpenAI, the tie up strengthens its enterprise business by pushing its models into widely used corporate software, alongside its direct sales of AI tools.&nbsp;\n\n\n\n“As companies shift experimenting with AI to deploying it at scale, they need the power of multiple AI leaders working together, to deliver faster, better outcomes. Bringing together our engineering teams and our respective technologies will drive faster value for customers and more intuitive ways of working with AI,” Amit Zavery, president, chief operating officer, and chief product officer at ServiceNow said.\n\n\n\nServiceNow said its AI platform will use OpenAI models to support natural language assistance, automated summarisation and content generation for incidents and service cases, tools that convert intent into workflows and automation, and intelligent search across enterprise systems.&nbsp;\n\n\n\nEmployees will be able to make requests in plain language, such as viewing benefits or escalating a customer issue, with the system not only answering but also acting on those requests. ServiceNow engineers will build the new AI powered products with technical support from OpenAI. The company will also use its forward deployed engineers to help customers roll out and use the new features.\n\n\n\nBoth companies have partnerships with other AI and software providers, and ServiceNow said it intends to keep its platform open to multiple model makers depending on use cases.\n\n\n\nThe rise of AI agents is expected to add pressure on the IT job market, which has already been weakening as companies slow hiring and invest more in automation. Brad Lightcap, chief operating officer at OpenAI said, “With OpenAI frontier models and multimodal capabilities in ServiceNow, enterprises across every industry will benefit from intelligence that handles work end to end in even the most complex environments.”\n\n\n\nServiceNow said the partnership extends OpenAI’s work with large enterprises that already use its technology, including Accenture, Walmart, PayPal, Intuit, Target, Thermo Fisher, BNY, Morgan Stanley, and BBVA. OpenAI said more than 1 million business customers worldwide are now using its services.\n\n\n\nIn June last year, OpenAI also entered into a multi-year strategic collaboration with HLCTech to drive large-scale enterprise AI transformation, becoming one of the first strategic services partnerships of OpenAI.\nThe post OpenAI, ServiceNow Partner to Build AI Agents for Business Workflows appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/openai-servicenow-partner-to-build-ai-agents-for-business-workflows/",
      "author": "Mohit Pandey",
      "published": "2026-01-20T14:50:58",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "OpenAI and ServiceNow signed a three-year partnership to embed OpenAI models including GPT-5.2 into ServiceNow's enterprise platform, which handles 80 billion workflows annually. The deal includes native voice and speech-to-speech capabilities with revenue commitments tied to customer adoption.",
      "importance_score": 78.0,
      "reasoning": "Major enterprise partnership with GPT-5.2 mention suggests advanced model availability; significant scale at 80B workflows gives OpenAI massive enterprise footprint",
      "themes": [
        "Enterprise AI",
        "Strategic Partnerships",
        "OpenAI"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI and ServiceNow signed a three-year partnership to embed OpenAI models including GPT-5.2 into ServiceNow's enterprise platform, which handles 80 billion workflows annually. The deal includes native voice and speech-to-speech capabilities with revenue commitments tied to customer adoption.</p>",
      "content_html": "<p>OpenAI and ServiceNow have signed a three year partnership to embed OpenAI’s AI models into ServiceNow’s enterprise software, a move that deepens the push to place autonomous AI agents inside core business workflows.</p>\n<p>Under the agreement, OpenAI will become a preferred intelligence capability for enterprises that collectively run more than 80 billion workflows each year on the ServiceNow platform.&nbsp;</p>\n<p>The tie up expands customer access to OpenAI models such as GPT-5.2 and adds native voice and speech to speech capabilities inside ServiceNow’s products.</p>\n<p>The deal is also tied to how many customers adopt OpenAI’s technology on the ServiceNow platform and includes a revenue commitment from ServiceNow to OpenAI.&nbsp;</p>\n<p>Financial terms were not disclosed.</p>\n<p>The partnership reflects a broader shift in enterprise software toward AI agents that can act on behalf of users. Rivals such as Salesforce, SAP and Workday are already promoting built in AI agents as a key way for companies to get value from AI.</p>\n<p>For OpenAI, the tie up strengthens its enterprise business by pushing its models into widely used corporate software, alongside its direct sales of AI tools.&nbsp;</p>\n<p>“As companies shift experimenting with AI to deploying it at scale, they need the power of multiple AI leaders working together, to deliver faster, better outcomes. Bringing together our engineering teams and our respective technologies will drive faster value for customers and more intuitive ways of working with AI,” Amit Zavery, president, chief operating officer, and chief product officer at ServiceNow said.</p>\n<p>ServiceNow said its AI platform will use OpenAI models to support natural language assistance, automated summarisation and content generation for incidents and service cases, tools that convert intent into workflows and automation, and intelligent search across enterprise systems.&nbsp;</p>\n<p>Employees will be able to make requests in plain language, such as viewing benefits or escalating a customer issue, with the system not only answering but also acting on those requests. ServiceNow engineers will build the new AI powered products with technical support from OpenAI. The company will also use its forward deployed engineers to help customers roll out and use the new features.</p>\n<p>Both companies have partnerships with other AI and software providers, and ServiceNow said it intends to keep its platform open to multiple model makers depending on use cases.</p>\n<p>The rise of AI agents is expected to add pressure on the IT job market, which has already been weakening as companies slow hiring and invest more in automation. Brad Lightcap, chief operating officer at OpenAI said, “With OpenAI frontier models and multimodal capabilities in ServiceNow, enterprises across every industry will benefit from intelligence that handles work end to end in even the most complex environments.”</p>\n<p>ServiceNow said the partnership extends OpenAI’s work with large enterprises that already use its technology, including Accenture, Walmart, PayPal, Intuit, Target, Thermo Fisher, BNY, Morgan Stanley, and BBVA. OpenAI said more than 1 million business customers worldwide are now using its services.</p>\n<p>In June last year, OpenAI also entered into a multi-year strategic collaboration with HLCTech to drive large-scale enterprise AI transformation, becoming one of the first strategic services partnerships of OpenAI.</p>\n<p>The post OpenAI, ServiceNow Partner to Build AI Agents for Business Workflows appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "6924badf023f",
      "title": "Wikipedia Parent Announces AI Deal with Amazon, Meta, Perplexity",
      "content": "Under the deal, tech vendors are paying to access Wikimedia's data to train and develop their large language models.",
      "url": "https://aibusiness.com/foundation-models/wikipedia-ai-deal-amazon-meta-perplexity",
      "author": "Scarlett Evans",
      "published": "2026-01-20T21:54:41",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "The Wikimedia Foundation announced paid data licensing agreements with Amazon, Meta, and Perplexity for access to Wikipedia data to train and develop large language models. This formalizes what has been an informal data source for AI training.",
      "importance_score": 73.0,
      "reasoning": "Sets important precedent for AI training data licensing; involves multiple major AI players and one of the most-used training data sources",
      "themes": [
        "Training Data",
        "Data Licensing",
        "AI Ethics"
      ],
      "continuation": null,
      "summary_html": "<p>The Wikimedia Foundation announced paid data licensing agreements with Amazon, Meta, and Perplexity for access to Wikipedia data to train and develop large language models. This formalizes what has been an informal data source for AI training.</p>",
      "content_html": "<p>Under the deal, tech vendors are paying to access Wikimedia's data to train and develop their large language models.</p>"
    },
    {
      "id": "cacf74acbf5b",
      "title": "Microsoft Research Releases OptiMind: A 20B Parameter Model that Turns Natural Language into Solver Ready Optimization Models",
      "content": "Microsoft Research has released OptiMind, an AI based system that converts natural language descriptions of complex decision problems into mathematical formulations that optimization solvers can execute. It targets a long standing bottleneck in operations research, where translating business intent into mixed integer linear programs usually needs expert modelers and days of work.\n\n\n\nWhat OptiMind Is And What It Outputs?\n\n\n\nOptiMind-SFT is a specialized 20B parameter Mixture of Experts model in the gpt oss transformer family. About 3.6B parameters are active per token, so inference cost is closer to a mid sized model while keeping high capacity. The context length is 128,000 tokens, which allows long specifications and multi step reasoning traces inside a single request.\n\n\n\nThe model takes a natural language description of an optimization problem as input. The output is a mathematical formulation along with executable Python code that uses GurobiPy. The generated script defines decision variables, constraints, and objective, calls the Gurobi solver, and prints the optimal objective value and decisions.\n\n\n\nOptiMind acts as a formulation layer between domain experts and standard MILP solvers. It does not replace the solver, it generates the MILP that the solver will optimize.\n\n\n\nArchitecture, Training Setup, And Datasets\n\n\n\nThe base model is openai/gpt-oss-20b, fine tuned into microsoft/OptiMind-SFT using cleaned optimization datasets. The architecture is a Mixture of Experts transformer, with routing that activates a subset of experts per token. The model is released under the MIT license. \n\n\n\nTraining uses 8 NVIDIA B200 GPUs, and inference and evaluation in the reference setup use 8 NVIDIA H100 GPUs. Reported fine tuning time is about 8 hours. For regular use, the team recommends at least 32 GB of GPU memory on hardware such as A100, H100, or B200.\n\n\n\nFor supervised fine tuning, the research team construct cleaned versions of OR Instruct and OptMATH Train. For testing, they use expert validated and re-cleaned versions of IndustryOR, Mamo Complex, and OptMATH. These benchmarks cover hard formulation tasks where existing models often reach only 20 to 50 percent accuracy on the original noisy versions. \n\n\n\nClass Based Error Analysis And Data Cleaning\n\n\n\nA key technical idea in OptiMind is to combine optimization expertise with LLM training. The research team classifies problems from OR-Instruct and OptMATH into 53 seed classes, for example set cover, flow shop scheduling, or traveling salesman problem.\n\n\n\nFor each class, they run the gpt-oss-20b-base model on a sample of problems and select instances where the model output disagrees with the ground truth. Optimization experts inspect these items, identify the recurring formulation mistakes, and write short error descriptions and preventive hints. These hints describe correct constraints, variable bounds, or modeling tricks, such as the proper Miller Tucker Zemlin constraints for TSP.\n\n\n\nThe research team then uses a semi-automated pipeline. They regenerate solutions with a larger model that is prompted with the class specific hints, apply majority voting across samples to improve solution quality, and drop items that remain inconsistent. They also detect missing parameters and ambiguous statements and regenerate problem descriptions when needed. The result is a cleaned training corpus that is better aligned with correct mathematical formulations.\n\n\n\nInference Pipeline, Hints, And Test Time Scaling\n\n\n\nAt inference time, OptiMind behaves as a multi stage system, not just a single prompt. The default pipeline first classifies each test instance into one of the 53 optimization classes used during error analysis. It then augments the prompt with the error summary and hint pairs associated with that class. \n\n\n\nThe model then generates a reasoning trace, the mathematical formulation, and the GurobiPy code. When more compute is available, the system can apply self consistency with majority voting. It generates several candidate scripts, executes them, and selects the solution that appears most often within set numerical tolerances. \n\n\n\nA multi turn correction mode can also be enabled. The system runs the generated code, captures solver logs or execution errors, feeds this feedback back to the model, and lets the model revise the formulation and code for a few rounds. This closes some modeling and coding errors at the cost of higher latency. \n\n\n\nQuantitative Gains On Optimization Benchmarks\n\n\n\nOn cleaned versions of IndustryOR, Mamo-Complex, and OptMATH, the OptiMind framework significantly improves solution accuracy. The fine-tuned model improves formulation accuracy by 20.7 percent across multiple optimization benchmarks, with further gains when test time scaling techniques such as self consistency and multi turn feedback are applied.\n\n\n\nAcross these benchmarks, OptiMind improves absolute accuracy over the gpt-oss-20b-base model and outperforms other open source models of similar or larger size. It reaches performance that is competitive with proprietary frontier models such as GPT-o4 mini and GPT-5 under the evaluation settings.\n\n\n\nThese results rely on careful cleaning of both training and test data. The research team report that many apparent model errors on original benchmarks actually came from missing data, ambiguous descriptions, or incorrect reference solutions, and that re-cleaning can lift apparent accuracy for a fixed model from about 40 to 60 percent into the 70 to 90 percent range on the corrected sets.\n\n\n\nKey Takeaways\n\n\n\n\nOptiMind is a 20B parameter Mixture of Experts transformer in the gpt-oss-family that takes natural language optimization problems as input and outputs both a mathematical formulation and executable GurobiPy code, with about 3.6B parameters activated per token and a 128,000 token context length. \n\n\n\nThe model is fine tuned from openai/gpt-oss-20b on cleaned optimization datasets such as OR-Instruct and OptMATH, and evaluated on expert validated benchmarks including IndustryOR and Mamo Complex, focusing on mixed integer linear programming formulations. \n\n\n\nOptiMind uses class based error analysis and expert written hints for 53 optimization classes, then applies these hints both in data cleaning and at inference time, which systematically reduces common modeling mistakes in generated MILPs. \n\n\n\nThe framework improves formulation accuracy by 20.7 percent across multiple optimization benchmarks compared to the base model, and with test time scaling methods such as self consistency and multi turn feedback it reaches performance that is competitive with larger proprietary systems. \n\n\n\nOptiMind-SFT is released as microsoft/OptiMind-SFT on Hugging Face and as microsoft-optimind-sft in Azure AI Foundry, where it can be served via SGLang as an OpenAI compatible endpoint, enabling practical integration into decision support pipelines for supply chains, manufacturing, logistics, and scheduling. \n\n\n\n\n\n\n\n\nCheck out the Model Weights and Technical details. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Microsoft Research Releases OptiMind: A 20B Parameter Model that Turns Natural Language into Solver Ready Optimization Models appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/19/microsoft-research-releases-optimind-a-20b-parameter-model-that-turns-natural-language-into-solver-ready-optimization-models/",
      "author": "Asif Razzaq",
      "published": "2026-01-20T04:06:43",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Microsoft Research released OptiMind, a 20B-parameter MoE model (3.6B active) that converts natural language descriptions into mathematical optimization formulations. The model supports 128K context length and targets operations research bottlenecks.",
      "importance_score": 72.0,
      "reasoning": "Specialized model release from major lab addressing practical enterprise problem; demonstrates trend toward domain-specific AI tools",
      "themes": [
        "Model Release",
        "Microsoft",
        "Enterprise AI"
      ],
      "continuation": null,
      "summary_html": "<p>Microsoft Research released OptiMind, a 20B-parameter MoE model (3.6B active) that converts natural language descriptions into mathematical optimization formulations. The model supports 128K context length and targets operations research bottlenecks.</p>",
      "content_html": "<p>Microsoft Research has released OptiMind, an AI based system that converts natural language descriptions of complex decision problems into mathematical formulations that optimization solvers can execute. It targets a long standing bottleneck in operations research, where translating business intent into mixed integer linear programs usually needs expert modelers and days of work.</p>\n<p>What OptiMind Is And What It Outputs?</p>\n<p>OptiMind-SFT is a specialized 20B parameter Mixture of Experts model in the gpt oss transformer family. About 3.6B parameters are active per token, so inference cost is closer to a mid sized model while keeping high capacity. The context length is 128,000 tokens, which allows long specifications and multi step reasoning traces inside a single request.</p>\n<p>The model takes a natural language description of an optimization problem as input. The output is a mathematical formulation along with executable Python code that uses GurobiPy. The generated script defines decision variables, constraints, and objective, calls the Gurobi solver, and prints the optimal objective value and decisions.</p>\n<p>OptiMind acts as a formulation layer between domain experts and standard MILP solvers. It does not replace the solver, it generates the MILP that the solver will optimize.</p>\n<p>Architecture, Training Setup, And Datasets</p>\n<p>The base model is openai/gpt-oss-20b, fine tuned into microsoft/OptiMind-SFT using cleaned optimization datasets. The architecture is a Mixture of Experts transformer, with routing that activates a subset of experts per token. The model is released under the MIT license.</p>\n<p>Training uses 8 NVIDIA B200 GPUs, and inference and evaluation in the reference setup use 8 NVIDIA H100 GPUs. Reported fine tuning time is about 8 hours. For regular use, the team recommends at least 32 GB of GPU memory on hardware such as A100, H100, or B200.</p>\n<p>For supervised fine tuning, the research team construct cleaned versions of OR Instruct and OptMATH Train. For testing, they use expert validated and re-cleaned versions of IndustryOR, Mamo Complex, and OptMATH. These benchmarks cover hard formulation tasks where existing models often reach only 20 to 50 percent accuracy on the original noisy versions.</p>\n<p>Class Based Error Analysis And Data Cleaning</p>\n<p>A key technical idea in OptiMind is to combine optimization expertise with LLM training. The research team classifies problems from OR-Instruct and OptMATH into 53 seed classes, for example set cover, flow shop scheduling, or traveling salesman problem.</p>\n<p>For each class, they run the gpt-oss-20b-base model on a sample of problems and select instances where the model output disagrees with the ground truth. Optimization experts inspect these items, identify the recurring formulation mistakes, and write short error descriptions and preventive hints. These hints describe correct constraints, variable bounds, or modeling tricks, such as the proper Miller Tucker Zemlin constraints for TSP.</p>\n<p>The research team then uses a semi-automated pipeline. They regenerate solutions with a larger model that is prompted with the class specific hints, apply majority voting across samples to improve solution quality, and drop items that remain inconsistent. They also detect missing parameters and ambiguous statements and regenerate problem descriptions when needed. The result is a cleaned training corpus that is better aligned with correct mathematical formulations.</p>\n<p>Inference Pipeline, Hints, And Test Time Scaling</p>\n<p>At inference time, OptiMind behaves as a multi stage system, not just a single prompt. The default pipeline first classifies each test instance into one of the 53 optimization classes used during error analysis. It then augments the prompt with the error summary and hint pairs associated with that class.</p>\n<p>The model then generates a reasoning trace, the mathematical formulation, and the GurobiPy code. When more compute is available, the system can apply self consistency with majority voting. It generates several candidate scripts, executes them, and selects the solution that appears most often within set numerical tolerances.</p>\n<p>A multi turn correction mode can also be enabled. The system runs the generated code, captures solver logs or execution errors, feeds this feedback back to the model, and lets the model revise the formulation and code for a few rounds. This closes some modeling and coding errors at the cost of higher latency.</p>\n<p>Quantitative Gains On Optimization Benchmarks</p>\n<p>On cleaned versions of IndustryOR, Mamo-Complex, and OptMATH, the OptiMind framework significantly improves solution accuracy. The fine-tuned model improves formulation accuracy by 20.7 percent across multiple optimization benchmarks, with further gains when test time scaling techniques such as self consistency and multi turn feedback are applied.</p>\n<p>Across these benchmarks, OptiMind improves absolute accuracy over the gpt-oss-20b-base model and outperforms other open source models of similar or larger size. It reaches performance that is competitive with proprietary frontier models such as GPT-o4 mini and GPT-5 under the evaluation settings.</p>\n<p>These results rely on careful cleaning of both training and test data. The research team report that many apparent model errors on original benchmarks actually came from missing data, ambiguous descriptions, or incorrect reference solutions, and that re-cleaning can lift apparent accuracy for a fixed model from about 40 to 60 percent into the 70 to 90 percent range on the corrected sets.</p>\n<p>Key Takeaways</p>\n<p>OptiMind is a 20B parameter Mixture of Experts transformer in the gpt-oss-family that takes natural language optimization problems as input and outputs both a mathematical formulation and executable GurobiPy code, with about 3.6B parameters activated per token and a 128,000 token context length.</p>\n<p>The model is fine tuned from openai/gpt-oss-20b on cleaned optimization datasets such as OR-Instruct and OptMATH, and evaluated on expert validated benchmarks including IndustryOR and Mamo Complex, focusing on mixed integer linear programming formulations.</p>\n<p>OptiMind uses class based error analysis and expert written hints for 53 optimization classes, then applies these hints both in data cleaning and at inference time, which systematically reduces common modeling mistakes in generated MILPs.</p>\n<p>The framework improves formulation accuracy by 20.7 percent across multiple optimization benchmarks compared to the base model, and with test time scaling methods such as self consistency and multi turn feedback it reaches performance that is competitive with larger proprietary systems.</p>\n<p>OptiMind-SFT is released as microsoft/OptiMind-SFT on Hugging Face and as microsoft-optimind-sft in Azure AI Foundry, where it can be served via SGLang as an OpenAI compatible endpoint, enabling practical integration into decision support pipelines for supply chains, manufacturing, logistics, and scheduling.</p>\n<p>Check out the&nbsp;Model Weights&nbsp;and&nbsp;Technical details.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Microsoft Research Releases OptiMind: A 20B Parameter Model that Turns Natural Language into Solver Ready Optimization Models appeared first on MarkTechPost.</p>"
    },
    {
      "id": "83c846af5b74",
      "title": "Zhipu AI Releases GLM-4.7-Flash: A 30B-A3B MoE Model for Efficient Local Coding and Agents",
      "content": "GLM-4.7-Flash is a new member of the GLM 4.7 family and targets developers who want strong coding and reasoning performance in a model that is practical to run locally. Zhipu AI (Z.ai) describes GLM-4.7-Flash as a 30B-A3B MoE model and presents it as the strongest model in the 30B class, designed for lightweight deployment where performance and efficiency both matter.\n\n\n\nModel class and position inside the GLM 4.7 family\n\n\n\nGLM-4.7-Flash is a text generation model with 31B params, BF16 and F32 tensor types, and the architecture tag glm4_moe_lite. It supports English and Chinese, and it is configured for conversational use. GLM-4.7-Flash sits in the GLM-4.7 collection next to the larger GLM-4.7 and GLM-4.7-FP8 models. \n\n\n\nZ.ai positions GLM-4.7-Flash as a free tier and lightweight deployment option relative to the full GLM-4.7 model, while still targeting coding, reasoning, and general text generation tasks. This makes it interesting for developers who cannot deploy a 358B class model but still want a modern MoE design and strong benchmark results. \n\n\n\nArchitecture and context length\n\n\n\nIn a Mixture of Experts architecture of this type, the model stores more parameters than it activates for each token. That allows specialization across experts while keeping the effective compute per token closer to a smaller dense model.\n\n\n\nGLM 4.7 Flash supports a context length of 128k tokens and achieves strong performance on coding benchmarks among models of similar scale. This context size is suitable for large codebases, multi-file repositories, and long technical documents, where many existing models would need aggressive chunking.\n\n\n\nGLM-4.7-Flash uses a standard causal language modeling interface and a chat template, which allows integration into existing LLM stacks with minimal changes.\n\n\n\nBenchmark performance in the 30B class\n\n\n\nThe Z.ai team compares GLM-4.7-Flash with Qwen3-30B-A3B-Thinking-2507 and GPT-OSS-20B. GLM-4.7-Flash leads or is competitive across a mix of math, reasoning, long horizon, and coding agent benchmarks.\n\n\n\nhttps://huggingface.co/zai-org/GLM-4.7-Flash\n\n\nThis above table showcase why GLM-4.7-Flash is one of the strongest model in the 30B class, at least among the models included in this comparison. The important point is that GLM-4.7-Flash is not only a compact deployment of GLM but also a high performing model on established coding and agent benchmarks.\n\n\n\nEvaluation parameters and thinking mode\n\n\n\nFor most tasks, the default settings are: temperature 1.0, top p 0.95, and max new tokens 131072. This defines a relatively open sampling regime with a large generation budget.\n\n\n\nFor Terminal Bench and SWE-bench Verified, the configuration uses temperature 0.7, top p 1.0, and max new tokens 16384. For τ²-Bench, the configuration uses temperature 0 and max new tokens 16,384. These stricter settings reduce randomness for tasks that need stable tool use and multi step interaction.\n\n\n\nZ.ai team also recommends turning on Preserved Thinking mode for multi turn agentic tasks such as τ²-Bench and Terminal Bench 2. This mode preserves internal reasoning traces across turns. That is useful when you build agents that need long chains of function calls and corrections.\n\n\n\nHow GLM-4.7-Flash fits developer workflows\n\n\n\nGLM-4.7-Flash combines several properties that are relevant for agentic, coding focused applications:\n\n\n\n\nA 30B-A3B MoE architecture with 31B params and a 128k token context length.\n\n\n\nStrong benchmark results on AIME 25, GPQA, SWE-bench Verified, τ²-Bench, and BrowseComp compared to other models in the same table. \n\n\n\nDocumented evaluation parameters and a Preserved Thinking mode for multi turn agent tasks.\n\n\n\nFirst class support for vLLM, SGLang, and Transformers based inference, with ready to use commands. \n\n\n\nA growing set of finetunes and quantizations, including MLX conversions, in the Hugging Face ecosystem. \n\n\n\n\n\n\n\n\nCheck out the Model weight. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Zhipu AI Releases GLM-4.7-Flash: A 30B-A3B MoE Model for Efficient Local Coding and Agents appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/20/zhipu-ai-releases-glm-4-7-flash-a-30b-a3b-moe-model-for-efficient-local-coding-and-agents/",
      "author": "Michal Sutter",
      "published": "2026-01-20T19:54:29",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "As first reported in [Social](/?date=2026-01-20&category=reddit#item-3d47fe87b1f2) yesterday, Zhipu AI released GLM-4.7-Flash, a 30B-A3B MoE model optimized for local deployment, coding, and agent use cases. The model supports English and Chinese with strong reasoning performance at lower compute requirements.",
      "importance_score": 70.0,
      "reasoning": "Notable release from leading Chinese AI lab; efficient MoE architecture advances local deployment capabilities",
      "themes": [
        "Model Release",
        "China AI",
        "Open Source"
      ],
      "continuation": {
        "original_item_id": "3d47fe87b1f2",
        "original_date": "2026-01-20",
        "original_category": "reddit",
        "original_title": "zai-org/GLM-4.7-Flash · Hugging Face",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As first reported in **Social** yesterday"
      },
      "summary_html": "<p>As first reported in <a href=\"/?date=2026-01-20&amp;category=reddit#item-3d47fe87b1f2\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> yesterday, Zhipu AI released GLM-4.7-Flash, a 30B-A3B MoE model optimized for local deployment, coding, and agent use cases. The model supports English and Chinese with strong reasoning performance at lower compute requirements.</p>",
      "content_html": "<p>GLM-4.7-Flash is a new member of the GLM 4.7 family and targets developers who want strong coding and reasoning performance in a model that is practical to run locally. Zhipu AI (Z.ai) describes GLM-4.7-Flash as a 30B-A3B MoE model and presents it as the strongest model in the 30B class, designed for lightweight deployment where performance and efficiency both matter.</p>\n<p>Model class and position inside the GLM 4.7 family</p>\n<p>GLM-4.7-Flash is a text generation model with 31B params, BF16 and F32 tensor types, and the architecture tag glm4_moe_lite. It supports English and Chinese, and it is configured for conversational use. GLM-4.7-Flash sits in the GLM-4.7 collection next to the larger GLM-4.7 and GLM-4.7-FP8 models.</p>\n<p>Z.ai positions GLM-4.7-Flash as a free tier and lightweight deployment option relative to the full GLM-4.7 model, while still targeting coding, reasoning, and general text generation tasks. This makes it interesting for developers who cannot deploy a 358B class model but still want a modern MoE design and strong benchmark results.</p>\n<p>Architecture and context length</p>\n<p>In a Mixture of Experts architecture of this type, the model stores more parameters than it activates for each token. That allows specialization across experts while keeping the effective compute per token closer to a smaller dense model.</p>\n<p>GLM 4.7 Flash supports a context length of 128k tokens and achieves strong performance on coding benchmarks among models of similar scale. This context size is suitable for large codebases, multi-file repositories, and long technical documents, where many existing models would need aggressive chunking.</p>\n<p>GLM-4.7-Flash uses a standard causal language modeling interface and a chat template, which allows integration into existing LLM stacks with minimal changes.</p>\n<p>Benchmark performance in the 30B class</p>\n<p>The Z.ai team compares GLM-4.7-Flash with Qwen3-30B-A3B-Thinking-2507 and GPT-OSS-20B. GLM-4.7-Flash leads or is competitive across a mix of math, reasoning, long horizon, and coding agent benchmarks.</p>\n<p>https://huggingface.co/zai-org/GLM-4.7-Flash</p>\n<p>This above table showcase why GLM-4.7-Flash is one of the strongest model in the 30B class, at least among the models included in this comparison. The important point is that GLM-4.7-Flash is not only a compact deployment of GLM but also a high performing model on established coding and agent benchmarks.</p>\n<p>Evaluation parameters and thinking mode</p>\n<p>For most tasks, the default settings are: temperature 1.0, top p 0.95, and max new tokens 131072. This defines a relatively open sampling regime with a large generation budget.</p>\n<p>For Terminal Bench and SWE-bench Verified, the configuration uses temperature 0.7, top p 1.0, and max new tokens 16384. For τ²-Bench, the configuration uses temperature 0 and max new tokens 16,384. These stricter settings reduce randomness for tasks that need stable tool use and multi step interaction.</p>\n<p>Z.ai team also recommends turning on Preserved Thinking mode for multi turn agentic tasks such as τ²-Bench and Terminal Bench 2. This mode preserves internal reasoning traces across turns. That is useful when you build agents that need long chains of function calls and corrections.</p>\n<p>How GLM-4.7-Flash fits developer workflows</p>\n<p>GLM-4.7-Flash combines several properties that are relevant for agentic, coding focused applications:</p>\n<p>A 30B-A3B MoE architecture with 31B params and a 128k token context length.</p>\n<p>Strong benchmark results on AIME 25, GPQA, SWE-bench Verified, τ²-Bench, and BrowseComp compared to other models in the same table.</p>\n<p>Documented evaluation parameters and a Preserved Thinking mode for multi turn agent tasks.</p>\n<p>First class support for vLLM, SGLang, and Transformers based inference, with ready to use commands.</p>\n<p>A growing set of finetunes and quantizations, including MLX conversions, in the Hugging Face ecosystem.</p>\n<p>Check out the&nbsp;Model weight.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Zhipu AI Releases GLM-4.7-Flash: A 30B-A3B MoE Model for Efficient Local Coding and Agents appeared first on MarkTechPost.</p>"
    },
    {
      "id": "90264fc9057a",
      "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
      "content": "",
      "url": "https://huggingface.co/blog/waypoint-1",
      "author": "Unknown",
      "published": "2026-01-20T00:00:00",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Overworld released Waypoint-1, a real-time interactive video diffusion model. Limited details available but represents advancement in interactive video generation capabilities.",
      "importance_score": 68.0,
      "reasoning": "Real-time interactive video generation is a frontier capability; limited content makes full assessment difficult",
      "themes": [
        "Video Generation",
        "Model Release",
        "Diffusion Models"
      ],
      "continuation": null,
      "summary_html": "<p>Overworld released Waypoint-1, a real-time interactive video diffusion model. Limited details available but represents advancement in interactive video generation capabilities.</p>",
      "content_html": ""
    },
    {
      "id": "25d0480d05b8",
      "title": "Cheap ChatGPT Tier on Offer for $8 a Month; Ads Coming Soon",
      "content": "OpenAI is evolving ChatGPT as it looks for ways to boost revenue while ensuring ad-free options for premium users.",
      "url": "https://aibusiness.com/foundation-models/cheap-chatgpt-tier-month-ads-coming-soon",
      "author": "Graham Hope",
      "published": "2026-01-20T22:39:13",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "OpenAI is developing a cheaper ChatGPT tier at $8/month and planning to introduce advertising to the platform. Premium users will retain ad-free options as the company seeks to boost revenue.",
      "importance_score": 65.0,
      "reasoning": "Significant business model evolution for the leading consumer AI product; ads represent major strategic shift for OpenAI",
      "themes": [
        "OpenAI",
        "Business Models",
        "Consumer AI"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI is developing a cheaper ChatGPT tier at $8/month and planning to introduce advertising to the platform. Premium users will retain ad-free options as the company seeks to boost revenue.</p>",
      "content_html": "<p>OpenAI is evolving ChatGPT as it looks for ways to boost revenue while ensuring ad-free options for premium users.</p>"
    },
    {
      "id": "64900f37913d",
      "title": "Differential Transformer V2",
      "content": "",
      "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
      "author": "Unknown",
      "published": "2026-01-20T03:20:57",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Microsoft published Differential Transformer V2 on Hugging Face, an advancement to the attention mechanism architecture. Limited content available in the provided article.",
      "importance_score": 62.0,
      "reasoning": "Architecture improvements from Microsoft are noteworthy; differential attention showed promise in V1 for reducing noise",
      "themes": [
        "Research",
        "Architecture",
        "Microsoft"
      ],
      "continuation": null,
      "summary_html": "<p>Microsoft published Differential Transformer V2 on Hugging Face, an advancement to the attention mechanism architecture. Limited content available in the provided article.</p>",
      "content_html": ""
    },
    {
      "id": "91fadee7cd0f",
      "title": "OpenAI, ServiceNow Enter Into Strategic Multiyear Partnership",
      "content": "The partnership reflects OpenAI's broader strategy to establish itself in the enterprise IT market by collaborating with major providers, while embedding its technology in their products without direct sales.",
      "url": "https://aibusiness.com/generative-ai/openai-servicenow-enter-into-multiyear-partnership",
      "author": "Esther Shittu",
      "published": "2026-01-20T22:37:53",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "OpenAI and ServiceNow entered a strategic multi-year partnership to embed OpenAI technology in ServiceNow products without direct sales. The collaboration targets enterprise IT market expansion.",
      "importance_score": 60.0,
      "reasoning": "Same partnership as detailed article but with less information; represents significant enterprise strategy",
      "themes": [
        "Enterprise AI",
        "Strategic Partnerships",
        "OpenAI"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI and ServiceNow entered a strategic multi-year partnership to embed OpenAI technology in ServiceNow products without direct sales. The collaboration targets enterprise IT market expansion.</p>",
      "content_html": "<p>The partnership reflects OpenAI's broader strategy to establish itself in the enterprise IT market by collaborating with major providers, while embedding its technology in their products without direct sales.</p>"
    },
    {
      "id": "8c1ea14ae122",
      "title": "Thousands of Companies Are Driving China’s AI Boom. A Government Registry Tracks Them All",
      "content": "How the Cyberspace Administration of China inadvertently made a guide to the country’s homegrown AI revolution.",
      "url": "https://www.wired.com/story/china-ai-boom-algorithm-registry/",
      "author": "Yi-Ling Liu",
      "published": "2026-01-20T11:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "The Big Story",
        "Business",
        "Business / Artificial Intelligence",
        "longreads",
        "The China Issue",
        "artificial intelligence",
        "government",
        "China",
        "Model Citizens"
      ],
      "summary": "China's Cyberspace Administration maintains an algorithm registry that inadvertently documents the country's entire AI ecosystem. The registry provides unprecedented visibility into thousands of Chinese AI companies.",
      "importance_score": 58.0,
      "reasoning": "Valuable insight into China's AI industry structure and regulatory approach; useful for understanding AI competition",
      "themes": [
        "China AI",
        "Regulation",
        "AI Ecosystem"
      ],
      "continuation": null,
      "summary_html": "<p>China's Cyberspace Administration maintains an algorithm registry that inadvertently documents the country's entire AI ecosystem. The registry provides unprecedented visibility into thousands of Chinese AI companies.</p>",
      "content_html": "<p>How the Cyberspace Administration of China inadvertently made a guide to the country’s homegrown AI revolution.</p>"
    },
    {
      "id": "208dd7adb3af",
      "title": "UK exposed to ‘serious harm’ by failure to tackle AI risks, MPs warn",
      "content": "Government, Bank of England and FCA criticised for taking ‘wait-and-see’ approach to AI use in financial sectorConsumers and the UK financial system are being exposed to “serious harm” by the failure of government and the Bank of England to get a grip on the risks posed by artificial intelligence, an influential parliamentary committee has warned.In a new report, MPs on the Treasury committee criticise ministers and City regulators, including the Financial Conduct Authority (FCA), for taking a “wait-and-see” approach to AI use across the financial sector. Continue reading...",
      "url": "https://www.theguardian.com/business/2026/jan/20/uk-ai-risks-mps-government-bank-of-england-fca",
      "author": "Kalyeena Makortoff Banking correspondent",
      "published": "2026-01-20T00:01:08",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Business",
        "AI (artificial intelligence)",
        "Bank of England",
        "Financial Conduct Authority",
        "Regulators",
        "Technology",
        "UK news"
      ],
      "summary": "UK Treasury committee MPs criticized the government, Bank of England, and FCA for a 'wait-and-see' approach to AI risks in finance, warning of 'serious harm' to consumers and the financial system.",
      "importance_score": 55.0,
      "reasoning": "Regulatory pressure in major financial market; highlights growing tension between AI adoption and risk management",
      "themes": [
        "Regulation",
        "AI Risk",
        "Financial Services"
      ],
      "continuation": null,
      "summary_html": "<p>UK Treasury committee MPs criticized the government, Bank of England, and FCA for a 'wait-and-see' approach to AI risks in finance, warning of 'serious harm' to consumers and the financial system.</p>",
      "content_html": "<p>Government, Bank of England and FCA criticised for taking ‘wait-and-see’ approach to AI use in financial sectorConsumers and the UK financial system are being exposed to “serious harm” by the failure of government and the Bank of England to get a grip on the risks posed by artificial intelligence, an influential parliamentary committee has warned.In a new report, MPs on the Treasury committee criticise ministers and City regulators, including the Financial Conduct Authority (FCA), for taking a “wait-and-see” approach to AI use across the financial sector. Continue reading...</p>"
    },
    {
      "id": "74f29593b90c",
      "title": "Inside IAIRO, India’s ISRO for AI",
      "content": "On December 30, last year, the Gujarat government approved something India has been debating for more than a decade, but never quite managed to build: a serious national AI research institution. The Indian AI Research Organisation (IAIRO) began operations on January 1 as a Section 8 non-profit inside the GIFT City in Gandhinagar.&nbsp;\n\n\n\nDespite the low-key launch, the intent is anything but small.\n\n\n\nIAIRO is being set up as a public-private partnership between the state government, the Indian government through the IndiaAI Mission, and the Indian Pharmaceutical Alliance (IPA).\n\n\n\nFor the first five years, IAIRO will operate on a budget of roughly ₹300 crore, split equally between the state, the Centre, and the private partner. IPA alone is putting in ₹25 crore in the first year. The pharma group includes companies like Cipla, Torrent and Sun Pharma.&nbsp;\n\n\n\nThis is not a startup incubator. It is not a skills programme.&nbsp;\n\n\n\nIAIRO is being positioned as a national research institution for AI, closer in spirit to ISRO than to anything India has tried in software before.\n\n\n\nThe official note describes it as a multidisciplinary AI hub that will conduct research, build products, generate IP, train people, and work with industry and startups. It will run on a hybrid compute model, using its own GPUs along with India AI Cloud.&nbsp;\n\n\n\nNot Just About LLMs\n\n\n\nOn paper, it could sound like another government AI centre. What makes it different is who is behind it and what they are trying to fix.\n\n\n\nAmit Sheth, a professor of computer science and engineering at the University of South Carolina, who is leading IAIRO, has spent decades in AI research in the US. He said the idea did not start in Gandhinagar. It started two years ago in Delhi, in a meeting with Prime Minister Narendra Modi.&nbsp;\n\n\n\nSheth had taken the proposal for Ekagrid University that was being coordinated by Shail Kumar and involved some of the top AI leaders of Indian origin, for developing a world-class research university with an AI-first strategy. That plan did not go through. And the problem did not go away.&nbsp;\n\n\n\nIndia produces a large number of AI users and service engineers. It does not produce frontier AI researchers or people who can build deep systems at scale.\n\n\n\nSheth is blunt about the gap.\n\n\n\n“Jensen Huang has pointed out that nine of the world’s top 10 AI institutions are in China, over 70% of papers at leading AI conferences now come from China, and that supports ecosystems with deep research capability capable of producing organisations like DeepSeek. India currently lacks this depth. IAIRO exists to deliberately build it,” he said.\n\n\n\nThat sentence captures what IAIRO is really about. This is not about chatbots or coding tools. It is about whether India has the kind of research base that can create the next DeepSeek—not just use it.\n\n\n\n\n\n\n\n\n\nAn ISRO Moment for AI\n\n\n\nFrom those early discussions, a position paper titled Sovereign AI for India’s Strategic Autonomy made its way to key decision-makers, including S Krishnan, secretary at MeitY; Abhishek Singh, CEO of IndiaAI; and Abhay Karandikar, secretary at the science and technology department. What followed were three goals that now define IAIRO.\n\n\n\nThe first is talent. Not broad skilling, but building a deep pool of AI researchers, scientists, and system builders capable of genuine frontier work.\n\n\n\nThe second is original IP: foundation models, applied platforms, and domain AI systems that can become Indian-origin global products.\n\n\n\nThe third is what Sheth calls a prototype-to-product ecosystem—turning papers into companies.\n\n\n\nThat is why IAIRO is being set up as a national platform rather than a single lab. Sheth describes a growing network that already includes senior researchers with h-index between 70 and 120, technical leaders from Google, Apple, DeepMind, OpenAI, IBM Research and Amazon, a reverse brain-drain cohort of Indian researchers returning or working part-time from abroad, founders who have built IP-driven companies, and investors across stages.\n\n\n\nLayered on top of that is a formal programme called From Breakthrough to Breakout, bringing together entrepreneurs-in-residence, applied labs, and venture creation tracks.\n\n\n\nAlong with Sheth, IAIRO was founded by prof Ramesh Jain, the founding director of UC Irvine’s Institute for Future Health; prof Dev Niyogi, UNESCO Chair for AI, water and cities at the University of Texas at Austin; prof Sanjay Chaudhary from Ahmedabad University; investor and entrepreneur Juhi Bhatnagar; and Selvam Velmurugan, senior technical advisor at BlinkRx.\n\n\n\nThe board includes Ajai Chowdhry, co-founder of HCL; Sharad Sharma, founder of iSPIRT Foundation; IndiaAI’s Singh; and P Bharathi, secretary of the science and technology department.\n\n\n\n“The ambition is not incremental improvement. It is to create, for AI, what ISRO created for space: a concentrated national capability that compounds talent, IP, and execution over decades,” Sheth said.\n\n\n\nA Different Model Than the US &amp; China\n\n\n\nThe first real test of that ambition will come in the sectors IAIRO has chosen to start with.\n\n\n\nUnlike big labs in the US that chase general-purpose models, IAIRO is deliberately starting with enterprise and mission-critical AI. The logic is simple: India cannot afford to burn billions training broad LLMs that do not solve local problems.\n\n\n\n“From day one, we are building AI systems that can power discovery, decision making, and operations inside strategic sectors,” Sheth said.\n\n\n\nThe flagship domain is pharma, which is exactly why the IPA is the anchor partner. Indian pharma already spends more on R&amp;D than any other domestic industry, but it remains largely stuck in generics. IAIRO is bringing in people who have built AI systems for drug discovery, clinical trials, and intelligent manufacturing in the West.\n\n\n\n“This programme spans the full stack: from AI-enabled discovery platforms, to clinical trial intelligence, to manufacturing optimisation and quality systems,” he said.\n\n\n\nThe second pillar is sustainability, and what Sheth calls ‘Digital Earth’. This is about climate, weather, agriculture, energy, and resilience.\n\n\n\nIndia’s economy is intensely weather-sensitive. Floods, fog, heatwaves, and shifting rain patterns directly hit infrastructure, crops, and supply chains. IAIRO wants to build AI systems that can deliver hyperlocal prediction, digital twins of cities and ecosystems, and decision tools that governments and companies can actually use.\n\n\n\n“Future proofing infrastructure and investment risks to weather and climatic extremes is possible with AI-based novel weather and environmental predictive tools,” Sheth said.\n\n\n\nThe third pillar is health. The idea here is not just hospital AI, but continuous, everyday health, starting with crises like diabetes.\n\n\n\n“IAIRO is reinventing healthcare, replacing episodic care with continuous, everyday health support that helps people understand what is happening, adapt gradually over time, and engage with medical care more effectively,” he said.\n\n\n\nBehind all of this sits IndiaAI. IAIRO is being created as a Focused Research Organisation under the IndiaAI Mission, again using a PPP model. The compute, policy backing, and national coordination will come from there.\n\n\n\n“We consider ourselves lucky that the Indian Pharmaceutical Alliance stepped in as an inaugural private partner,” Sheth said, noting how reluctant Indian corporates usually are to fund real research.\n\n\n\nWorking From -1 to 0\n\n\n\nOne of the most misunderstood parts of IAIRO is its startup plan. It is not an incubator like the ones at IITs. It does not write early cheques to every idea.\n\n\n\nIAIRO is designed to work at what Sheth calls ‘-1 to 0’. The emphasis starts with ideas that are nationally relevant—essentially solving sovereign problems—and only then moves to the technical depth of the solution.&nbsp;&nbsp;\n\n\n\nThis happens before a company even exists. Founders are selected first. Problems are discovered and validated. IP is created inside IAIRO. Only then are companies spun out, with deep technical and ecosystem backing already in place.\n\n\n\n“We operate at −1 to 0, not 0 to 1,” he said. “We focus on creating a small number of deeply technical, AI- and IP-led companies rather than supporting large volumes of early startups.”\n\n\n\nThere will also be a second funnel for existing startups that need serious technical reinforcement to reach a breakthrough level.\n\n\n\nIAIRO will not usually fund TRL 1 to 3 startups with equity cheques. Instead, it absorbs the risk by paying for research, people, infrastructure and validation until the science is real enough for venture capital.\n\n\n\nIt is a very different model from most Indian programmes that hand out grants and hope for the best.\n\n\n\nSo, is IAIRO the DeepMind of India?\n\n\n\nSheth says no—not in the near term, at least.\n\n\n\n“DeepMind operates at a completely different financial scale. While we have high ambitions, the current level of investment in IAIRO is far more modest, multiple orders of magnitude lower than Big Tech, and nowhere close to even relatively new players like DeepSeek,” he said.\n\n\n\nThat is why the focus is on mission-critical enterprise AI rather than generic models. It is also why the goal is to start reversing the brain drain over a long cycle, not overnight.\n\n\n\nCreating World Class Talent\n\n\n\nAnother important reality is that India is significantly behind and has real distance to cover. China began its Thousand Talents program nearly 15 years ago, and today an estimated 80-90% of Chinese students and experts trained in the US have returned to build in China.&nbsp;\n\n\n\nIAIRO is an early step towards creating a similar momentum for India—providing a serious institutional pull to start reversing the flow, with the clear understanding that this is long-cycle work. Better late than never.\n\n\n\nWhy does India publish so little cutting-edge AI research? Sheth argues it is not a talent problem. It is an ecosystem problem.\n\n\n\n“Breakthrough research comes from institutions, not individuals,” he said. “We did our undergraduate education in India, but our serious research training, PhDs, and early high-impact work happened in the US. Not because Indians are better there, but because the ecosystem there makes excellence far more likely.”\n\n\n\nOver the last few years, Sheth’s group in the US has published 40-60 papers annually. “More than half had Indian students working as interns with my team as co-authors. Many of them went on to do a PhD with me or join other top, fully funded PhD programmes,” he said.\n\n\n\nIAIRO is trying to recreate that loop inside India with funded labs, strong mentors, ambitious peers, and a culture of publishing and building.\n\n\n\nThe government partnership is what makes this possible. It also creates risk.\n\n\n\n“The risk is bureaucracy. The upside is nation-scale capability,” Sheth said.\n\n\n\nPrivate capital and most Indian corporates simply are not wired to bankroll research that takes a long time to pay off. That kind of long-horizon ambition has always belonged to the state. But IAIRO is trying to keep things sharp, running as a hybrid with a third of its funding coming from private sources.\n\n\n\nWhat Makes Sheth Think It Will Work?\n\n\n\nSheth believes this model can actually hold, as the foundation is already being laid with unusual seriousness.\n\n\n\n“IAIRO has already attracted exceptional expertise across every role required to build a world-class AI research and deep-tech ecosystem,” he said, pointing to senior researchers with global publication records, engineers who’ve worked inside the biggest AI labs, experienced programme leaders, and an embedded investment network designed to move ideas from labs to real deployments.\n\n\n\nOn paper, the Gujarat government may have announced IAIRO as another PPP in GIFT City. In reality, however, what is actually taking shape is something far more ambitious. It is a bet that India can finally build a home for serious AI, where frontier research, real-world product building, and national missions don’t live in silos but under one roof.&nbsp;\n\n\n\nWhether ₹300 crore is enough to pull that off is still up for debate. But what stands out is the intent. For the first time in a long time, India’s ambition is not modest.\nThe post Inside IAIRO, India’s ISRO for AI appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-features/inside-iairo-indias-isro-for-ai/",
      "author": "Mohit Pandey",
      "published": "2026-01-20T12:13:13",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI Features"
      ],
      "summary": "India launched IAIRO (Indian AI Research Organisation) on January 1st as a public-private partnership with ~₹300 crore budget over five years. The institution operates from GIFT City, Gandhinagar with pharmaceutical industry backing.",
      "importance_score": 55.0,
      "reasoning": "New national AI research infrastructure represents India's commitment to AI development; modest budget limits immediate impact",
      "themes": [
        "National AI Strategy",
        "India",
        "Research Institutions"
      ],
      "continuation": null,
      "summary_html": "<p>India launched IAIRO (Indian AI Research Organisation) on January 1st as a public-private partnership with ~₹300 crore budget over five years. The institution operates from GIFT City, Gandhinagar with pharmaceutical industry backing.</p>",
      "content_html": "<p>On December 30, last year, the Gujarat government approved something India has been debating for more than a decade, but never quite managed to build: a serious national AI research institution. The Indian AI Research Organisation (IAIRO) began operations on January 1 as a Section 8 non-profit inside the GIFT City in Gandhinagar.&nbsp;</p>\n<p>Despite the low-key launch, the intent is anything but small.</p>\n<p>IAIRO is being set up as a public-private partnership between the state government, the Indian government through the IndiaAI Mission, and the Indian Pharmaceutical Alliance (IPA).</p>\n<p>For the first five years, IAIRO will operate on a budget of roughly ₹300 crore, split equally between the state, the Centre, and the private partner. IPA alone is putting in ₹25 crore in the first year. The pharma group includes companies like Cipla, Torrent and Sun Pharma.&nbsp;</p>\n<p>This is not a startup incubator. It is not a skills programme.&nbsp;</p>\n<p>IAIRO is being positioned as a national research institution for AI, closer in spirit to ISRO than to anything India has tried in software before.</p>\n<p>The official note describes it as a multidisciplinary AI hub that will conduct research, build products, generate IP, train people, and work with industry and startups. It will run on a hybrid compute model, using its own GPUs along with India AI Cloud.&nbsp;</p>\n<p>Not Just About LLMs</p>\n<p>On paper, it could sound like another government AI centre. What makes it different is who is behind it and what they are trying to fix.</p>\n<p>Amit Sheth, a professor of computer science and engineering at the University of South Carolina, who is leading IAIRO, has spent decades in AI research in the US. He said the idea did not start in Gandhinagar. It started two years ago in Delhi, in a meeting with Prime Minister Narendra Modi.&nbsp;</p>\n<p>Sheth had taken the proposal for Ekagrid University that was being coordinated by Shail Kumar and involved some of the top AI leaders of Indian origin, for developing a world-class research university with an AI-first strategy. That plan did not go through. And the problem did not go away.&nbsp;</p>\n<p>India produces a large number of AI users and service engineers. It does not produce frontier AI researchers or people who can build deep systems at scale.</p>\n<p>Sheth is blunt about the gap.</p>\n<p>“Jensen Huang has pointed out that nine of the world’s top 10 AI institutions are in China, over 70% of papers at leading AI conferences now come from China, and that supports ecosystems with deep research capability capable of producing organisations like DeepSeek. India currently lacks this depth. IAIRO exists to deliberately build it,” he said.</p>\n<p>That sentence captures what IAIRO is really about. This is not about chatbots or coding tools. It is about whether India has the kind of research base that can create the next DeepSeek—not just use it.</p>\n<p>An ISRO Moment for AI</p>\n<p>From those early discussions, a position paper titled Sovereign AI for India’s Strategic Autonomy made its way to key decision-makers, including S Krishnan, secretary at MeitY; Abhishek Singh, CEO of IndiaAI; and Abhay Karandikar, secretary at the science and technology department. What followed were three goals that now define IAIRO.</p>\n<p>The first is talent. Not broad skilling, but building a deep pool of AI researchers, scientists, and system builders capable of genuine frontier work.</p>\n<p>The second is original IP: foundation models, applied platforms, and domain AI systems that can become Indian-origin global products.</p>\n<p>The third is what Sheth calls a prototype-to-product ecosystem—turning papers into companies.</p>\n<p>That is why IAIRO is being set up as a national platform rather than a single lab. Sheth describes a growing network that already includes senior researchers with h-index between 70 and 120, technical leaders from Google, Apple, DeepMind, OpenAI, IBM Research and Amazon, a reverse brain-drain cohort of Indian researchers returning or working part-time from abroad, founders who have built IP-driven companies, and investors across stages.</p>\n<p>Layered on top of that is a formal programme called From Breakthrough to Breakout, bringing together entrepreneurs-in-residence, applied labs, and venture creation tracks.</p>\n<p>Along with Sheth, IAIRO was founded by prof Ramesh Jain, the founding director of UC Irvine’s Institute for Future Health; prof Dev Niyogi, UNESCO Chair for AI, water and cities at the University of Texas at Austin; prof Sanjay Chaudhary from Ahmedabad University; investor and entrepreneur Juhi Bhatnagar; and Selvam Velmurugan, senior technical advisor at BlinkRx.</p>\n<p>The board includes Ajai Chowdhry, co-founder of HCL; Sharad Sharma, founder of iSPIRT Foundation; IndiaAI’s Singh; and P Bharathi, secretary of the science and technology department.</p>\n<p>“The ambition is not incremental improvement. It is to create, for AI, what ISRO created for space: a concentrated national capability that compounds talent, IP, and execution over decades,” Sheth said.</p>\n<p>A Different Model Than the US &amp; China</p>\n<p>The first real test of that ambition will come in the sectors IAIRO has chosen to start with.</p>\n<p>Unlike big labs in the US that chase general-purpose models, IAIRO is deliberately starting with enterprise and mission-critical AI. The logic is simple: India cannot afford to burn billions training broad LLMs that do not solve local problems.</p>\n<p>“From day one, we are building AI systems that can power discovery, decision making, and operations inside strategic sectors,” Sheth said.</p>\n<p>The flagship domain is pharma, which is exactly why the IPA is the anchor partner. Indian pharma already spends more on R&amp;D than any other domestic industry, but it remains largely stuck in generics. IAIRO is bringing in people who have built AI systems for drug discovery, clinical trials, and intelligent manufacturing in the West.</p>\n<p>“This programme spans the full stack: from AI-enabled discovery platforms, to clinical trial intelligence, to manufacturing optimisation and quality systems,” he said.</p>\n<p>The second pillar is sustainability, and what Sheth calls ‘Digital Earth’. This is about climate, weather, agriculture, energy, and resilience.</p>\n<p>India’s economy is intensely weather-sensitive. Floods, fog, heatwaves, and shifting rain patterns directly hit infrastructure, crops, and supply chains. IAIRO wants to build AI systems that can deliver hyperlocal prediction, digital twins of cities and ecosystems, and decision tools that governments and companies can actually use.</p>\n<p>“Future proofing infrastructure and investment risks to weather and climatic extremes is possible with AI-based novel weather and environmental predictive tools,” Sheth said.</p>\n<p>The third pillar is health. The idea here is not just hospital AI, but continuous, everyday health, starting with crises like diabetes.</p>\n<p>“IAIRO is reinventing healthcare, replacing episodic care with continuous, everyday health support that helps people understand what is happening, adapt gradually over time, and engage with medical care more effectively,” he said.</p>\n<p>Behind all of this sits IndiaAI. IAIRO is being created as a Focused Research Organisation under the IndiaAI Mission, again using a PPP model. The compute, policy backing, and national coordination will come from there.</p>\n<p>“We consider ourselves lucky that the Indian Pharmaceutical Alliance stepped in as an inaugural private partner,” Sheth said, noting how reluctant Indian corporates usually are to fund real research.</p>\n<p>Working From -1 to 0</p>\n<p>One of the most misunderstood parts of IAIRO is its startup plan. It is not an incubator like the ones at IITs. It does not write early cheques to every idea.</p>\n<p>IAIRO is designed to work at what Sheth calls ‘-1 to 0’. The emphasis starts with ideas that are nationally relevant—essentially solving sovereign problems—and only then moves to the technical depth of the solution.&nbsp;&nbsp;</p>\n<p>This happens before a company even exists. Founders are selected first. Problems are discovered and validated. IP is created inside IAIRO. Only then are companies spun out, with deep technical and ecosystem backing already in place.</p>\n<p>“We operate at −1 to 0, not 0 to 1,” he said. “We focus on creating a small number of deeply technical, AI- and IP-led companies rather than supporting large volumes of early startups.”</p>\n<p>There will also be a second funnel for existing startups that need serious technical reinforcement to reach a breakthrough level.</p>\n<p>IAIRO will not usually fund TRL 1 to 3 startups with equity cheques. Instead, it absorbs the risk by paying for research, people, infrastructure and validation until the science is real enough for venture capital.</p>\n<p>It is a very different model from most Indian programmes that hand out grants and hope for the best.</p>\n<p>So, is IAIRO the DeepMind of India?</p>\n<p>Sheth says no—not in the near term, at least.</p>\n<p>“DeepMind operates at a completely different financial scale. While we have high ambitions, the current level of investment in IAIRO is far more modest, multiple orders of magnitude lower than Big Tech, and nowhere close to even relatively new players like DeepSeek,” he said.</p>\n<p>That is why the focus is on mission-critical enterprise AI rather than generic models. It is also why the goal is to start reversing the brain drain over a long cycle, not overnight.</p>\n<p>Creating World Class Talent</p>\n<p>Another important reality is that India is significantly behind and has real distance to cover. China began its Thousand Talents program nearly 15 years ago, and today an estimated 80-90% of Chinese students and experts trained in the US have returned to build in China.&nbsp;</p>\n<p>IAIRO is an early step towards creating a similar momentum for India—providing a serious institutional pull to start reversing the flow, with the clear understanding that this is long-cycle work. Better late than never.</p>\n<p>Why does India publish so little cutting-edge AI research? Sheth argues it is not a talent problem. It is an ecosystem problem.</p>\n<p>“Breakthrough research comes from institutions, not individuals,” he said. “We did our undergraduate education in India, but our serious research training, PhDs, and early high-impact work happened in the US. Not because Indians are better there, but because the ecosystem there makes excellence far more likely.”</p>\n<p>Over the last few years, Sheth’s group in the US has published 40-60 papers annually. “More than half had Indian students working as interns with my team as co-authors. Many of them went on to do a PhD with me or join other top, fully funded PhD programmes,” he said.</p>\n<p>IAIRO is trying to recreate that loop inside India with funded labs, strong mentors, ambitious peers, and a culture of publishing and building.</p>\n<p>The government partnership is what makes this possible. It also creates risk.</p>\n<p>“The risk is bureaucracy. The upside is nation-scale capability,” Sheth said.</p>\n<p>Private capital and most Indian corporates simply are not wired to bankroll research that takes a long time to pay off. That kind of long-horizon ambition has always belonged to the state. But IAIRO is trying to keep things sharp, running as a hybrid with a third of its funding coming from private sources.</p>\n<p>What Makes Sheth Think It Will Work?</p>\n<p>Sheth believes this model can actually hold, as the foundation is already being laid with unusual seriousness.</p>\n<p>“IAIRO has already attracted exceptional expertise across every role required to build a world-class AI research and deep-tech ecosystem,” he said, pointing to senior researchers with global publication records, engineers who’ve worked inside the biggest AI labs, experienced programme leaders, and an embedded investment network designed to move ideas from labs to real deployments.</p>\n<p>On paper, the Gujarat government may have announced IAIRO as another PPP in GIFT City. In reality, however, what is actually taking shape is something far more ambitious. It is a bet that India can finally build a home for serious AI, where frontier research, real-world product building, and national missions don’t live in silos but under one roof.&nbsp;</p>\n<p>Whether ₹300 crore is enough to pull that off is still up for debate. But what stands out is the intent. For the first time in a long time, India’s ambition is not modest.</p>\n<p>The post Inside IAIRO, India’s ISRO for AI appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "8c4c43f42065",
      "title": "San Francisco AI Startup Emergent Secures $70 Mn Series B Funding to Grow Its Team",
      "content": "San Francisco-based AI startup Emergent has secured $70 million in Series B funding from Khosla Ventures and SoftBank Vision Fund 2, along with contributions from Prosus, Lightspeed, Together, and Y Combinator.\n\n\n\nSince its launch seven months ago, Emergent has raised $100 million in funding. The company has over 5 million users across more than 190 nations, Emergent said in a statement.\n\n\n\nIt will use the funding to support ongoing team growth, enhance product development, and facilitate entry into new markets as demand for AI-driven software development rises among entrepreneurs and small businesses worldwide.&nbsp;\n\n\n\n“Software creation is undergoing a structural shift,” Mukund Jha, co-founder and CEO of Emergent, said in a statement. “It used to be that only people with technical training or capital got to turn ideas into real products. Emergent flips that model. We are seeing millions of people build and ship real businesses, workflows, and products in days.”\n\n\n\nSince its establishment, Emergent has achieved $50 million in annual recurring revenue (ARR) and the company projects to exceed $100 million in ARR by April 2026.\n\n\n\n“Emergent is harnessing AI to unlock a massive wave of entrepreneurship by removing the technical and capital barriers that have historically limited who can build software,” said Sarthak Misra, partner at SoftBank Investment Advisers.&nbsp;\n\n\n\nEmergent has secured Series B funding three months after completing its Series A round. This new funding round follows support from Google and marks SoftBank’s return to investing in AI companies in India.\n\n\n\nThe startup provides a complete development team that quickly designs, builds, tests, and scales reliable software at a lower cost. Entrepreneurs can transform ideas into revenue in hours with production-quality software ready for launch and integrated with billing providers like Stripe.\nThe post San Francisco AI Startup Emergent Secures $70 Mn Series B Funding to Grow Its Team appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/san-francisco-ai-startup-emergent-secures-70-mn-series-b-funding-to-grow-its-team/",
      "author": "Smruthi Nadig",
      "published": "2026-01-20T11:31:19",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News",
        "emergent",
        "Funding",
        "khosla ventures",
        "Softbank"
      ],
      "summary": "AI startup Emergent raised $70M Series B from Khosla Ventures and SoftBank, bringing total funding to $100M. The company has 5 million users across 190 countries for AI-driven software development.",
      "importance_score": 55.0,
      "reasoning": "Solid funding round but not exceptional; shows continued investor interest in AI coding tools",
      "themes": [
        "Funding",
        "Startups",
        "AI Coding"
      ],
      "continuation": null,
      "summary_html": "<p>AI startup Emergent raised $70M Series B from Khosla Ventures and SoftBank, bringing total funding to $100M. The company has 5 million users across 190 countries for AI-driven software development.</p>",
      "content_html": "<p>San Francisco-based AI startup Emergent has secured $70 million in Series B funding from Khosla Ventures and SoftBank Vision Fund 2, along with contributions from Prosus, Lightspeed, Together, and Y Combinator.</p>\n<p>Since its launch seven months ago, Emergent has raised $100 million in funding. The company has over 5 million users across more than 190 nations, Emergent said in a statement.</p>\n<p>It will use the funding to support ongoing team growth, enhance product development, and facilitate entry into new markets as demand for AI-driven software development rises among entrepreneurs and small businesses worldwide.&nbsp;</p>\n<p>“Software creation is undergoing a structural shift,” Mukund Jha, co-founder and CEO of Emergent, said in a statement. “It used to be that only people with technical training or capital got to turn ideas into real products. Emergent flips that model. We are seeing millions of people build and ship real businesses, workflows, and products in days.”</p>\n<p>Since its establishment, Emergent has achieved $50 million in annual recurring revenue (ARR) and the company projects to exceed $100 million in ARR by April 2026.</p>\n<p>“Emergent is harnessing AI to unlock a massive wave of entrepreneurship by removing the technical and capital barriers that have historically limited who can build software,” said Sarthak Misra, partner at SoftBank Investment Advisers.&nbsp;</p>\n<p>Emergent has secured Series B funding three months after completing its Series A round. This new funding round follows support from Google and marks SoftBank’s return to investing in AI companies in India.</p>\n<p>The startup provides a complete development team that quickly designs, builds, tests, and scales reliable software at a lower cost. Entrepreneurs can transform ideas into revenue in hours with production-quality software ready for launch and integrated with billing providers like Stripe.</p>\n<p>The post San Francisco AI Startup Emergent Secures $70 Mn Series B Funding to Grow Its Team appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "3cf1b292fc16",
      "title": "From Traces to Insights: Understanding Agent Behavior at Scale",
      "content": "Visibility is the easiest piece. The hard part is analyzing and understanding what you&#x2019;re observing. I&#x2019;ve spoken to teams recording 100k+ traces every single day. What are they doing with those traces? Literally nothing. Because it&#x2019;s impossible to read and summarize 100,000 traces at any human scale.- Dev ShahTracing is critical for agent development - it powers evaluation, debugging, and annotation. But when you have agents in production generating thousands of traces daily, manual review doesn&apos;t scale.When building traditional software, we have product analytics to help with this problem. You can track agent metrics too&#x2014;latency, error rates, feedback. But these metrics only surface problems. To understand the patterns behind them, you need to analyze unstructured conversations&#x2014;not predefined event flows. Traditional product analytics wasn&apos;t built for this. This is exactly why we built LangSmith Insights Agent.Why You Can&apos;t Predict Agent BehaviorBuilding agents is different than building software. There are three key differences from traditional software:Non-determinism.&#xa0;When you run software multiple times, you get the same result. This doesn&#x2019;t happen with agents. Each call to an LLM may produce different results. When you let an agent run for a while and make hundreds of calls in a row, it is highly likely that the same input may produce very different paths.Prompt sensitivity.&#xa0;Software doesn&apos;t change dramatically with small input changes. It is robust to small changes in user input. LLMs have a characteristic called &#x201c;prompt sensitivity&#x201d;. This means small changes in the prompt space can produce large changes in output.Unbounded input space. Software structures user input through UIs. The input space is naturally bounded and scoped by what is in the UI. Agents accept natural language, which is unbounded. Users can enter anything\n\n\n\n\n\nSoftware\nAgents\n\n\n\n\nDeterministic\nYes\nNo\n\n\nRobust to small changes in user input\nYes\nNo\n\n\nBounded input space\nYes\nNo\n\n\n\n\n&#x1f4a1;&#xa0;Because agents are non-deterministic and accept unbounded input, you can&apos;t predict what they&apos;ll do or how users will use them until production.You Need Production AnalyticsSoftware is deterministic, robust to small changes in user input, and bounded in input space. As a result, you can be pretty sure software is behaving as you expect before you launch to production, and have a suite of tests to assert that. You can discover user behavior by observing production events, but this is constrained by the actions they can take in the UI (this is traditional product analytics).Agents are non-deterministic and sensitive to small changes in user input. As a result - you don&apos;t know what agents will actually do until production. Agents also usually have a natural language interface, giving them a completely unbounded input space. This means there is way more user intent you can capture and analyze.&#x1f4a1;&#xa0;When building agents, you need to iterate on production data much more than when building traditional software.You need to iterate on how agents fail.&#xa0;With software, you catch correctness issues pre-production with tests. Maybe some edge cases slip through. With agents, it&apos;s the opposite - most failures emerge in production.You need to iterate on how users actually use agents.&#xa0;With software, users can only use the product as you let them. With agents, natural language means they can use it in far more ways.Traditional product analytics are necessary but not sufficient&#xa0;for agent iterationTraditional software generates tons of events (clicks, page views, sessions). Product analytics tools (Mixpanel, Amplitude, etc.) solve the &quot;too much data&quot; problem by:Aggregating discrete events into metricsBuilding funnels and cohortsA/B testingYou can (and should) do this with agents as well. We&#x2019;ve found that most companies in production track end user feedback, latency and tool calls.These metrics help you understand what is going on, but not why. In order to understand why these metrics are moving, you need to analyze the traces themselves.Online evals help with known questions. You can run evaluators over traces to score for specific things - user frustration, topic tags, success criteria. But online evals require you to know what you&apos;re looking for upfront.What about exploratory questions? &apos;How are users actually using my agent?&apos; &apos;What failure patterns exist?&apos; You can&apos;t write an evaluator for patterns you haven&apos;t discovered yet.So what does agent analytics actually look like? You need a tool that:Analyzes unstructured conversations, not discrete eventsDiscovers patterns you didn&apos;t know to look forSurfaces clusters at scaleThis is LangSmith Insights Agent.LangSmith Insights AgentLangSmith Insights Agent uses clustering to automatically discover patterns in your traces. Instead of defining what to look for upfront, it analyzes thousands of conversations and surfaces the clusters that matter - usage patterns, error modes, or any dimension you specify. It handles the exploratory questions that would otherwise require manually reviewing hundreds of traces.Insights Agent will look for patterns and produce a report. The report will be based around a clustered analysis of the runs. There will be multiple different hierarchies of clusters. There will be top level clusters, then a second level of more detailed groupings, and then individual runs beneath that. You can click through those levels to explore the data at whatever level you think best. This lets you zoom in on specific issues or zoom out for high-level patternsYou can configure Insights Agent to look for whatever patterns you want. The two most common patterns we see people looking for are:How are user&#x2019;s using my agent?How might my agent be failing?We have prebuilt prompts for these two use cases, but if you want to configure it to look for something else you can specify an arbitrary prompt. Because every agent is different - you might care about compliance, tone, accuracy, or domain-specific failuresSometimes you may only want to run Insights Agent over a specific subset of runs. This lets you combine Insights Agent with online evals or traditional product analytics. For example, you may only want to investigate runs that had negative end user feedback (e.g a thumbs down). You can do this by specifying a filtered set of runs to run Insights Agent over. Combine quantitative signals (thumbs down) with qualitative analysis (what patterns exist in those thumbs down).You can also filter by attributes that don&apos;t exist yet. Example: &apos;Why are users frustrated?&apos; LangSmith Insights Agent can calculate &apos;user is frustrated&apos; on the fly, then filter and cluster based on it.When clusters are produced, it will then aggregate attributes associated with the traces in those groups so you can quickly spot outlier clusters. You can also specify attributes to calculate on the fly (that are then also included in the aggregated stats). Discover patterns you didn&apos;t know to track.Try LangSmith Insights AgentAgents generate thousands of unstructured traces daily. Traditional metrics tell you&#xa0;that&#xa0;something changed, but not&#xa0;what&#xa0;or&#xa0;why. Manual review doesn&apos;t scale.LangSmith Insights Agent makes pattern discovery practical - surfacing usage patterns and failure modes automatically. It&apos;s what makes iterating on production agent data possible.Try it today in LangSmith today.",
      "url": "https://www.blog.langchain.com/from-traces-to-insights-understanding-agent-behavior-at-scale/",
      "author": "Harrison Chase",
      "published": "2026-01-20T17:16:56",
      "source": "LangChain Blog",
      "source_type": "rss",
      "tags": [
        "Harrison's Hot Takes"
      ],
      "summary": "LangChain published guidance on analyzing agent behavior at scale, addressing the challenge of processing thousands of traces daily. The post discusses moving beyond basic metrics to understand patterns in agent behavior.",
      "importance_score": 52.0,
      "reasoning": "Practical tooling insight for agent development; addresses real production challenges but not frontier research",
      "themes": [
        "AI Agents",
        "Observability",
        "Developer Tools"
      ],
      "continuation": null,
      "summary_html": "<p>LangChain published guidance on analyzing agent behavior at scale, addressing the challenge of processing thousands of traces daily. The post discusses moving beyond basic metrics to understand patterns in agent behavior.</p>",
      "content_html": "<p>Visibility is the easiest piece. The hard part is analyzing and understanding what you’re observing. I’ve spoken to teams recording 100k+ traces every single day. What are they doing with those traces? Literally nothing. Because it’s impossible to read and summarize 100,000 traces at any human scale.- Dev ShahTracing is critical for agent development - it powers evaluation, debugging, and annotation. But when you have agents in production generating thousands of traces daily, manual review doesn't scale.When building traditional software, we have product analytics to help with this problem. You can track agent metrics too—latency, error rates, feedback. But these metrics only surface problems. To understand the patterns behind them, you need to analyze unstructured conversations—not predefined event flows. Traditional product analytics wasn't built for this. This is exactly why we built LangSmith Insights Agent.Why You Can't Predict Agent BehaviorBuilding agents is different than building software. There are three key differences from traditional software:Non-determinism.&nbsp;When you run software multiple times, you get the same result. This doesn’t happen with agents. Each call to an LLM may produce different results. When you let an agent run for a while and make hundreds of calls in a row, it is highly likely that the same input may produce very different paths.Prompt sensitivity.&nbsp;Software doesn't change dramatically with small input changes. It is robust to small changes in user input. LLMs have a characteristic called “prompt sensitivity”. This means small changes in the prompt space can produce large changes in output.Unbounded input space. Software structures user input through UIs. The input space is naturally bounded and scoped by what is in the UI. Agents accept natural language, which is unbounded. Users can enter anything</p>\n<p>Software</p>\n<p>Agents</p>\n<p>Deterministic</p>\n<p>Yes</p>\n<p>No</p>\n<p>Robust to small changes in user input</p>\n<p>Yes</p>\n<p>No</p>\n<p>Bounded input space</p>\n<p>Yes</p>\n<p>No</p>\n<p>💡&nbsp;Because agents are non-deterministic and accept unbounded input, you can't predict what they'll do or how users will use them until production.You Need Production AnalyticsSoftware is deterministic, robust to small changes in user input, and bounded in input space. As a result, you can be pretty sure software is behaving as you expect before you launch to production, and have a suite of tests to assert that. You can discover user behavior by observing production events, but this is constrained by the actions they can take in the UI (this is traditional product analytics).Agents are non-deterministic and sensitive to small changes in user input. As a result - you don't know what agents will actually do until production. Agents also usually have a natural language interface, giving them a completely unbounded input space. This means there is way more user intent you can capture and analyze.💡&nbsp;When building agents, you need to iterate on production data much more than when building traditional software.You need to iterate on how agents fail.&nbsp;With software, you catch correctness issues pre-production with tests. Maybe some edge cases slip through. With agents, it's the opposite - most failures emerge in production.You need to iterate on how users actually use agents.&nbsp;With software, users can only use the product as you let them. With agents, natural language means they can use it in far more ways.Traditional product analytics are necessary but not sufficient&nbsp;for agent iterationTraditional software generates tons of events (clicks, page views, sessions). Product analytics tools (Mixpanel, Amplitude, etc.) solve the \"too much data\" problem by:Aggregating discrete events into metricsBuilding funnels and cohortsA/B testingYou can (and should) do this with agents as well. We’ve found that most companies in production track end user feedback, latency and tool calls.These metrics help you understand what is going on, but not why. In order to understand why these metrics are moving, you need to analyze the traces themselves.Online evals help with known questions. You can run evaluators over traces to score for specific things - user frustration, topic tags, success criteria. But online evals require you to know what you're looking for upfront.What about exploratory questions? 'How are users actually using my agent?' 'What failure patterns exist?' You can't write an evaluator for patterns you haven't discovered yet.So what does agent analytics actually look like? You need a tool that:Analyzes unstructured conversations, not discrete eventsDiscovers patterns you didn't know to look forSurfaces clusters at scaleThis is LangSmith Insights Agent.LangSmith Insights AgentLangSmith Insights Agent uses clustering to automatically discover patterns in your traces. Instead of defining what to look for upfront, it analyzes thousands of conversations and surfaces the clusters that matter - usage patterns, error modes, or any dimension you specify. It handles the exploratory questions that would otherwise require manually reviewing hundreds of traces.Insights Agent will look for patterns and produce a report. The report will be based around a clustered analysis of the runs. There will be multiple different hierarchies of clusters. There will be top level clusters, then a second level of more detailed groupings, and then individual runs beneath that. You can click through those levels to explore the data at whatever level you think best. This lets you zoom in on specific issues or zoom out for high-level patternsYou can configure Insights Agent to look for whatever patterns you want. The two most common patterns we see people looking for are:How are user’s using my agent?How might my agent be failing?We have prebuilt prompts for these two use cases, but if you want to configure it to look for something else you can specify an arbitrary prompt. Because every agent is different - you might care about compliance, tone, accuracy, or domain-specific failuresSometimes you may only want to run Insights Agent over a specific subset of runs. This lets you combine Insights Agent with online evals or traditional product analytics. For example, you may only want to investigate runs that had negative end user feedback (e.g a thumbs down). You can do this by specifying a filtered set of runs to run Insights Agent over. Combine quantitative signals (thumbs down) with qualitative analysis (what patterns exist in those thumbs down).You can also filter by attributes that don't exist yet. Example: 'Why are users frustrated?' LangSmith Insights Agent can calculate 'user is frustrated' on the fly, then filter and cluster based on it.When clusters are produced, it will then aggregate attributes associated with the traces in those groups so you can quickly spot outlier clusters. You can also specify attributes to calculate on the fly (that are then also included in the aggregated stats). Discover patterns you didn't know to track.Try LangSmith Insights AgentAgents generate thousands of unstructured traces daily. Traditional metrics tell you&nbsp;that&nbsp;something changed, but not&nbsp;what&nbsp;or&nbsp;why. Manual review doesn't scale.LangSmith Insights Agent makes pattern discovery practical - surfacing usage patterns and failure modes automatically. It's what makes iterating on production agent data possible.Try it today in LangSmith today.</p>"
    },
    {
      "id": "8224fbf40e53",
      "title": "Big tech continues to bend the knee to Trump a year after his inauguration",
      "content": "Inside the big rewards tech titans have reaped from caving to Trump. Plus, a look at the US datacenter boom and the effects of Australia’s social media banHello, and welcome to TechScape. I’m your host, Blake Montgomery, the Guardian’s US tech editor.One year ago today, Donald Trump was inaugurated as president of the United States. Standing alongside him that day were the leaders of the tech industry’s most powerful companies, who had donated to him in an unprecedented bending of the knee. In the ensuing year, the companies have reaped enormous rewards from their alliance with Trump, which my colleague Nick Robins-Early and I wrote about last month after Trump signed an executive order prohibiting states from passing laws regulating AI. Trump has sponsored the tech industry with billions in government funding and with diplomatic visits that featured CEOs as his fellow negotiators in massive, lucrative deals. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/20/trump-tech-alliance-datacenters-social-media",
      "author": "Blake Montgomery",
      "published": "2026-01-20T15:55:55",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Technology",
        "Donald Trump",
        "Energy",
        "US news",
        "Social media bans",
        "AI (artificial intelligence)"
      ],
      "summary": "Analysis of tech industry's relationship with Trump administration one year after inauguration, covering rewards reaped from political alignment including an executive order prohibiting state AI regulations.",
      "importance_score": 50.0,
      "reasoning": "Policy context relevant to AI regulation landscape but primarily political analysis rather than AI news",
      "themes": [
        "AI Policy",
        "Regulation",
        "US Politics"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis of tech industry's relationship with Trump administration one year after inauguration, covering rewards reaped from political alignment including an executive order prohibiting state AI regulations.</p>",
      "content_html": "<p>Inside the big rewards tech titans have reaped from caving to Trump. Plus, a look at the US datacenter boom and the effects of Australia’s social media banHello, and welcome to TechScape. I’m your host, Blake Montgomery, the Guardian’s US tech editor.One year ago today, Donald Trump was inaugurated as president of the United States. Standing alongside him that day were the leaders of the tech industry’s most powerful companies, who had donated to him in an unprecedented bending of the knee. In the ensuing year, the companies have reaped enormous rewards from their alliance with Trump, which my colleague Nick Robins-Early and I wrote about last month after Trump signed an executive order prohibiting states from passing laws regulating AI. Trump has sponsored the tech industry with billions in government funding and with diplomatic visits that featured CEOs as his fellow negotiators in massive, lucrative deals. Continue reading...</p>"
    },
    {
      "id": "ceae3517b1b4",
      "title": "China’s AI Boyfriend Business Is Taking On a Life of Its Own",
      "content": "Gen Z women in China are all in on digital companionship—even setting up dates with real-world versions of their AI boyfriends.",
      "url": "https://www.wired.com/story/china-ai-boyfriends/",
      "author": "Johanna Costigan",
      "published": "2026-01-20T11:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "The Big Story",
        "Culture",
        "Culture / Digital Culture",
        "longreads",
        "Dating",
        "artificial intelligence",
        "China",
        "The China Issue",
        "chatbots",
        "Love Language"
      ],
      "summary": "Gen Z women in China are embracing AI boyfriend chatbots for digital companionship, with some arranging dates with real-world versions of their AI companions. The trend highlights growing AI companion adoption.",
      "importance_score": 45.0,
      "reasoning": "Interesting cultural phenomenon but primarily social/cultural story rather than frontier AI development",
      "themes": [
        "AI Companions",
        "China",
        "Consumer Behavior"
      ],
      "continuation": null,
      "summary_html": "<p>Gen Z women in China are embracing AI boyfriend chatbots for digital companionship, with some arranging dates with real-world versions of their AI companions. The trend highlights growing AI companion adoption.</p>",
      "content_html": "<p>Gen Z women in China are all in on digital companionship—even setting up dates with real-world versions of their AI boyfriends.</p>"
    },
    {
      "id": "6117b8610a94",
      "title": "Google’s Gemini Signs IPL Sponsorship Deal Worth ₹270 Crore",
      "content": "Google’s AI platform Gemini has entered the Indian Premier League (IPL) as a sponsor through a three-year agreement estimated at around ₹270 crore, The Economic Times reported.&nbsp;\n\n\n\nThe deal reflects the growing presence of AI companies in Indian cricket sponsorships. While the exact terms of the agreement are not public, the deal is expected to give Gemini branding visibility across IPL properties, including pitch-side hoardings and media backdrops.\n\n\n\nThe partnership follows ChatGPT’s entry into cricket advertising late last year, when it became a sponsor of the Women’s Premier League under a two-year deal valued at about ₹16 crore.&nbsp;\n\n\n\nFollowing India’s recent women’s cricket World Cup victory, interest in women’s matches has surged, making them increasingly attractive for brands looking to tap into a fast-growing and more engaged audience. Notably, Gemini also served as the global partner for the ICC Women’s Cricket World Cup in 2025.\n\n\n\nIndustry executives say AI-driven brands are increasingly competing with established companies for high-profile cricket sponsorships.&nbsp;\n\n\n\nIn 2024, design platform Canva bid ₹554 crore for the Board of Control for Cricket in India (BCCI) shirt sponsorship but lost to Apollo Tyres, which secured the rights for the 2025-2028 cycle at ₹579 crore.\n\n\n\nAccording to the ET report, more partnerships between AI platforms and cricket properties are expected, with such companies likely to spend more than ₹300 crore on sponsorships alone, excluding television and digital advertising.&nbsp;\n\n\n\nThe TATA IPL 2025 reached a combined viewership of around one billion across television and digital platforms during the season, marking one of the biggest audiences in IPL history. Some reports put the combined reach even higher at about 1.19 billion viewers over the full season.&nbsp;\n\n\n\nAI platforms could play a role similar to that of fantasy sports in sustaining interest in sports advertising. Cricket’s appeal as an advertising platform has only intensified as other high-spending categories have pulled back.&nbsp;\n\n\n\nThe government’s ban on real-money gaming and fantasy sports removed an estimated ₹7,000 crore in advertising spend from the market, particularly affecting television and digital sports advertising.&nbsp;\n\n\n\nWith India seen as a key growth market, AI platforms are increasingly turning to cricket to reach large audiences and drive user adoption.\nThe post Google’s Gemini Signs IPL Sponsorship Deal Worth ₹270 Crore appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/googles-gemini-signs-ipl-sponsorship-deal-worth-%e2%82%b9270-crore/",
      "author": "Siddharth Jindal",
      "published": "2026-01-20T12:30:53",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News",
        "Google"
      ],
      "summary": "Google's Gemini signed a three-year IPL cricket sponsorship worth approximately ₹270 crore (~$32M). This follows ChatGPT's Women's Premier League sponsorship deal.",
      "importance_score": 42.0,
      "reasoning": "Marketing expansion shows AI companies competing for consumer awareness; not technically significant",
      "themes": [
        "Marketing",
        "Google",
        "India"
      ],
      "continuation": null,
      "summary_html": "<p>Google's Gemini signed a three-year IPL cricket sponsorship worth approximately ₹270 crore (~$32M). This follows ChatGPT's Women's Premier League sponsorship deal.</p>",
      "content_html": "<p>Google’s AI platform Gemini has entered the Indian Premier League (IPL) as a sponsor through a three-year agreement estimated at around ₹270 crore, The Economic Times reported.&nbsp;</p>\n<p>The deal reflects the growing presence of AI companies in Indian cricket sponsorships. While the exact terms of the agreement are not public, the deal is expected to give Gemini branding visibility across IPL properties, including pitch-side hoardings and media backdrops.</p>\n<p>The partnership follows ChatGPT’s entry into cricket advertising late last year, when it became a sponsor of the Women’s Premier League under a two-year deal valued at about ₹16 crore.&nbsp;</p>\n<p>Following India’s recent women’s cricket World Cup victory, interest in women’s matches has surged, making them increasingly attractive for brands looking to tap into a fast-growing and more engaged audience. Notably, Gemini also served as the global partner for the ICC Women’s Cricket World Cup in 2025.</p>\n<p>Industry executives say AI-driven brands are increasingly competing with established companies for high-profile cricket sponsorships.&nbsp;</p>\n<p>In 2024, design platform Canva bid ₹554 crore for the Board of Control for Cricket in India (BCCI) shirt sponsorship but lost to Apollo Tyres, which secured the rights for the 2025-2028 cycle at ₹579 crore.</p>\n<p>According to the ET report, more partnerships between AI platforms and cricket properties are expected, with such companies likely to spend more than ₹300 crore on sponsorships alone, excluding television and digital advertising.&nbsp;</p>\n<p>The TATA IPL 2025 reached a combined viewership of around one billion across television and digital platforms during the season, marking one of the biggest audiences in IPL history. Some reports put the combined reach even higher at about 1.19 billion viewers over the full season.&nbsp;</p>\n<p>AI platforms could play a role similar to that of fantasy sports in sustaining interest in sports advertising. Cricket’s appeal as an advertising platform has only intensified as other high-spending categories have pulled back.&nbsp;</p>\n<p>The government’s ban on real-money gaming and fantasy sports removed an estimated ₹7,000 crore in advertising spend from the market, particularly affecting television and digital sports advertising.&nbsp;</p>\n<p>With India seen as a key growth market, AI platforms are increasingly turning to cricket to reach large audiences and drive user adoption.</p>\n<p>The post Google’s Gemini Signs IPL Sponsorship Deal Worth ₹270 Crore appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "169d8beff4c8",
      "title": "Tell us: has a chatbot helped you out of a difficult time in your life?",
      "content": "We would like to hear from people who have used chatbots for companionship or mental health supportAI Chatbots are now a part of everyday life. ChatGPT surpassed 800 million weekly active users in late 2025.\n Some people are forming relationships with these chatbots, using them for companionship, mental health support, and even as therapists.\n Has a chatbot helped you get through a difficult period in life? If so, we’d like to hear about it.If you’re having trouble using the form click here. Read terms of service here and privacy policy here. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/20/tell-us-has-a-chatbot-helped-you-out-of-a-difficult-time-in-your-life",
      "author": "Guardian community team",
      "published": "2026-01-20T14:45:08",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Computing",
        "Technology"
      ],
      "summary": "The Guardian soliciting reader stories about chatbot use for mental health support. Article notes ChatGPT surpassed 800 million weekly active users in late 2025.",
      "importance_score": 40.0,
      "reasoning": "Reader engagement piece with limited news value; 800M user stat is notable but not the article's focus",
      "themes": [
        "AI Companions",
        "Mental Health",
        "Consumer AI"
      ],
      "continuation": null,
      "summary_html": "<p>The Guardian soliciting reader stories about chatbot use for mental health support. Article notes ChatGPT surpassed 800 million weekly active users in late 2025.</p>",
      "content_html": "<p>We would like to hear from people who have used chatbots for companionship or mental health supportAI Chatbots are now a part of everyday life. ChatGPT surpassed 800 million weekly active users in late 2025.</p>\n<p>Some people are forming relationships with these chatbots, using them for companionship, mental health support, and even as therapists.</p>\n<p>Has a chatbot helped you get through a difficult period in life? If so, we’d like to hear about it.If you’re having trouble using the form click here. Read terms of service here and privacy policy here. Continue reading...</p>"
    },
    {
      "id": "fae441a8a12a",
      "title": "How to Design a Fully Streaming Voice Agent with End-to-End Latency Budgets, Incremental ASR, LLM Streaming, and Real-Time TTS",
      "content": "In this tutorial, we build an end-to-end streaming voice agent that mirrors how modern low-latency conversational systems operate in real time. We simulate the complete pipeline, from chunked audio input and streaming speech recognition to incremental language model reasoning and streamed text-to-speech output, while explicitly tracking latency at every stage. By working with strict latency budgets and observing metrics such as time to first token and time to first audio, we focus on the practical engineering trade-offs that shape responsive voice-based user experiences. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserimport time\nimport asyncio\nimport numpy as np\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import List, AsyncIterator\nfrom enum import Enum\nimport matplotlib.pyplot as plt\n\n\n@dataclass\nclass LatencyMetrics:\n   audio_chunk_received: float = 0.0\n   asr_started: float = 0.0\n   asr_partial: float = 0.0\n   asr_complete: float = 0.0\n   llm_started: float = 0.0\n   llm_first_token: float = 0.0\n   llm_complete: float = 0.0\n   tts_started: float = 0.0\n   tts_first_chunk: float = 0.0\n   tts_complete: float = 0.0\n\n\n   def get_time_to_first_audio(self) -> float:\n       return self.tts_first_chunk - self.asr_complete if self.tts_first_chunk and self.asr_complete else 0.0\n\n\n   def get_total_latency(self) -> float:\n       return self.tts_complete - self.audio_chunk_received if self.tts_complete else 0.0\n\n\n@dataclass\nclass LatencyBudgets:\n   asr_processing: float = 0.1\n   asr_finalization: float = 0.3\n   llm_first_token: float = 0.5\n   llm_token_generation: float = 0.02\n   tts_first_chunk: float = 0.2\n   tts_chunk_generation: float = 0.05\n   time_to_first_audio: float = 1.0\n\n\nclass AgentState(Enum):\n   LISTENING = \"listening\"\n   PROCESSING_SPEECH = \"processing_speech\"\n   THINKING = \"thinking\"\n   SPEAKING = \"speaking\"\n   INTERRUPTED = \"interrupted\"\n\n\n\nWe define the core data structures and state representations that allow us to track latency across the entire voice pipeline. We formalize timing signals for ASR, LLM, and TTS to ensure consistent measurement across all stages. We also establish a clear agent state machine that guides how the system transitions during a conversational turn. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass AudioInputStream:\n   def __init__(self, sample_rate: int = 16000, chunk_duration_ms: int = 100):\n       self.sample_rate = sample_rate\n       self.chunk_duration_ms = chunk_duration_ms\n       self.chunk_size = int(sample_rate * chunk_duration_ms / 1000)\n\n\n   async def stream_audio(self, text: str) -> AsyncIterator[np.ndarray]:\n       chars_per_second = (150 * 5) / 60\n       duration_seconds = len(text) / chars_per_second\n       num_chunks = int(duration_seconds * 1000 / self.chunk_duration_ms)\n\n\n       for _ in range(num_chunks):\n           chunk = np.random.randn(self.chunk_size).astype(np.float32) * 0.1\n           await asyncio.sleep(self.chunk_duration_ms / 1000)\n           yield chunk\n\n\n\nWe simulate real-time audio input by breaking speech into fixed-duration chunks that arrive asynchronously. We model realistic speaking rates and streaming behavior to mimic live microphone input. We use this stream as the foundation for testing downstream latency-sensitive components. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass StreamingASR:\n   def __init__(self, latency_budget: float = 0.1):\n       self.latency_budget = latency_budget\n       self.silence_threshold = 0.5\n\n\n   async def transcribe_stream(\n       self,\n       audio_stream: AsyncIterator[np.ndarray],\n       ground_truth: str\n   ) -> AsyncIterator[tuple[str, bool]]:\n       words = ground_truth.split()\n       words_transcribed = 0\n       silence_duration = 0.0\n       chunk_count = 0\n\n\n       async for chunk in audio_stream:\n           chunk_count += 1\n           await asyncio.sleep(self.latency_budget)\n\n\n           if chunk_count % 3 == 0 and words_transcribed &lt; len(words):\n               words_transcribed += 1\n               yield \" \".join(words[:words_transcribed]), False\n\n\n           audio_power = np.mean(np.abs(chunk))\n           silence_duration = silence_duration + 0.1 if audio_power &lt; 0.05 else 0.0\n\n\n           if silence_duration >= self.silence_threshold:\n               await asyncio.sleep(0.2)\n               yield ground_truth, True\n               return\n\n\n       yield ground_truth, True\n\n\n\nWe implement a streaming ASR module that produces partial transcriptions before emitting a final result. We progressively reveal words to reflect how modern ASR systems operate in real time. We also introduce silence-based finalization to approximate end-of-utterance detection. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass StreamingLLM:\n   def __init__(self, time_to_first_token: float = 0.3, tokens_per_second: float = 50):\n       self.time_to_first_token = time_to_first_token\n       self.tokens_per_second = tokens_per_second\n\n\n   async def generate_response(self, prompt: str) -> AsyncIterator[str]:\n       responses = {\n           \"hello\": \"Hello! How can I help you today?\",\n           \"weather\": \"The weather is sunny with a temperature of 72°F.\",\n           \"time\": \"The current time is 2:30 PM.\",\n           \"default\": \"I understand. Let me help you with that.\"\n       }\n\n\n       response = responses[\"default\"]\n       for key in responses:\n           if key in prompt.lower():\n               response = responses[key]\n               break\n\n\n       await asyncio.sleep(self.time_to_first_token)\n\n\n       for word in response.split():\n           yield word + \" \"\n           await asyncio.sleep(1.0 / self.tokens_per_second)\n\n\nclass StreamingTTS:\n   def __init__(self, time_to_first_chunk: float = 0.2, chars_per_second: float = 15):\n       self.time_to_first_chunk = time_to_first_chunk\n       self.chars_per_second = chars_per_second\n\n\n   async def synthesize_stream(self, text_stream: AsyncIterator[str]) -> AsyncIterator[np.ndarray]:\n       first_chunk = True\n       buffer = \"\"\n\n\n       async for text in text_stream:\n           buffer += text\n           if len(buffer) >= 20 or first_chunk:\n               if first_chunk:\n                   await asyncio.sleep(self.time_to_first_chunk)\n                   first_chunk = False\n\n\n               duration = len(buffer) / self.chars_per_second\n               yield np.random.randn(int(16000 * duration)).astype(np.float32) * 0.1\n               buffer = \"\"\n               await asyncio.sleep(duration * 0.5)\n\n\n\nIn this snippet, we model a streaming language model and a streaming text-to-speech engine working together. We generate responses token by token to capture time-to-first-token behavior. We then convert incremental text into audio chunks to simulate early and continuous speech synthesis. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass StreamingVoiceAgent:\n   def __init__(self, latency_budgets: LatencyBudgets):\n       self.budgets = latency_budgets\n       self.audio_stream = AudioInputStream()\n       self.asr = StreamingASR(latency_budgets.asr_processing)\n       self.llm = StreamingLLM(\n           latency_budgets.llm_first_token,\n           1.0 / latency_budgets.llm_token_generation\n       )\n       self.tts = StreamingTTS(\n           latency_budgets.tts_first_chunk,\n           1.0 / latency_budgets.tts_chunk_generation\n       )\n       self.state = AgentState.LISTENING\n       self.metrics_history: List[LatencyMetrics] = []\n\n\n   async def process_turn(self, user_input: str) -> LatencyMetrics:\n       metrics = LatencyMetrics()\n       start_time = time.time()\n\n\n       metrics.audio_chunk_received = time.time() - start_time\n       audio_gen = self.audio_stream.stream_audio(user_input)\n\n\n       metrics.asr_started = time.time() - start_time\n       async for text, final in self.asr.transcribe_stream(audio_gen, user_input):\n           if final:\n               metrics.asr_complete = time.time() - start_time\n               transcription = text\n\n\n       metrics.llm_started = time.time() - start_time\n       response = \"\"\n       async for token in self.llm.generate_response(transcription):\n           if not metrics.llm_first_token:\n               metrics.llm_first_token = time.time() - start_time\n           response += token\n\n\n       metrics.llm_complete = time.time() - start_time\n       metrics.tts_started = time.time() - start_time\n\n\n       async def text_stream():\n           for word in response.split():\n               yield word + \" \"\n\n\n       async for _ in self.tts.synthesize_stream(text_stream()):\n           if not metrics.tts_first_chunk:\n               metrics.tts_first_chunk = time.time() - start_time\n\n\n       metrics.tts_complete = time.time() - start_time\n       self.metrics_history.append(metrics)\n       return metrics\n\n\n\nWe orchestrate the full voice agent by wiring audio input, ASR, LLM, and TTS into a single asynchronous flow. We record precise timestamps at each transition to compute critical latency metrics. We treat each user turn as an isolated experiment to enable systematic performance analysis. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserasync def run_demo():\n   budgets = LatencyBudgets(\n       asr_processing=0.08,\n       llm_first_token=0.3,\n       llm_token_generation=0.02,\n       tts_first_chunk=0.15,\n       time_to_first_audio=0.8\n   )\n\n\n   agent = StreamingVoiceAgent(budgets)\n\n\n   inputs = [\n       \"Hello, how are you today?\",\n       \"What's the weather like?\",\n       \"Can you tell me the time?\"\n   ]\n\n\n   for text in inputs:\n       await agent.process_turn(text)\n       await asyncio.sleep(1)\n\n\nif __name__ == \"__main__\":\n   asyncio.run(run_demo())\n\n\n\nWe run the entire system across multiple conversational turns to observe latency consistency and variance. We apply aggressive latency budgets to stress the pipeline under realistic constraints. We use these runs to validate whether the system meets responsiveness targets across interactions.\n\n\n\nIn conclusion, we demonstrated how a fully streaming voice agent can be orchestrated as a single asynchronous pipeline with clear stage boundaries and measurable performance guarantees. We showed that combining partial ASR, token-level LLM streaming, and early-start TTS reduces perceived latency, even when total computation time remains non-trivial. This approach helps us reason systematically about turn-taking, responsiveness, and optimization levers, and it provides a solid foundation for extending the system toward real-world deployments using production ASR, LLM, and TTS models.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How to Design a Fully Streaming Voice Agent with End-to-End Latency Budgets, Incremental ASR, LLM Streaming, and Real-Time TTS appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/19/how-to-design-a-fully-streaming-voice-agent-with-end-to-end-latency-budgets-incremental-asr-llm-streaming-and-real-time-tts/",
      "author": "Asif Razzaq",
      "published": "2026-01-20T04:24:05",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "Artificial Intelligence",
        "Audio Language Model",
        "Editors Pick",
        "Language Model",
        "Sound",
        "Staff",
        "Technology",
        "TTS",
        "Tutorials"
      ],
      "summary": "Technical tutorial on building streaming voice agents with latency optimization, covering ASR, LLM streaming, and real-time TTS pipelines. Educational content rather than news.",
      "importance_score": 35.0,
      "reasoning": "Useful technical tutorial but educational content, not news; no new capabilities announced",
      "themes": [
        "Voice AI",
        "Tutorials",
        "Developer Education"
      ],
      "continuation": null,
      "summary_html": "<p>Technical tutorial on building streaming voice agents with latency optimization, covering ASR, LLM streaming, and real-time TTS pipelines. Educational content rather than news.</p>",
      "content_html": "<p>In this tutorial, we build an end-to-end streaming voice agent that mirrors how modern low-latency conversational systems operate in real time. We simulate the complete pipeline, from chunked audio input and streaming speech recognition to incremental language model reasoning and streamed text-to-speech output, while explicitly tracking latency at every stage. By working with strict latency budgets and observing metrics such as time to first token and time to first audio, we focus on the practical engineering trade-offs that shape responsive voice-based user experiences. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserimport time</p>\n<p>import asyncio</p>\n<p>import numpy as np</p>\n<p>from collections import deque</p>\n<p>from dataclasses import dataclass</p>\n<p>from typing import List, AsyncIterator</p>\n<p>from enum import Enum</p>\n<p>import matplotlib.pyplot as plt</p>\n<p>@dataclass</p>\n<p>class LatencyMetrics:</p>\n<p>audio_chunk_received: float = 0.0</p>\n<p>asr_started: float = 0.0</p>\n<p>asr_partial: float = 0.0</p>\n<p>asr_complete: float = 0.0</p>\n<p>llm_started: float = 0.0</p>\n<p>llm_first_token: float = 0.0</p>\n<p>llm_complete: float = 0.0</p>\n<p>tts_started: float = 0.0</p>\n<p>tts_first_chunk: float = 0.0</p>\n<p>tts_complete: float = 0.0</p>\n<p>def get_time_to_first_audio(self) -&gt; float:</p>\n<p>return self.tts_first_chunk - self.asr_complete if self.tts_first_chunk and self.asr_complete else 0.0</p>\n<p>def get_total_latency(self) -&gt; float:</p>\n<p>return self.tts_complete - self.audio_chunk_received if self.tts_complete else 0.0</p>\n<p>@dataclass</p>\n<p>class LatencyBudgets:</p>\n<p>asr_processing: float = 0.1</p>\n<p>asr_finalization: float = 0.3</p>\n<p>llm_first_token: float = 0.5</p>\n<p>llm_token_generation: float = 0.02</p>\n<p>tts_first_chunk: float = 0.2</p>\n<p>tts_chunk_generation: float = 0.05</p>\n<p>time_to_first_audio: float = 1.0</p>\n<p>class AgentState(Enum):</p>\n<p>LISTENING = \"listening\"</p>\n<p>PROCESSING_SPEECH = \"processing_speech\"</p>\n<p>THINKING = \"thinking\"</p>\n<p>SPEAKING = \"speaking\"</p>\n<p>INTERRUPTED = \"interrupted\"</p>\n<p>We define the core data structures and state representations that allow us to track latency across the entire voice pipeline. We formalize timing signals for ASR, LLM, and TTS to ensure consistent measurement across all stages. We also establish a clear agent state machine that guides how the system transitions during a conversational turn. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass AudioInputStream:</p>\n<p>def __init__(self, sample_rate: int = 16000, chunk_duration_ms: int = 100):</p>\n<p>self.sample_rate = sample_rate</p>\n<p>self.chunk_duration_ms = chunk_duration_ms</p>\n<p>self.chunk_size = int(sample_rate * chunk_duration_ms / 1000)</p>\n<p>async def stream_audio(self, text: str) -&gt; AsyncIterator[np.ndarray]:</p>\n<p>chars_per_second = (150 * 5) / 60</p>\n<p>duration_seconds = len(text) / chars_per_second</p>\n<p>num_chunks = int(duration_seconds * 1000 / self.chunk_duration_ms)</p>\n<p>for _ in range(num_chunks):</p>\n<p>chunk = np.random.randn(self.chunk_size).astype(np.float32) * 0.1</p>\n<p>await asyncio.sleep(self.chunk_duration_ms / 1000)</p>\n<p>yield chunk</p>\n<p>We simulate real-time audio input by breaking speech into fixed-duration chunks that arrive asynchronously. We model realistic speaking rates and streaming behavior to mimic live microphone input. We use this stream as the foundation for testing downstream latency-sensitive components. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass StreamingASR:</p>\n<p>def __init__(self, latency_budget: float = 0.1):</p>\n<p>self.latency_budget = latency_budget</p>\n<p>self.silence_threshold = 0.5</p>\n<p>async def transcribe_stream(</p>\n<p>self,</p>\n<p>audio_stream: AsyncIterator[np.ndarray],</p>\n<p>ground_truth: str</p>\n<p>) -&gt; AsyncIterator[tuple[str, bool]]:</p>\n<p>words = ground_truth.split()</p>\n<p>words_transcribed = 0</p>\n<p>silence_duration = 0.0</p>\n<p>chunk_count = 0</p>\n<p>async for chunk in audio_stream:</p>\n<p>chunk_count += 1</p>\n<p>await asyncio.sleep(self.latency_budget)</p>\n<p>if chunk_count % 3 == 0 and words_transcribed &lt; len(words):</p>\n<p>words_transcribed += 1</p>\n<p>yield \" \".join(words[:words_transcribed]), False</p>\n<p>audio_power = np.mean(np.abs(chunk))</p>\n<p>silence_duration = silence_duration + 0.1 if audio_power &lt; 0.05 else 0.0</p>\n<p>if silence_duration &gt;= self.silence_threshold:</p>\n<p>await asyncio.sleep(0.2)</p>\n<p>yield ground_truth, True</p>\n<p>return</p>\n<p>yield ground_truth, True</p>\n<p>We implement a streaming ASR module that produces partial transcriptions before emitting a final result. We progressively reveal words to reflect how modern ASR systems operate in real time. We also introduce silence-based finalization to approximate end-of-utterance detection. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass StreamingLLM:</p>\n<p>def __init__(self, time_to_first_token: float = 0.3, tokens_per_second: float = 50):</p>\n<p>self.time_to_first_token = time_to_first_token</p>\n<p>self.tokens_per_second = tokens_per_second</p>\n<p>async def generate_response(self, prompt: str) -&gt; AsyncIterator[str]:</p>\n<p>responses = {</p>\n<p>\"hello\": \"Hello! How can I help you today?\",</p>\n<p>\"weather\": \"The weather is sunny with a temperature of 72°F.\",</p>\n<p>\"time\": \"The current time is 2:30 PM.\",</p>\n<p>\"default\": \"I understand. Let me help you with that.\"</p>\n<p>}</p>\n<p>response = responses[\"default\"]</p>\n<p>for key in responses:</p>\n<p>if key in prompt.lower():</p>\n<p>response = responses[key]</p>\n<p>break</p>\n<p>await asyncio.sleep(self.time_to_first_token)</p>\n<p>for word in response.split():</p>\n<p>yield word + \" \"</p>\n<p>await asyncio.sleep(1.0 / self.tokens_per_second)</p>\n<p>class StreamingTTS:</p>\n<p>def __init__(self, time_to_first_chunk: float = 0.2, chars_per_second: float = 15):</p>\n<p>self.time_to_first_chunk = time_to_first_chunk</p>\n<p>self.chars_per_second = chars_per_second</p>\n<p>async def synthesize_stream(self, text_stream: AsyncIterator[str]) -&gt; AsyncIterator[np.ndarray]:</p>\n<p>first_chunk = True</p>\n<p>buffer = \"\"</p>\n<p>async for text in text_stream:</p>\n<p>buffer += text</p>\n<p>if len(buffer) &gt;= 20 or first_chunk:</p>\n<p>if first_chunk:</p>\n<p>await asyncio.sleep(self.time_to_first_chunk)</p>\n<p>first_chunk = False</p>\n<p>duration = len(buffer) / self.chars_per_second</p>\n<p>yield np.random.randn(int(16000 * duration)).astype(np.float32) * 0.1</p>\n<p>buffer = \"\"</p>\n<p>await asyncio.sleep(duration * 0.5)</p>\n<p>In this snippet, we model a streaming language model and a streaming text-to-speech engine working together. We generate responses token by token to capture time-to-first-token behavior. We then convert incremental text into audio chunks to simulate early and continuous speech synthesis. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass StreamingVoiceAgent:</p>\n<p>def __init__(self, latency_budgets: LatencyBudgets):</p>\n<p>self.budgets = latency_budgets</p>\n<p>self.audio_stream = AudioInputStream()</p>\n<p>self.asr = StreamingASR(latency_budgets.asr_processing)</p>\n<p>self.llm = StreamingLLM(</p>\n<p>latency_budgets.llm_first_token,</p>\n<p>1.0 / latency_budgets.llm_token_generation</p>\n<p>)</p>\n<p>self.tts = StreamingTTS(</p>\n<p>latency_budgets.tts_first_chunk,</p>\n<p>1.0 / latency_budgets.tts_chunk_generation</p>\n<p>)</p>\n<p>self.state = AgentState.LISTENING</p>\n<p>self.metrics_history: List[LatencyMetrics] = []</p>\n<p>async def process_turn(self, user_input: str) -&gt; LatencyMetrics:</p>\n<p>metrics = LatencyMetrics()</p>\n<p>start_time = time.time()</p>\n<p>metrics.audio_chunk_received = time.time() - start_time</p>\n<p>audio_gen = self.audio_stream.stream_audio(user_input)</p>\n<p>metrics.asr_started = time.time() - start_time</p>\n<p>async for text, final in self.asr.transcribe_stream(audio_gen, user_input):</p>\n<p>if final:</p>\n<p>metrics.asr_complete = time.time() - start_time</p>\n<p>transcription = text</p>\n<p>metrics.llm_started = time.time() - start_time</p>\n<p>response = \"\"</p>\n<p>async for token in self.llm.generate_response(transcription):</p>\n<p>if not metrics.llm_first_token:</p>\n<p>metrics.llm_first_token = time.time() - start_time</p>\n<p>response += token</p>\n<p>metrics.llm_complete = time.time() - start_time</p>\n<p>metrics.tts_started = time.time() - start_time</p>\n<p>async def text_stream():</p>\n<p>for word in response.split():</p>\n<p>yield word + \" \"</p>\n<p>async for _ in self.tts.synthesize_stream(text_stream()):</p>\n<p>if not metrics.tts_first_chunk:</p>\n<p>metrics.tts_first_chunk = time.time() - start_time</p>\n<p>metrics.tts_complete = time.time() - start_time</p>\n<p>self.metrics_history.append(metrics)</p>\n<p>return metrics</p>\n<p>We orchestrate the full voice agent by wiring audio input, ASR, LLM, and TTS into a single asynchronous flow. We record precise timestamps at each transition to compute critical latency metrics. We treat each user turn as an isolated experiment to enable systematic performance analysis. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserasync def run_demo():</p>\n<p>budgets = LatencyBudgets(</p>\n<p>asr_processing=0.08,</p>\n<p>llm_first_token=0.3,</p>\n<p>llm_token_generation=0.02,</p>\n<p>tts_first_chunk=0.15,</p>\n<p>time_to_first_audio=0.8</p>\n<p>)</p>\n<p>agent = StreamingVoiceAgent(budgets)</p>\n<p>inputs = [</p>\n<p>\"Hello, how are you today?\",</p>\n<p>\"What's the weather like?\",</p>\n<p>\"Can you tell me the time?\"</p>\n<p>]</p>\n<p>for text in inputs:</p>\n<p>await agent.process_turn(text)</p>\n<p>await asyncio.sleep(1)</p>\n<p>if __name__ == \"__main__\":</p>\n<p>asyncio.run(run_demo())</p>\n<p>We run the entire system across multiple conversational turns to observe latency consistency and variance. We apply aggressive latency budgets to stress the pipeline under realistic constraints. We use these runs to validate whether the system meets responsiveness targets across interactions.</p>\n<p>In conclusion, we demonstrated how a fully streaming voice agent can be orchestrated as a single asynchronous pipeline with clear stage boundaries and measurable performance guarantees. We showed that combining partial ASR, token-level LLM streaming, and early-start TTS reduces perceived latency, even when total computation time remains non-trivial. This approach helps us reason systematically about turn-taking, responsiveness, and optimization levers, and it provides a solid foundation for extending the system toward real-world deployments using production ASR, LLM, and TTS models.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How to Design a Fully Streaming Voice Agent with End-to-End Latency Budgets, Incremental ASR, LLM Streaming, and Real-Time TTS appeared first on MarkTechPost.</p>"
    }
  ]
}