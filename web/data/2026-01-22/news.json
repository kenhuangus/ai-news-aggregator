{
  "category": "news",
  "date": "2026-01-22",
  "category_summary": "**Massive funding rounds** dominated the week: **Anthropic** [raised **$10B**](/?date=2026-01-22&category=news#item-a87e5f90c246) at a **$350B valuation**, while **xAI** secured **$20B**. **OpenAI** [announced monetization targets](/?date=2026-01-22&category=news#item-07c41520041c) of **$1.4T** in commitments by 2034. **NVIDIA** [invested **$150M**](/?date=2026-01-22&category=news#item-1d1deb93baf1) in inference startup **Baseten** (now valued at **$5B**).\n\n**Strategic partnerships and infrastructure investments** are reshaping the landscape:\n- **Apple** [partnered with **Google Gemini**](/?date=2026-01-22&category=news#item-329c838a3e3d) to power next-generation **Siri**\n- **India** [expects up to **$150B**](/?date=2026-01-22&category=news#item-47d219e6589a) in AI infrastructure investment by end of 2026 from **Google**, **Microsoft**, and **Amazon**\n- **JP Morgan** CEO **Jamie Dimon** [warned at Davos](/?date=2026-01-22&category=news#item-96782a00dbfe) that AI rollout may need slowing to prevent civil unrest\n\n**Model developments** included **Liquid AI's** release of **LFM2.5-1.2B-Thinking**, a reasoning model fitting under 1GB for on-device deployment. VC strategy is shifting as **Sequoia** and **a16z** [now back competing AI labs](/?date=2026-01-22&category=news#item-49572afc0907) simultaneously.",
  "category_summary_html": "<p><strong>Massive funding rounds</strong> dominated the week: <strong>Anthropic</strong> <a href=\"/?date=2026-01-22&category=news#item-a87e5f90c246\" class=\"internal-link\" rel=\"noopener noreferrer\">raised <strong>$10B</strong></a> at a <strong>$350B valuation</strong>, while <strong>xAI</strong> secured <strong>$20B</strong>. <strong>OpenAI</strong> <a href=\"/?date=2026-01-22&category=news#item-07c41520041c\" class=\"internal-link\" rel=\"noopener noreferrer\">announced monetization targets</a> of <strong>$1.4T</strong> in commitments by 2034. <strong>NVIDIA</strong> <a href=\"/?date=2026-01-22&category=news#item-1d1deb93baf1\" class=\"internal-link\" rel=\"noopener noreferrer\">invested <strong>$150M</strong></a> in inference startup <strong>Baseten</strong> (now valued at <strong>$5B</strong>).</p>\n<p><strong>Strategic partnerships and infrastructure investments</strong> are reshaping the landscape:</p>\n<ul>\n<li><strong>Apple</strong> <a href=\"/?date=2026-01-22&category=news#item-329c838a3e3d\" class=\"internal-link\" rel=\"noopener noreferrer\">partnered with <strong>Google Gemini</strong></a> to power next-generation <strong>Siri</strong></li>\n<li><strong>India</strong> <a href=\"/?date=2026-01-22&category=news#item-47d219e6589a\" class=\"internal-link\" rel=\"noopener noreferrer\">expects up to <strong>$150B</strong></a> in AI infrastructure investment by end of 2026 from <strong>Google</strong>, <strong>Microsoft</strong>, and <strong>Amazon</strong></li>\n<li><strong>JP Morgan</strong> CEO <strong>Jamie Dimon</strong> <a href=\"/?date=2026-01-22&category=news#item-96782a00dbfe\" class=\"internal-link\" rel=\"noopener noreferrer\">warned at Davos</a> that AI rollout may need slowing to prevent civil unrest</li>\n</ul>\n<p><strong>Model developments</strong> included <strong>Liquid AI's</strong> release of <strong>LFM2.5-1.2B-Thinking</strong>, a reasoning model fitting under 1GB for on-device deployment. VC strategy is shifting as <strong>Sequoia</strong> and <strong>a16z</strong> <a href=\"/?date=2026-01-22&category=news#item-49572afc0907\" class=\"internal-link\" rel=\"noopener noreferrer\">now back competing AI labs</a> simultaneously.</p>",
  "themes": [
    {
      "name": "AI Funding & Valuations",
      "description": "Record-breaking funding rounds for major AI labs including Anthropic ($10B), xAI ($20B), and infrastructure investments",
      "item_count": 5,
      "example_items": [],
      "importance": 88.0
    },
    {
      "name": "Big Tech Partnerships & Strategy",
      "description": "Major strategic moves including Apple-Google Gemini partnership and VC investment pattern changes",
      "item_count": 4,
      "example_items": [],
      "importance": 75.0
    },
    {
      "name": "AI Infrastructure & Hardware",
      "description": "Investments in AI infrastructure, memory shortages, and inference optimization",
      "item_count": 4,
      "example_items": [],
      "importance": 68.0
    },
    {
      "name": "AI Policy & Safety",
      "description": "Warnings from industry leaders, US-China collaboration, and political influence on regulation",
      "item_count": 4,
      "example_items": [],
      "importance": 65.0
    },
    {
      "name": "Enterprise AI Deployment",
      "description": "Large-scale AI adoption at major enterprises like Citi and data sovereignty concerns",
      "item_count": 3,
      "example_items": [],
      "importance": 60.0
    },
    {
      "name": "Model Releases & Research",
      "description": "New model releases including Liquid AI's edge reasoning model and research advances",
      "item_count": 5,
      "example_items": [],
      "importance": 58.0
    }
  ],
  "total_items": 24,
  "items": [
    {
      "id": "a87e5f90c246",
      "title": "LWiAI Podcast #231 - Claude Cowork, Anthropic $10B, Deep Delta Learning",
      "content": "Our 231st episode with a summary and discussion of last week&#8217;s big AI news!Recorded on 01/16/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:Anthropic&#8217;s new cowork tool integrates Claude code, potentially simplifying multiple computing tasks from editing videos to compiling spreadsheets.Significant funding rounds see Anthropic raising $10B at a valuation of $350B, while XAI raises $20B, underscoring the immense market interest in AI startups.Nvidia faces supply challenges for H200 AI chips due to overwhelming demand from China, despite high costs per unit and its potential impact on U.S. company revenue.Policy debates highlight tensions around U.S. export controls to China, with leaders like Justin Lin from Alibaba and Jake Sullivan, former national security advisor, weighing in on the ramifications for the AI industry&#8217;s future.Timestamps:(00:00:10) Intro / Banter(00:01:30) News PreviewTools &amp; Apps(00:02:13) Anthropic&#8217;s new Cowork tool offers Claude Code without the code | TechCrunch(00:09:45) Google&#8217;s Gemini AI will use what it knows about you from Gmail, Search, and YouTube | The Verge(00:12:45) Google removes some AI health summaries after investigation finds &#8220;dangerous&#8221; flaws - Ars Technica(00:16:29) Gmail is getting a Gemini AI overhaul(00:18:12) Slackbot is an AI agent now | TechCrunchApplications &amp; Business(00:20:11) Anthropic Raising $10 Billion at $350 Billion Value(00:22:25) Elon Musk xAI raises $20 billion from Nvidia, Cisco, investors(00:24:47) NVIDIA Needs a Supply Chain &#8216;Miracle&#8217; From TSMC as China&#8217;s H200 AI Chip Orders Overwhelm Supply, Triggering a Bottleneck(00:29:26) OpenAI signs deal, worth $10B, for compute from Cerebras | TechCrunch(00:31:49) CoreWeave in focus as it amends credit agreement(00:34:30) LMArena lands $1.7B valuation four months after launching its product | TechCrunchProjects &amp; Open Source(00:35:54) Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models(00:43:15) mHC: Manifold-Constrained Hyper-Connections(00:49:53) IQuest_Coder_Technical_Report(00:54:58) TII Abu-Dhabi Released Falcon H1R-7B: A New Reasoning Model Outperforming Others in Math and Coding with only 7B Params with 256k Context Window - MarkTechPostResearch &amp; Advancements(01:01:42) Deep Delta Learning(01:07:47) Recursive Language Models(01:13:39) Conditional memory via scalable lookup(01:18:54) Extending the Context of Pretrained LLMs by Dropping their Positional EmbeddingsPolicy &amp; Safety(01:26:06) Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks(01:31:00) Nvidia CEO says purchase orders, not formal declaration, will signal Chinese approval of H200(01:32:24) China AI Leaders Warn of Widening Gap With US After $1B IPO Week(01:37:25) Jake Sullivan is furious that Trump removed Biden&#8217;s AI chip export controls | The Verge",
      "url": "https://lastweekin.ai/p/lwiai-podcast-231-claude-cowork-anthropic",
      "author": "Last Week in AI",
      "published": "2026-01-21T03:22:53",
      "source": "Last Week in AI",
      "source_type": "rss",
      "tags": [],
      "summary": "Continuing our coverage of Anthropic's funding from [yesterday](/?date=2026-01-20&category=news#item-0c17f09e39ed), Podcast covers major AI news including Anthropic raising $10B at $350B valuation, xAI raising $20B, and Anthropic's new Claude Cowork tool. Also discusses NVIDIA H200 supply challenges from China demand.",
      "importance_score": 88.0,
      "reasoning": "Contains multiple major funding announcements: Anthropic $10B at $350B valuation and xAI $20B raise are industry-defining events. Also covers significant product launch.",
      "themes": [
        "Funding",
        "Anthropic",
        "xAI",
        "Claude",
        "NVIDIA"
      ],
      "continuation": {
        "original_item_id": "0c17f09e39ed",
        "original_date": "2026-01-20",
        "original_category": "news",
        "original_title": "Sequoia Breaks Ranks to Back Anthropic in $25 Bn Mega Round: Report",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage of Anthropic's funding from yesterday"
      },
      "summary_html": "<p>Continuing our coverage of Anthropic's funding from <a href=\"/?date=2026-01-20&amp;category=news#item-0c17f09e39ed\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Podcast covers major AI news including Anthropic raising $10B at $350B valuation, xAI raising $20B, and Anthropic's new Claude Cowork tool. Also discusses NVIDIA H200 supply challenges from China demand.</p>",
      "content_html": "<p>Our 231st episode with a summary and discussion of last week’s big AI news!Recorded on 01/16/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:Anthropic’s new cowork tool integrates Claude code, potentially simplifying multiple computing tasks from editing videos to compiling spreadsheets.Significant funding rounds see Anthropic raising $10B at a valuation of $350B, while XAI raises $20B, underscoring the immense market interest in AI startups.Nvidia faces supply challenges for H200 AI chips due to overwhelming demand from China, despite high costs per unit and its potential impact on U.S. company revenue.Policy debates highlight tensions around U.S. export controls to China, with leaders like Justin Lin from Alibaba and Jake Sullivan, former national security advisor, weighing in on the ramifications for the AI industry’s future.Timestamps:(00:00:10) Intro / Banter(00:01:30) News PreviewTools &amp; Apps(00:02:13) Anthropic’s new Cowork tool offers Claude Code without the code | TechCrunch(00:09:45) Google’s Gemini AI will use what it knows about you from Gmail, Search, and YouTube | The Verge(00:12:45) Google removes some AI health summaries after investigation finds “dangerous” flaws - Ars Technica(00:16:29) Gmail is getting a Gemini AI overhaul(00:18:12) Slackbot is an AI agent now | TechCrunchApplications &amp; Business(00:20:11) Anthropic Raising $10 Billion at $350 Billion Value(00:22:25) Elon Musk xAI raises $20 billion from Nvidia, Cisco, investors(00:24:47) NVIDIA Needs a Supply Chain ‘Miracle’ From TSMC as China’s H200 AI Chip Orders Overwhelm Supply, Triggering a Bottleneck(00:29:26) OpenAI signs deal, worth $10B, for compute from Cerebras | TechCrunch(00:31:49) CoreWeave in focus as it amends credit agreement(00:34:30) LMArena lands $1.7B valuation four months after launching its product | TechCrunchProjects &amp; Open Source(00:35:54) Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models(00:43:15) mHC: Manifold-Constrained Hyper-Connections(00:49:53) IQuest_Coder_Technical_Report(00:54:58) TII Abu-Dhabi Released Falcon H1R-7B: A New Reasoning Model Outperforming Others in Math and Coding with only 7B Params with 256k Context Window - MarkTechPostResearch &amp; Advancements(01:01:42) Deep Delta Learning(01:07:47) Recursive Language Models(01:13:39) Conditional memory via scalable lookup(01:18:54) Extending the Context of Pretrained LLMs by Dropping their Positional EmbeddingsPolicy &amp; Safety(01:26:06) Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks(01:31:00) Nvidia CEO says purchase orders, not formal declaration, will signal Chinese approval of H200(01:32:24) China AI Leaders Warn of Widening Gap With US After $1B IPO Week(01:37:25) Jake Sullivan is furious that Trump removed Biden’s AI chip export controls | The Verge</p>"
    },
    {
      "id": "329c838a3e3d",
      "title": "Has Gemini surpassed ChatGPT? We put the AI models to the test.",
      "content": "The last time we did comparative tests of AI models from OpenAI and Google at Ars was in late 2023, when Google's offering was still called Bard. In the roughly two years since, a lot has happened in the world of artificial intelligence. And now that Apple has made the consequential decision to partner with Google Gemini to power the next generation of its Siri voice assistant, we thought it was high time to do some new tests to see where the models from these AI giants stand today.\nFor this test, we're comparing the default models that both OpenAI and Google present to users who don't pay for a regular subscription—ChatGPT 5.2 for OpenAI and Gemini 3.2 Fast for Google. While other models might be more powerful, we felt this test best recreates the AI experience as it would work for the vast majority of Siri users, who don't pay to subscribe to either company's services.\nAs in the past, we'll feed the same prompts to both models and evaluate the results using a combination of objective evaluation and subjective feel. Rather than re-using the relatively simple prompts we ran back in 2023, though, we'll be running these models on an updated set of more complex prompts that we first used when pitting GPT-5 against GPT-4o last summer.Read full article\nComments",
      "url": "https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/",
      "author": "Kyle Orland",
      "published": "2026-01-21T15:03:39",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Features",
        "Artificial Intelligence",
        "ChatGPT",
        "gemini",
        "google",
        "openai"
      ],
      "summary": "Ars Technica conducts comparative tests between ChatGPT 5.2 and Gemini 3.2 Fast. Article reveals Apple has partnered with Google Gemini to power the next generation of Siri voice assistant.",
      "importance_score": 82.0,
      "reasoning": "Apple-Google Gemini partnership for Siri is a major industry development. Comparison of latest free-tier models from OpenAI and Google provides valuable benchmark context.",
      "themes": [
        "Model Comparison",
        "Big Tech Partnerships",
        "Voice Assistants"
      ],
      "continuation": null,
      "summary_html": "<p>Ars Technica conducts comparative tests between ChatGPT 5.2 and Gemini 3.2 Fast. Article reveals Apple has partnered with Google Gemini to power the next generation of Siri voice assistant.</p>",
      "content_html": "<p>The last time we did comparative tests of AI models from OpenAI and Google at Ars was in late 2023, when Google's offering was still called Bard. In the roughly two years since, a lot has happened in the world of artificial intelligence. And now that Apple has made the consequential decision to partner with Google Gemini to power the next generation of its Siri voice assistant, we thought it was high time to do some new tests to see where the models from these AI giants stand today.</p>\n<p>For this test, we're comparing the default models that both OpenAI and Google present to users who don't pay for a regular subscription—ChatGPT 5.2 for OpenAI and Gemini 3.2 Fast for Google. While other models might be more powerful, we felt this test best recreates the AI experience as it would work for the vast majority of Siri users, who don't pay to subscribe to either company's services.</p>\n<p>As in the past, we'll feed the same prompts to both models and evaluate the results using a combination of objective evaluation and subjective feel. Rather than re-using the relatively simple prompts we ran back in 2023, though, we'll be running these models on an updated set of more complex prompts that we first used when pitting GPT-5 against GPT-4o last summer.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "07c41520041c",
      "title": "OpenAI Targets Monetization, $1.4T Commitments by 2034",
      "content": "With the company investing massive amounts in infrastructure, prompting scrutiny of its finances, increased use of its products is a clear focus.",
      "url": "https://aibusiness.com/generative-ai/openai-targets-monetization-commitments-2034",
      "author": "Graham Hope",
      "published": "2026-01-21T16:43:08",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "OpenAI is targeting monetization with projected commitments of $1.4 trillion by 2034, amid scrutiny of its massive infrastructure investments.",
      "importance_score": 75.0,
      "reasoning": "Major financial news about the leading AI company. $1.4T commitment target signals the scale of expected AI industry growth.",
      "themes": [
        "OpenAI",
        "AI Business",
        "Funding"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI is targeting monetization with projected commitments of $1.4 trillion by 2034, amid scrutiny of its massive infrastructure investments.</p>",
      "content_html": "<p>With the company investing massive amounts in infrastructure, prompting scrutiny of its finances, increased use of its products is a clear focus.</p>"
    },
    {
      "id": "1d1deb93baf1",
      "title": "NVIDIA Invests $150 Million in AI Inference Startup Baseten",
      "content": "NVIDIA has invested $150 million in AI inference startup Baseten, which has raised $300 million in a funding round valuing the company at $5 billion—more than double its previous valuation, The Wall Street Journal reported.\n\n\n\nThe round was led by venture capital firm Institutional Venture Partners and CapitalG, Alphabet’s independent growth fund, with participation from NVIDIA. The deal highlights NVIDIA’s aggressive push into inference-focused startups, as the AI industry shifts its attention from training large models to running them efficiently at scale. It also marks another instance of NVIDIA backing a direct customer of its AI chips.\n\n\n\nFounded in 2019, San Francisco–based Baseten helps companies such as AI code editor Cursor and note-taking platform Notion deploy and operate large language models in production environments. Including the latest raise, the company has now secured $585 million in total funding. Co-founder and chief executive Tuhin Srivastava has described Baseten’s ambition as building the “AWS for inference”.\n\n\n\nFor NVIDIA, the investment reinforces a strategic pivot championed by chief executive Jensen Huang, who has repeatedly argued that inference will ultimately become a much larger market than model training. As enterprises move from experimentation to full-scale deployment, demand for reliable and cost-efficient inference infrastructure is accelerating, placing companies like Baseten at the centre of this transition.\n\n\n\nBaseten’s platform is optimised for NVIDIA’s latest GPU architectures, including the H100 and next-generation B200 chips. By enabling high-performance inference workloads on these GPUs, Baseten effectively extends NVIDIA’s ecosystem, helping ensure its hardware remains the default choice as AI adoption spreads across enterprises.\n\n\n\nCapitalG’s participation adds a competitive dimension, given Alphabet’s own investments in AI infrastructure and model deployment. Nevertheless, the collaboration underlines the strategic importance of inference, even among industry rivals.\n\n\n\nAt a $5 billion valuation, Baseten now joins a small group of AI infrastructure startups commanding premium multiples. Investors argue that inference platforms are well-positioned to capture long-term value as AI moves beyond Big Tech into sectors such as productivity software, finance and creative tools.\n\n\n\nBaseten has also gained traction among developers through Truss, its open-source framework that simplifies model deployment. Truss allows teams to package models, manage dependencies and scale inference workloads with minimal friction, an increasingly critical capability as AI features are embedded directly into consumer and enterprise products.\nThe post NVIDIA Invests $150 Million in AI Inference Startup Baseten appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/nvidia-invests-150-million-in-ai-inference-startup-baseten/",
      "author": "Pallavi Chakravorty",
      "published": "2026-01-21T12:16:35",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "NVIDIA invested $150 million in AI inference startup Baseten as part of a $300 million funding round valuing the company at $5 billion. Baseten serves customers including Cursor and Notion for deploying large AI models.",
      "importance_score": 72.0,
      "reasoning": "Significant funding round with major strategic investor. Highlights industry shift from training to inference optimization.",
      "themes": [
        "Funding",
        "NVIDIA",
        "AI Infrastructure",
        "Inference"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA invested $150 million in AI inference startup Baseten as part of a $300 million funding round valuing the company at $5 billion. Baseten serves customers including Cursor and Notion for deploying large AI models.</p>",
      "content_html": "<p>NVIDIA has invested $150 million in AI inference startup Baseten, which has raised $300 million in a funding round valuing the company at $5 billion—more than double its previous valuation, The Wall Street Journal reported.</p>\n<p>The round was led by venture capital firm Institutional Venture Partners and CapitalG, Alphabet’s independent growth fund, with participation from NVIDIA. The deal highlights NVIDIA’s aggressive push into inference-focused startups, as the AI industry shifts its attention from training large models to running them efficiently at scale. It also marks another instance of NVIDIA backing a direct customer of its AI chips.</p>\n<p>Founded in 2019, San Francisco–based Baseten helps companies such as AI code editor Cursor and note-taking platform Notion deploy and operate large language models in production environments. Including the latest raise, the company has now secured $585 million in total funding. Co-founder and chief executive Tuhin Srivastava has described Baseten’s ambition as building the “AWS for inference”.</p>\n<p>For NVIDIA, the investment reinforces a strategic pivot championed by chief executive Jensen Huang, who has repeatedly argued that inference will ultimately become a much larger market than model training. As enterprises move from experimentation to full-scale deployment, demand for reliable and cost-efficient inference infrastructure is accelerating, placing companies like Baseten at the centre of this transition.</p>\n<p>Baseten’s platform is optimised for NVIDIA’s latest GPU architectures, including the H100 and next-generation B200 chips. By enabling high-performance inference workloads on these GPUs, Baseten effectively extends NVIDIA’s ecosystem, helping ensure its hardware remains the default choice as AI adoption spreads across enterprises.</p>\n<p>CapitalG’s participation adds a competitive dimension, given Alphabet’s own investments in AI infrastructure and model deployment. Nevertheless, the collaboration underlines the strategic importance of inference, even among industry rivals.</p>\n<p>At a $5 billion valuation, Baseten now joins a small group of AI infrastructure startups commanding premium multiples. Investors argue that inference platforms are well-positioned to capture long-term value as AI moves beyond Big Tech into sectors such as productivity software, finance and creative tools.</p>\n<p>Baseten has also gained traction among developers through Truss, its open-source framework that simplifies model deployment. Truss allows teams to package models, manage dependencies and scale inference workloads with minimal friction, an increasingly critical capability as AI features are embedded directly into consumer and enterprise products.</p>\n<p>The post NVIDIA Invests $150 Million in AI Inference Startup Baseten appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "47d219e6589a",
      "title": "India to See Up To $150 Bn AI Infrastructure Investments in 2026: Ashwini Vaishnaw",
      "content": "India may see an investment of up to $150 billion in AI infrastructure by the end of 2026, Ashwini Vaishnaw, union minister for information technology, railways and information and broadcasting, said in an exclusive interview with CNBC TV18 at the World Economic Forum in Davos, Switzerland.\n\n\n\nVaishnaw said India has already secured investment commitments worth $70 billion, with a further $50–80 billion likely over the next 12 months.\n\n\n\nThe past year has seen a wave of major investment announcements. Google committed $15 billion to set up an AI hub in Visakhapatnam, Microsoft raised its India investment commitment to over $20 billion through 2030, and Amazon pledged $35 billion in investments over the same period.\n\n\n\nVaishnaw also confirmed that India’s homegrown large language models (LLMs) will be unveiled at the upcoming India AI Summit. IndiaAI Mission CEO Abhishek Singh had earlier told AIM that at least two LLMs—developed by Sarvam AI and BharatGen—would be launched ahead of the summit.\n\n\n\nThe minister said India will eventually develop 12 foundational AI models, each ranging between 50–120 billion parameters, designed to run on relatively small GPU clusters. This, he said, would enable low-cost AI services at scale. Vaishnaw noted that early live testing of Indian models has shown “very encouraging” real-world performance.\n\n\n\nUnder the IndiaAI Mission, the government has selected 12 startups and organisations to build sovereign foundational models: Sarvam AI, Soket AI Labs, Gnani.ai, Gan.AI, Avataar AI, BharatGen, Fractal Analytics, Tech Mahindra (Maker’s Lab), ZenteiQ Aitech Innovations, Genloop Intelligence, NeuroDX (IntelliHealth), and Shodh AI.\n\n\n\nAddressing industry leaders, Vaishnaw urged companies to play a more active role in skilling India’s youth, calling for collaboration on curriculum development to create a robust AI talent pipeline.\nThe post India to See Up To $150 Bn AI Infrastructure Investments in 2026: Ashwini Vaishnaw appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/india-to-see-up-to-150-bn-ai-infrastructure-investments-in-2026-ashwini-vaishnaw/",
      "author": "Pallavi Chakravorty",
      "published": "2026-01-21T10:26:07",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "India may see up to $150 billion in AI infrastructure investment by end of 2026, according to IT Minister Ashwini Vaishnaw at Davos. Google committed $15B, Microsoft over $20B, and Amazon $35B for Indian AI development.",
      "importance_score": 72.0,
      "reasoning": "Massive infrastructure investment announcements from multiple tech giants. Significant for global AI development landscape.",
      "themes": [
        "AI Infrastructure",
        "India",
        "Big Tech Investment",
        "Davos 2026"
      ],
      "continuation": null,
      "summary_html": "<p>India may see up to $150 billion in AI infrastructure investment by end of 2026, according to IT Minister Ashwini Vaishnaw at Davos. Google committed $15B, Microsoft over $20B, and Amazon $35B for Indian AI development.</p>",
      "content_html": "<p>India may see an investment of up to $150 billion in AI infrastructure by the end of 2026, Ashwini Vaishnaw, union minister for information technology, railways and information and broadcasting, said in an exclusive interview with CNBC TV18 at the World Economic Forum in Davos, Switzerland.</p>\n<p>Vaishnaw said India has already secured investment commitments worth $70 billion, with a further $50–80 billion likely over the next 12 months.</p>\n<p>The past year has seen a wave of major investment announcements. Google committed $15 billion to set up an AI hub in Visakhapatnam, Microsoft raised its India investment commitment to over $20 billion through 2030, and Amazon pledged $35 billion in investments over the same period.</p>\n<p>Vaishnaw also confirmed that India’s homegrown large language models (LLMs) will be unveiled at the upcoming India AI Summit. IndiaAI Mission CEO Abhishek Singh had earlier told AIM that at least two LLMs—developed by Sarvam AI and BharatGen—would be launched ahead of the summit.</p>\n<p>The minister said India will eventually develop 12 foundational AI models, each ranging between 50–120 billion parameters, designed to run on relatively small GPU clusters. This, he said, would enable low-cost AI services at scale. Vaishnaw noted that early live testing of Indian models has shown “very encouraging” real-world performance.</p>\n<p>Under the IndiaAI Mission, the government has selected 12 startups and organisations to build sovereign foundational models: Sarvam AI, Soket AI Labs, Gnani.ai, Gan.AI, Avataar AI, BharatGen, Fractal Analytics, Tech Mahindra (Maker’s Lab), ZenteiQ Aitech Innovations, Genloop Intelligence, NeuroDX (IntelliHealth), and Shodh AI.</p>\n<p>Addressing industry leaders, Vaishnaw urged companies to play a more active role in skilling India’s youth, calling for collaboration on curriculum development to create a robust AI talent pipeline.</p>\n<p>The post India to See Up To $150 Bn AI Infrastructure Investments in 2026: Ashwini Vaishnaw appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "e2e61dd579ba",
      "title": "Liquid AI Releases LFM2.5-1.2B-Thinking: a 1.2B Parameter Reasoning Model That Fits Under 1 GB On-Device",
      "content": "Liquid AI has released LFM2.5-1.2B-Thinking, a 1.2 billion parameter reasoning model that runs fully on device and fits in about 900 MB on a modern phone. What needed a data center 2 years ago can now run offline on consumer hardware, with a focus on structured reasoning traces, tool use, and math, rather than general chat. \n\n\n\nPosition in the LFM2.5 family and core specs\n\n\n\nLFM2.5-1.2B-Thinking is part of the LFM2.5 family of Liquid Foundation Models, which extends the earlier LFM2 architecture with more pre-training and multi stage reinforcement learning for edge deployment.\n\n\n\nThe model is text only and general purpose with the following configuration:\n\n\n\n\n1.17B parameters, reported as a 1.2B class model\n\n\n\n16 layers, with 10 double gated LIV convolution blocks and 6 GQA blocks\n\n\n\nTraining budget of 28T tokens\n\n\n\nContext length of 32,768 tokens\n\n\n\nVocabulary size of 65,536\n\n\n\n8 languages, English, Arabic, Chinese, French, German, Japanese, Korean, Spanish\n\n\n\n\nReasoning first behavior and thinking traces\n\n\n\nThe &#8216;Thinking&#8217; variant is trained specifically for reasoning. At inference time it produces internal thinking traces before the final answer. These traces are chains of intermediate steps that the model uses to plan tool calls, verify partial results, and work through multi step instructions.\n\n\n\nLiquid AI team recommends this model for agentic tasks, data extraction pipelines, and retrieval augmented generation flows where you want explicit reasoning and verifiable intermediate steps. A practical way to think about it, you use LFM2.5-1.2B-Thinking as the planning brain inside agents and tools, and use other models when you need broad world knowledge or code heavy workflows.\n\n\n\nBenchmarks versus other 1B class models\n\n\n\nLiquid AI team evaluates LFM2.5-1.2B-Thinking against models around 1B parameters on a suite of reasoning and instruction benchmarks. \n\n\n\nhttps://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking\n\n\nCompared to LFM2.5-1.2B-Instruct, three metrics improve strongly, math reasoning rises from about 63 to 88 on MATH 500, instruction following rises from about 61 to 69 on Multi IF, and tool use rises from about 49 to 57 on BFCLv3. \n\n\n\nLFM2.5-1.2B-Thinking competes with Qwen3-1.7B in thinking mode on most reasoning benchmarks while using around 40 percent fewer parameters and fewer output tokens on average. It also outperforms other 1B class baselines such as Granite-4.0-H-1B, Granite-4.0-1B, Gemma-3-1B-IT, and Llama-3.2-1B Instruct on many of these tasks.\n\n\n\nTraining recipe and doom looping mitigation\n\n\n\nReasoning models often suffer from doom looping, where the model repeats fragments of its chain of thought instead of finishing the answer. LFM2.5-1.2B-Thinking uses a multi stage training pipeline to reduce this. \n\n\n\nThe process starts with mid training that includes reasoning traces so the model learns a &#8216;reason first then answer&#8217; pattern. Then supervised fine tuning on synthetic chains improves chain of thought generation. After that, preference alignment and RLVR are applied. In preference alignment, the research team generates 5 temperature sampled candidates and 1 greedy candidate per prompt and uses an LLM judge to pick preferred and rejected outputs, while also labeling looping outputs explicitly. During RLVR they add an n gram repetition penalty early in training. This reduces the doom loop rate from 15.74 percent at mid training to 0.36 percent after RLVR on a set of representative prompts.\n\n\n\nThe result is a small reasoning model that can produce thinking traces without getting stuck in long repetitive outputs, which is important for interactive agents and on device UX.\n\n\n\nInference performance and hardware footprint\n\n\n\nA key design target is fast inference with a small memory footprint on CPUs and NPUs. LFM2.5-1.2B-Thinking can decode at about 239 tokens per second on an AMD CPU and about 82 tokens per second on a mobile NPU, while running under 1 GB of memory, with broad day one support for llama.cpp, MLX, and vLLM. \n\n\n\nThe detailed hardware table uses 1K prefill and 100 decode tokens and gives the following examples for LFM2.5-1.2B-Thinking\n\n\n\nhttps://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking\n\n\nThese numbers show that the model fits comfortably under 1 GB on phones and embedded devices while sustaining useful throughputs even at long contexts. \n\n\n\nKey Takeaways\n\n\n\n\nLFM2.5-1.2B-Thinking is a 1.17B parameter reasoning model with 32,768 context length and runs under 1 GB on phones and laptops.\n\n\n\nThe model is optimized for explicit thinking traces, agentic workflows, data extraction, and RAG.\n\n\n\nIt reaches strong scores for a 1B class model, for example 87.96 on MATH 500, 85.60 on GSM8K, and competitive performance with Qwen3 1.7B in thinking mode with fewer parameters.\n\n\n\nThe training pipeline uses midtraining with reasoning traces, supervised fine tuning, preference alignment with 5 sampled along with 1 greedy candidate, and RLVR with n gram penalties, which reduces doom loops from 15.74 percent to 0.36 percent.\n\n\n\nThe model runs efficiently on AMD and Qualcomm NPUs and CPUs with runtimes like llama.cpp, FastFlowLM, and NexaML, is available in GGUF, ONNX, and MLX formats, and can be loaded easily from Hugging Face for on device deployment.\n\n\n\n\n\n\n\n\nHosting Providers/Deployment\n\n\n\nYou can access or host the model through the following providers and platforms:\n\n\n\nCloud &amp; API Providers\n\n\n\n\nOpenRouter\n\n\n\nLiquid AI Playground\n\n\n\nLEAP (Liquid’s Edge AI Platform)\n\n\n\n\nModel Repositories (Self-Hosting)\n\n\n\nIf you want to run the model locally or on your own infrastructure, the weights are available in various formats:\n\n\n\n\nHugging Face\n\nThe post Liquid AI Releases LFM2.5-1.2B-Thinking: a 1.2B Parameter Reasoning Model That Fits Under 1 GB On-Device appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/20/liquid-ai-releases-lfm2-5-1-2b-thinking-a-1-2b-parameter-reasoning-model-that-fits-under-1-gb-on-device/",
      "author": "Asif Razzaq",
      "published": "2026-01-21T03:43:22",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Machine Learning",
        "New Releases",
        "Python",
        "Staff",
        "Tech News",
        "Technology",
        "Uncategorized"
      ],
      "summary": "As first reported in [Social](/?date=2026-01-21&category=reddit#item-eb1de682d6db) yesterday, Liquid AI released LFM2.5-1.2B-Thinking, a 1.2B parameter reasoning model that runs on-device in ~900MB. Features structured reasoning traces, tool use, and math capabilities using hybrid convolution and attention architecture.",
      "importance_score": 70.0,
      "reasoning": "Notable model release enabling reasoning capabilities on edge devices. Significant for on-device AI but from smaller lab.",
      "themes": [
        "Edge AI",
        "Reasoning Models",
        "On-Device AI",
        "Model Release"
      ],
      "continuation": {
        "original_item_id": "eb1de682d6db",
        "original_date": "2026-01-21",
        "original_category": "reddit",
        "original_title": "Liquid AI released the best thinking Language Model Under 1GB",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "As first reported in **Social** yesterday"
      },
      "summary_html": "<p>As first reported in <a href=\"/?date=2026-01-21&amp;category=reddit#item-eb1de682d6db\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> yesterday, Liquid AI released LFM2.5-1.2B-Thinking, a 1.2B parameter reasoning model that runs on-device in ~900MB. Features structured reasoning traces, tool use, and math capabilities using hybrid convolution and attention architecture.</p>",
      "content_html": "<p>Liquid AI has released LFM2.5-1.2B-Thinking, a 1.2 billion parameter reasoning model that runs fully on device and fits in about 900 MB on a modern phone. What needed a data center 2 years ago can now run offline on consumer hardware, with a focus on structured reasoning traces, tool use, and math, rather than general chat.</p>\n<p>Position in the LFM2.5 family and core specs</p>\n<p>LFM2.5-1.2B-Thinking is part of the LFM2.5 family of Liquid Foundation Models, which extends the earlier LFM2 architecture with more pre-training and multi stage reinforcement learning for edge deployment.</p>\n<p>The model is text only and general purpose with the following configuration:</p>\n<p>1.17B parameters, reported as a 1.2B class model</p>\n<p>16 layers, with 10 double gated LIV convolution blocks and 6 GQA blocks</p>\n<p>Training budget of 28T tokens</p>\n<p>Context length of 32,768 tokens</p>\n<p>Vocabulary size of 65,536</p>\n<p>8 languages, English, Arabic, Chinese, French, German, Japanese, Korean, Spanish</p>\n<p>Reasoning first behavior and thinking traces</p>\n<p>The ‘Thinking’ variant is trained specifically for reasoning. At inference time it produces internal thinking traces before the final answer. These traces are chains of intermediate steps that the model uses to plan tool calls, verify partial results, and work through multi step instructions.</p>\n<p>Liquid AI team recommends this model for agentic tasks, data extraction pipelines, and retrieval augmented generation flows where you want explicit reasoning and verifiable intermediate steps. A practical way to think about it, you use LFM2.5-1.2B-Thinking as the planning brain inside agents and tools, and use other models when you need broad world knowledge or code heavy workflows.</p>\n<p>Benchmarks versus other 1B class models</p>\n<p>Liquid AI team evaluates LFM2.5-1.2B-Thinking against models around 1B parameters on a suite of reasoning and instruction benchmarks.</p>\n<p>https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking</p>\n<p>Compared to LFM2.5-1.2B-Instruct, three metrics improve strongly, math reasoning rises from about 63 to 88 on MATH 500, instruction following rises from about 61 to 69 on Multi IF, and tool use rises from about 49 to 57 on BFCLv3.</p>\n<p>LFM2.5-1.2B-Thinking competes with Qwen3-1.7B in thinking mode on most reasoning benchmarks while using around 40 percent fewer parameters and fewer output tokens on average. It also outperforms other 1B class baselines such as Granite-4.0-H-1B, Granite-4.0-1B, Gemma-3-1B-IT, and Llama-3.2-1B Instruct on many of these tasks.</p>\n<p>Training recipe and doom looping mitigation</p>\n<p>Reasoning models often suffer from doom looping, where the model repeats fragments of its chain of thought instead of finishing the answer. LFM2.5-1.2B-Thinking uses a multi stage training pipeline to reduce this.</p>\n<p>The process starts with mid training that includes reasoning traces so the model learns a ‘reason first then answer’ pattern. Then supervised fine tuning on synthetic chains improves chain of thought generation. After that, preference alignment and RLVR are applied. In preference alignment, the research team generates 5 temperature sampled candidates and 1 greedy candidate per prompt and uses an LLM judge to pick preferred and rejected outputs, while also labeling looping outputs explicitly. During RLVR they add an n gram repetition penalty early in training. This reduces the doom loop rate from 15.74 percent at mid training to 0.36 percent after RLVR on a set of representative prompts.</p>\n<p>The result is a small reasoning model that can produce thinking traces without getting stuck in long repetitive outputs, which is important for interactive agents and on device UX.</p>\n<p>Inference performance and hardware footprint</p>\n<p>A key design target is fast inference with a small memory footprint on CPUs and NPUs. LFM2.5-1.2B-Thinking can decode at about 239 tokens per second on an AMD CPU and about 82 tokens per second on a mobile NPU, while running under 1 GB of memory, with broad day one support for llama.cpp, MLX, and vLLM.</p>\n<p>The detailed hardware table uses 1K prefill and 100 decode tokens and gives the following examples for LFM2.5-1.2B-Thinking</p>\n<p>https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking</p>\n<p>These numbers show that the model fits comfortably under 1 GB on phones and embedded devices while sustaining useful throughputs even at long contexts.</p>\n<p>Key Takeaways</p>\n<p>LFM2.5-1.2B-Thinking is a 1.17B parameter reasoning model with 32,768 context length and runs under 1 GB on phones and laptops.</p>\n<p>The model is optimized for explicit thinking traces, agentic workflows, data extraction, and RAG.</p>\n<p>It reaches strong scores for a 1B class model, for example 87.96 on MATH 500, 85.60 on GSM8K, and competitive performance with Qwen3 1.7B in thinking mode with fewer parameters.</p>\n<p>The training pipeline uses midtraining with reasoning traces, supervised fine tuning, preference alignment with 5 sampled along with 1 greedy candidate, and RLVR with n gram penalties, which reduces doom loops from 15.74 percent to 0.36 percent.</p>\n<p>The model runs efficiently on AMD and Qualcomm NPUs and CPUs with runtimes like llama.cpp, FastFlowLM, and NexaML, is available in GGUF, ONNX, and MLX formats, and can be loaded easily from Hugging Face for on device deployment.</p>\n<p>Hosting Providers/Deployment</p>\n<p>You can access or host the model through the following providers and platforms:</p>\n<p>Cloud &amp; API Providers</p>\n<p>OpenRouter</p>\n<p>Liquid AI Playground</p>\n<p>LEAP (Liquid’s Edge AI Platform)</p>\n<p>Model Repositories (Self-Hosting)</p>\n<p>If you want to run the model locally or on your own infrastructure, the weights are available in various formats:</p>\n<p>Hugging Face</p>\n<p>The post Liquid AI Releases LFM2.5-1.2B-Thinking: a 1.2B Parameter Reasoning Model That Fits Under 1 GB On-Device appeared first on MarkTechPost.</p>"
    },
    {
      "id": "96782a00dbfe",
      "title": "Rollout of AI may need to be slowed to ‘save society’, says JP Morgan boss",
      "content": "Jamie Dimon warns of civil unrest but Nvidia’s Jensen Huang argues tech will create rather than destroy jobsJamie Dimon, the boss of JP Morgan, has said artificial intelligence “may go too fast for society” and cause “civil unrest” unless governments and business support displaced workers.While advances in AI will have huge benefits, from increasing productivity to curing diseases, the technology may need to be phased in to “save society”, he said. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/21/rollout-ai-slowed-save-society-jp-morgan-jamie-dimon-jensen-huang",
      "author": "John Collingridge and Graeme Wearden in Davos",
      "published": "2026-01-21T18:18:20",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Davos 2026",
        "Jamie Dimon",
        "JP Morgan",
        "AI (artificial intelligence)",
        "Nvidia",
        "Business",
        "Technology",
        "World news"
      ],
      "summary": "JP Morgan CEO Jamie Dimon warns at Davos that AI 'may go too fast for society' and could cause civil unrest if displaced workers aren't supported. He suggests AI rollout may need to be phased to 'save society.'",
      "importance_score": 68.0,
      "reasoning": "Major financial leader making significant public statements about AI risks at Davos. Influential voice that could shape policy discussions.",
      "themes": [
        "AI Safety",
        "Davos 2026",
        "Industry Leadership",
        "Labor"
      ],
      "continuation": null,
      "summary_html": "<p>JP Morgan CEO Jamie Dimon warns at Davos that AI 'may go too fast for society' and could cause civil unrest if displaced workers aren't supported. He suggests AI rollout may need to be phased to 'save society.'</p>",
      "content_html": "<p>Jamie Dimon warns of civil unrest but Nvidia’s Jensen Huang argues tech will create rather than destroy jobsJamie Dimon, the boss of JP Morgan, has said artificial intelligence “may go too fast for society” and cause “civil unrest” unless governments and business support displaced workers.While advances in AI will have huge benefits, from increasing productivity to curing diseases, the technology may need to be phased in to “save society”, he said. Continue reading...</p>"
    },
    {
      "id": "f2875205038c",
      "title": "The US and China Are Collaborating More Closely on AI Than You Think",
      "content": "WIRED analyzed more than 5,000 papers from NeurIPS using OpenAI’s Codex to understand the areas where the US and China actually work together on AI research.",
      "url": "https://www.wired.com/story/us-china-collaboration-neurips-papers/",
      "author": "Will Knight",
      "published": "2026-01-21T19:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "AI Lab",
        "artificial intelligence",
        "China",
        "Donald Trump",
        "Regulation",
        "politics"
      ],
      "summary": "WIRED analysis of 5,000+ NeurIPS papers reveals substantial US-China collaboration on AI research despite geopolitical tensions. The analysis used OpenAI's Codex to understand collaboration patterns.",
      "importance_score": 65.0,
      "reasoning": "Important policy and research insight showing continued scientific collaboration despite trade restrictions. Relevant for understanding AI research dynamics.",
      "themes": [
        "AI Research",
        "US-China Relations",
        "Policy"
      ],
      "continuation": null,
      "summary_html": "<p>WIRED analysis of 5,000+ NeurIPS papers reveals substantial US-China collaboration on AI research despite geopolitical tensions. The analysis used OpenAI's Codex to understand collaboration patterns.</p>",
      "content_html": "<p>WIRED analyzed more than 5,000 papers from NeurIPS using OpenAI’s Codex to understand the areas where the US and China actually work together on AI research.</p>"
    },
    {
      "id": "a9432c4dce85",
      "title": "The quiet work behind Citi’s 4,000-person internal AI rollout",
      "content": "For many large companies, artificial intelligence still lives in side projects. Small teams test tools, run pilots, and present results that struggle to spread beyond a few departments. Citi has taken a different path, where instead of keeping AI limited to specialists, the bank has spent the past two years pushing the technology into daily work in the organisation.\nThat effort has resulted in an internal AI workforce of roughly 4,000 employees, drawn from roles that range from technology and operations to risk and customer support. The figure was first reported by Business Insider, which detailed how Citi built its &#8220;AI Champions&#8221; and &#8220;AI Accelerators&#8221; programmes to encourage participation not central control.\nThe scale of integration is notable, as Citi employs around 182,000 people globally, and more than 70% of them now use firm-approved AI tools in some form, according to the same report. That level of use places Citi ahead of many peers that still restrict AI access to technical teams or innovation labs.\nFrom central pilots to team-level adoption\nRather than start with tools, Citi focused on people. The bank invited employees to volunteer as AI Champions, giving them access to training, internal resources, and early versions of approved AI systems. The employees then supported colleagues in their own teams, acting as local points of contact not formal trainers.\nThe approach reflects a practical view of adoption. New tools often fail not because they lack features, but because staff do not know when or how to use them. By embedding support inside teams, Citi reduced the gap between experimentation and routine work.\nTraining played a central role. Employees could earn internal badges by completing courses or demonstrating how they used AI to improve their own tasks. The badges did not come with promotions or pay rises, but they helped create visibility and credibility in the organisation. According to Business Insider, this peer-driven model helped AI spread faster than top-down mandates.\nEveryday use, with guardrails\nCiti&#8217;s leadership has framed the effort as a response to scale not novelty. With operations spanning retail banking, investment services, compliance, and customer support, small efficiency gains can add up quickly. AI tools are being used to summarise documents, draft internal notes, analyse data sets, and assist with software development. None of these uses are new on their own, but the difference lies in how they are applied.\nThe focus on everyday tasks also shapes Citi&#8217;s risk posture. The bank has limited employees to firm-approved tools, with guardrails around what data can be used and how outputs are handled. That constraint has slowed some experiments, but it has also made managers more comfortable allowing broader access. In regulated industries, trust often matters more than speed.\nWhat Citi&#8217;s approach shows about scaling AI\nThe structure of Citi&#8217;s programme suggests a lesson for other large enterprises. AI adoption does not require every employee to become an expert. It requires enough people to understand the tools well enough to apply them responsibly and explain them to others. By training thousands instead of dozens, Citi reduced its reliance on a small group of specialists.\nThere is also a cultural signal at play. Encouraging employees from non-technical roles to participate sends a message that AI is not only for engineers or data scientists. It becomes part of how work gets done, similar to spreadsheets or presentation software in earlier decades.\nThat shift aligns with broader industry trends. Surveys from firms like McKinsey have shown that many companies struggle to move AI projects into production, often citing talent gaps and unclear ownership. Citi&#8217;s model sidesteps some of those issues by distributing ownership in teams, while keeping governance centralised.\nStill, the approach is not without limits. Peer-led adoption depends on sustained interest, and not all teams move at the same pace. There is also the risk that informal support networks become uneven, with some groups benefiting more than others. Citi has tried to address this by rotating Champions and updating training content as tools change.\nWhat stands out is the bank&#8217;s willingness to treat AI as infrastructure not innovation. Instead of asking whether AI could transform the business, Citi asked where it could remove friction from existing work. That framing makes progress easier to measure and reduces pressure to produce dramatic results.\nThe experience also challenges a common assumption that AI adoption must start at the top. Citi&#8217;s senior leadership supported the effort, but much of the momentum came from employees who volunteered time to learn and teach. In large organisations, that bottom-up energy can be hard to generate, yet it often determines whether new technology sticks.\nAs more companies move from pilots to production, Citi&#8217;s experiment offers a useful case study. It shows that scale does not come from buying more tools, but from helping people feel confident using the ones they already have. For enterprises wondering why AI progress feels slow, the answer may lie less in strategy decks and more in how work actually gets done, one team at a time.\n(Photo by Declan Sun)\nSee also: JPMorgan Chase treats AI spending as core infrastructure\n\n\n\nWant to learn more about AI and big data from industry leaders? Check outAI &amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post The quiet work behind Citi’s 4,000-person internal AI rollout appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/the-quiet-work-behind-citi-4000-person-internal-ai-rollout/",
      "author": "Muhammad Zulhusni",
      "published": "2026-01-21T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "AI in Action",
        "Artificial Intelligence",
        "Features",
        "Finance AI",
        "Workforce & HR AI",
        "World of Work",
        "ai",
        "artificial intelligence",
        "banking",
        "data analysis",
        "infrastructure",
        "research"
      ],
      "summary": "Citi has built an internal AI workforce of 4,000 employees through 'AI Champions' and 'AI Accelerators' programs over two years. The bank is pushing AI into daily operations across technology, risk, and customer support roles.",
      "importance_score": 62.0,
      "reasoning": "Notable example of large-scale enterprise AI deployment. Demonstrates maturation of AI adoption in regulated industries.",
      "themes": [
        "Enterprise AI",
        "Finance AI",
        "Workforce"
      ],
      "continuation": null,
      "summary_html": "<p>Citi has built an internal AI workforce of 4,000 employees through 'AI Champions' and 'AI Accelerators' programs over two years. The bank is pushing AI into daily operations across technology, risk, and customer support roles.</p>",
      "content_html": "<p>For many large companies, artificial intelligence still lives in side projects. Small teams test tools, run pilots, and present results that struggle to spread beyond a few departments. Citi has taken a different path, where instead of keeping AI limited to specialists, the bank has spent the past two years pushing the technology into daily work in the organisation.</p>\n<p>That effort has resulted in an internal AI workforce of roughly 4,000 employees, drawn from roles that range from technology and operations to risk and customer support. The figure was first reported by Business Insider, which detailed how Citi built its “AI Champions” and “AI Accelerators” programmes to encourage participation not central control.</p>\n<p>The scale of integration is notable, as Citi employs around 182,000 people globally, and more than 70% of them now use firm-approved AI tools in some form, according to the same report. That level of use places Citi ahead of many peers that still restrict AI access to technical teams or innovation labs.</p>\n<p>From central pilots to team-level adoption</p>\n<p>Rather than start with tools, Citi focused on people. The bank invited employees to volunteer as AI Champions, giving them access to training, internal resources, and early versions of approved AI systems. The employees then supported colleagues in their own teams, acting as local points of contact not formal trainers.</p>\n<p>The approach reflects a practical view of adoption. New tools often fail not because they lack features, but because staff do not know when or how to use them. By embedding support inside teams, Citi reduced the gap between experimentation and routine work.</p>\n<p>Training played a central role. Employees could earn internal badges by completing courses or demonstrating how they used AI to improve their own tasks. The badges did not come with promotions or pay rises, but they helped create visibility and credibility in the organisation. According to Business Insider, this peer-driven model helped AI spread faster than top-down mandates.</p>\n<p>Everyday use, with guardrails</p>\n<p>Citi’s leadership has framed the effort as a response to scale not novelty. With operations spanning retail banking, investment services, compliance, and customer support, small efficiency gains can add up quickly. AI tools are being used to summarise documents, draft internal notes, analyse data sets, and assist with software development. None of these uses are new on their own, but the difference lies in how they are applied.</p>\n<p>The focus on everyday tasks also shapes Citi’s risk posture. The bank has limited employees to firm-approved tools, with guardrails around what data can be used and how outputs are handled. That constraint has slowed some experiments, but it has also made managers more comfortable allowing broader access. In regulated industries, trust often matters more than speed.</p>\n<p>What Citi’s approach shows about scaling AI</p>\n<p>The structure of Citi’s programme suggests a lesson for other large enterprises. AI adoption does not require every employee to become an expert. It requires enough people to understand the tools well enough to apply them responsibly and explain them to others. By training thousands instead of dozens, Citi reduced its reliance on a small group of specialists.</p>\n<p>There is also a cultural signal at play. Encouraging employees from non-technical roles to participate sends a message that AI is not only for engineers or data scientists. It becomes part of how work gets done, similar to spreadsheets or presentation software in earlier decades.</p>\n<p>That shift aligns with broader industry trends. Surveys from firms like McKinsey have shown that many companies struggle to move AI projects into production, often citing talent gaps and unclear ownership. Citi’s model sidesteps some of those issues by distributing ownership in teams, while keeping governance centralised.</p>\n<p>Still, the approach is not without limits. Peer-led adoption depends on sustained interest, and not all teams move at the same pace. There is also the risk that informal support networks become uneven, with some groups benefiting more than others. Citi has tried to address this by rotating Champions and updating training content as tools change.</p>\n<p>What stands out is the bank’s willingness to treat AI as infrastructure not innovation. Instead of asking whether AI could transform the business, Citi asked where it could remove friction from existing work. That framing makes progress easier to measure and reduces pressure to produce dramatic results.</p>\n<p>The experience also challenges a common assumption that AI adoption must start at the top. Citi’s senior leadership supported the effort, but much of the momentum came from employees who volunteered time to learn and teach. In large organisations, that bottom-up energy can be hard to generate, yet it often determines whether new technology sticks.</p>\n<p>As more companies move from pilots to production, Citi’s experiment offers a useful case study. It shows that scale does not come from buying more tools, but from helping people feel confident using the ones they already have. For enterprises wondering why AI progress feels slow, the answer may lie less in strategy decks and more in how work actually gets done, one team at a time.</p>\n<p>(Photo by Declan Sun)</p>\n<p>See also: JPMorgan Chase treats AI spending as core infrastructure</p>\n<p>Want to learn more about AI and big data from industry leaders? Check outAI &amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post The quiet work behind Citi’s 4,000-person internal AI rollout appeared first on AI News.</p>"
    },
    {
      "id": "9e7e0e497e0f",
      "title": "DeepSeek’s Rise, Pause, and How Local Competition Took Over",
      "content": "In 2024, NVIDIA had established its long-standing dominance in the sphere of the AI supply chain with advanced GPUs. That was until DeepSeek entered its domain in early 2025, challenging assumptions about scale and rewriting the rules of AI development.&nbsp;\n\n\n\nThe Chinese lab released a reasoning model that matched or exceeded several Western closed-source systems while training the base model, DeepSeek-V2, on just 2,048 GPUs—far fewer than what frontier labs then used for similar results.&nbsp;\n\n\n\n\n\n\n\nSource: Andrej Karpathy, former OpenAI researcher in a post on X, December 2024. \n\n\n\nAs attention shifted from sheer compute to architecture, training strategy, and efficiency, NVIDIA’s market capitalisation fell by as much as $600 billion in a single day—the largest such drop for a US company—as investors reassessed the need for ever-larger GPU spending.\n\n\n\nSoon, China started deploying DeepSeek across public service platforms and government cloud infrastructure. Universities rolled out customised instances for automated learning assistance, research support, academic planning, and 24/7 student services as part of wider AI-augmented education pilots. Even healthcare, automotive manufacturing, and the military integrated DeepSeek’s LLMs.&nbsp;\n\n\n\nThe competitive environment pushed US cloud providers to catch up.&nbsp;\n\n\n\nWithin weeks, AWS added DeepSeek-R1 to its AI services. Microsoft integrated it into Azure AI Foundry and its model catalogue. Google made DeepSeek-R1 available in Vertex AI’s Model Garden, enabling developers to deploy the model within existing cloud workflows.&nbsp;\n\n\n\nOver the last year, DeepSeek’s journey has been defined by R1, the absence of its successor, and local rivals taking over.&nbsp;\n\n\n\nUsage Patterns and Market Evolution\n\n\n\nOpenRouter, a unified API gateway that provides access to hundreds of AI models through a single interface, offers a useful lens on DeepSeek’s adoption. Its routing dataset—covering roughly 100 trillion tokens—shows DeepSeek models accounting for around 14.4 trillion tokens between November 2024 and November 2025.&nbsp;\n\n\n\n\n\n\n\nAfter the release of DeepSeek V3 and DeepSeek R1, the two together represented more than half of all open-source token traffic on the platform. No other open-weight model family reached comparable concentration during that period.\n\n\n\nPricing played a significant role in the model’s adoption, as DeepSeek consistently ranked among the lowest-cost options for sustained, high-volume routing. But this dominance peaked around mid-2025 and then declined.&nbsp;\n\n\n\nOpenRouter’s report identifies a clear summer inflexion.&nbsp;\n\n\n\n\n\n\n\nSource: OpenRouter\n\n\n\nThere wasn’t a collapse in absolute usage, but a rapid diversification of the Chinese open-source ecosystem. New releases from Qwen, Moonshot AI (Kimi), MiniMax, and others captured production traffic within weeks.\n\n\n\nAnd by late 2025, no single open-source model accounted for more than 20–25% of token share. DeepSeek’s earlier flagpole position declined, giving way to a more pluralistic distribution.\n\n\n\nCurrently, DeepSeek is going toe-to-toe with other Chinese AI models in absolute benchmarks. But it is in no hurry to recapture the throne.&nbsp;\n\n\n\n\n\n\n\nThe Research Focus\n\n\n\nRather than accelerating product releases like Western AI labs and Chinese rivals, DeepSeek pivoted towards research, training methods, and infrastructure.&nbsp;\n\n\n\nPost-R1, DeepSeek focused on exploring the nitty-gritty details of an LLM’s architecture and incrementally fixing bottlenecks that hampered compute efficiency. This was in line with the struggles the country faced due to export restrictions the US government placed on NVIDIA for selling GPUs to a Chinese tech company.&nbsp;\n\n\n\nIn February 2025, with a five-day open-source initiative, DeepSeek released frameworks focused on execution efficiency, pipeline scheduling, core computation, parallel workload coordination, and large-scale storage. These addressed the bottlenecks that determine whether models can be trained and served cheaply at scale, and were aimed squarely at production engineers.&nbsp;\n\n\n\nThroughout the year, the DeepSeek-R1 also received incremental updates that boosted its performance on benchmarks, and the company also ran numerous research works and experiments on the base model, the DeepSeek-V3.&nbsp;\n\n\n\nNotably, in September, DeepSeek released V3.2-Exp, an experimental model designed to push long-context capabilities while keeping efficiency central, with 3.5x lower prefill costs and up to 10x cheaper decoding during inference for a 128k context window.\n\n\n\nIn October, it released DeepSeek-OCR. The model converts text into compact visual tokens, enabling compression ratios of 9–10x with over 96% precision, and around 60% accuracy even at 20xcompression. The work suggested a new efficiency path in which visual modalities are used not for perception but for memory and context optimisation in language models.\n\n\n\nAnd, in November, it published research on a model that achieved gold-medal-level performance at the International Math Olympiad 2025. It became the only company to achieve the status after OpenAI and Google DeepMind. This model, the DeepSeek-Math-v2, addressed a growing concern in reasoning and math benchmarks, namely that many models arrive at correct answers without sound or inspectable reasoning.&nbsp;\n\n\n\nWhat Next?&nbsp;\n\n\n\nA recent Microsoft report showed DeepSeek achieving significant market penetration outside China, with about 43% usage in Russia and roughly 56% in Belarus, making these among the highest adoption rates globally. In China, DeepSeek’s share of generative AI usage is approximately 89%.&nbsp;\n\n\n\n\n\n\n\nSource: Microsoft\n\n\n\nBy contrast, adoption in Western Europe and North America remains low, often under 5%. In many African countries, usage is 2–4 times higher than in Western Europe or North America, driven by DeepSeek’s free or low-cost access with minimal subscription barriers, which makes it appealing in price-sensitive markets where Western alternatives are less accessible.&nbsp;\n\n\n\nWith growth concentrated in developing regions, how DeepSeek adapts its future models and services to meet these markets’ specific needs will be an important indicator of its global strategy.\n\n\n\nHowever, the lack of a successor to its R1 model is noteworthy.&nbsp;\n\n\n\nAccording to a Reuters report in mid-2025, DeepSeek did not release the expected DeepSeek R2 at the anticipated time because the company’s leadership, including founder Liang Wenfeng, was not satisfied with the model’s performance and stability, pushing its launch beyond the originally planned May 2025 date.&nbsp;\n\n\n\nAdditional factors included slow data labelling, technical problems tied to hardware choices (such as instability and connectivity issues with Huawei chips that DeepSeek had been encouraged to adopt), which forced the company to prioritise stability by sticking with NVIDIA GPUs for training and using those other chips only for inference.\n\n\n\nNow, DeepSeek is planning to launch a new base model, DeepSeek-v4, with a clear emphasis on coding and math performance, according to The Information. But this time, it will not have the first mover’s advantage in the open-source ecosystem.&nbsp;\nThe post DeepSeek’s Rise, Pause, and How Local Competition Took Over appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/global-tech/deepseeks-rise-pause-and-how-local-competition-took-over/",
      "author": "Supreeth Koundinya",
      "published": "2026-01-21T14:23:12",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "Global Tech",
        "AI (Artificial Intelligence)"
      ],
      "summary": "Analysis of DeepSeek's rise in early 2025, when it challenged NVIDIA's dominance by training competitive models on just 2,048 GPUs. NVIDIA's market cap dropped $600B in a single day following DeepSeek's revelations.",
      "importance_score": 60.0,
      "reasoning": "Retrospective analysis of significant 2025 event. Important context but not new breaking news.",
      "themes": [
        "DeepSeek",
        "NVIDIA",
        "Efficient Training"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis of DeepSeek's rise in early 2025, when it challenged NVIDIA's dominance by training competitive models on just 2,048 GPUs. NVIDIA's market cap dropped $600B in a single day following DeepSeek's revelations.</p>",
      "content_html": "<p>In 2024, NVIDIA had established its long-standing dominance in the sphere of the AI supply chain with advanced GPUs. That was until DeepSeek entered its domain in early 2025, challenging assumptions about scale and rewriting the rules of AI development.&nbsp;</p>\n<p>The Chinese lab released a reasoning model that matched or exceeded several Western closed-source systems while training the base model, DeepSeek-V2, on just 2,048 GPUs—far fewer than what frontier labs then used for similar results.&nbsp;</p>\n<p>Source: Andrej Karpathy, former OpenAI researcher in a post on X, December 2024.</p>\n<p>As attention shifted from sheer compute to architecture, training strategy, and efficiency, NVIDIA’s market capitalisation fell by as much as $600 billion in a single day—the largest such drop for a US company—as investors reassessed the need for ever-larger GPU spending.</p>\n<p>Soon, China started deploying DeepSeek across public service platforms and government cloud infrastructure. Universities rolled out customised instances for automated learning assistance, research support, academic planning, and 24/7 student services as part of wider AI-augmented education pilots. Even healthcare, automotive manufacturing, and the military integrated DeepSeek’s LLMs.&nbsp;</p>\n<p>The competitive environment pushed US cloud providers to catch up.&nbsp;</p>\n<p>Within weeks, AWS added DeepSeek-R1 to its AI services. Microsoft integrated it into Azure AI Foundry and its model catalogue. Google made DeepSeek-R1 available in Vertex AI’s Model Garden, enabling developers to deploy the model within existing cloud workflows.&nbsp;</p>\n<p>Over the last year, DeepSeek’s journey has been defined by R1, the absence of its successor, and local rivals taking over.&nbsp;</p>\n<p>Usage Patterns and Market Evolution</p>\n<p>OpenRouter, a unified API gateway that provides access to hundreds of AI models through a single interface, offers a useful lens on DeepSeek’s adoption. Its routing dataset—covering roughly 100 trillion tokens—shows DeepSeek models accounting for around 14.4 trillion tokens between November 2024 and November 2025.&nbsp;</p>\n<p>After the release of DeepSeek V3 and DeepSeek R1, the two together represented more than half of all open-source token traffic on the platform. No other open-weight model family reached comparable concentration during that period.</p>\n<p>Pricing played a significant role in the model’s adoption, as DeepSeek consistently ranked among the lowest-cost options for sustained, high-volume routing. But this dominance peaked around mid-2025 and then declined.&nbsp;</p>\n<p>OpenRouter’s report identifies a clear summer inflexion.&nbsp;</p>\n<p>Source: OpenRouter</p>\n<p>There wasn’t a collapse in absolute usage, but a rapid diversification of the Chinese open-source ecosystem. New releases from Qwen, Moonshot AI (Kimi), MiniMax, and others captured production traffic within weeks.</p>\n<p>And by late 2025, no single open-source model accounted for more than 20–25% of token share. DeepSeek’s earlier flagpole position declined, giving way to a more pluralistic distribution.</p>\n<p>Currently, DeepSeek is going toe-to-toe with other Chinese AI models in absolute benchmarks. But it is in no hurry to recapture the throne.&nbsp;</p>\n<p>The Research Focus</p>\n<p>Rather than accelerating product releases like Western AI labs and Chinese rivals, DeepSeek pivoted towards research, training methods, and infrastructure.&nbsp;</p>\n<p>Post-R1, DeepSeek focused on exploring the nitty-gritty details of an LLM’s architecture and incrementally fixing bottlenecks that hampered compute efficiency. This was in line with the struggles the country faced due to export restrictions the US government placed on NVIDIA for selling GPUs to a Chinese tech company.&nbsp;</p>\n<p>In February 2025, with a five-day open-source initiative, DeepSeek released frameworks focused on execution efficiency, pipeline scheduling, core computation, parallel workload coordination, and large-scale storage. These addressed the bottlenecks that determine whether models can be trained and served cheaply at scale, and were aimed squarely at production engineers.&nbsp;</p>\n<p>Throughout the year, the DeepSeek-R1 also received incremental updates that boosted its performance on benchmarks, and the company also ran numerous research works and experiments on the base model, the DeepSeek-V3.&nbsp;</p>\n<p>Notably, in September, DeepSeek released V3.2-Exp, an experimental model designed to push long-context capabilities while keeping efficiency central, with 3.5x lower prefill costs and up to 10x cheaper decoding during inference for a 128k context window.</p>\n<p>In October, it released DeepSeek-OCR. The model converts text into compact visual tokens, enabling compression ratios of 9–10x with over 96% precision, and around 60% accuracy even at 20xcompression. The work suggested a new efficiency path in which visual modalities are used not for perception but for memory and context optimisation in language models.</p>\n<p>And, in November, it published research on a model that achieved gold-medal-level performance at the International Math Olympiad 2025. It became the only company to achieve the status after OpenAI and Google DeepMind. This model, the DeepSeek-Math-v2, addressed a growing concern in reasoning and math benchmarks, namely that many models arrive at correct answers without sound or inspectable reasoning.&nbsp;</p>\n<p>What Next?&nbsp;</p>\n<p>A recent Microsoft report showed DeepSeek achieving significant market penetration outside China, with about 43% usage in Russia and roughly 56% in Belarus, making these among the highest adoption rates globally. In China, DeepSeek’s share of generative AI usage is approximately 89%.&nbsp;</p>\n<p>Source: Microsoft</p>\n<p>By contrast, adoption in Western Europe and North America remains low, often under 5%. In many African countries, usage is 2–4 times higher than in Western Europe or North America, driven by DeepSeek’s free or low-cost access with minimal subscription barriers, which makes it appealing in price-sensitive markets where Western alternatives are less accessible.&nbsp;</p>\n<p>With growth concentrated in developing regions, how DeepSeek adapts its future models and services to meet these markets’ specific needs will be an important indicator of its global strategy.</p>\n<p>However, the lack of a successor to its R1 model is noteworthy.&nbsp;</p>\n<p>According to a Reuters report in mid-2025, DeepSeek did not release the expected DeepSeek R2 at the anticipated time because the company’s leadership, including founder Liang Wenfeng, was not satisfied with the model’s performance and stability, pushing its launch beyond the originally planned May 2025 date.&nbsp;</p>\n<p>Additional factors included slow data labelling, technical problems tied to hardware choices (such as instability and connectivity issues with Huawei chips that DeepSeek had been encouraged to adopt), which forced the company to prioritise stability by sticking with NVIDIA GPUs for training and using those other chips only for inference.</p>\n<p>Now, DeepSeek is planning to launch a new base model, DeepSeek-v4, with a clear emphasis on coding and math performance, according to The Information. But this time, it will not have the first mover’s advantage in the open-source ecosystem.&nbsp;</p>\n<p>The post DeepSeek’s Rise, Pause, and How Local Competition Took Over appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "49572afc0907",
      "title": "AI is Redrawing VC Rules as Investors Hedge Bets Against Rival Startups",
      "content": "Sequoia Capital’s decision to invest in Anthropic marks a departure from the traditional venture capital rulebook and is being viewed as the death of the ‘one winner’ venture capital model. Historically, VCs have avoided backing rival companies in the same sector, preferring to bet on a single winner. Yet Sequoia—already an investor in OpenAI and Elon Musk’s xAI—has now added Anthropic to its portfolio.\n\n\n\nIt is not alone in breaking this long-standing taboo and hedging bets against its portfolio companies in the same sector. Andreessen Horowitz, which recently raised $15 billion to expand investments across infrastructure, healthcare and defence, has backed OpenAI, xAI, and OpenAI co-founder Ilya Sutskever’s Safe Superintelligence (SSI). Fidelity and Ark Invest have invested in both OpenAI and xAI, while Sound Ventures and Wisdom Ventures both hold stakes in OpenAI and Anthropic.&nbsp;\n\n\n\nSequoia itself invested in OpenAI in 2021 and later backed SSI in September 2024.\n\n\n\nWhile Sequoia’s earlier investment in xAI appeared to contradict the traditional VC approach of choosing a single winner, that move was widely seen as an extension of the VC firm’s long-standing relationship with Elon Musk. It also holds stakes in SpaceX and The Boring Company, and is a major investor in Neuralink.\n\n\n\nSequoia is now joining a $25-billion funding round for Anthropic led by Singapore’s GIC and US investor Coatue, with the AI company seeking a valuation of $350 billion—more than double its $170 billion valuation from just four months ago.\n\n\n\nAI is Changing Investment Strategies\n\n\n\nSequoia’s shift is particularly striking given its earlier stance on portfolio conflicts. In 2020, the firm exited payments startup Finix after concluding it competed with Stripe, another Sequoia-backed company, forfeiting its $21-million stake along with a board seat.\n\n\n\nThe Anthropic investment also follows a leadership transition at Sequoia in 2025, with veteran partner Roelof Botha stepping back from oversight of the US and Europe business, and Alfred Lin and Pat Grady taking charge. This came after a turbulent period marked by internal friction, a $200-million loss from cryptocurrency exchange FTX’s collapse, and a renewed focus on AI.\n\n\n\nSome investors describe this approach as “spray and pray”—once applied to fintech and e-commerce, and now increasingly to AI. A similar pattern is emerging in India. Peak XV Partners (formerly Sequoia Capital India &amp; SEA) has backed Sarvam AI, which is developing a foundation model, while also investing in application-layer companies such as Atlan and WizCommerce, which could eventually build competing in-house models.\n\n\n\nAntler India, one of the country’s most active AI investors, backs multiple agentic AI startups, while Accel’s Atoms programme has funded a wide range of AI tools. Although these firms claim to maintain strict “Chinese walls” between partners to manage conflicts, they are increasingly open to backing multiple companies in the same vertical.\n\n\n\nTraditionally, VCs picked sides—Uber or Lyft—and avoided funding direct rivals to preserve focus and loyalty. The scale and uncertainty of AI, however, have pushed many investors to abandon this rule.&nbsp;\n\n\n\n“In foundational AI, there may not be a single winner,” Akhil Gupta, CTO and co-founder of NoBroker, tells AIM. “The stack is deeper, applications are broader, and variables like regulation, compute access, and talent make outcomes far less predictable.”\n\n\n\nSuhail Sameer, founder and managing partner of OTP Ventures, notes that most Indian funds are investing in AI applications built atop infrastructure developed outside India. “These companies face the risk that core AI providers could outpace them with future upgrades.” While OTP avoids investing in direct competitors from the same fund, Sameer advises founders to avoid raising capital from investors with clear conflicts of interest.\n\n\n\nEthics And Trust\n\n\n\nThe debate over portfolio conflicts intensified after OpenAI CEO Sam Altman testified under oath last year in a lawsuit brought by Elon Musk. Altman acknowledged that investors with access to OpenAI’s confidential information would lose those rights if they made non-passive investments in competitors, describing this as an industry-standard safeguard.\n\n\n\nFor founders, the concern is straightforward. Investors often gain access to sensitive information on strategy, pricing, hiring and product roadmaps, raising fears that insights—deliberately or otherwise—could give an advantage to rival portfolio companies. Even with formal information barriers, perceived conflicts can erode trust and distort boardroom dynamics.\n\n\n\n“When Nexus Venture Partners invested in Snapdeal while backing ShopClues, I was worried,” Sandeep Aggarwal, founder of e-commerce marketplace ShopClues and used vehicles startup Droom, remarks. Although Nexus argued the companies were not direct rivals at the time, Snapdeal later pivoted into a competing marketplace. Aggarwal questions whether a VC can genuinely maintain equal commitment to two rival businesses.\n\n\n\nMeanwhile, Gupta believes conflicts are not inherently unethical but can become problematic if boundaries blur or influence is misused. “From a founder’s standpoint, transparency is critical—knowing upfront what exposure an investor has and what safeguards exist.”\n\n\n\nAshish Fafadia, partner at Blume Ventures, adds that pivots often cause company paths to converge. “VCs must ensure separate partners manage competing investments and maintain near-perfect Chinese walls to prevent information leakage.”\n\n\n\nIf the trend of investing in rival businesses is global, how can founders protect themselves?&nbsp;\n\n\n\n“Founders can mitigate risks by setting guardrails early. This includes negotiating strict conflict-of-interest clauses, limiting information rights if investors back rivals, and enforcing board-level firewalls with recusals from sensitive discussions,” advises Aggarwal. Some founders may also require investors to take passive-only positions in competitors or forfeit information rights altogether if conflicts arise.\n\n\n\nAs capital increasingly concentrates around a handful of AI labs, traditional VCs have to navigate whether backing rivals accelerates innovation or undermines trust and confidentiality that startups cherish.\nThe post AI is Redrawing VC Rules as Investors Hedge Bets Against Rival Startups appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-features/ai-is-redrawing-vc-rules-as-investors-hedge-bets-against-rival-startups/",
      "author": "Pallavi Chakravorty",
      "published": "2026-01-21T11:08:15",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI Features"
      ],
      "summary": "Major VCs including Sequoia Capital and Andreessen Horowitz are breaking traditional rules by investing in competing AI companies. Sequoia now backs OpenAI, xAI, and Anthropic simultaneously.",
      "importance_score": 60.0,
      "reasoning": "Interesting shift in VC strategy reflecting uncertainty about AI market winners. Signals maturation of AI investment landscape.",
      "themes": [
        "Venture Capital",
        "AI Investment",
        "Industry Dynamics"
      ],
      "continuation": null,
      "summary_html": "<p>Major VCs including Sequoia Capital and Andreessen Horowitz are breaking traditional rules by investing in competing AI companies. Sequoia now backs OpenAI, xAI, and Anthropic simultaneously.</p>",
      "content_html": "<p>Sequoia Capital’s decision to invest in Anthropic marks a departure from the traditional venture capital rulebook and is being viewed as the death of the ‘one winner’ venture capital model. Historically, VCs have avoided backing rival companies in the same sector, preferring to bet on a single winner. Yet Sequoia—already an investor in OpenAI and Elon Musk’s xAI—has now added Anthropic to its portfolio.</p>\n<p>It is not alone in breaking this long-standing taboo and hedging bets against its portfolio companies in the same sector. Andreessen Horowitz, which recently raised $15 billion to expand investments across infrastructure, healthcare and defence, has backed OpenAI, xAI, and OpenAI co-founder Ilya Sutskever’s Safe Superintelligence (SSI). Fidelity and Ark Invest have invested in both OpenAI and xAI, while Sound Ventures and Wisdom Ventures both hold stakes in OpenAI and Anthropic.&nbsp;</p>\n<p>Sequoia itself invested in OpenAI in 2021 and later backed SSI in September 2024.</p>\n<p>While Sequoia’s earlier investment in xAI appeared to contradict the traditional VC approach of choosing a single winner, that move was widely seen as an extension of the VC firm’s long-standing relationship with Elon Musk. It also holds stakes in SpaceX and The Boring Company, and is a major investor in Neuralink.</p>\n<p>Sequoia is now joining a $25-billion funding round for Anthropic led by Singapore’s GIC and US investor Coatue, with the AI company seeking a valuation of $350 billion—more than double its $170 billion valuation from just four months ago.</p>\n<p>AI is Changing Investment Strategies</p>\n<p>Sequoia’s shift is particularly striking given its earlier stance on portfolio conflicts. In 2020, the firm exited payments startup Finix after concluding it competed with Stripe, another Sequoia-backed company, forfeiting its $21-million stake along with a board seat.</p>\n<p>The Anthropic investment also follows a leadership transition at Sequoia in 2025, with veteran partner Roelof Botha stepping back from oversight of the US and Europe business, and Alfred Lin and Pat Grady taking charge. This came after a turbulent period marked by internal friction, a $200-million loss from cryptocurrency exchange FTX’s collapse, and a renewed focus on AI.</p>\n<p>Some investors describe this approach as “spray and pray”—once applied to fintech and e-commerce, and now increasingly to AI. A similar pattern is emerging in India. Peak XV Partners (formerly Sequoia Capital India &amp; SEA) has backed Sarvam AI, which is developing a foundation model, while also investing in application-layer companies such as Atlan and WizCommerce, which could eventually build competing in-house models.</p>\n<p>Antler India, one of the country’s most active AI investors, backs multiple agentic AI startups, while Accel’s Atoms programme has funded a wide range of AI tools. Although these firms claim to maintain strict “Chinese walls” between partners to manage conflicts, they are increasingly open to backing multiple companies in the same vertical.</p>\n<p>Traditionally, VCs picked sides—Uber or Lyft—and avoided funding direct rivals to preserve focus and loyalty. The scale and uncertainty of AI, however, have pushed many investors to abandon this rule.&nbsp;</p>\n<p>“In foundational AI, there may not be a single winner,” Akhil Gupta, CTO and co-founder of NoBroker, tells AIM. “The stack is deeper, applications are broader, and variables like regulation, compute access, and talent make outcomes far less predictable.”</p>\n<p>Suhail Sameer, founder and managing partner of OTP Ventures, notes that most Indian funds are investing in AI applications built atop infrastructure developed outside India. “These companies face the risk that core AI providers could outpace them with future upgrades.” While OTP avoids investing in direct competitors from the same fund, Sameer advises founders to avoid raising capital from investors with clear conflicts of interest.</p>\n<p>Ethics And Trust</p>\n<p>The debate over portfolio conflicts intensified after OpenAI CEO Sam Altman testified under oath last year in a lawsuit brought by Elon Musk. Altman acknowledged that investors with access to OpenAI’s confidential information would lose those rights if they made non-passive investments in competitors, describing this as an industry-standard safeguard.</p>\n<p>For founders, the concern is straightforward. Investors often gain access to sensitive information on strategy, pricing, hiring and product roadmaps, raising fears that insights—deliberately or otherwise—could give an advantage to rival portfolio companies. Even with formal information barriers, perceived conflicts can erode trust and distort boardroom dynamics.</p>\n<p>“When Nexus Venture Partners invested in Snapdeal while backing ShopClues, I was worried,” Sandeep Aggarwal, founder of e-commerce marketplace ShopClues and used vehicles startup Droom, remarks. Although Nexus argued the companies were not direct rivals at the time, Snapdeal later pivoted into a competing marketplace. Aggarwal questions whether a VC can genuinely maintain equal commitment to two rival businesses.</p>\n<p>Meanwhile, Gupta believes conflicts are not inherently unethical but can become problematic if boundaries blur or influence is misused. “From a founder’s standpoint, transparency is critical—knowing upfront what exposure an investor has and what safeguards exist.”</p>\n<p>Ashish Fafadia, partner at Blume Ventures, adds that pivots often cause company paths to converge. “VCs must ensure separate partners manage competing investments and maintain near-perfect Chinese walls to prevent information leakage.”</p>\n<p>If the trend of investing in rival businesses is global, how can founders protect themselves?&nbsp;</p>\n<p>“Founders can mitigate risks by setting guardrails early. This includes negotiating strict conflict-of-interest clauses, limiting information rights if investors back rivals, and enforcing board-level firewalls with recusals from sensitive discussions,” advises Aggarwal. Some founders may also require investors to take passive-only positions in competitors or forfeit information rights altogether if conflicts arise.</p>\n<p>As capital increasingly concentrates around a handful of AI labs, traditional VCs have to navigate whether backing rivals accelerates innovation or undermines trust and confidentiality that startups cherish.</p>\n<p>The post AI is Redrawing VC Rules as Investors Hedge Bets Against Rival Startups appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "9b2944f05966",
      "title": "Kioxia's memory is \"sold out\" for 2026, prolonging a \"high-end and expensive phase\"",
      "content": "The companies that make RAM and flash memory chips are enjoying record profits because of the AI-induced memory crunch—and they’re also indicating that they don’t expect conditions to improve much if at all in 2026. And while RAM kits have been hit the fastest and hardest by shortages and price increases, we shouldn't expect SSD pricing to improve any time soon, either.\nThat's the message from Shunsuke Nakato (via PC Gamer), managing director of the memory division of Kioxia, the Japanese memory company that was spun off from Toshiba at the end of the 2010s. Nakato says that Kioxia’s manufacturing capacity is sold out through the rest of 2026, driving the market for both enterprise and consumer SSDs to a “high-end and expensive phase.”\n“There is a sense of crisis that companies will be eliminated the moment they stop investing in AI, so they have no choice but to continue investing,” said Nakato, as reported by the Korean-language publication Digital Daily. Absent a big change in the demand for generative AI data centers, that cycle of investments will keep prices high for the foreseeable future.Read full article\nComments",
      "url": "https://arstechnica.com/gadgets/2026/01/kioxias-memory-is-sold-out-for-2026-prolonging-a-high-end-and-expensive-phase/",
      "author": "Andrew Cunningham",
      "published": "2026-01-21T20:37:00",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "Tech",
        "kioxia",
        "memory shortage",
        "SSDs"
      ],
      "summary": "Kioxia's memory division announces manufacturing capacity is sold out through 2026, driving enterprise and consumer SSDs into a 'high-end and expensive phase.' AI-induced memory demand is creating record profits for memory makers while keeping prices elevated.",
      "importance_score": 58.0,
      "reasoning": "Important infrastructure story affecting AI industry costs, but not a direct frontier AI development. Supply constraints are a secondary concern for AI advancement.",
      "themes": [
        "AI Infrastructure",
        "Hardware Supply"
      ],
      "continuation": null,
      "summary_html": "<p>Kioxia's memory division announces manufacturing capacity is sold out through 2026, driving enterprise and consumer SSDs into a 'high-end and expensive phase.' AI-induced memory demand is creating record profits for memory makers while keeping prices elevated.</p>",
      "content_html": "<p>The companies that make RAM and flash memory chips are enjoying record profits because of the AI-induced memory crunch—and they’re also indicating that they don’t expect conditions to improve much if at all in 2026. And while RAM kits have been hit the fastest and hardest by shortages and price increases, we shouldn't expect SSD pricing to improve any time soon, either.</p>\n<p>That's the message from Shunsuke Nakato (via PC Gamer), managing director of the memory division of Kioxia, the Japanese memory company that was spun off from Toshiba at the end of the 2010s. Nakato says that Kioxia’s manufacturing capacity is sold out through the rest of 2026, driving the market for both enterprise and consumer SSDs to a “high-end and expensive phase.”</p>\n<p>“There is a sense of crisis that companies will be eliminated the moment they stop investing in AI, so they have no choice but to continue investing,” said Nakato, as reported by the Korean-language publication Digital Daily. Absent a big change in the demand for generative AI data centers, that cycle of investments will keep prices high for the foreseeable future.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "170d5e96c3ec",
      "title": "Pro-AI Super PACs Are Already All In on the Midterms",
      "content": "Silicon Valley’s battle against AI regulation is already shaping the next US election cycle.",
      "url": "https://www.wired.com/story/ai-super-pacs-trying-to-influence-midterms/",
      "author": "Maxwell Zeff",
      "published": "2026-01-21T11:30:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Politics",
        "Politics / Global Elections",
        "artificial intelligence",
        "government",
        "elections",
        "Silicon Valley",
        "politics",
        "Vote.AI"
      ],
      "summary": "Pro-AI Super PACs backed by Silicon Valley are already working to influence US midterm elections, focusing on opposing AI regulation measures.",
      "importance_score": 58.0,
      "reasoning": "Relevant policy development but limited detail. Political influence on AI regulation is significant but this is early-stage coverage.",
      "themes": [
        "AI Policy",
        "Politics",
        "Regulation"
      ],
      "continuation": null,
      "summary_html": "<p>Pro-AI Super PACs backed by Silicon Valley are already working to influence US midterm elections, focusing on opposing AI regulation measures.</p>",
      "content_html": "<p>Silicon Valley’s battle against AI regulation is already shaping the next US election cycle.</p>"
    },
    {
      "id": "d66810bbe1ae",
      "title": "Salesforce AI Introduces FOFPred: A Language-Driven Future Optical Flow Prediction Framework that Enables Improved Robot Control and Video Generation",
      "content": "Salesforce AI research team present FOFPred, a language driven future optical flow prediction framework that connects large vision language models with diffusion transformers for dense motion forecasting in control and video generation settings. FOFPred takes one or more images and a natural language instruction such as &#8216;moving the bottle from right to left&#8217; and predicts 4 future optical flow frames that describe how every pixel is expected to move over time. \n\n\n\nhttps://arxiv.org/pdf/2601.10781\n\n\nFuture optical flow as a motion representation\n\n\n\nOptical flow is the apparent per pixel displacement between two frames. FOFPred focuses on future optical flow, which means predicting dense displacement fields for future frames given only current observations and text, without access to future images at inference.\n\n\n\nFuture optical flow is a compact motion only representation. It removes static appearance and keeps only pixel level motion, so it is well suited as an intermediate state for robot control policies and as a conditioning signal for video diffusion models. Compared to predicting future RGB frames, it reduces the complexity of the output distribution and avoids modeling textures and high frequency details that are not required for motion planning. \n\n\n\nTo plug into existing latent diffusion infrastructure, the research team encode optical flow as RGB images. They map flow magnitude and direction from polar form into HSV channels, then convert to RGB. The scaling of each channel is tuned so that consecutive flow frames are visually smooth and resemble animated graphics. A standard Flux.1 variational autoencoder then encodes and decodes these flow images. \n\n\n\nUnified VLM Diffusion backbone\n\n\n\nFOFPred uses a unified architecture that combines a frozen vision language model, a frozen VAE and a trainable diffusion transformer. The pipeline is:\n\n\n\n\nQwen2.5-VL is used as the vision language encoder to jointly encode the caption and visual inputs.\n\n\n\nFlux.1 VAE encodes the input images and the training optical flow targets into latent tensors.\n\n\n\nAn OmniGen style diffusion transformer, DiT, takes projected visual and textual features as conditional inputs and generates latent future flow sequences.\n\n\n\n\nOnly the DiT and small MLP projectors are trained. The Qwen2.5-VL and Flux.1 weights stay frozen, which lets the model reuse image editing pretraining and multimodal reasoning ability from prior work. Temporal modeling is added by extending the RoPE positional encoding and attention blocks from two dimensional spatial positions to full spatio-temporal positions across input and output frame sequences. This gives full spatio-temporal attention without adding extra parameters, so the DiT can reuse OmniGen image pretraining directly. \n\n\n\nhttps://arxiv.org/pdf/2601.10781\n\n\nTraining on noisy web videos with relative optical flow\n\n\n\nThe core model is trained on web scale human activity videos with paired captions. The research team uses the Something Something V2 dataset and the EgoDex egocentric manipulation dataset to obtain around 500,000 video caption pairs.\n\n\n\nTraining uses an end to end flow matching objective in latent space. Future optical flow sequences are first computed offline, then encoded by the VAE and used as targets in a flow matching diffusion loss for the DiT. During training the method also applies classifier free guidance on both text and visual conditions and masks some frames and viewpoints to improve robustness.\n\n\n\nA critical contribution is the relative optical flow calculation used to build clean training targets from noisy egocentric videos. For each frame pair the method:\n\n\n\n\nComputes dense optical flow with an off the shelf estimator.\n\n\n\nEstimates camera motion via homography using deep features.\n\n\n\nUses projective geometry to subtract camera motion and obtain object centric relative flow vectors.\n\n\n\nFilters frame pairs by selecting those where the top k percent flow magnitudes exceed a threshold, which focuses training on segments with meaningful motion. \n\n\n\n\nThese steps are run offline at lower resolution for efficiency, then recomputed at original resolution for the final targets. The ablation study shows that static frame targets or raw flow without camera motion removal harm downstream performance, while disentangled relative flow targets give the best results. \n\n\n\nhttps://arxiv.org/pdf/2601.10781\n\n\nLanguage driven robot manipulation\n\n\n\nThe first downstream use case is robot control. FOFPred is finetuned on robot video caption data to predict future optical flow from both fixed and wrist mounted cameras. On top of FOFPred, the research team attach a diffusion policy network that takes predicted flow, text and robot state, and outputs continuous actions. This setup follows prior diffusion policy work but uses future optical flow instead of predicted RGB frames as the core representation. \n\n\n\nOn the CALVIN ABCD benchmark, which evaluates long horizon zero shot chains of 5 language specified manipulation tasks, FOFPred reaches an average chain length of 4.48. VPP reaches 4.33 and DreamVLA reaches 4.44 under the same protocol. FOFPred also attains a Task 5 success rate of 78.7 percent, which is the best among reported methods. In a low data setting with 10 percent of CALVIN demonstrations, FOFPred still reaches 3.43 average length, higher than the 3.25 of VPP. \n\n\n\nOn RoboTwin 2.0, a dual arm manipulation benchmark with 5 tasks that require both arms, FOFPred attains an average success rate of 68.6 percent. The VPP baseline reaches 61.8 percent under identical training settings. FOFPred improves success on every task in the subset. \n\n\n\nhttps://arxiv.org/pdf/2601.10781\n\n\nMotion aware text to video generation\n\n\n\nThe second downstream task is motion control in text to video generation. The research team build a two stage pipeline by connecting FOFPred with the Go with the Flow video diffusion model. FOFPred takes an initial frame and a language description of motion, predicts a sequence of future flow frames, and interpolates them into a dense motion field. Go with the Flow then uses this motion field and the initial frame to synthesize the final video, enforcing the described motion pattern. \n\n\n\nOn the motion heavy Something Something V2 benchmark, the FOFPred along with Go with the Flow pipeline improves over the CogVideoX baseline under identical conditions. The method reaches SSIM 68.4, PSNR 22.26, LPIPS 28.5, FVD 75.39, KVD 11.38, and motion fidelity 0.662, which are consistently better than CogVideoX. Importantly, FOFPred only uses language and a single frame at inference, while several controllable video baselines require hand or object masks or trajectories as extra inputs.\n\n\n\nhttps://arxiv.org/pdf/2601.10781\n\n\nKey Take aways\n\n\n\n\nFOFPred reframes motion prediction as language driven future optical flow, predicting 4 dense optical flow frames from one or more current images and a text instruction, which provides a compact motion only representation for downstream tasks.\n\n\n\nThe model uses a unified VLM Diffusion backbone, with Qwen2.5-VL as a frozen vision language encoder, Flux.1-VAE as a frozen latent encoder for images and flow, and an OmniGen style DiT as the only trained component with spatio temporal RoPE based attention.\n\n\n\nTraining relies on large scale web and egocentric video from Something Something-V2 and EgoDex, and builds relative optical flow targets by estimating ego-motion via homography, subtracting camera flow and filtering for high motion segments, which significantly improves downstream performance.\n\n\n\nIn robot manipulation, FOFPred acts as a motion backbone for a diffusion policy head and achieves state of the art or better results on CALVIN ABCD and RoboTwin 2.0, including 4.48 average task chain length on CALVIN and 68.6 percent average success on RoboTwin, outperforming VPP and DreamVLA variants.\n\n\n\nFor text to video generation, connecting FOFPred to Go with the Flow yields better SSv2 metrics than CogVideoX, with higher SSIM and PSNR, lower FVD and KVD, and improved motion fidelity, while requiring only language and a single frame at inference, making FOFPred a reusable motion controller for both robotics and video synthesis pipelines.\n\n\n\n\n\n\n\n\nCheck out the Paper, Model and Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Salesforce AI Introduces FOFPred: A Language-Driven Future Optical Flow Prediction Framework that Enables Improved Robot Control and Video Generation appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/21/salesforce-ai-introduces-fofpred-a-language-driven-future-optical-flow-prediction-framework-that-enables-improved-robot-control-and-video-generation/",
      "author": "Michal Sutter",
      "published": "2026-01-21T08:55:38",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Computer Vision",
        "Editors Pick",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Salesforce AI research introduces FOFPred, a framework connecting vision-language models with diffusion transformers for predicting future optical flow from images and natural language instructions.",
      "importance_score": 56.0,
      "reasoning": "Interesting research from major company connecting VLMs with robotics/video generation. Technical advancement but incremental.",
      "themes": [
        "Computer Vision",
        "Robotics",
        "Research"
      ],
      "continuation": null,
      "summary_html": "<p>Salesforce AI research introduces FOFPred, a framework connecting vision-language models with diffusion transformers for predicting future optical flow from images and natural language instructions.</p>",
      "content_html": "<p>Salesforce AI research team present FOFPred, a language driven future optical flow prediction framework that connects large vision language models with diffusion transformers for dense motion forecasting in control and video generation settings. FOFPred takes one or more images and a natural language instruction such as ‘moving the bottle from right to left’ and predicts 4 future optical flow frames that describe how every pixel is expected to move over time.</p>\n<p>https://arxiv.org/pdf/2601.10781</p>\n<p>Future optical flow as a motion representation</p>\n<p>Optical flow is the apparent per pixel displacement between two frames. FOFPred focuses on future optical flow, which means predicting dense displacement fields for future frames given only current observations and text, without access to future images at inference.</p>\n<p>Future optical flow is a compact motion only representation. It removes static appearance and keeps only pixel level motion, so it is well suited as an intermediate state for robot control policies and as a conditioning signal for video diffusion models. Compared to predicting future RGB frames, it reduces the complexity of the output distribution and avoids modeling textures and high frequency details that are not required for motion planning.</p>\n<p>To plug into existing latent diffusion infrastructure, the research team encode optical flow as RGB images. They map flow magnitude and direction from polar form into HSV channels, then convert to RGB. The scaling of each channel is tuned so that consecutive flow frames are visually smooth and resemble animated graphics. A standard Flux.1 variational autoencoder then encodes and decodes these flow images.</p>\n<p>Unified VLM Diffusion backbone</p>\n<p>FOFPred uses a unified architecture that combines a frozen vision language model, a frozen VAE and a trainable diffusion transformer. The pipeline is:</p>\n<p>Qwen2.5-VL is used as the vision language encoder to jointly encode the caption and visual inputs.</p>\n<p>Flux.1 VAE encodes the input images and the training optical flow targets into latent tensors.</p>\n<p>An OmniGen style diffusion transformer, DiT, takes projected visual and textual features as conditional inputs and generates latent future flow sequences.</p>\n<p>Only the DiT and small MLP projectors are trained. The Qwen2.5-VL and Flux.1 weights stay frozen, which lets the model reuse image editing pretraining and multimodal reasoning ability from prior work. Temporal modeling is added by extending the RoPE positional encoding and attention blocks from two dimensional spatial positions to full spatio-temporal positions across input and output frame sequences. This gives full spatio-temporal attention without adding extra parameters, so the DiT can reuse OmniGen image pretraining directly.</p>\n<p>https://arxiv.org/pdf/2601.10781</p>\n<p>Training on noisy web videos with relative optical flow</p>\n<p>The core model is trained on web scale human activity videos with paired captions. The research team uses the Something Something V2 dataset and the EgoDex egocentric manipulation dataset to obtain around 500,000 video caption pairs.</p>\n<p>Training uses an end to end flow matching objective in latent space. Future optical flow sequences are first computed offline, then encoded by the VAE and used as targets in a flow matching diffusion loss for the DiT. During training the method also applies classifier free guidance on both text and visual conditions and masks some frames and viewpoints to improve robustness.</p>\n<p>A critical contribution is the relative optical flow calculation used to build clean training targets from noisy egocentric videos. For each frame pair the method:</p>\n<p>Computes dense optical flow with an off the shelf estimator.</p>\n<p>Estimates camera motion via homography using deep features.</p>\n<p>Uses projective geometry to subtract camera motion and obtain object centric relative flow vectors.</p>\n<p>Filters frame pairs by selecting those where the top k percent flow magnitudes exceed a threshold, which focuses training on segments with meaningful motion.</p>\n<p>These steps are run offline at lower resolution for efficiency, then recomputed at original resolution for the final targets. The ablation study shows that static frame targets or raw flow without camera motion removal harm downstream performance, while disentangled relative flow targets give the best results.</p>\n<p>https://arxiv.org/pdf/2601.10781</p>\n<p>Language driven robot manipulation</p>\n<p>The first downstream use case is robot control. FOFPred is finetuned on robot video caption data to predict future optical flow from both fixed and wrist mounted cameras. On top of FOFPred, the research team attach a diffusion policy network that takes predicted flow, text and robot state, and outputs continuous actions. This setup follows prior diffusion policy work but uses future optical flow instead of predicted RGB frames as the core representation.</p>\n<p>On the CALVIN ABCD benchmark, which evaluates long horizon zero shot chains of 5 language specified manipulation tasks, FOFPred reaches an average chain length of 4.48. VPP reaches 4.33 and DreamVLA reaches 4.44 under the same protocol. FOFPred also attains a Task 5 success rate of 78.7 percent, which is the best among reported methods. In a low data setting with 10 percent of CALVIN demonstrations, FOFPred still reaches 3.43 average length, higher than the 3.25 of VPP.</p>\n<p>On RoboTwin 2.0, a dual arm manipulation benchmark with 5 tasks that require both arms, FOFPred attains an average success rate of 68.6 percent. The VPP baseline reaches 61.8 percent under identical training settings. FOFPred improves success on every task in the subset.</p>\n<p>https://arxiv.org/pdf/2601.10781</p>\n<p>Motion aware text to video generation</p>\n<p>The second downstream task is motion control in text to video generation. The research team build a two stage pipeline by connecting FOFPred with the Go with the Flow video diffusion model. FOFPred takes an initial frame and a language description of motion, predicts a sequence of future flow frames, and interpolates them into a dense motion field. Go with the Flow then uses this motion field and the initial frame to synthesize the final video, enforcing the described motion pattern.</p>\n<p>On the motion heavy Something Something V2 benchmark, the FOFPred along with Go with the Flow pipeline improves over the CogVideoX baseline under identical conditions. The method reaches SSIM 68.4, PSNR 22.26, LPIPS 28.5, FVD 75.39, KVD 11.38, and motion fidelity 0.662, which are consistently better than CogVideoX. Importantly, FOFPred only uses language and a single frame at inference, while several controllable video baselines require hand or object masks or trajectories as extra inputs.</p>\n<p>https://arxiv.org/pdf/2601.10781</p>\n<p>Key Take aways</p>\n<p>FOFPred reframes motion prediction as language driven future optical flow, predicting 4 dense optical flow frames from one or more current images and a text instruction, which provides a compact motion only representation for downstream tasks.</p>\n<p>The model uses a unified VLM Diffusion backbone, with Qwen2.5-VL as a frozen vision language encoder, Flux.1-VAE as a frozen latent encoder for images and flow, and an OmniGen style DiT as the only trained component with spatio temporal RoPE based attention.</p>\n<p>Training relies on large scale web and egocentric video from Something Something-V2 and EgoDex, and builds relative optical flow targets by estimating ego-motion via homography, subtracting camera flow and filtering for high motion segments, which significantly improves downstream performance.</p>\n<p>In robot manipulation, FOFPred acts as a motion backbone for a diffusion policy head and achieves state of the art or better results on CALVIN ABCD and RoboTwin 2.0, including 4.48 average task chain length on CALVIN and 68.6 percent average success on RoboTwin, outperforming VPP and DreamVLA variants.</p>\n<p>For text to video generation, connecting FOFPred to Go with the Flow yields better SSv2 metrics than CogVideoX, with higher SSIM and PSNR, lower FVD and KVD, and improved motion fidelity, while requiring only language and a single frame at inference, making FOFPred a reusable motion controller for both robotics and video synthesis pipelines.</p>\n<p>Check out the&nbsp;Paper, Model and Repo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Salesforce AI Introduces FOFPred: A Language-Driven Future Optical Flow Prediction Framework that Enables Improved Robot Control and Video Generation appeared first on MarkTechPost.</p>"
    },
    {
      "id": "7ce8f770ef89",
      "title": "Adobe Unveils AI Video Innovations, $10 Million Grants Ahead of Sundance Film Festival",
      "content": "Adobe has revealed that it is enhancing video creators’ capabilities through new AI advancements and investments in professional development. Ahead of the Sundance Film Festival, where 85% of all films utilised Adobe products, the company introduced innovations to streamline post-production.\n\n\n\nThese upgrades include AI-enhanced masking in Premiere and new features in After Effects, such as updated typography, materials, and 3D options, that expand motion design and storytelling. Premiere now integrates with Firefly Boards, Adobe&#8217;s AI-powered brainstorming platform, enabling teams to generate concepts using Adobe&#8217;s, Google&#8217;s, and OpenAI&#8217;s leading AI models.\n\n\n\nAdditionally, Adobe is investing $10 million this year to support video professionals from underrepresented communities through the Adobe Film &amp; TV Fund, bringing the total to $20 million since its launch in 2024. The fund offers grants and training opportunities and will showcase films at Sundance while fostering industry partnerships.\n\n\n\nNew integrations between Adobe Firefly and Premiere enhance Adobe Firefly&#8217;s video AI capabilities, an all-in-one creative AI studio that combines leading AI models with top creative tools.\n\n\n\nThese innovations include precision controls for prompt-based edits, camera motion refinement, and the public beta of the Firefly video editor. This lightweight tool allows creators to combine generative clips, footage, graphics, and audio into polished stories directly in the browser. Adobe has also partnered with Runway to deliver next-generation AI video models across its workflows.&nbsp;\n\n\n\nThese advancements enable video professionals to move quickly from concept to edit, providing full creative control and exceptional flexibility.\n\n\n\nAccording to the company, recent updates in Adobe Premiere and After Effects are transforming workflows for video professionals, enabling quicker execution of previously time-intensive tasks.&nbsp;\n\n\n\nThe new Object Selection and Mask tool simplifies selection, enabling faster tracking of complex subjects, while redesigned Shape Masks provide greater creative control for effects like blurring faces or relighting areas of a frame.\n\n\n\nIn After Effects, enhancements such as Native 3D Parametric Meshes allow for the creation of intricate 3D shapes with realistic shadows. Additionally, access to over 1,300 free Substance 3D Materials enhances motion graphics quality, and the Variable Font Animation feature adds dynamism to text elements.&nbsp;\n\n\n\n“We’re thrilled to see so many filmmakers creating their stories with Adobe’s industry-leading tools,&#8221; Deepa Subramaniam, vice president of product marketing and creative professionals at Adobe, said in a statement. “The creative community inspires everything we do, and we’re committed to advancing AI video tools with new innovations and investments for the next generation of storytellers.”\nThe post Adobe Unveils AI Video Innovations, $10 Million Grants Ahead of Sundance Film Festival appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/adobe-unveils-ai-video-innovations-10-million-grants-ahead-of-sundance-film-festival/",
      "author": "Smruthi Nadig",
      "published": "2026-01-21T10:11:36",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News",
        "Adobe",
        "video AI"
      ],
      "summary": "Adobe announced AI video enhancements including AI-enhanced masking in Premiere and new After Effects features ahead of Sundance. The company is investing $10 million to support underrepresented video professionals.",
      "importance_score": 55.0,
      "reasoning": "Product updates from major creative software company. Incremental improvements rather than breakthrough features.",
      "themes": [
        "Adobe",
        "Video AI",
        "Creative Tools"
      ],
      "continuation": null,
      "summary_html": "<p>Adobe announced AI video enhancements including AI-enhanced masking in Premiere and new After Effects features ahead of Sundance. The company is investing $10 million to support underrepresented video professionals.</p>",
      "content_html": "<p>Adobe has revealed that it is enhancing video creators’ capabilities through new AI advancements and investments in professional development. Ahead of the Sundance Film Festival, where 85% of all films utilised Adobe products, the company introduced innovations to streamline post-production.</p>\n<p>These upgrades include AI-enhanced masking in Premiere and new features in After Effects, such as updated typography, materials, and 3D options, that expand motion design and storytelling. Premiere now integrates with Firefly Boards, Adobe’s AI-powered brainstorming platform, enabling teams to generate concepts using Adobe’s, Google’s, and OpenAI’s leading AI models.</p>\n<p>Additionally, Adobe is investing $10 million this year to support video professionals from underrepresented communities through the Adobe Film &amp; TV Fund, bringing the total to $20 million since its launch in 2024. The fund offers grants and training opportunities and will showcase films at Sundance while fostering industry partnerships.</p>\n<p>New integrations between Adobe Firefly and Premiere enhance Adobe Firefly’s video AI capabilities, an all-in-one creative AI studio that combines leading AI models with top creative tools.</p>\n<p>These innovations include precision controls for prompt-based edits, camera motion refinement, and the public beta of the Firefly video editor. This lightweight tool allows creators to combine generative clips, footage, graphics, and audio into polished stories directly in the browser. Adobe has also partnered with Runway to deliver next-generation AI video models across its workflows.&nbsp;</p>\n<p>These advancements enable video professionals to move quickly from concept to edit, providing full creative control and exceptional flexibility.</p>\n<p>According to the company, recent updates in Adobe Premiere and After Effects are transforming workflows for video professionals, enabling quicker execution of previously time-intensive tasks.&nbsp;</p>\n<p>The new Object Selection and Mask tool simplifies selection, enabling faster tracking of complex subjects, while redesigned Shape Masks provide greater creative control for effects like blurring faces or relighting areas of a frame.</p>\n<p>In After Effects, enhancements such as Native 3D Parametric Meshes allow for the creation of intricate 3D shapes with realistic shadows. Additionally, access to over 1,300 free Substance 3D Materials enhances motion graphics quality, and the Variable Font Animation feature adds dynamism to text elements.&nbsp;</p>\n<p>“We’re thrilled to see so many filmmakers creating their stories with Adobe’s industry-leading tools,” Deepa Subramaniam, vice president of product marketing and creative professionals at Adobe, said in a statement. “The creative community inspires everything we do, and we’re committed to advancing AI video tools with new innovations and investments for the next generation of storytellers.”</p>\n<p>The post Adobe Unveils AI Video Innovations, $10 Million Grants Ahead of Sundance Film Festival appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "3ffaa6eb2739",
      "title": "Inworld AI Releases TTS-1.5 For Realtime, Production Grade Voice Agents",
      "content": "Inworld AI has introduced Inworld TTS-1.5, an upgrade to its TTS-1 family that targets realtime voice agents with strict constraints on latency, quality, and cost. TTS-1.5 is described as the number top ranked text to speech system on Artificial Analysis and is designed to be more expressive and more stable than prior generations while remaining suitable for large scale consumer deployments. \n\n\n\nRealtime latency for interactive agents\n\n\n\nTTS-1.5 focuses on P90 time to first audio latency, which is a critical metric for user perceived responsiveness. For TTS-1.5 Max, P90 time to first audio is below 250 ms. For TTS-1.5 Mini, P90 time to first audio is below 130 ms. These values are about 4 times faster than the prior TTS generation according to Inworld.\n\n\n\nThe TTS-1.5 stack supports streaming over WebSocket so synthesis and playback can start as soon as the first audio chunk is generated. In practice this keeps end to end interaction latency in the same range as typical realtime language model responses when models run on modern GPUs, which is important when TTS is part of a full agent pipeline. \n\n\n\nInworld recommends TTS-1.5 Max for most applications because it balances latency near 200 ms with higher stability and quality. TTS-1.5 Mini is positioned for latency sensitive workloads such as real time gaming or ultra responsive voice agents where every millisecond is important. \n\n\n\nExpression, stability and benchmark position\n\n\n\nTTS-1.5 builds on TTS-1 and it delivers about 30 percent more expressive range and about 40 percent better stability than the earlier models. \n\n\n\nHere expression refers to features such as prosody, emphasis, and emotional variation. Stability is measured by metrics such as word error rate and output consistency across long sequences and varied prompts. The reduction in word error rate reduces issues like truncated sentences, unintended word substitutions, or artifacts, which is important when TTS output is driven directly from generated language model text. \n\n\n\nPricing and cost profile at consumer scale\n\n\n\nTTS-1.5 is priced with two main configurations. Inworld TTS-1.5 Mini costs 5 dollars per 1 million characters, which is about 0.005 dollars per minute of speech. TTS-1.5 Max costs 10 dollars per 1 million characters, which is about 0.01 dollars per minute.\n\n\n\nThis cost profile makes it feasible to run TTS continuously in high usage products such as voice native companions, education platforms, or customer support lines without TTS becoming the dominant variable cost.\n\n\n\nMultilingual support, voice cloning and deployment options\n\n\n\nInworld TTS-1.5 supports 15 languages. The list includes English, Spanish, French, Korean, Dutch, Chinese, German, Italian, Japanese, Polish, Portuguese, Russian, Hindi, Arabic, and Hebrew. This allows a single TTS pipeline to cover a wide set of markets without separate models per region.\n\n\n\nThe system provides instant voice cloning and professional voice cloning. Instant voice cloning can create a custom voice from about 15 seconds of audio and is exposed directly in the Inworld portal and through API. Professional voice cloning uses at least 30 minutes of clean audio, with 20 minutes or more recommended for best results, and targets branded voices and less common accents. \n\n\n\nFor deployment, TTS-1.5 is available as a cloud API and also as an on prem solution, where the full model runs inside the customer infrastructure for data sovereignty and compliance. The same quality profile is maintained across both deployment modes, and the models integrate with partner platforms such as LiveKit, Pipecat, and Vapi for end to end voice agent stacks. \n\n\n\nKey Takeaways\n\n\n\n\nInworld TTS 1.5 delivers realtime performance, with P90 time to first audio under 250 ms for the Max model and under 130 ms for the Mini model, about 4 times faster than the prior generation.\n\n\n\nThe model increases expressiveness by about 30 percent and improves stability with about 40 percent lower word error rate.\n\n\n\nPricing is optimized for consumer scale, TTS 1.5 Mini costs about 5 dollars per 1 million characters and TTS 1.5 Max costs about 10 dollars per 1 million characters, which is significantly cheaper per minute than many competing systems.\n\n\n\nTTS 1.5 supports 15 languages and offers instant and professional voice cloning, enabling custom and branded voices from short reference audio or longer recorded datasets.\n\n\n\nThe system is available as a cloud API and as an on prem deployment, and integrates with existing voice agent stacks, which makes it suitable for production realtime agents that require explicit guarantees on latency, quality, and data control.\n\n\n\n\n\n\n\n\nCheck out the Technical details. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Inworld AI Releases TTS-1.5 For Realtime, Production Grade Voice Agents appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/21/inworld-ai-releases-tts-1-5-for-realtime-production-grade-voice-agents/",
      "author": "Asif Razzaq",
      "published": "2026-01-21T23:23:42",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Audio Language Model",
        "Editors Pick",
        "Language Model",
        "New Releases",
        "Sound",
        "Staff",
        "Technology",
        "TTS"
      ],
      "summary": "Inworld AI released TTS-1.5, claiming top ranking on Artificial Analysis for text-to-speech. The system achieves P90 latency under 130ms for Mini and under 250ms for Max versions.",
      "importance_score": 55.0,
      "reasoning": "Solid product release for voice AI but from a smaller player. Useful for real-time applications but not frontier-defining.",
      "themes": [
        "Voice AI",
        "Text-to-Speech",
        "Latency"
      ],
      "continuation": null,
      "summary_html": "<p>Inworld AI released TTS-1.5, claiming top ranking on Artificial Analysis for text-to-speech. The system achieves P90 latency under 130ms for Mini and under 250ms for Max versions.</p>",
      "content_html": "<p>Inworld AI has introduced Inworld TTS-1.5, an upgrade to its TTS-1 family that targets realtime voice agents with strict constraints on latency, quality, and cost. TTS-1.5 is described as the number top ranked text to speech system on Artificial Analysis and is designed to be more expressive and more stable than prior generations while remaining suitable for large scale consumer deployments.</p>\n<p>Realtime latency for interactive agents</p>\n<p>TTS-1.5 focuses on P90 time to first audio latency, which is a critical metric for user perceived responsiveness. For TTS-1.5 Max, P90 time to first audio is below 250 ms. For TTS-1.5 Mini, P90 time to first audio is below 130 ms. These values are about 4 times faster than the prior TTS generation according to Inworld.</p>\n<p>The TTS-1.5 stack supports streaming over WebSocket so synthesis and playback can start as soon as the first audio chunk is generated. In practice this keeps end to end interaction latency in the same range as typical realtime language model responses when models run on modern GPUs, which is important when TTS is part of a full agent pipeline.</p>\n<p>Inworld recommends TTS-1.5 Max for most applications because it balances latency near 200 ms with higher stability and quality. TTS-1.5 Mini is positioned for latency sensitive workloads such as real time gaming or ultra responsive voice agents where every millisecond is important.</p>\n<p>Expression, stability and benchmark position</p>\n<p>TTS-1.5 builds on TTS-1 and it delivers about 30 percent more expressive range and about 40 percent better stability than the earlier models.</p>\n<p>Here expression refers to features such as prosody, emphasis, and emotional variation. Stability is measured by metrics such as word error rate and output consistency across long sequences and varied prompts. The reduction in word error rate reduces issues like truncated sentences, unintended word substitutions, or artifacts, which is important when TTS output is driven directly from generated language model text.</p>\n<p>Pricing and cost profile at consumer scale</p>\n<p>TTS-1.5 is priced with two main configurations. Inworld TTS-1.5 Mini costs 5 dollars per 1 million characters, which is about 0.005 dollars per minute of speech. TTS-1.5 Max costs 10 dollars per 1 million characters, which is about 0.01 dollars per minute.</p>\n<p>This cost profile makes it feasible to run TTS continuously in high usage products such as voice native companions, education platforms, or customer support lines without TTS becoming the dominant variable cost.</p>\n<p>Multilingual support, voice cloning and deployment options</p>\n<p>Inworld TTS-1.5 supports 15 languages. The list includes English, Spanish, French, Korean, Dutch, Chinese, German, Italian, Japanese, Polish, Portuguese, Russian, Hindi, Arabic, and Hebrew. This allows a single TTS pipeline to cover a wide set of markets without separate models per region.</p>\n<p>The system provides instant voice cloning and professional voice cloning. Instant voice cloning can create a custom voice from about 15 seconds of audio and is exposed directly in the Inworld portal and through API. Professional voice cloning uses at least 30 minutes of clean audio, with 20 minutes or more recommended for best results, and targets branded voices and less common accents.</p>\n<p>For deployment, TTS-1.5 is available as a cloud API and also as an on prem solution, where the full model runs inside the customer infrastructure for data sovereignty and compliance. The same quality profile is maintained across both deployment modes, and the models integrate with partner platforms such as LiveKit, Pipecat, and Vapi for end to end voice agent stacks.</p>\n<p>Key Takeaways</p>\n<p>Inworld TTS 1.5 delivers realtime performance, with P90 time to first audio under 250 ms for the Max model and under 130 ms for the Mini model, about 4 times faster than the prior generation.</p>\n<p>The model increases expressiveness by about 30 percent and improves stability with about 40 percent lower word error rate.</p>\n<p>Pricing is optimized for consumer scale, TTS 1.5 Mini costs about 5 dollars per 1 million characters and TTS 1.5 Max costs about 10 dollars per 1 million characters, which is significantly cheaper per minute than many competing systems.</p>\n<p>TTS 1.5 supports 15 languages and offers instant and professional voice cloning, enabling custom and branded voices from short reference audio or longer recorded datasets.</p>\n<p>The system is available as a cloud API and as an on prem deployment, and integrates with existing voice agent stacks, which makes it suitable for production realtime agents that require explicit guarantees on latency, quality, and data control.</p>\n<p>Check out the&nbsp;Technical details.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Inworld AI Releases TTS-1.5 For Realtime, Production Grade Voice Agents appeared first on MarkTechPost.</p>"
    },
    {
      "id": "e45aa3012a41",
      "title": "Deploy agents instantly with Agent Builder templates",
      "content": "LangSmith Agent Builder allows anyone to build an agent with a simple prompt. Ask it to build you a market research agent, and it will follow up with relevant questions to create what you need.But sometimes you want to start with something that&#x2019;s ready to go. Today we&#x2019;re introducing the Agent Builder Template Library and expanding our tool integrations to help you get from idea to working agent even faster.Agent Builder templates are prebuilt agents for common jobs, with tools connected and agent instructions included. They&#x2019;re ready to deploy and fully customizable. You can update your agent&#x2019;s instructions, add tools, and set approval requirements.Unlike traditional workflow automations, you don&#x2019;t need to map every step and spend hours debugging changes. Just give your agent feedback like you would a teammate, and it learns.\n            \n                \n                \n                    \n                        \n                            \n                        \n                    \n                \n                \n                    \n                        \n                            \n                                \n                            \n                        \n                        \n                            \n                                \n                                \n                            \n                        \n                        0:00\n                        \n                            /0:33\n                        \n                        \n                        1&#xd7;\n                        \n                            \n                                \n                            \n                        \n                        \n                            \n                                \n                            \n                        \n                        \n                    \n                \n            \n            \n        We built these templates with the companies who know their domains best, including Tavily, PagerDuty, Exa, Box, and Arcade, and we&apos;re adding new templates regularly. Explore the Template LibraryTry out these agent templates today:Calendar Brief (Google Calendar): Reviews your calendar each morning and sends you a summary with research on meeting participants.Email Assistant (Gmail): Categorizes your emails and drafts replies for your approval.Incident Responder (PagerDuty): Analyzes alerts, cross-references your runbook, and recommends actions.Document intake review (Box): Reviews file submissions and prepares a summary for your approval.Talent sourcing (Exa): Searches LinkedIn based on your job description and sends recommended candidate profiles.Competitor research (Tavily): Conducts deep competitive research and delivers concise reports.Social Media Monitor (X + Slack): Monitors X and sends a daily digest to Slack with the latest news.&#x201c;Agents are a powerful way to turn unstructured content into usable data. Across enterprises, a lot of document work is still manual today: checking completeness, validating accuracy, and extracting context for decision making. By combining the power of Box and Agent Builder, we are making it easy to add an agent to that loop, so teams can focus their time on decisions, not busywork.&#x201d;&#x2014; Ben Kus, CTO, BoxSee what&#x2019;s possible with ArcadeToday, Agent Builder provides a set of ready made tool integrations and templates. However, there are a nearly infinite number of tools your team may want to connect via MCP. Arcade&#x2019;s MCP Gateway makes an additional 8,000 tools available to Agent Builder for use cases spanning marketing, sales, recruiting, customer success, product, engineering, and general productivity.To show what&#x2019;s possible, Arcade developed a collection of 60+ ready-to-deploy Agent Builder templates, available in their own hosted gallery. Each template includes a step-by-step guide to set up and start using your agent. Explore Arcade templates.Choose the best model for your agentWhether you&apos;re starting from a prompt or a template, different jobs may call for different models. Cost, latency, and reasoning requirements vary depending on whether your agent is summarizing emails overnight or responding to questions in real time. That&#x2019;s why Agent Builder doesn&#x2019;t lock you into just one model. It supports OpenAI, Anthropic, and Google Gemini models, plus any custom or open source models that follow OpenAI or Anthropic specs.&#xa0;To show what&#x2019;s possible, Baseten built an agent using their GLM 4.7 model that responds quickly for real-time user interaction. Connect your preferred model provider to Agent Builder, and then you can get building.Turn your idea into a community templateThis is only the beginning and we&apos;re building alongside the community. If you&apos;ve built an agent you love, whether it&apos;s automating sales outreach, monitoring production systems, or conducting research, we&#x2019;d love to hear about it.&#xa0;Join our Community Slack and share it in #agent-builder-templates. We&#x2019;re turning the best community agents into first-class templates.What&apos;s nextWe&#x2019;re just getting started with Agent Builder and learning every day as more people build agents. Try Agent Builder for free today and share what you build in #agent-builder-templates.Get started:Try Agent Builder freeExplore the Template LibraryJoin the Community Slack",
      "url": "https://www.blog.langchain.com/introducing-agent-builder-template-library/",
      "author": "LangChain Accounts",
      "published": "2026-01-21T17:00:00",
      "source": "LangChain Blog",
      "source_type": "rss",
      "tags": [
        "agent builder",
        "agents"
      ],
      "summary": "LangChain introduces Agent Builder Template Library with prebuilt agents and expanded tool integrations. Templates come with tools connected and instructions included, ready for deployment.",
      "importance_score": 55.0,
      "reasoning": "Useful product update from leading agent framework company. Incremental improvement to existing platform.",
      "themes": [
        "LangChain",
        "AI Agents",
        "Developer Tools"
      ],
      "continuation": null,
      "summary_html": "<p>LangChain introduces Agent Builder Template Library with prebuilt agents and expanded tool integrations. Templates come with tools connected and instructions included, ready for deployment.</p>",
      "content_html": "<p>LangSmith Agent Builder allows anyone to build an agent with a simple prompt. Ask it to build you a market research agent, and it will follow up with relevant questions to create what you need.But sometimes you want to start with something that’s ready to go. Today we’re introducing the Agent Builder Template Library and expanding our tool integrations to help you get from idea to working agent even faster.Agent Builder templates are prebuilt agents for common jobs, with tools connected and agent instructions included. They’re ready to deploy and fully customizable. You can update your agent’s instructions, add tools, and set approval requirements.Unlike traditional workflow automations, you don’t need to map every step and spend hours debugging changes. Just give your agent feedback like you would a teammate, and it learns.</p>\n<p>0:00</p>\n<p>/0:33</p>\n<p>1×</p>\n<p>We built these templates with the companies who know their domains best, including Tavily, PagerDuty, Exa, Box, and Arcade, and we're adding new templates regularly. Explore the Template LibraryTry out these agent templates today:Calendar Brief (Google Calendar): Reviews your calendar each morning and sends you a summary with research on meeting participants.Email Assistant (Gmail): Categorizes your emails and drafts replies for your approval.Incident Responder (PagerDuty): Analyzes alerts, cross-references your runbook, and recommends actions.Document intake review (Box): Reviews file submissions and prepares a summary for your approval.Talent sourcing (Exa): Searches LinkedIn based on your job description and sends recommended candidate profiles.Competitor research (Tavily): Conducts deep competitive research and delivers concise reports.Social Media Monitor (X + Slack): Monitors X and sends a daily digest to Slack with the latest news.“Agents are a powerful way to turn unstructured content into usable data. Across enterprises, a lot of document work is still manual today: checking completeness, validating accuracy, and extracting context for decision making. By combining the power of Box and Agent Builder, we are making it easy to add an agent to that loop, so teams can focus their time on decisions, not busywork.”— Ben Kus, CTO, BoxSee what’s possible with ArcadeToday, Agent Builder provides a set of ready made tool integrations and templates. However, there are a nearly infinite number of tools your team may want to connect via MCP. Arcade’s MCP Gateway makes an additional 8,000 tools available to Agent Builder for use cases spanning marketing, sales, recruiting, customer success, product, engineering, and general productivity.To show what’s possible, Arcade developed a collection of 60+ ready-to-deploy Agent Builder templates, available in their own hosted gallery. Each template includes a step-by-step guide to set up and start using your agent. Explore Arcade templates.Choose the best model for your agentWhether you're starting from a prompt or a template, different jobs may call for different models. Cost, latency, and reasoning requirements vary depending on whether your agent is summarizing emails overnight or responding to questions in real time. That’s why Agent Builder doesn’t lock you into just one model. It supports OpenAI, Anthropic, and Google Gemini models, plus any custom or open source models that follow OpenAI or Anthropic specs.&nbsp;To show what’s possible, Baseten built an agent using their GLM 4.7 model that responds quickly for real-time user interaction. Connect your preferred model provider to Agent Builder, and then you can get building.Turn your idea into a community templateThis is only the beginning and we're building alongside the community. If you've built an agent you love, whether it's automating sales outreach, monitoring production systems, or conducting research, we’d love to hear about it.&nbsp;Join our Community Slack and share it in #agent-builder-templates. We’re turning the best community agents into first-class templates.What's nextWe’re just getting started with Agent Builder and learning every day as more people build agents. Try Agent Builder for free today and share what you build in #agent-builder-templates.Get started:Try Agent Builder freeExplore the Template LibraryJoin the Community Slack</p>"
    },
    {
      "id": "86967e7b3f79",
      "title": "Wikipedia volunteers spent years cataloging AI tells. Now there's a plugin to avoid them.",
      "content": "On Saturday, tech entrepreneur Siqi Chen released an open source plugin for Anthropic's Claude Code AI assistant that instructs the AI model to stop writing like an AI model. Called \"Humanizer,\" the simple prompt plugin feeds Claude a list of 24 language and formatting patterns that Wikipedia editors have listed as chatbot giveaways. Chen published the plugin on GitHub, where it has picked up over 1,600 stars as of Monday.\n\"It's really handy that Wikipedia went and collated a detailed list of 'signs of AI writing,'\" Chen wrote on X. \"So much so that you can just tell your LLM to... not do that.\"\nThe source material is a guide from WikiProject AI Cleanup, a group of Wikipedia editors who have been hunting AI-generated articles since late 2023. French Wikipedia editor Ilyas Lebleu founded the project. The volunteers have tagged over 500 articles for review and, in August 2025, published a formal list of the patterns they kept seeing.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/01/new-ai-plugin-uses-wikipedias-ai-writing-detection-rules-to-help-it-sound-human/",
      "author": "Benj Edwards",
      "published": "2026-01-21T12:15:23",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "AI detection",
        "AI detectors",
        "ai slop",
        "AI writing",
        "Anthropic",
        "chatbots",
        "ChatGPT",
        "chatgtp",
        "Claude",
        "generative ai",
        "large language models",
        "machine learning",
        "wikipedia"
      ],
      "summary": "Tech entrepreneur Siqi Chen released 'Humanizer,' an open-source plugin for Claude that instructs it to avoid AI writing patterns identified by Wikipedia editors. The plugin feeds 24 language patterns that WikiProject AI Cleanup has cataloged as chatbot giveaways.",
      "importance_score": 52.0,
      "reasoning": "Interesting cultural development in AI detection/evasion, but not frontier AI news. More about usage patterns than technological advancement.",
      "themes": [
        "AI Detection",
        "AI Writing",
        "Open Source"
      ],
      "continuation": null,
      "summary_html": "<p>Tech entrepreneur Siqi Chen released 'Humanizer,' an open-source plugin for Claude that instructs it to avoid AI writing patterns identified by Wikipedia editors. The plugin feeds 24 language patterns that WikiProject AI Cleanup has cataloged as chatbot giveaways.</p>",
      "content_html": "<p>On Saturday, tech entrepreneur Siqi Chen released an open source plugin for Anthropic's Claude Code AI assistant that instructs the AI model to stop writing like an AI model. Called \"Humanizer,\" the simple prompt plugin feeds Claude a list of 24 language and formatting patterns that Wikipedia editors have listed as chatbot giveaways. Chen published the plugin on GitHub, where it has picked up over 1,600 stars as of Monday.</p>\n<p>\"It's really handy that Wikipedia went and collated a detailed list of 'signs of AI writing,'\" Chen wrote on X. \"So much so that you can just tell your LLM to... not do that.\"</p>\n<p>The source material is a guide from WikiProject AI Cleanup, a group of Wikipedia editors who have been hunting AI-generated articles since late 2023. French Wikipedia editor Ilyas Lebleu founded the project. The volunteers have tagged over 500 articles for review and, in August 2025, published a formal list of the patterns they kept seeing.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "a14e3067f440",
      "title": "Balancing AI cost efficiency with data sovereignty",
      "content": "AI cost efficiency and data sovereignty are at odds, forcing a rethink of enterprise risk frameworks for global organisations.\n\n\n\nFor over a year, the generative AI narrative focused on a race for capability, often measuring success by parameter counts and flawed benchmark scores. Boardroom conversations, however, are undergoing a necessary correction.\n\n\n\nWhile the allure of low-cost, high-performance models offers a tempting path to rapid innovation, the hidden liabilities associated with data residency and state influence are forcing a reassessment of vendor selection. China-based AI laboratory DeepSeek recently became a focal point for this industry-wide debate.\n\n\n\n\n\n\n\nAccording to Bill Conner, former adviser to Interpol and GCHQ, and current CEO of Jitterbit, DeepSeek’s initial reception was positive because it challenged the status quo by demonstrating that “high-performing large language models do not necessarily require Silicon Valley–scale budgets.&#8221;\n\n\n\nFor businesses looking to trim the immense costs associated with generative AI pilots, this efficiency was understandably attractive. Conner observes that these &#8220;reported low training costs undeniably reignited industry conversations around efficiency, optimisation, and ‘good enough’ AI.&#8221;\n\n\n\nAI and data sovereignty risks\n\n\n\nEnthusiasm for cut-price performance has collided with geopolitical realities. Operational efficiency cannot be decoupled from data security, particularly when that data fuels models hosted in jurisdictions with different legal frameworks regarding privacy and state access.\n\n\n\nRecent disclosures regarding DeepSeek have altered the math for Western enterprises. Conner highlights &#8220;recent US government revelations indicating DeepSeek is not only storing data in China but actively sharing it with state intelligence services.&#8221;\n\n\n\nThis disclosure moves the issue beyond standard GDPR or CCPA compliance. The &#8220;risk profile escalates beyond typical privacy concerns into the realm of national security.&#8221;\n\n\n\nFor enterprise leaders, this presents a specific hazard. LLM integration is rarely a standalone event; it involves connecting the model to proprietary data lakes, customer information systems, and intellectual property repositories. If the underlying AI model possesses a &#8220;back door&#8221; or obliges data sharing with a foreign intelligence apparatus, sovereignty is eliminated and the enterprise effectively bypasses its own security perimeter and erases any cost efficiency benefits.\n\n\n\nConner warns that &#8220;DeepSeek&#8217;s entanglement with military procurement networks and alleged export control evasion tactics should serve as a critical warning sign for CEOs, CIOs, and risk officers alike.&#8221; Utilising such technology could inadvertently entangle a company in sanctions violations or supply chain compromises.\n\n\n\nSuccess is no longer just about code generation or document summaries; it is about the provider&#8217;s legal and ethical framework. Especially in industries like finance, healthcare, and defence, tolerance for ambiguity regarding data lineage is zero.\n\n\n\nTechnical teams may prioritise AI performance benchmarks and ease of integration during the proof-of-concept phase, potentially overlooking the geopolitical provenance of the tool and the need for data sovereignty. Risk officers and CIOs must enforce a governance layer that interrogates the &#8220;who&#8221; and &#8220;where&#8221; of the model, not just the &#8220;what.&#8221;\n\n\n\nGovernance over AI cost efficiency\n\n\n\nDeciding to adopt or ban a specific AI model is a matter of corporate responsibility. Shareholders and customers expect that their data remains secure and used solely for intended business purposes.\n\n\n\nConner frames this explicitly for Western leadership, stating that &#8220;for Western CEOs, CIOs, and risk officers, this is not a question of model performance or cost efficiency.&#8221; Instead, &#8220;it is a governance, accountability, and fiduciary responsibility issue.&#8221;\n\n\n\nEnterprises &#8220;cannot justify integrating a system where data residency, usage intent, and state influence are fundamentally opaque.&#8221; This opacity creates an unacceptable liability. Even if a model offers 95 percent of a competitor&#8217;s performance at half the cost, the potential for regulatory fines, reputational damage, and loss of intellectual property erases those savings instantly.\n\n\n\nThe DeepSeek case study serves as a prompt to audit current AI supply chains. Leaders must ensure they have full visibility into where model inference occurs and who holds the keys to the underlying data.&nbsp;\n\n\n\nAs the market for generative AI matures, trust, transparency, and data sovereignty will likely outweigh the appeal of raw cost efficiency.\n\n\n\nSee also: SAP and Fresenius to build sovereign AI backbone for healthcare\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Balancing AI cost efficiency with data sovereignty appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/balancing-ai-cost-efficiency-with-data-sovereignty/",
      "author": "Ryan Daws",
      "published": "2026-01-21T10:51:23",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Features",
        "Governance, Regulation & Policy",
        "Inside AI",
        "Opinion",
        "data",
        "enterprise",
        "governance",
        "privacy",
        "security",
        "sovereignty",
        "strategy"
      ],
      "summary": "Enterprise organizations are reassessing AI vendor selection as data sovereignty concerns conflict with cost efficiency goals. DeepSeek has become a focal point for debates about state influence and data residency risks.",
      "importance_score": 52.0,
      "reasoning": "Analysis piece on enterprise AI considerations. Important strategic topic but more opinion/analysis than breaking news.",
      "themes": [
        "Enterprise AI",
        "Data Sovereignty",
        "DeepSeek"
      ],
      "continuation": null,
      "summary_html": "<p>Enterprise organizations are reassessing AI vendor selection as data sovereignty concerns conflict with cost efficiency goals. DeepSeek has become a focal point for debates about state influence and data residency risks.</p>",
      "content_html": "<p>AI cost efficiency and data sovereignty are at odds, forcing a rethink of enterprise risk frameworks for global organisations.</p>\n<p>For over a year, the generative AI narrative focused on a race for capability, often measuring success by parameter counts and flawed benchmark scores. Boardroom conversations, however, are undergoing a necessary correction.</p>\n<p>While the allure of low-cost, high-performance models offers a tempting path to rapid innovation, the hidden liabilities associated with data residency and state influence are forcing a reassessment of vendor selection. China-based AI laboratory DeepSeek recently became a focal point for this industry-wide debate.</p>\n<p>According to Bill Conner, former adviser to Interpol and GCHQ, and current CEO of Jitterbit, DeepSeek’s initial reception was positive because it challenged the status quo by demonstrating that “high-performing large language models do not necessarily require Silicon Valley–scale budgets.”</p>\n<p>For businesses looking to trim the immense costs associated with generative AI pilots, this efficiency was understandably attractive. Conner observes that these “reported low training costs undeniably reignited industry conversations around efficiency, optimisation, and ‘good enough’ AI.”</p>\n<p>AI and data sovereignty risks</p>\n<p>Enthusiasm for cut-price performance has collided with geopolitical realities. Operational efficiency cannot be decoupled from data security, particularly when that data fuels models hosted in jurisdictions with different legal frameworks regarding privacy and state access.</p>\n<p>Recent disclosures regarding DeepSeek have altered the math for Western enterprises. Conner highlights “recent US government revelations indicating DeepSeek is not only storing data in China but actively sharing it with state intelligence services.”</p>\n<p>This disclosure moves the issue beyond standard GDPR or CCPA compliance. The “risk profile escalates beyond typical privacy concerns into the realm of national security.”</p>\n<p>For enterprise leaders, this presents a specific hazard. LLM integration is rarely a standalone event; it involves connecting the model to proprietary data lakes, customer information systems, and intellectual property repositories. If the underlying AI model possesses a “back door” or obliges data sharing with a foreign intelligence apparatus, sovereignty is eliminated and the enterprise effectively bypasses its own security perimeter and erases any cost efficiency benefits.</p>\n<p>Conner warns that “DeepSeek’s entanglement with military procurement networks and alleged export control evasion tactics should serve as a critical warning sign for CEOs, CIOs, and risk officers alike.” Utilising such technology could inadvertently entangle a company in sanctions violations or supply chain compromises.</p>\n<p>Success is no longer just about code generation or document summaries; it is about the provider’s legal and ethical framework. Especially in industries like finance, healthcare, and defence, tolerance for ambiguity regarding data lineage is zero.</p>\n<p>Technical teams may prioritise AI performance benchmarks and ease of integration during the proof-of-concept phase, potentially overlooking the geopolitical provenance of the tool and the need for data sovereignty. Risk officers and CIOs must enforce a governance layer that interrogates the “who” and “where” of the model, not just the “what.”</p>\n<p>Governance over AI cost efficiency</p>\n<p>Deciding to adopt or ban a specific AI model is a matter of corporate responsibility. Shareholders and customers expect that their data remains secure and used solely for intended business purposes.</p>\n<p>Conner frames this explicitly for Western leadership, stating that “for Western CEOs, CIOs, and risk officers, this is not a question of model performance or cost efficiency.” Instead, “it is a governance, accountability, and fiduciary responsibility issue.”</p>\n<p>Enterprises “cannot justify integrating a system where data residency, usage intent, and state influence are fundamentally opaque.” This opacity creates an unacceptable liability. Even if a model offers 95 percent of a competitor’s performance at half the cost, the potential for regulatory fines, reputational damage, and loss of intellectual property erases those savings instantly.</p>\n<p>The DeepSeek case study serves as a prompt to audit current AI supply chains. Leaders must ensure they have full visibility into where model inference occurs and who holds the keys to the underlying data.&nbsp;</p>\n<p>As the market for generative AI matures, trust, transparency, and data sovereignty will likely outweigh the appeal of raw cost efficiency.</p>\n<p>See also: SAP and Fresenius to build sovereign AI backbone for healthcare</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Balancing AI cost efficiency with data sovereignty appeared first on AI News.</p>"
    },
    {
      "id": "57304b48a4ed",
      "title": "Building Multi-Agent Applications with Deep Agents",
      "content": "By Sydney Runkle and Vivek TrendyBreaking down complex tasks across specialized agents is one of the most effective approaches to building capable AI systems.Deep Agents makes this easy with two first-class primitives:subagents: delegating to isolated agentsskills: progressively disclosing capabilitiesIn this post, we&apos;ll show you how to build multi-agent systems with Deep Agents.Using Subagents: Specialized, Isolated WorkersSubagents tackle a fundamental problem in agent engineering: context bloat. This is when an agent&#x2019;s context window becomes close to full as it works on a task.Why is this important? There&#x2019;s great work from Chroma on context rot showing that models struggle to complete tasks as their context window gets filled. Our friends at HumanLayer call this high context regime the &#x201c;dumb zone&#x201d;. Subagents isolate context from the main agent to help avoid quickly entering the dumb zone.When your agent makes dozens of web searches or file reads, the context window fills with intermediate results. Subagents isolate work by running with their own context window. So if the subagent is doing a lot of exploratory work before coming with its final answer, the main agent still only gets the final result, not the 20 tool calls that produced it.Here&#x2019;s a look at the basic subagents architecture:When to Use SubagentsContext Preservation: A task requiring multiple steps can clutter the main agent&apos;s context (ex: codebase exploration).Specialization: Use domain specific instructions or tools. Subagents developed by distinct teams can specialize in different verticals.Multi-Model: Subagents can use different models than the main agent. For example, choosing a smaller model for lower latency.Parallelization: Subagents can run simultaneously and return their outputs to the main agent. This reduces latency.Creating SubagentsDefine subagents as dictionaries and pass them to create_deep_agent():from deepagents import create_deep_agent\n\nresearch_subagent = {\n    &quot;name&quot;: &quot;research-agent&quot;,\n    &quot;description&quot;: &quot;Used to research more in depth questions&quot;,\n    &quot;system_prompt&quot;: &quot;You are a great researcher&quot;,\n    &quot;tools&quot;: [internet_search],\n    &quot;model&quot;: &quot;openai:gpt-4o&quot;,  # Optional: override main agent model\n}\n\nagent = create_deep_agent(\n    model=&quot;claude-sonnet-4-5-20250929&quot;,\n    subagents=[research_subagent]\n)\nSee the subagents documentation for configuration details.The General-Purpose SubagentDeep Agents include a built-in general-purpose subagent that mirrors your main agent&apos;s capabilities. It has the same system prompt, tools, and model. This is perfect for context isolation without specialized behavior.Example: Instead of your main agent making 10 web searches and filling its context, it can delegate to the general-purpose subagent with task(name=&quot;general-purpose&quot;, task=&quot;Research quantum computing trends&quot;). The subagent performs all searches internally and returns only a summary.Best Practices for SubagentsWrite clear descriptions. Your main agent uses descriptions to decide which subagent to call:&#x2705; Good: &quot;Analyzes financial data and generates investment insights with confidence scores&quot; &#x274c; Bad: &quot;Does finance stuff&quot;Keep system prompts detailed. Include tool usage guidance and output format requirements:research_subagent = {\n    &quot;name&quot;: &quot;research-agent&quot;,\n    &quot;description&quot;: &quot;Conducts in-depth research using web search and synthesizes findings&quot;,\n    &quot;system_prompt&quot;: &quot;&quot;&quot;You are a thorough researcher. Your job is to:\n\n    1. Break down the research question into searchable queries\n    2. Use internet_search to find relevant information\n    3. Synthesize findings into a comprehensive but concise summary\n    4. Cite sources when making claims\n\n    Output format:\n    - Summary (2-3 paragraphs)\n    - Key findings (bullet points)\n    - Sources (with URLs)\n\n    Keep your response under 500 words to maintain clean context.&quot;&quot;&quot;,\n    &quot;tools&quot;: [internet_search],\n}\nMinimize tool sets. Only give subagents the tools they need:# &#x2705; Good: Focused tool set\nemail_agent = {\n    &quot;name&quot;: &quot;email-sender&quot;,\n    # Only email-related\n    &quot;tools&quot;: [send_email, validate_email],\n}\n\n# &#x274c; Bad: Too many tools\nemail_agent = {\n    &quot;name&quot;: &quot;email-sender&quot;,\n    # Unfocused\n    &quot;tools&quot;: [send_email, web_search, database_query, file_upload],\n}\nUsing Skills: Progressive Disclosure of CapabilitiesSkills provide a different pattern: progressive disclosure. Instead of giving your agent dozens of tools upfront, you define specialized capabilities in SKILL.md files. Your agent sees skill names and descriptions, then reads the full instructions only when needed.Skill descriptions are pre-loaded into the context window. The skill body is only loaded when the agent decides the skill is needed based on the description and previous context.Caption: skill descriptions are pre-loaded into the context window. The skill body is only loaded when the agent decides the skill is needed based on the description and previous context.Setting Up SkillsSkills use the agentskills.io spec. Here&apos;s the structure:.deepagents/skills/\n&#x251c;&#x2500;&#x2500; deploy/SKILL.md\n&#x2514;&#x2500;&#x2500; review-pr/SKILL.md\nEach SKILL.md file has YAML frontmatter with metadata and a main body:---\nname: deploy\ndescription: Deploy to production\nversion: 1.0.0  # Optional\ntags: [deployment, production]  # Optional\n---\n\n# Deploy to Production\n\nWhen the user asks to deploy, follow these steps:\n\n1. Run tests: `npm test`\n2. Build the application: `npm run build`\n3. Deploy to production: `npm run deploy:prod`\n4. Verify deployment: Check the health endpoint\n\nAlways confirm with the user before deploying to production.\nAdding Skills to Your AgentUse the skills argument to create_deep_agent to load skills from the filesystem:from deepagents import create_deep_agent\nfrom deepagents.backends import FilesystemBackend\n\nagent = create_deep_agent(\n    model=&quot;claude-sonnet-4-5-20250929&quot;,\n    backend=FilesystemBackend(root_dir=&quot;/&quot;),\n    skills=[&quot;.deepagents/skills&quot;],\n)\nThe agent now sees your skills. When it needs detailed instructions, it reads the full SKILL.md file.You can also use other backends (such as a StateBackend or StoreBackend), then invoke the agent with a files specification:from deepagents.middleware.filesystem import FileData\n\n# default backend is a StateBackend\nagent = create_deep_agent(\n    model=&quot;anthropic:claude-sonnet-4-20250514&quot;,\n    skills=[&quot;/skills/&quot;],\n)\n\nskill_content = &quot;&quot;&quot;\n---\nname: deploy\n...\n&quot;&quot;&quot;\n\n# Invoke the agent with the skill and virtual files\nresult = agent.invoke({\n    &quot;messages&quot;: [HumanMessage(content=&quot;Research the latest Python releases&quot;)],\n    &quot;files&quot;: {\n        &quot;/skills/web-research/SKILL.md&quot;: FileData(\n            content=skill_content.split(&quot;\\n&quot;),\n            created_at=&quot;2024-01-01T00:00:00Z&quot;,\n            modified_at=&quot;2024-01-01T00:00:00Z&quot;,\n        ),\n    },\n})\nChoosing the Right PatternHere&#x2019;s a quick set of questions to guide you:\n\n\n\n\nWhen you need to...\nUse...\nBoth?\n\n\n\n\nDelegate complex, multi-step work\nSubagents for context isolation\n\n\n\nReuse procedures or instructions\nSkills for progressive disclosure\n\n\n\nProvide specialized tools for specific tasks\nSubagents with focused tool sets\n&#x2705;\n\n\nShare capabilities across multiple agents\nSkills (they&#x2019;re just files)\n&#x2705;\n\n\nWork with large tool sets\nSkills to avoid token bloat\n\n\n\n\n\n&#x1f4a1;Note, this doesn&#x2019;t have to be an either-or decision.Many systems use both. Skills define procedures; subagents execute complex multi-step work. Your subagents can use skills to effectively manage their context windows!Next StepsTo learn more about multi-agent patterns in Deep Agents, check out our:Subagents documentation - Detailed API reference and examplesSkills documentation - Detailed API reference and examplesMulti-agent patterns guide - General guidance on choosing patternsThe key insight: multi-agent patterns don&apos;t have to be complicated. With the right abstractions (middleware for plumbing, tool calling for invocation), they become simple building blocks you can compose into capable, sophisticated systems.Start with subagents for context management, add skills for progressive disclosure, and build from there.",
      "url": "https://www.blog.langchain.com/building-multi-agent-applications-with-deep-agents/",
      "author": "Sydney Runkle",
      "published": "2026-01-21T16:30:00",
      "source": "LangChain Blog",
      "source_type": "rss",
      "tags": [
        "deep agents",
        "agents",
        "Engineering"
      ],
      "summary": "LangChain details Deep Agents architecture using subagents and skills primitives to address context bloat in complex AI systems. Subagents provide isolated workers while skills enable progressive capability disclosure.",
      "importance_score": 52.0,
      "reasoning": "Technical deep-dive on agent architecture from leading framework. Engineering content rather than breakthrough news.",
      "themes": [
        "Multi-Agent Systems",
        "LangChain",
        "Architecture"
      ],
      "continuation": null,
      "summary_html": "<p>LangChain details Deep Agents architecture using subagents and skills primitives to address context bloat in complex AI systems. Subagents provide isolated workers while skills enable progressive capability disclosure.</p>",
      "content_html": "<p>By Sydney Runkle and Vivek TrendyBreaking down complex tasks across specialized agents is one of the most effective approaches to building capable AI systems.Deep Agents makes this easy with two first-class primitives:subagents: delegating to isolated agentsskills: progressively disclosing capabilitiesIn this post, we'll show you how to build multi-agent systems with Deep Agents.Using Subagents: Specialized, Isolated WorkersSubagents tackle a fundamental problem in agent engineering: context bloat. This is when an agent’s context window becomes close to full as it works on a task.Why is this important? There’s great work from Chroma on context rot showing that models struggle to complete tasks as their context window gets filled. Our friends at HumanLayer call this high context regime the “dumb zone”. Subagents isolate context from the main agent to help avoid quickly entering the dumb zone.When your agent makes dozens of web searches or file reads, the context window fills with intermediate results. Subagents isolate work by running with their own context window. So if the subagent is doing a lot of exploratory work before coming with its final answer, the main agent still only gets the final result, not the 20 tool calls that produced it.Here’s a look at the basic subagents architecture:When to Use SubagentsContext Preservation: A task requiring multiple steps can clutter the main agent's context (ex: codebase exploration).Specialization: Use domain specific instructions or tools. Subagents developed by distinct teams can specialize in different verticals.Multi-Model: Subagents can use different models than the main agent. For example, choosing a smaller model for lower latency.Parallelization: Subagents can run simultaneously and return their outputs to the main agent. This reduces latency.Creating SubagentsDefine subagents as dictionaries and pass them to create_deep_agent():from deepagents import create_deep_agent</p>\n<p>research_subagent = {</p>\n<p>\"name\": \"research-agent\",</p>\n<p>\"description\": \"Used to research more in depth questions\",</p>\n<p>\"system_prompt\": \"You are a great researcher\",</p>\n<p>\"tools\": [internet_search],</p>\n<p>\"model\": \"openai:gpt-4o\",  # Optional: override main agent model</p>\n<p>}</p>\n<p>agent = create_deep_agent(</p>\n<p>model=\"claude-sonnet-4-5-20250929\",</p>\n<p>subagents=[research_subagent]</p>\n<p>)</p>\n<p>See the subagents documentation for configuration details.The General-Purpose SubagentDeep Agents include a built-in general-purpose subagent that mirrors your main agent's capabilities. It has the same system prompt, tools, and model. This is perfect for context isolation without specialized behavior.Example: Instead of your main agent making 10 web searches and filling its context, it can delegate to the general-purpose subagent with task(name=\"general-purpose\", task=\"Research quantum computing trends\"). The subagent performs all searches internally and returns only a summary.Best Practices for SubagentsWrite clear descriptions. Your main agent uses descriptions to decide which subagent to call:✅ Good: \"Analyzes financial data and generates investment insights with confidence scores\" ❌ Bad: \"Does finance stuff\"Keep system prompts detailed. Include tool usage guidance and output format requirements:research_subagent = {</p>\n<p>\"name\": \"research-agent\",</p>\n<p>\"description\": \"Conducts in-depth research using web search and synthesizes findings\",</p>\n<p>\"system_prompt\": \"\"\"You are a thorough researcher. Your job is to:</p>\n<p>1. Break down the research question into searchable queries</p>\n<p>2. Use internet_search to find relevant information</p>\n<p>3. Synthesize findings into a comprehensive but concise summary</p>\n<p>4. Cite sources when making claims</p>\n<p>Output format:</p>\n<ul>\n<li>Summary (2-3 paragraphs)</li>\n<li>Key findings (bullet points)</li>\n<li>Sources (with URLs)</li>\n</ul>\n<p>Keep your response under 500 words to maintain clean context.\"\"\",</p>\n<p>\"tools\": [internet_search],</p>\n<p>}</p>\n<p>Minimize tool sets. Only give subagents the tools they need:# ✅ Good: Focused tool set</p>\n<p>email_agent = {</p>\n<p>\"name\": \"email-sender\",</p>\n<p># Only email-related</p>\n<p>\"tools\": [send_email, validate_email],</p>\n<p>}</p>\n<p># ❌ Bad: Too many tools</p>\n<p>email_agent = {</p>\n<p>\"name\": \"email-sender\",</p>\n<p># Unfocused</p>\n<p>\"tools\": [send_email, web_search, database_query, file_upload],</p>\n<p>}</p>\n<p>Using Skills: Progressive Disclosure of CapabilitiesSkills provide a different pattern: progressive disclosure. Instead of giving your agent dozens of tools upfront, you define specialized capabilities in SKILL.md files. Your agent sees skill names and descriptions, then reads the full instructions only when needed.Skill descriptions are pre-loaded into the context window. The skill body is only loaded when the agent decides the skill is needed based on the description and previous context.Caption: skill descriptions are pre-loaded into the context window. The skill body is only loaded when the agent decides the skill is needed based on the description and previous context.Setting Up SkillsSkills use the agentskills.io spec. Here's the structure:.deepagents/skills/</p>\n<p>├── deploy/SKILL.md</p>\n<p>└── review-pr/SKILL.md</p>\n<p>Each SKILL.md file has YAML frontmatter with metadata and a main body:---</p>\n<p>name: deploy</p>\n<p>description: Deploy to production</p>\n<p>version: 1.0.0  # Optional</p>\n<p>tags: [deployment, production]  # Optional</p>\n<p>---</p>\n<p># Deploy to Production</p>\n<p>When the user asks to deploy, follow these steps:</p>\n<p>1. Run tests: `npm test`</p>\n<p>2. Build the application: `npm run build`</p>\n<p>3. Deploy to production: `npm run deploy:prod`</p>\n<p>4. Verify deployment: Check the health endpoint</p>\n<p>Always confirm with the user before deploying to production.</p>\n<p>Adding Skills to Your AgentUse the skills argument to create_deep_agent to load skills from the filesystem:from deepagents import create_deep_agent</p>\n<p>from deepagents.backends import FilesystemBackend</p>\n<p>agent = create_deep_agent(</p>\n<p>model=\"claude-sonnet-4-5-20250929\",</p>\n<p>backend=FilesystemBackend(root_dir=\"/\"),</p>\n<p>skills=[\".deepagents/skills\"],</p>\n<p>)</p>\n<p>The agent now sees your skills. When it needs detailed instructions, it reads the full SKILL.md file.You can also use other backends (such as a StateBackend or StoreBackend), then invoke the agent with a files specification:from deepagents.middleware.filesystem import FileData</p>\n<p># default backend is a StateBackend</p>\n<p>agent = create_deep_agent(</p>\n<p>model=\"anthropic:claude-sonnet-4-20250514\",</p>\n<p>skills=[\"/skills/\"],</p>\n<p>)</p>\n<p>skill_content = \"\"\"</p>\n<p>---</p>\n<p>name: deploy</p>\n<p>...</p>\n<p>\"\"\"</p>\n<p># Invoke the agent with the skill and virtual files</p>\n<p>result = agent.invoke({</p>\n<p>\"messages\": [HumanMessage(content=\"Research the latest Python releases\")],</p>\n<p>\"files\": {</p>\n<p>\"/skills/web-research/SKILL.md\": FileData(</p>\n<p>content=skill_content.split(\"\\n\"),</p>\n<p>created_at=\"2024-01-01T00:00:00Z\",</p>\n<p>modified_at=\"2024-01-01T00:00:00Z\",</p>\n<p>),</p>\n<p>},</p>\n<p>})</p>\n<p>Choosing the Right PatternHere’s a quick set of questions to guide you:</p>\n<p>When you need to...</p>\n<p>Use...</p>\n<p>Both?</p>\n<p>Delegate complex, multi-step work</p>\n<p>Subagents for context isolation</p>\n<p>Reuse procedures or instructions</p>\n<p>Skills for progressive disclosure</p>\n<p>Provide specialized tools for specific tasks</p>\n<p>Subagents with focused tool sets</p>\n<p>✅</p>\n<p>Share capabilities across multiple agents</p>\n<p>Skills (they’re just files)</p>\n<p>✅</p>\n<p>Work with large tool sets</p>\n<p>Skills to avoid token bloat</p>\n<p>💡Note, this doesn’t have to be an either-or decision.Many systems use both. Skills define procedures; subagents execute complex multi-step work. Your subagents can use skills to effectively manage their context windows!Next StepsTo learn more about multi-agent patterns in Deep Agents, check out our:Subagents documentation - Detailed API reference and examplesSkills documentation - Detailed API reference and examplesMulti-agent patterns guide - General guidance on choosing patternsThe key insight: multi-agent patterns don't have to be complicated. With the right abstractions (middleware for plumbing, tool calling for invocation), they become simple building blocks you can compose into capable, sophisticated systems.Start with subagents for context management, add skills for progressive disclosure, and build from there.</p>"
    },
    {
      "id": "d1bdd2f03e05",
      "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
      "content": "",
      "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
      "author": "Unknown",
      "published": "2026-01-21T06:25:31",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "IBM Research introduces AssetOpsBench, a benchmark for evaluating AI agents in industrial asset operations scenarios.",
      "importance_score": 48.0,
      "reasoning": "New benchmark from IBM Research but limited content provided. Industrial AI focus is niche.",
      "themes": [
        "Benchmarks",
        "Industrial AI",
        "IBM"
      ],
      "continuation": null,
      "summary_html": "<p>IBM Research introduces AssetOpsBench, a benchmark for evaluating AI agents in industrial asset operations scenarios.</p>",
      "content_html": ""
    },
    {
      "id": "117a58879e4c",
      "title": "A Coding Guide to Anemoi-Style Semi-Centralized Agentic Systems Using Peer-to-Peer Critic Loops in LangGraph",
      "content": "In this tutorial, we demonstrate how a semi-centralized Anemoi-style multi-agent system works by letting two peer agents negotiate directly without a manager or supervisor. We show how a Drafter and a Critic iteratively refine an output through peer-to-peer feedback, reducing coordination overhead while preserving quality. We implement this pattern end-to-end in Colab using LangGraph, focusing on clarity, control flow, and practical execution rather than abstract orchestration theory. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U langgraph langchain-openai langchain-core\n\n\nimport os\nimport json\nfrom getpass import getpass\nfrom typing import TypedDict\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END\n\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n   os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY (hidden): \")\n\n\nMODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\nllm = ChatOpenAI(model=MODEL, temperature=0.2)\n\n\n\nWe set up the Colab environment by installing the required LangGraph and LangChain packages and securely collecting the OpenAI API key as a hidden input. We initialize the language model that will be shared by all agents, keeping the configuration minimal and reproducible. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass AnemoiState(TypedDict):\n   task: str\n   max_rounds: int\n   round: int\n   draft: str\n   critique: str\n   agreed: bool\n   final: str\n   trace: bool\n\n\n\nWe define a typed state that acts as the shared communication surface between agents during negotiation. We explicitly track the task, draft, critique, agreement flag, and iteration count to keep the flow transparent and debuggable. This state obviates the need for a central manager or for implicit memory. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different BrowserDRAFTER_SYSTEM = \"\"\"You are Agent A (Drafter) in a peer-to-peer loop.\nYou write a high-quality solution to the user's task.\nIf you receive critique, you revise decisively and incorporate it.\nReturn only the improved draft text.\"\"\"\n\n\ndef drafter_node(state: AnemoiState) -> AnemoiState:\n   task = state[\"task\"]\n   critique = state.get(\"critique\", \"\").strip()\n   r = state.get(\"round\", 0) + 1\n\n\n   if critique:\n       user_msg = f\"\"\"TASK:\n{task}\n\n\nCRITIQUE:\n{critique}\n\n\nRevise the draft.\"\"\"\n   else:\n       user_msg = f\"\"\"TASK:\n{task}\n\n\nWrite the first draft.\"\"\"\n\n\n   draft = llm.invoke(\n       [\n           {\"role\": \"system\", \"content\": DRAFTER_SYSTEM},\n           {\"role\": \"user\", \"content\": user_msg},\n       ]\n   ).content.strip()\n\n\n   if state.get(\"trace\", False):\n       print(f\"\\n--- Drafter Round {r} ---\\n{draft}\\n\")\n\n\n   return {**state, \"round\": r, \"draft\": draft, \"agreed\": False}\n\n\n\nWe implement the Drafter agent, which produces the initial response and revises it whenever peer feedback is available. We keep the Drafter focused purely on improving the user-facing draft, without awareness of control logic or termination conditions. It mirrors the Anemoi idea of agents optimizing locally while observing peer signals. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different BrowserCRITIC_SYSTEM = \"\"\"You are Agent B (Critic).\nReturn strict JSON:\n{\"agree\": true/false, \"critique\": \"...\"}\"\"\"\n\n\ndef critic_node(state: AnemoiState) -> AnemoiState:\n   task = state[\"task\"]\n   draft = state.get(\"draft\", \"\")\n\n\n   raw = llm.invoke(\n       [\n           {\"role\": \"system\", \"content\": CRITIC_SYSTEM},\n           {\n               \"role\": \"user\",\n               \"content\": f\"TASK:\\n{task}\\n\\nDRAFT:\\n{draft}\",\n           },\n       ]\n   ).content.strip()\n\n\n   cleaned = raw.strip(\"```\").replace(\"json\", \"\").strip()\n\n\n   try:\n       data = json.loads(cleaned)\n       agree = bool(data.get(\"agree\", False))\n       critique = str(data.get(\"critique\", \"\")).strip()\n   except Exception:\n       agree = False\n       critique = raw\n\n\n   if state.get(\"trace\", False):\n       print(f\"--- Critic Decision ---\\nAGREE: {agree}\\n{critique}\\n\")\n\n\n   final = draft if agree else state.get(\"final\", \"\")\n   return {**state, \"agreed\": agree, \"critique\": critique, \"final\": final}\n\n\n\nWe implement the Critic agent, which evaluates the draft and decides whether it is ready to ship or needs revision. We enforce a strict agree-or-revise decision to avoid vague feedback and ensure fast convergence. This peer evaluation step allows quality control without introducing a supervisory agent. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef continue_or_end(state: AnemoiState) -> str:\n   if state.get(\"agreed\", False):\n       return \"end\"\n   if state.get(\"round\", 0) >= state.get(\"max_rounds\", 3):\n       return \"force_ship\"\n   return \"loop\"\n\n\ndef force_ship_node(state: AnemoiState) -> AnemoiState:\n   return {**state, \"final\": state.get(\"final\") or state.get(\"draft\", \"\")}\n\n\ngraph = StateGraph(AnemoiState)\ngraph.add_node(\"drafter\", drafter_node)\ngraph.add_node(\"critic\", critic_node)\ngraph.add_node(\"force_ship\", force_ship_node)\n\n\ngraph.set_entry_point(\"drafter\")\ngraph.add_edge(\"drafter\", \"critic\")\ngraph.add_conditional_edges(\n   \"critic\",\n   continue_or_end,\n   {\"loop\": \"drafter\", \"force_ship\": \"force_ship\", \"end\": END},\n)\ngraph.add_edge(\"force_ship\", END)\n\n\nanemoi_critic_loop = graph.compile()\n\n\ndemo_task = \"\"\"Explain the Anemoi semi-centralized agent pattern and why peer-to-peer critic loops reduce bottlenecks.\"\"\"\n\n\nresult = anemoi_critic_loop.invoke(\n   {\n       \"task\": demo_task,\n       \"max_rounds\": 3,\n       \"round\": 0,\n       \"draft\": \"\",\n       \"critique\": \"\",\n       \"agreed\": False,\n       \"final\": \"\",\n       \"trace\": False,\n   }\n)\n\n\nprint(\"\\n====================\")\nprint(\" FINAL OUTPUT\")\nprint(\"====================\\n\")\nprint(result[\"final\"])\n\n\n\nWe assemble the LangGraph workflow that routes control between Drafter and Critic until agreement is reached or the maximum round limit is reached. We rely on simple conditional routing rather than centralized planning, thereby preserving the system&#8217;s semi-centralized nature. Finally, we execute the graph and return the best available output to the user.\n\n\n\nIn conclusion, we demonstrated that Anemoi-style peer negotiation is a practical alternative to manager-worker architectures, offering lower latency, reduced context bloat, and simpler agent coordination. By allowing agents to monitor and correct each other directly, we achieved convergence with fewer tokens and less orchestration complexity. In this tutorial, we provided a reusable blueprint for building scalable, semi-centralized agent systems. It lays the foundation for extending the pattern to multi-peer meshes, red-team loops, or protocol-based agent interoperability.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post A Coding Guide to Anemoi-Style Semi-Centralized Agentic Systems Using Peer-to-Peer Critic Loops in LangGraph appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/20/a-coding-guide-to-anemoi-style-semi-centralized-agentic-systems-using-peer-to-peer-critic-loops-in-langgraph/",
      "author": "Asif Razzaq",
      "published": "2026-01-21T02:43:58",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Editors Pick",
        "Staff",
        "Tutorials"
      ],
      "summary": "Tutorial demonstrating Anemoi-style semi-centralized multi-agent systems using peer-to-peer critic loops in LangGraph framework.",
      "importance_score": 40.0,
      "reasoning": "Technical tutorial on agent architecture patterns. Useful for practitioners but not news.",
      "themes": [
        "Multi-Agent Systems",
        "Tutorial",
        "LangGraph"
      ],
      "continuation": null,
      "summary_html": "<p>Tutorial demonstrating Anemoi-style semi-centralized multi-agent systems using peer-to-peer critic loops in LangGraph framework.</p>",
      "content_html": "<p>In this tutorial, we demonstrate how a semi-centralized Anemoi-style multi-agent system works by letting two peer agents negotiate directly without a manager or supervisor. We show how a Drafter and a Critic iteratively refine an output through peer-to-peer feedback, reducing coordination overhead while preserving quality. We implement this pattern end-to-end in Colab using LangGraph, focusing on clarity, control flow, and practical execution rather than abstract orchestration theory. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U langgraph langchain-openai langchain-core</p>\n<p>import os</p>\n<p>import json</p>\n<p>from getpass import getpass</p>\n<p>from typing import TypedDict</p>\n<p>from langchain_openai import ChatOpenAI</p>\n<p>from langgraph.graph import StateGraph, END</p>\n<p>if not os.environ.get(\"OPENAI_API_KEY\"):</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY (hidden): \")</p>\n<p>MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")</p>\n<p>llm = ChatOpenAI(model=MODEL, temperature=0.2)</p>\n<p>We set up the Colab environment by installing the required LangGraph and LangChain packages and securely collecting the OpenAI API key as a hidden input. We initialize the language model that will be shared by all agents, keeping the configuration minimal and reproducible. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass AnemoiState(TypedDict):</p>\n<p>task: str</p>\n<p>max_rounds: int</p>\n<p>round: int</p>\n<p>draft: str</p>\n<p>critique: str</p>\n<p>agreed: bool</p>\n<p>final: str</p>\n<p>trace: bool</p>\n<p>We define a typed state that acts as the shared communication surface between agents during negotiation. We explicitly track the task, draft, critique, agreement flag, and iteration count to keep the flow transparent and debuggable. This state obviates the need for a central manager or for implicit memory. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different BrowserDRAFTER_SYSTEM = \"\"\"You are Agent A (Drafter) in a peer-to-peer loop.</p>\n<p>You write a high-quality solution to the user's task.</p>\n<p>If you receive critique, you revise decisively and incorporate it.</p>\n<p>Return only the improved draft text.\"\"\"</p>\n<p>def drafter_node(state: AnemoiState) -&gt; AnemoiState:</p>\n<p>task = state[\"task\"]</p>\n<p>critique = state.get(\"critique\", \"\").strip()</p>\n<p>r = state.get(\"round\", 0) + 1</p>\n<p>if critique:</p>\n<p>user_msg = f\"\"\"TASK:</p>\n<p>{task}</p>\n<p>CRITIQUE:</p>\n<p>{critique}</p>\n<p>Revise the draft.\"\"\"</p>\n<p>else:</p>\n<p>user_msg = f\"\"\"TASK:</p>\n<p>{task}</p>\n<p>Write the first draft.\"\"\"</p>\n<p>draft = llm.invoke(</p>\n<p>[</p>\n<p>{\"role\": \"system\", \"content\": DRAFTER_SYSTEM},</p>\n<p>{\"role\": \"user\", \"content\": user_msg},</p>\n<p>]</p>\n<p>).content.strip()</p>\n<p>if state.get(\"trace\", False):</p>\n<p>print(f\"\\n--- Drafter Round {r} ---\\n{draft}\\n\")</p>\n<p>return {<strong>state, \"round\": r, \"draft\": draft, \"agreed\": False}</strong></p><strong>\n<p>We implement the Drafter agent, which produces the initial response and revises it whenever peer feedback is available. We keep the Drafter focused purely on improving the user-facing draft, without awareness of control logic or termination conditions. It mirrors the Anemoi idea of agents optimizing locally while observing peer signals. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different BrowserCRITIC_SYSTEM = \"\"\"You are Agent B (Critic).</p>\n<p>Return strict JSON:</p>\n<p>{\"agree\": true/false, \"critique\": \"...\"}\"\"\"</p>\n<p>def critic_node(state: AnemoiState) -&gt; AnemoiState:</p>\n<p>task = state[\"task\"]</p>\n<p>draft = state.get(\"draft\", \"\")</p>\n<p>raw = llm.invoke(</p>\n<p>[</p>\n<p>{\"role\": \"system\", \"content\": CRITIC_SYSTEM},</p>\n<p>{</p>\n<p>\"role\": \"user\",</p>\n<p>\"content\": f\"TASK:\\n{task}\\n\\nDRAFT:\\n{draft}\",</p>\n<p>},</p>\n<p>]</p>\n<p>).content.strip()</p>\n<p>cleaned = raw.strip(\"```\").replace(\"json\", \"\").strip()</p>\n<p>try:</p>\n<p>data = json.loads(cleaned)</p>\n<p>agree = bool(data.get(\"agree\", False))</p>\n<p>critique = str(data.get(\"critique\", \"\")).strip()</p>\n<p>except Exception:</p>\n<p>agree = False</p>\n<p>critique = raw</p>\n<p>if state.get(\"trace\", False):</p>\n<p>print(f\"--- Critic Decision ---\\nAGREE: {agree}\\n{critique}\\n\")</p>\n<p>final = draft if agree else state.get(\"final\", \"\")</p>\n</strong><p><strong>return {</strong>state, \"agreed\": agree, \"critique\": critique, \"final\": final}</p>\n<p>We implement the Critic agent, which evaluates the draft and decides whether it is ready to ship or needs revision. We enforce a strict agree-or-revise decision to avoid vague feedback and ensure fast convergence. This peer evaluation step allows quality control without introducing a supervisory agent. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef continue_or_end(state: AnemoiState) -&gt; str:</p>\n<p>if state.get(\"agreed\", False):</p>\n<p>return \"end\"</p>\n<p>if state.get(\"round\", 0) &gt;= state.get(\"max_rounds\", 3):</p>\n<p>return \"force_ship\"</p>\n<p>return \"loop\"</p>\n<p>def force_ship_node(state: AnemoiState) -&gt; AnemoiState:</p>\n<p>return {**state, \"final\": state.get(\"final\") or state.get(\"draft\", \"\")}</p>\n<p>graph = StateGraph(AnemoiState)</p>\n<p>graph.add_node(\"drafter\", drafter_node)</p>\n<p>graph.add_node(\"critic\", critic_node)</p>\n<p>graph.add_node(\"force_ship\", force_ship_node)</p>\n<p>graph.set_entry_point(\"drafter\")</p>\n<p>graph.add_edge(\"drafter\", \"critic\")</p>\n<p>graph.add_conditional_edges(</p>\n<p>\"critic\",</p>\n<p>continue_or_end,</p>\n<p>{\"loop\": \"drafter\", \"force_ship\": \"force_ship\", \"end\": END},</p>\n<p>)</p>\n<p>graph.add_edge(\"force_ship\", END)</p>\n<p>anemoi_critic_loop = graph.compile()</p>\n<p>demo_task = \"\"\"Explain the Anemoi semi-centralized agent pattern and why peer-to-peer critic loops reduce bottlenecks.\"\"\"</p>\n<p>result = anemoi_critic_loop.invoke(</p>\n<p>{</p>\n<p>\"task\": demo_task,</p>\n<p>\"max_rounds\": 3,</p>\n<p>\"round\": 0,</p>\n<p>\"draft\": \"\",</p>\n<p>\"critique\": \"\",</p>\n<p>\"agreed\": False,</p>\n<p>\"final\": \"\",</p>\n<p>\"trace\": False,</p>\n<p>}</p>\n<p>)</p>\n<p>print(\"\\n====================\")</p>\n<p>print(\" FINAL OUTPUT\")</p>\n<p>print(\"====================\\n\")</p>\n<p>print(result[\"final\"])</p>\n<p>We assemble the LangGraph workflow that routes control between Drafter and Critic until agreement is reached or the maximum round limit is reached. We rely on simple conditional routing rather than centralized planning, thereby preserving the system’s semi-centralized nature. Finally, we execute the graph and return the best available output to the user.</p>\n<p>In conclusion, we demonstrated that Anemoi-style peer negotiation is a practical alternative to manager-worker architectures, offering lower latency, reduced context bloat, and simpler agent coordination. By allowing agents to monitor and correct each other directly, we achieved convergence with fewer tokens and less orchestration complexity. In this tutorial, we provided a reusable blueprint for building scalable, semi-centralized agent systems. It lays the foundation for extending the pattern to multi-peer meshes, red-team loops, or protocol-based agent interoperability.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post A Coding Guide to Anemoi-Style Semi-Centralized Agentic Systems Using Peer-to-Peer Critic Loops in LangGraph appeared first on MarkTechPost.</p>"
    },
    {
      "id": "8ffb3f07ae78",
      "title": "How AutoGluon Enables Modern AutoML Pipelines for Production-Grade Tabular Models with Ensembling and Distillation",
      "content": "In this tutorial, we build a production-grade tabular machine learning pipeline using AutoGluon, taking a real-world mixed-type dataset from raw ingestion through to deployment-ready artifacts. We train high-quality stacked and bagged ensembles, evaluate performance with robust metrics, perform subgroup and feature-level analysis, and then optimize the model for real-time inference using refit-full and distillation. Throughout the workflow, we focus on practical decisions that balance accuracy, latency, and deployability. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U \"autogluon==1.5.0\" \"scikit-learn>=1.3\" \"pandas>=2.0\" \"numpy>=1.24\"\n\n\nimport os, time, json, warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, log_loss, accuracy_score, classification_report, confusion_matrix\n\n\nfrom autogluon.tabular import TabularPredictor\n\n\n\nWe set up the environment by installing the required libraries and importing all core dependencies used throughout the pipeline. We configure warnings to keep outputs clean and ensure numerical, tabular, and evaluation utilities are ready. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserfrom sklearn.datasets import fetch_openml\ndf = fetch_openml(data_id=40945, as_frame=True).frame\n\n\ntarget = \"survived\"\ndf[target] = df[target].astype(int)\n\n\ndrop_cols = [c for c in [\"boat\", \"body\", \"home.dest\"] if c in df.columns]\ndf = df.drop(columns=drop_cols, errors=\"ignore\")\n\n\ndf = df.replace({None: np.nan})\nprint(\"Shape:\", df.shape)\nprint(\"Target positive rate:\", df[target].mean().round(4))\nprint(\"Columns:\", list(df.columns))\n\n\ntrain_df, test_df = train_test_split(\n   df,\n   test_size=0.2,\n   random_state=42,\n   stratify=df[target],\n)\n\n\n\n\nWe load a real-world mixed-type dataset and perform light preprocessing to prepare a clean training signal. We define the target, remove highly leaky columns, and validate the dataset structure. We then create a stratified train–test split to preserve class balance. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef has_gpu():\n   try:\n       import torch\n       return torch.cuda.is_available()\n   except Exception:\n       return False\n\n\npresets = \"extreme\" if has_gpu() else \"best_quality\"\n\n\nsave_path = \"/content/autogluon_titanic_advanced\"\nos.makedirs(save_path, exist_ok=True)\n\n\npredictor = TabularPredictor(\n   label=target,\n   eval_metric=\"roc_auc\",\n   path=save_path,\n   verbosity=2\n)\n\n\n\n\nWe detect hardware availability to dynamically select the most suitable AutoGluon training preset. We configure a persistent model directory and initialize the tabular predictor with an appropriate evaluation metric. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserstart = time.time()\npredictor.fit(\n   train_data=train_df,\n   presets=presets,\n   time_limit=7 * 60,\n   num_bag_folds=5,\n   num_stack_levels=2,\n   refit_full=False\n)\ntrain_time = time.time() - start\nprint(f\"\\nTraining done in {train_time:.1f}s with presets='{presets}'\")\n\n\n\nWe train a high-quality ensemble using bagging and stacking within a controlled time budget. We rely on AutoGluon’s automated model search to efficiently explore strong architectures. We also record training time to understand computational cost. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserlb = predictor.leaderboard(test_df, silent=True)\nprint(\"\\n=== Leaderboard (top 15) ===\")\ndisplay(lb.head(15))\n\n\nproba = predictor.predict_proba(test_df)\npred = predictor.predict(test_df)\n\n\ny_true = test_df[target].values\nif isinstance(proba, pd.DataFrame) and 1 in proba.columns:\n   y_proba = proba[1].values\nelse:\n   y_proba = np.asarray(proba).reshape(-1)\n\n\nprint(\"\\n=== Test Metrics ===\")\nprint(\"ROC-AUC:\", roc_auc_score(y_true, y_proba).round(5))\nprint(\"LogLoss:\", log_loss(y_true, np.clip(y_proba, 1e-6, 1 - 1e-6)).round(5))\nprint(\"Accuracy:\", accuracy_score(y_true, pred).round(5))\nprint(\"\\nClassification report:\\n\", classification_report(y_true, pred))\n\n\n\nWe evaluate the trained models using a held-out test set and inspect the leaderboard to compare performance. We compute probabilistic and discrete predictions and derive key classification metrics. It gives us a comprehensive view of model accuracy and calibration. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserif \"pclass\" in test_df.columns:\n   print(\"\\n=== Slice AUC by pclass ===\")\n   for grp, part in test_df.groupby(\"pclass\"):\n       part_proba = predictor.predict_proba(part)\n       part_proba = part_proba[1].values if isinstance(part_proba, pd.DataFrame) and 1 in part_proba.columns else np.asarray(part_proba).reshape(-1)\n       auc = roc_auc_score(part[target].values, part_proba)\n       print(f\"pclass={grp}: AUC={auc:.4f} (n={len(part)})\")\n\n\nfi = predictor.feature_importance(test_df, silent=True)\nprint(\"\\n=== Feature importance (top 20) ===\")\ndisplay(fi.head(20))\n\n\n\nWe analyze model behavior through subgroup performance slicing and permutation-based feature importance. We identify how performance varies across meaningful segments of the data. It helps us assess robustness and interpretability before deployment. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browsert0 = time.time()\nrefit_map = predictor.refit_full()\nt_refit = time.time() - t0\n\n\nprint(f\"\\nrefit_full completed in {t_refit:.1f}s\")\nprint(\"Refit mapping (sample):\", dict(list(refit_map.items())[:5]))\n\n\nlb_full = predictor.leaderboard(test_df, silent=True)\nprint(\"\\n=== Leaderboard after refit_full (top 15) ===\")\ndisplay(lb_full.head(15))\n\n\nbest_model = predictor.get_model_best()\nfull_candidates = [m for m in predictor.get_model_names() if m.endswith(\"_FULL\")]\n\n\ndef bench_infer(model_name, df_in, repeats=3):\n   times = []\n   for _ in range(repeats):\n       t1 = time.time()\n       _ = predictor.predict(df_in, model=model_name)\n       times.append(time.time() - t1)\n   return float(np.median(times))\n\n\nsmall_batch = test_df.drop(columns=[target]).head(256)\nlat_best = bench_infer(best_model, small_batch)\nprint(f\"\\nBest model: {best_model} | median predict() latency on 256 rows: {lat_best:.4f}s\")\n\n\nif full_candidates:\n   lb_full_sorted = lb_full.sort_values(by=\"score_test\", ascending=False)\n   best_full = lb_full_sorted[lb_full_sorted[\"model\"].str.endswith(\"_FULL\")].iloc[0][\"model\"]\n   lat_full = bench_infer(best_full, small_batch)\n   print(f\"Best FULL model: {best_full} | median predict() latency on 256 rows: {lat_full:.4f}s\")\n   print(f\"Speedup factor (best / full): {lat_best / max(lat_full, 1e-9):.2f}x\")\n\n\ntry:\n   t0 = time.time()\n   distill_result = predictor.distill(\n       train_data=train_df,\n       time_limit=4 * 60,\n       augment_method=\"spunge\",\n   )\n   t_distill = time.time() - t0\n   print(f\"\\nDistillation completed in {t_distill:.1f}s\")\nexcept Exception as e:\n   print(\"\\nDistillation step failed\")\n   print(\"Error:\", repr(e))\n\n\nlb2 = predictor.leaderboard(test_df, silent=True)\nprint(\"\\n=== Leaderboard after distillation attempt (top 20) ===\")\ndisplay(lb2.head(20))\n\n\npredictor.save()\nreloaded = TabularPredictor.load(save_path)\n\n\nsample = test_df.drop(columns=[target]).sample(8, random_state=0)\nsample_pred = reloaded.predict(sample)\nsample_proba = reloaded.predict_proba(sample)\n\n\nprint(\"\\n=== Reloaded predictor sanity-check ===\")\nprint(sample.assign(pred=sample_pred).head())\n\n\nprint(\"\\nProbabilities (head):\")\ndisplay(sample_proba.head())\n\n\nartifacts = {\n   \"path\": save_path,\n   \"presets\": presets,\n   \"best_model\": reloaded.get_model_best(),\n   \"model_names\": reloaded.get_model_names(),\n   \"leaderboard_top10\": lb2.head(10).to_dict(orient=\"records\"),\n}\nwith open(os.path.join(save_path, \"run_summary.json\"), \"w\") as f:\n   json.dump(artifacts, f, indent=2)\n\n\nprint(\"\\nSaved summary to:\", os.path.join(save_path, \"run_summary.json\"))\nprint(\"Done.\")\n\n\n\nWe optimize the trained ensemble for inference by collapsing bagged models and benchmarking latency improvements. We optionally distill the ensemble into faster models and validate persistence through save-reload checks. Also, we export structured artifacts required for production handoff.\n\n\n\nIn conclusion, we implemented an end-to-end workflow with AutoGluon that transforms raw tabular data into production-ready models with minimal manual intervention, while maintaining strong control over accuracy, robustness, and inference efficiency. We performed systematic error analysis and feature importance evaluation, optimized large ensembles through refitting and distillation, and validated deployment readiness using latency benchmarking and artifact packaging. This workflow enables the deployment of high-performing, scalable, interpretable, and well-suited tabular models for real-world production environments.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How AutoGluon Enables Modern AutoML Pipelines for Production-Grade Tabular Models with Ensembling and Distillation appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/21/how-autogluon-enables-modern-automl-pipelines-for-production-grade-tabular-models-with-ensembling-and-distillation/",
      "author": "Asif Razzaq",
      "published": "2026-01-21T08:07:48",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Machine Learning",
        "Staff",
        "Tech News",
        "Technology",
        "Tutorials"
      ],
      "summary": "Tutorial on building production-grade ML pipelines using AutoGluon for tabular models with ensembling and distillation techniques.",
      "importance_score": 38.0,
      "reasoning": "Educational tutorial content, not news. Useful resource but not frontier AI development.",
      "themes": [
        "AutoML",
        "Tutorial",
        "Machine Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Tutorial on building production-grade ML pipelines using AutoGluon for tabular models with ensembling and distillation techniques.</p>",
      "content_html": "<p>In this tutorial, we build a production-grade tabular machine learning pipeline using AutoGluon, taking a real-world mixed-type dataset from raw ingestion through to deployment-ready artifacts. We train high-quality stacked and bagged ensembles, evaluate performance with robust metrics, perform subgroup and feature-level analysis, and then optimize the model for real-time inference using refit-full and distillation. Throughout the workflow, we focus on practical decisions that balance accuracy, latency, and deployability. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U \"autogluon==1.5.0\" \"scikit-learn&gt;=1.3\" \"pandas&gt;=2.0\" \"numpy&gt;=1.24\"</p>\n<p>import os, time, json, warnings</p>\n<p>warnings.filterwarnings(\"ignore\")</p>\n<p>import numpy as np</p>\n<p>import pandas as pd</p>\n<p>from sklearn.model_selection import train_test_split</p>\n<p>from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, classification_report, confusion_matrix</p>\n<p>from autogluon.tabular import TabularPredictor</p>\n<p>We set up the environment by installing the required libraries and importing all core dependencies used throughout the pipeline. We configure warnings to keep outputs clean and ensure numerical, tabular, and evaluation utilities are ready. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserfrom sklearn.datasets import fetch_openml</p>\n<p>df = fetch_openml(data_id=40945, as_frame=True).frame</p>\n<p>target = \"survived\"</p>\n<p>df[target] = df[target].astype(int)</p>\n<p>drop_cols = [c for c in [\"boat\", \"body\", \"home.dest\"] if c in df.columns]</p>\n<p>df = df.drop(columns=drop_cols, errors=\"ignore\")</p>\n<p>df = df.replace({None: np.nan})</p>\n<p>print(\"Shape:\", df.shape)</p>\n<p>print(\"Target positive rate:\", df[target].mean().round(4))</p>\n<p>print(\"Columns:\", list(df.columns))</p>\n<p>train_df, test_df = train_test_split(</p>\n<p>df,</p>\n<p>test_size=0.2,</p>\n<p>random_state=42,</p>\n<p>stratify=df[target],</p>\n<p>)</p>\n<p>We load a real-world mixed-type dataset and perform light preprocessing to prepare a clean training signal. We define the target, remove highly leaky columns, and validate the dataset structure. We then create a stratified train–test split to preserve class balance. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef has_gpu():</p>\n<p>try:</p>\n<p>import torch</p>\n<p>return torch.cuda.is_available()</p>\n<p>except Exception:</p>\n<p>return False</p>\n<p>presets = \"extreme\" if has_gpu() else \"best_quality\"</p>\n<p>save_path = \"/content/autogluon_titanic_advanced\"</p>\n<p>os.makedirs(save_path, exist_ok=True)</p>\n<p>predictor = TabularPredictor(</p>\n<p>label=target,</p>\n<p>eval_metric=\"roc_auc\",</p>\n<p>path=save_path,</p>\n<p>verbosity=2</p>\n<p>)</p>\n<p>We detect hardware availability to dynamically select the most suitable AutoGluon training preset. We configure a persistent model directory and initialize the tabular predictor with an appropriate evaluation metric. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserstart = time.time()</p>\n<p>predictor.fit(</p>\n<p>train_data=train_df,</p>\n<p>presets=presets,</p>\n<p>time_limit=7 * 60,</p>\n<p>num_bag_folds=5,</p>\n<p>num_stack_levels=2,</p>\n<p>refit_full=False</p>\n<p>)</p>\n<p>train_time = time.time() - start</p>\n<p>print(f\"\\nTraining done in {train_time:.1f}s with presets='{presets}'\")</p>\n<p>We train a high-quality ensemble using bagging and stacking within a controlled time budget. We rely on AutoGluon’s automated model search to efficiently explore strong architectures. We also record training time to understand computational cost. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserlb = predictor.leaderboard(test_df, silent=True)</p>\n<p>print(\"\\n=== Leaderboard (top 15) ===\")</p>\n<p>display(lb.head(15))</p>\n<p>proba = predictor.predict_proba(test_df)</p>\n<p>pred = predictor.predict(test_df)</p>\n<p>y_true = test_df[target].values</p>\n<p>if isinstance(proba, pd.DataFrame) and 1 in proba.columns:</p>\n<p>y_proba = proba[1].values</p>\n<p>else:</p>\n<p>y_proba = np.asarray(proba).reshape(-1)</p>\n<p>print(\"\\n=== Test Metrics ===\")</p>\n<p>print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba).round(5))</p>\n<p>print(\"LogLoss:\", log_loss(y_true, np.clip(y_proba, 1e-6, 1 - 1e-6)).round(5))</p>\n<p>print(\"Accuracy:\", accuracy_score(y_true, pred).round(5))</p>\n<p>print(\"\\nClassification report:\\n\", classification_report(y_true, pred))</p>\n<p>We evaluate the trained models using a held-out test set and inspect the leaderboard to compare performance. We compute probabilistic and discrete predictions and derive key classification metrics. It gives us a comprehensive view of model accuracy and calibration. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserif \"pclass\" in test_df.columns:</p>\n<p>print(\"\\n=== Slice AUC by pclass ===\")</p>\n<p>for grp, part in test_df.groupby(\"pclass\"):</p>\n<p>part_proba = predictor.predict_proba(part)</p>\n<p>part_proba = part_proba[1].values if isinstance(part_proba, pd.DataFrame) and 1 in part_proba.columns else np.asarray(part_proba).reshape(-1)</p>\n<p>auc = roc_auc_score(part[target].values, part_proba)</p>\n<p>print(f\"pclass={grp}: AUC={auc:.4f} (n={len(part)})\")</p>\n<p>fi = predictor.feature_importance(test_df, silent=True)</p>\n<p>print(\"\\n=== Feature importance (top 20) ===\")</p>\n<p>display(fi.head(20))</p>\n<p>We analyze model behavior through subgroup performance slicing and permutation-based feature importance. We identify how performance varies across meaningful segments of the data. It helps us assess robustness and interpretability before deployment. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browsert0 = time.time()</p>\n<p>refit_map = predictor.refit_full()</p>\n<p>t_refit = time.time() - t0</p>\n<p>print(f\"\\nrefit_full completed in {t_refit:.1f}s\")</p>\n<p>print(\"Refit mapping (sample):\", dict(list(refit_map.items())[:5]))</p>\n<p>lb_full = predictor.leaderboard(test_df, silent=True)</p>\n<p>print(\"\\n=== Leaderboard after refit_full (top 15) ===\")</p>\n<p>display(lb_full.head(15))</p>\n<p>best_model = predictor.get_model_best()</p>\n<p>full_candidates = [m for m in predictor.get_model_names() if m.endswith(\"_FULL\")]</p>\n<p>def bench_infer(model_name, df_in, repeats=3):</p>\n<p>times = []</p>\n<p>for _ in range(repeats):</p>\n<p>t1 = time.time()</p>\n<p>_ = predictor.predict(df_in, model=model_name)</p>\n<p>times.append(time.time() - t1)</p>\n<p>return float(np.median(times))</p>\n<p>small_batch = test_df.drop(columns=[target]).head(256)</p>\n<p>lat_best = bench_infer(best_model, small_batch)</p>\n<p>print(f\"\\nBest model: {best_model} | median predict() latency on 256 rows: {lat_best:.4f}s\")</p>\n<p>if full_candidates:</p>\n<p>lb_full_sorted = lb_full.sort_values(by=\"score_test\", ascending=False)</p>\n<p>best_full = lb_full_sorted[lb_full_sorted[\"model\"].str.endswith(\"_FULL\")].iloc[0][\"model\"]</p>\n<p>lat_full = bench_infer(best_full, small_batch)</p>\n<p>print(f\"Best FULL model: {best_full} | median predict() latency on 256 rows: {lat_full:.4f}s\")</p>\n<p>print(f\"Speedup factor (best / full): {lat_best / max(lat_full, 1e-9):.2f}x\")</p>\n<p>try:</p>\n<p>t0 = time.time()</p>\n<p>distill_result = predictor.distill(</p>\n<p>train_data=train_df,</p>\n<p>time_limit=4 * 60,</p>\n<p>augment_method=\"spunge\",</p>\n<p>)</p>\n<p>t_distill = time.time() - t0</p>\n<p>print(f\"\\nDistillation completed in {t_distill:.1f}s\")</p>\n<p>except Exception as e:</p>\n<p>print(\"\\nDistillation step failed\")</p>\n<p>print(\"Error:\", repr(e))</p>\n<p>lb2 = predictor.leaderboard(test_df, silent=True)</p>\n<p>print(\"\\n=== Leaderboard after distillation attempt (top 20) ===\")</p>\n<p>display(lb2.head(20))</p>\n<p>predictor.save()</p>\n<p>reloaded = TabularPredictor.load(save_path)</p>\n<p>sample = test_df.drop(columns=[target]).sample(8, random_state=0)</p>\n<p>sample_pred = reloaded.predict(sample)</p>\n<p>sample_proba = reloaded.predict_proba(sample)</p>\n<p>print(\"\\n=== Reloaded predictor sanity-check ===\")</p>\n<p>print(sample.assign(pred=sample_pred).head())</p>\n<p>print(\"\\nProbabilities (head):\")</p>\n<p>display(sample_proba.head())</p>\n<p>artifacts = {</p>\n<p>\"path\": save_path,</p>\n<p>\"presets\": presets,</p>\n<p>\"best_model\": reloaded.get_model_best(),</p>\n<p>\"model_names\": reloaded.get_model_names(),</p>\n<p>\"leaderboard_top10\": lb2.head(10).to_dict(orient=\"records\"),</p>\n<p>}</p>\n<p>with open(os.path.join(save_path, \"run_summary.json\"), \"w\") as f:</p>\n<p>json.dump(artifacts, f, indent=2)</p>\n<p>print(\"\\nSaved summary to:\", os.path.join(save_path, \"run_summary.json\"))</p>\n<p>print(\"Done.\")</p>\n<p>We optimize the trained ensemble for inference by collapsing bagged models and benchmarking latency improvements. We optionally distill the ensemble into faster models and validate persistence through save-reload checks. Also, we export structured artifacts required for production handoff.</p>\n<p>In conclusion, we implemented an end-to-end workflow with AutoGluon that transforms raw tabular data into production-ready models with minimal manual intervention, while maintaining strong control over accuracy, robustness, and inference efficiency. We performed systematic error analysis and feature importance evaluation, optimized large ensembles through refitting and distillation, and validated deployment readiness using latency benchmarking and artifact packaging. This workflow enables the deployment of high-performing, scalable, interpretable, and well-suited tabular models for real-world production environments.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How AutoGluon Enables Modern AutoML Pipelines for Production-Grade Tabular Models with Ensembling and Distillation appeared first on MarkTechPost.</p>"
    },
    {
      "id": "d38721a6bef8",
      "title": "What are Context Graphs?",
      "content": "Knowledge Graphs and their limitations\n\n\n\nWith the rapid growth of AI applications, Knowledge Graphs (KGs) have emerged as a foundational structure for representing knowledge in a machine-readable form. They organize information as triples—a head entity, a relation, and a tail entity—forming a graph-like structure where entities are nodes and relationships are edges. This representation allows machines to understand and reason over connected knowledge, supporting intelligent applications such as question answering, semantic analysis, and recommendation systems\n\n\n\nDespite their effectiveness, Knowledge Graphs (KGs) have notable limitations. They often lose important contextual information, making it difficult to capture the complexity and richness of real-world knowledge. Additionally, many KGs suffer from data sparsity, where entities and relationships are incomplete or poorly connected. This lack of full annotation limits the contextual signals available during inference, posing challenges for effective reasoning, even when integrated with large language models.\n\n\n\n\n\n\n\nContext Graphs\n\n\n\nContext Graphs (CGs) extend traditional Knowledge Graphs by adding extra information such as time, location, and source details. Instead of storing knowledge as isolated facts, they capture the situation in which a fact or decision occurred, leading to a clearer and more accurate understanding of real-world knowledge.\n\n\n\nWhen used with agent-based systems, context graphs also store how decisions were made. Agents need more than rules—they need to know how rules were applied before, when exceptions were allowed, who approved decisions, and how conflicts were handled. Since agents operate directly where decisions happen, they can naturally record this full context.\n\n\n\nOver time, these stored decision traces form a context graph that helps agents learn from past actions. This allows systems to understand not only what happened, but also why it happened, making agent behavior more consistent and reliable.\n\n\n\n\n\n\n\nWhat are the effects of Contextual Information?\n\n\n\nContextual information adds important layers to knowledge representation by going beyond simple entities–relation facts. It helps distinguish between facts that look similar but occur under different conditions, such as differences in time, location, scale, or surrounding circumstances. For example, two companies may be competitors in one market or time period but not in another. By capturing such context, systems can represent knowledge in a more detailed way and avoid treating all similar-looking facts as identical.\n\n\n\nIn context graphs, contextual information also plays a key role in reasoning and decision-making. It includes signals such as historical decisions, policies applied, exceptions granted, approvals involved, and related events from other systems. When agents record how a decision was made—what data was used, which rule was checked, and why an exception was allowed—this information becomes reusable context for future decisions. Over time, these records help connect entities that are not directly linked and allow systems to reason based on past outcomes and precedents, rather than relying only on fixed rules or isolated triples.\n\n\n\nShift from static tools to decision-making agents\n\n\n\nThere has been a clear shift in AI systems—from static tools to decision-making agents, driven largely by major industry players. Real-world decisions are rarely based on rules alone; they involve exceptions, approvals, and lessons from past cases. Context graphs address this gap by capturing how decisions are made across systems—what policies were checked, which data was used, who approved the decision, and what outcome followed. By structuring this decision history as context, agents can reuse prior judgments instead of repeatedly relearning the same edge cases. Some examples of this shift include:\n\n\n\nGoogle\n\n\n\n\nGmail’s Gemini features and Gemini 3–based agent frameworks both show AI shifting from simple help to active decision-making, whether that’s managing inbox priorities or running complex workflows.\n\n\n\nGmail relies on conversation history and user intent, while Gemini 3 agents use memory and state to handle longer tasks. In both cases, context matters more than single prompts.\n\n\n\nGemini 3 acts as an orchestration layer for multi-agent systems (ADK, Agno, Letta, Eigent), similar to how Gemini orchestrates summarization, writing, and prioritization inside Gmail.\n\n\n\nFeatures like AI Inbox and Suggested Replies rely on persistent understanding of user behavior, just as agent frameworks like Letta and mem0 rely on stateful memory to prevent context loss and ensure consistent behavior.\n\n\n\nGmail turns email into actionable summaries and to-dos, while Gemini-powered agents automate browsers, workflows, and enterprise tasks—both reflecting a broader shift toward AI systems that act, not just respond.\n\n\n\n\n\n\n\n\nOpenAI\n\n\n\n\nChatGPT Health brings health data from different sources—medical records, apps, wearables, and notes—into one place. This creates a clear, shared context that helps the system understand health patterns over time instead of answering isolated questions, similar to how context graphs link facts with their context.\n\n\n\nBy using personal health history and past interactions, ChatGPT Health helps users make better-informed decisions, such as preparing for doctor visits or understanding test results.\n\n\n\nHealth runs in a separate, secure space, keeping sensitive information private and contained. This ensures health context stays accurate and protected, which is essential for safely using context-based systems like context graphs.\n\n\n\n\n\n\n\n\nJP Morgan\n\n\n\n\nJP Morgan replacing proxy advisors with its AI tool, Proxy IQ, shows a shift toward building in-house decision systems that aggregate and analyze voting data across thousands of meetings, rather than relying on third-party recommendations.\n\n\n\nBy analyzing proxy data internally, the firm can incorporate historical voting behavior, company-specific details, and firm-level policies—aligning with the idea of context graphs that preserve how decisions are formed over time.\n\n\n\nInternal AI-based analysis gives JP Morgan more transparency, speed, and consistency in proxy voting, reflecting a broader move toward context-aware, AI-driven decision-making in enterprise settings.\n\n\n\n\n\n\n\n\nNVIDIA\n\n\n\n\nNVIDIA’s NeMo Agent Toolkit helps turn AI agents into production-ready systems by adding observability, evaluation, and deployment controls. By capturing execution traces, reasoning steps, and performance signals, it records how an agent arrived at an outcome—not just the final result—aligning closely with the idea of context graphs.\n\n\n\nTools like OpenTelemetry tracing and structured evaluations convert agent behavior into usable context. This makes it easier to debug decisions, compare different runs, and steadily improve reliability.\n\n\n\nSimilar to how DLSS 4.5 integrates AI deeply into real-time graphics pipelines, NAT integrates AI agents into enterprise workflows. Both highlight a broader shift toward AI systems that retain state, history, and context, which is critical for dependable, large-scale deployment.\n\n\n\n\n\n\n\n\nMicrosoft\n\n\n\n\nCopilot Checkout and Brand Agents turn shopping conversations into direct purchases. Questions, comparisons, and decisions happen in one place, creating clear context around why a customer chose a product.\n\n\n\nThese AI agents operate exactly where buying decisions happen—inside chats and brand websites—allowing them to guide users and complete checkout without extra steps.\n\n\n\nMerchants keep control of transactions and customer data. Over time, these interactions build useful context about customer intent and buying patterns, helping future decisions become faster and more accurate.\n\n\n\n\n\nThe post What are Context Graphs? appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/20/what-are-context-graphs/",
      "author": "Arham Islam",
      "published": "2026-01-21T02:58:05",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Career",
        "Editors Pick",
        "Staff"
      ],
      "summary": "Educational explainer on Context Graphs as an improvement over Knowledge Graphs for preserving contextual information in AI applications.",
      "importance_score": 38.0,
      "reasoning": "Educational content explaining concepts, not breaking news or new research.",
      "themes": [
        "Knowledge Graphs",
        "Education"
      ],
      "continuation": null,
      "summary_html": "<p>Educational explainer on Context Graphs as an improvement over Knowledge Graphs for preserving contextual information in AI applications.</p>",
      "content_html": "<p>Knowledge Graphs and their limitations</p>\n<p>With the rapid growth of AI applications, Knowledge Graphs (KGs) have emerged as a foundational structure for representing knowledge in a machine-readable form. They organize information as triples—a head entity, a relation, and a tail entity—forming a graph-like structure where entities are nodes and relationships are edges. This representation allows machines to understand and reason over connected knowledge, supporting intelligent applications such as question answering, semantic analysis, and recommendation systems</p>\n<p>Despite their effectiveness, Knowledge Graphs (KGs) have notable limitations. They often lose important contextual information, making it difficult to capture the complexity and richness of real-world knowledge. Additionally, many KGs suffer from data sparsity, where entities and relationships are incomplete or poorly connected. This lack of full annotation limits the contextual signals available during inference, posing challenges for effective reasoning, even when integrated with large language models.</p>\n<p>Context Graphs</p>\n<p>Context Graphs (CGs) extend traditional Knowledge Graphs by adding extra information such as time, location, and source details. Instead of storing knowledge as isolated facts, they capture the situation in which a fact or decision occurred, leading to a clearer and more accurate understanding of real-world knowledge.</p>\n<p>When used with agent-based systems, context graphs also store how decisions were made. Agents need more than rules—they need to know how rules were applied before, when exceptions were allowed, who approved decisions, and how conflicts were handled. Since agents operate directly where decisions happen, they can naturally record this full context.</p>\n<p>Over time, these stored decision traces form a context graph that helps agents learn from past actions. This allows systems to understand not only what happened, but also why it happened, making agent behavior more consistent and reliable.</p>\n<p>What are the effects of Contextual Information?</p>\n<p>Contextual information adds important layers to knowledge representation by going beyond simple entities–relation facts. It helps distinguish between facts that look similar but occur under different conditions, such as differences in time, location, scale, or surrounding circumstances. For example, two companies may be competitors in one market or time period but not in another. By capturing such context, systems can represent knowledge in a more detailed way and avoid treating all similar-looking facts as identical.</p>\n<p>In context graphs, contextual information also plays a key role in reasoning and decision-making. It includes signals such as historical decisions, policies applied, exceptions granted, approvals involved, and related events from other systems. When agents record how a decision was made—what data was used, which rule was checked, and why an exception was allowed—this information becomes reusable context for future decisions. Over time, these records help connect entities that are not directly linked and allow systems to reason based on past outcomes and precedents, rather than relying only on fixed rules or isolated triples.</p>\n<p>Shift from static tools to decision-making agents</p>\n<p>There has been a clear shift in AI systems—from static tools to decision-making agents, driven largely by major industry players. Real-world decisions are rarely based on rules alone; they involve exceptions, approvals, and lessons from past cases. Context graphs address this gap by capturing how decisions are made across systems—what policies were checked, which data was used, who approved the decision, and what outcome followed. By structuring this decision history as context, agents can reuse prior judgments instead of repeatedly relearning the same edge cases. Some examples of this shift include:</p>\n<p>Google</p>\n<p>Gmail’s Gemini features and Gemini 3–based agent frameworks both show AI shifting from simple help to active decision-making, whether that’s managing inbox priorities or running complex workflows.</p>\n<p>Gmail relies on conversation history and user intent, while Gemini 3 agents use memory and state to handle longer tasks. In both cases, context matters more than single prompts.</p>\n<p>Gemini 3 acts as an orchestration layer for multi-agent systems (ADK, Agno, Letta, Eigent), similar to how Gemini orchestrates summarization, writing, and prioritization inside Gmail.</p>\n<p>Features like AI Inbox and Suggested Replies rely on persistent understanding of user behavior, just as agent frameworks like Letta and mem0 rely on stateful memory to prevent context loss and ensure consistent behavior.</p>\n<p>Gmail turns email into actionable summaries and to-dos, while Gemini-powered agents automate browsers, workflows, and enterprise tasks—both reflecting a broader shift toward AI systems that act, not just respond.</p>\n<p>OpenAI</p>\n<p>ChatGPT Health brings health data from different sources—medical records, apps, wearables, and notes—into one place. This creates a clear, shared context that helps the system understand health patterns over time instead of answering isolated questions, similar to how context graphs link facts with their context.</p>\n<p>By using personal health history and past interactions, ChatGPT Health helps users make better-informed decisions, such as preparing for doctor visits or understanding test results.</p>\n<p>Health runs in a separate, secure space, keeping sensitive information private and contained. This ensures health context stays accurate and protected, which is essential for safely using context-based systems like context graphs.</p>\n<p>JP Morgan</p>\n<p>JP Morgan replacing proxy advisors with its AI tool, Proxy IQ, shows a shift toward building in-house decision systems that aggregate and analyze voting data across thousands of meetings, rather than relying on third-party recommendations.</p>\n<p>By analyzing proxy data internally, the firm can incorporate historical voting behavior, company-specific details, and firm-level policies—aligning with the idea of context graphs that preserve how decisions are formed over time.</p>\n<p>Internal AI-based analysis gives JP Morgan more transparency, speed, and consistency in proxy voting, reflecting a broader move toward context-aware, AI-driven decision-making in enterprise settings.</p>\n<p>NVIDIA</p>\n<p>NVIDIA’s NeMo Agent Toolkit helps turn AI agents into production-ready systems by adding observability, evaluation, and deployment controls. By capturing execution traces, reasoning steps, and performance signals, it records how an agent arrived at an outcome—not just the final result—aligning closely with the idea of context graphs.</p>\n<p>Tools like OpenTelemetry tracing and structured evaluations convert agent behavior into usable context. This makes it easier to debug decisions, compare different runs, and steadily improve reliability.</p>\n<p>Similar to how DLSS 4.5 integrates AI deeply into real-time graphics pipelines, NAT integrates AI agents into enterprise workflows. Both highlight a broader shift toward AI systems that retain state, history, and context, which is critical for dependable, large-scale deployment.</p>\n<p>Microsoft</p>\n<p>Copilot Checkout and Brand Agents turn shopping conversations into direct purchases. Questions, comparisons, and decisions happen in one place, creating clear context around why a customer chose a product.</p>\n<p>These AI agents operate exactly where buying decisions happen—inside chats and brand websites—allowing them to guide users and complete checkout without extra steps.</p>\n<p>Merchants keep control of transactions and customer data. Over time, these interactions build useful context about customer intent and buying patterns, helping future decisions become faster and more accurate.</p>\n<p>The post What are Context Graphs? appeared first on MarkTechPost.</p>"
    }
  ]
}