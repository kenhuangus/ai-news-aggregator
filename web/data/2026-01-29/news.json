{
  "category": "news",
  "date": "2026-01-29",
  "category_summary": "**Model Releases & Capabilities**: **Moonshot AI's Kimi K2.5** [claims SOTA](/?date=2026-01-29&category=news#item-eb2c2a3b3713) among open models, beating **Claude Sonnet 4.5** at half the cost with native multimodal understanding and 100-parallel agent swarm management. **MBZUAI's K2 Think V2** [advances open reasoning](/?date=2026-01-29&category=news#item-4400936812a9) with a fully transparent 70B parameter pipeline. **Tencent** [open-sourced **HPC-Ops**](/?date=2026-01-29&category=news#item-5b24258d5e63) delivering 17-30% inference improvements.\n\n**Agentic AI Deployment**: **Google** [launched **Auto Browse**](/?date=2026-01-29&category=news#item-0ce2f187c101) in Chrome, bringing autonomous browsing agents to billions of users. **Deloitte** [warned](/?date=2026-01-29&category=news#item-5c52a96c6901) that only 21% of organizations have AI agent governance despite 74% planning adoption within two years. The open-source **Moltbot** assistant [gained 69K GitHub stars](/?date=2026-01-29&category=news#item-f207a2857dd0) in one month despite security concerns.\n\n**Infrastructure & Geopolitics**: China [approved **400,000+ Nvidia H200**](/?date=2026-01-29&category=news#item-457455474f5c) chips for **ByteDance**, **Alibaba**, and **Tencent** after weeks of blocking imports. **Tesla** [discontinued Model S/X](/?date=2026-01-29&category=news#item-de863af52245) to pivot toward **Optimus** robotics. **Google DeepMind** [unveiled **AlphaGenome**](/?date=2026-01-29&category=news#item-d0deadf53add) for analyzing 1M DNA letters to identify disease drivers. The **CISA acting director** [accidentally leaked](/?date=2026-01-29&category=news#item-3542db936e3c) sensitive documents to ChatGPT, highlighting ongoing government AI security challenges.",
  "category_summary_html": "<p><strong>Model Releases & Capabilities</strong>: <strong>Moonshot AI's Kimi K2.5</strong> <a href=\"/?date=2026-01-29&category=news#item-eb2c2a3b3713\" class=\"internal-link\" rel=\"noopener noreferrer\">claims SOTA</a> among open models, beating <strong>Claude Sonnet 4.5</strong> at half the cost with native multimodal understanding and 100-parallel agent swarm management. <strong>MBZUAI's K2 Think V2</strong> <a href=\"/?date=2026-01-29&category=news#item-4400936812a9\" class=\"internal-link\" rel=\"noopener noreferrer\">advances open reasoning</a> with a fully transparent 70B parameter pipeline. <strong>Tencent</strong> <a href=\"/?date=2026-01-29&category=news#item-5b24258d5e63\" class=\"internal-link\" rel=\"noopener noreferrer\">open-sourced <strong>HPC-Ops</strong></a> delivering 17-30% inference improvements.</p>\n<p><strong>Agentic AI Deployment</strong>: <strong>Google</strong> <a href=\"/?date=2026-01-29&category=news#item-0ce2f187c101\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>Auto Browse</strong></a> in Chrome, bringing autonomous browsing agents to billions of users. <strong>Deloitte</strong> <a href=\"/?date=2026-01-29&category=news#item-5c52a96c6901\" class=\"internal-link\" rel=\"noopener noreferrer\">warned</a> that only 21% of organizations have AI agent governance despite 74% planning adoption within two years. The open-source <strong>Moltbot</strong> assistant <a href=\"/?date=2026-01-29&category=news#item-f207a2857dd0\" class=\"internal-link\" rel=\"noopener noreferrer\">gained 69K GitHub stars</a> in one month despite security concerns.</p>\n<p><strong>Infrastructure & Geopolitics</strong>: China <a href=\"/?date=2026-01-29&category=news#item-457455474f5c\" class=\"internal-link\" rel=\"noopener noreferrer\">approved <strong>400,000+ Nvidia H200</strong></a> chips for <strong>ByteDance</strong>, <strong>Alibaba</strong>, and <strong>Tencent</strong> after weeks of blocking imports. <strong>Tesla</strong> <a href=\"/?date=2026-01-29&category=news#item-de863af52245\" class=\"internal-link\" rel=\"noopener noreferrer\">discontinued Model S/X</a> to pivot toward <strong>Optimus</strong> robotics. <strong>Google DeepMind</strong> <a href=\"/?date=2026-01-29&category=news#item-d0deadf53add\" class=\"internal-link\" rel=\"noopener noreferrer\">unveiled <strong>AlphaGenome</strong></a> for analyzing 1M DNA letters to identify disease drivers. The <strong>CISA acting director</strong> <a href=\"/?date=2026-01-29&category=news#item-3542db936e3c\" class=\"internal-link\" rel=\"noopener noreferrer\">accidentally leaked</a> sensitive documents to ChatGPT, highlighting ongoing government AI security challenges.</p>",
  "themes": [
    {
      "name": "Agentic AI",
      "description": "Autonomous AI agents being deployed in browsers, enterprises, and consumer applications, with growing governance concerns",
      "item_count": 8,
      "example_items": [],
      "importance": 82.0
    },
    {
      "name": "Model Releases",
      "description": "New frontier models including Kimi K2.5 and K2 Think V2 advancing open source and multimodal capabilities",
      "item_count": 4,
      "example_items": [],
      "importance": 84.0
    },
    {
      "name": "AI Infrastructure & Geopolitics",
      "description": "Chip trade dynamics, datacenter investment, and US-China AI competition",
      "item_count": 5,
      "example_items": [],
      "importance": 78.0
    },
    {
      "name": "AI Governance & Safety",
      "description": "Enterprise governance gaps, government security incidents, and deployment risks outpacing safeguards",
      "item_count": 6,
      "example_items": [],
      "importance": 70.0
    },
    {
      "name": "Enterprise AI Adoption",
      "description": "Workplace AI deployment patterns, scaling challenges, and business results",
      "item_count": 5,
      "example_items": [],
      "importance": 58.0
    },
    {
      "name": "AI for Science",
      "description": "DeepMind's AlphaGenome and growing focus on scientific AI applications",
      "item_count": 3,
      "example_items": [],
      "importance": 72.0
    }
  ],
  "total_items": 32,
  "items": [
    {
      "id": "eb2c2a3b3713",
      "title": "[AINews] Moonshot Kimi K2.5 - Beats Sonnet 4.5 at half the cost, SOTA Open Model, first Native Image+Video, 100 parallel Agent Swarm manager",
      "content": "AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. AINews&#8217; website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Kimi has been on an absolute tear in the past year, and we last heard from them in November with Kimi K2 Thinking. Like K2, today&#8217;s K2.5 is still a 32B active-1T param model (384 experts), &#8220;built through continual pretraining on 15 trillion mixed visual and text tokens atop Kimi-K2-Base&#8221; (which itself was on 15T tokens), and with an EXTREMELY well produced video from their founder (3 minutes, just watch it):They again claim SOTA on HLE and BrowseComp (footnotes give confidence the tests are legit), but also open model SOTA for vision and coding tasks:tweetThere are a few notables here - Kimi K2.5 is &#8220;natively multimodal&#8221; for the first time, perhaps borrowing from Kimi VL, but is attributed to &#8220;massive-scale vision-text joint pre-training&#8221; including VIDEO understanding - &#8220;simply upload a screen recording&#8221; and K2.5 can reconstruct the website for you:The fact that this is a continued pretrain that changes arch (+400M param MoonViT vision encoder) is VERY exciting for model training folks who rarely get to see a scaled up model do stuff like this.The other 2 headline features are equally exciting: Agent Swarm (only for paid users on the Kimi app) which &#8220;learns to self-direct an agent swarm of up to 100 sub-agents, executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows.&#8221; This parallelism results in higher end result performance with up to 4.5x faster speed&#8230; ignoring token cost of course.For illustration, here is the output for:\"build a list of the top 100 funded ai startups (make sure they're actually Al - we want things like perplexity and cursor and cognition and elevenlabs and turbopuffer, NOT pretenders... use your best jdugement for criteria) and sort by valuation. use authoritative sources like Techcrunch and TheInformation and top VC firms like Sequoia, Benchmark, Redpoint, Greylock, and Conviction, as well as guests from the Latent Space podcast and Al Engineer conferences. augment the list with useful and interesting facts eg where they are based, size of team, short description of product, what their incumbents/competitors might be, what their bull case is, what Paul Graham would advise them to do.\"and &#8220;Office Productivity&#8221; with K2.5 Agent focused on &#8220;high-density, large-scale office work end to end&#8221;.This is not empty regurgitation - We saw enough to sign up as a paying subscriber of the Kimi App going forward. As Artificial Analysis notes, the China-Western gap in open models just took another big leap today.AI Twitter RecapMoonshotAI&#8217;s Kimi K2.5 ecosystem: open multimodal MoE + &#8220;Agent Swarm&#8221; pushKimi K2.5 model drop and positioning: Moonshot positions Kimi K2.5 as a flagship open-weights model with native multimodality (image + video), strong agentic performance, and aggressive API pricing/latency claims. Official launch media and messaging: founder intro video, pricing/throughput claims incl. &#8220;Turbo-level speed 60&#8211;100 tok/s&#8221;, plus early community reactions emphasizing &#8220;agent swarm&#8221; and multimodal capability (kimmonismus, kimmonismus on multimodal/video).Technical gist (as surfaced by the community): A useful unpacking of K2.5&#8217;s reported ingredients&#8212;~15T mixed visual+text tokens continual pretraining, context 128K&#8594;256K via YaRN, release in INT4 with selective quantization (only routed experts quantized), and the &#8220;Agent Swarm&#8221; orchestration concept (dynamic generation of subagents; up to 100 parallel subagents / 1,500 steps; wall-time improvements claimed 3&#8211;4.5&#215;) is summarized by @TheZachMueller (and points to the technical report).Benchmarks/third-party eval framing: Artificial Analysis positions K2.5 as &#8220;leading open weights&#8221; and closer to frontier labs, highlighting GDPval-AA Elo 1309 (agentic knowledge work harness), MMMU Pro 75%, INT4 ~595GB, and a 64% hallucination rate (improved vs K2 Thinking) among other stats: @ArtificialAnlys. LMArena announcements also place K2.5 Thinking at #1 open model in their Text Arena snapshot: @arena. (Treat leaderboards as point-in-time; harness/tooling and prompting matter.)Distribution and &#8220;runs at home&#8221; signals: K2.5 landed quickly across infra surfaces: Ollama cloud with launch integrations (@ollama), Together AI listing (@togethercompute), and Fireworks as a partner (Moonshot). A notable local-inference datapoint: K2.5 reportedly runs (slowly but &#8220;usable&#8221;) on 2&#215; M3 Ultra via MLX with sharded generation, ~21.9 tok/s at high memory use: @awnihannun (+ command snippet here).Product surface area around Kimi: Moonshot also pushed adjacent tooling: Kimi Code, an Apache-2.0 open-source coding agent integrating with common IDEs/editors (announcement), and an Agent SDK to build custom agents (link). A &#8220;Kimi Product&#8221; account is explicitly aimed at distributing prompts/use-cases (launch), with a viral demo of &#8220;video-to-code&#8221; website cloning (demo).Open &#8220;American comeback&#8221; at scale: Arcee/Prime Intellect Trinity Large Preview (400B MoE)Trinity Large Preview release: Arcee dropped Trinity Large initial weights as a &#8220;preview&#8221; release: @arcee_ai, with expanded details from @latkins. Prime Intellect frames it as an open 400B MoE with 13B active trained with Datology data: @PrimeIntellect. OpenRouter offered limited-time free access: @OpenRouterAI.Architecture/training details (most concrete technical tweet): A strong technical snapshot comes from @samsja19: 400B/A13B MoE, trained over 17T tokens; 3:1 interleaved local/global gated attention, SWA, NoPE on global layers + RoPE on local layers (as written in tweet), depth-scaled sandwich norm, sigmoid routing, trained with Muon; trained on ~2,000 B300s for a month on Prime Intellect infra, with data curation by DatologyAI.Data scaling emphasis: Datology&#8217;s involvement is highlighted as a major part of the project: &#8220;6.5T tokens overall&#8221; and &#8220;800B synthetic code&#8221; (plus multilingual curation) in one team member&#8217;s recap: @code_star. Separate recaps mention 8T synthetic as part of 17T: @pratyushmaini.Ecosystem readiness: vLLM announced day-0 support for serving Trinity Large: @vllm_project. The meta-story in the replies is that a Western org is again attempting frontier-ish pretraining from scratch with an open model, rather than only post-training/evals.Agents everywhere: orchestration, subagents, planning critics, and IDE/CLI integrationAgent &#8220;swarm&#8221; vs &#8220;subagents&#8221; convergence: Kimi&#8217;s &#8220;Agent Swarm&#8221; pitch (dynamic subagent creation) parallels the broader pattern of central orchestrator + parallel specialists. The most explicit &#8220;starter pattern&#8221; articulation is LangChain&#8217;s stateless subagent model (parallel execution + minimized context bloat): @sydneyrunkle. Meanwhile, Kimi&#8217;s swarm is framed as trainable orchestration via Parallel-Agent RL (PARL) in community summaries (Zach Mueller).Reliability via &#8220;critique before execute&#8221;: Google&#8217;s Jules introduced a Planning Critic&#8212;a second agent that critiques plans pre-execution, claiming a 9.5% drop in task failure rates: @julesagent. Jules also added &#8220;Suggested Tasks&#8221; for proactive optimizations: @julesagent.Coding-agent products intensifying: Mistral shipped Vibe 2.0 upgrades (subagents, user-defined agents, skills/slash commands, and paid plans): @mistralvibe and @qtnx_. MiniMax launched an &#8220;Agent Desktop&#8221; workspace pitched as more polished than Claude Cowork: @omarsar0 (and MiniMax&#8217;s own onboarding automation: @MiniMax_AI).IDE infrastructure and retrieval: Cursor claims semantic search materially improves coding-agent performance and that indexing for large codebases is &#8220;orders of magnitude faster&#8221;: @cursor_ai. VS Code continues tightening agent UX (e.g., safer command execution explanations): @aerezk, plus MCP servers returning UI via MCP Apps spec (LIFX control panel example): @burkeholland.Document AI &amp; multimodal systems: DeepSeek-OCR 2 and &#8220;Agentic Vision&#8221;DeepSeek-OCR 2: learned reading order + token compression: DeepSeek-OCR 2 is framed as a shift from fixed raster scans to learned Visual Causal Flow with DeepEncoder V2, including 16&#215; visual token compression (256&#8211;1120 tokens/image) and 91.09% OmniDocBench v1.5 (+3.73%); vLLM shipped day-0 support: @vllm_project. Unsloth notes similar headline improvements: @danielhanchen.Mechanistic intuition (why it matters for pipelines): Jerry Liu provides a clear &#8220;why learned order helps&#8221; explanation: avoid semantically shredding tables/forms by allowing query tokens to attend to contiguous regions instead of strict left-to-right: @jerryjliu0. Teortaxes adds a pragmatic eval take: OCR 2 is &#8220;on par with dots.ocr&#8221; and &#8220;nowhere near SOTA,&#8221; but the ideas may influence later multimodal products: @teortaxesTex.Gemini &#8220;Agentic Vision&#8221; = vision + code execution loop: Google is productizing a &#8220;Think, Act, Observe&#8221; loop where the model writes/executes Python to crop/zoom/annotate images, claiming 5&#8211;10% quality boosts across many vision benchmarks: @_philschmid and the official thread: @GoogleAI. This is an explicit move toward tool-augmented vision being first-class, not bolted on.AI for science &amp; research workflows: OpenAI Prism as &#8220;Overleaf with AI&#8221;Prism launch: OpenAI introduced Prism, a free &#8220;AI-native workspace for scientists&#8221; powered by GPT-5.2, positioned as a unified LaTeX collaboration environment: @OpenAI and @kevinweil. Community summaries frame it as &#8220;Overleaf with AI&#8221; (proofreading, citations, literature search): @scaling01.Data/IP clarification: Kevin Weil clarified that Prism follows your ChatGPT data controls and that OpenAI is not taking a share of individual discoveries; any IP-alignment deals would be bespoke for large orgs: @kevinweil.Why it matters technically: Prism is a product bet that collaboration context + tool integration (LaTeX, citations, project state) becomes a durable advantage&#8212;mirroring the &#8220;context &gt; intelligence&#8221; theme circulating in Chinese discussions about OpenAI infra and org design: @ZhihuFrontier.Research notes &amp; benchmarks worth tracking (RL, planning, multilingual scaling)Long-horizon planning benchmark: DeepPlanning proposes verifiable-constraint planning tasks (multi-day travel, shopping) and reports frontier agents still struggle; emphasizes explicit reasoning patterns and parallel tool use: @iScienceLuvr. (This pairs nicely with the &#8220;travel planning again&#8221; meme: @teortaxesTex.)RL efficiency and reuse of traces: PrefixRL idea&#8212;condition on off-policy prefixes to speed RL on hard reasoning, claiming 2&#215; faster to same reward vs strong baseline: @iScienceLuvr.Multilingual scaling laws: Google Research announced ATLAS scaling laws for massively multilingual LMs with data-driven guidance on balancing data mix vs model size: @GoogleResearch.Math research reality check: Epoch&#8217;s FrontierMath: Open Problems benchmark invites attempts; &#8220;AI hasn&#8217;t solved any of these yet&#8221;: @EpochAIResearch.Top tweets (by engagement)OpenAI launches Prism (AI LaTeX research workspace): @OpenAIMoonshot founder video introducing Kimi K2.5: @Kimi_MoonshotKimi &#8220;video-to-code&#8221; website cloning demo: @KimiProductOllama: Kimi K2.5 on Ollama cloud + integrations: @ollamaClaude generating 3Blue1Brown-style animations claim (education impact): @LiorOnAIFigure introduces Helix 02 autonomous whole-body robotics control: @Figure_robotAI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. New Model and Benchmark ReleasesIntroducing Kimi K2.5, Open-Source Visual Agentic Intelligence (Activity: 643): Kimi K2.5 is an open-source visual agentic intelligence model that achieves global state-of-the-art (SOTA) performance on agentic benchmarks, with scores of 50.2% on the HLE full set and 74.9% on BrowseComp. It also leads in open-source vision and coding benchmarks, scoring 78.5% on MMMU Pro, 86.6% on VideoMMMU, and 76.8% on SWE-bench Verified. The model introduces an Agent Swarm feature in beta, allowing up to 100 sub-agents to work in parallel, making 1,500 tool calls and operating 4.5&#215; faster than a single-agent setup. Kimi K2.5 is available in chat and agent modes on kimi.com, with additional resources on Hugging Face. A comment highlights the impressive capability of 100 sub-agents working in parallel, suggesting potential for enhanced performance in coding tasks. Another comment notes the banning of the original poster, raising questions about account authenticity.Asleep_Strike746 highlights the impressive capability of Kimi K2.5 to run 100 sub-agents in parallel, suggesting potential for complex task execution, such as coding tasks. This parallelism could significantly enhance performance in multi-threaded environments, making it a powerful tool for developers looking to automate or optimize workflows.illusoryMechanist points out the scale of Kimi K2.5 with &#8216;1T Activated Parameters&#8217; and &#8216;32B&#8217; (likely referring to the model&#8217;s parameter count), indicating a substantial computational capacity. This suggests that Kimi K2.5 could handle large-scale data processing and complex problem-solving tasks, positioning it as a competitive player in the open-source AI landscape.Capaj shares a practical test of Kimi K2.5 by prompting it to generate an SVG of a fox on a unicycle. The result was described as &#8216;not too bad&#8217;, implying that while the model can handle creative tasks, there might still be room for improvement in terms of output quality or creativity. This kind of testing is crucial for understanding the model&#8217;s capabilities in real-world applications.Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement (Activity: 333): The image is a bar chart titled &#8220;Aider Benchmark&#8221; that illustrates the performance of various coding models in terms of their pass rate for polyglot code editing. The &#8220;Jan-v3-4B-base-INSTRUCT&#8221; model leads with a score of 18, significantly outperforming other models like &#8220;Qwen3-4B-THINKING-2507&#8221; with 12.1 and &#8220;Ministral-3-8B-INSTRUCT-2512&#8221; with 6.8. This highlights the Jan-v3 model&#8217;s high efficiency and over 40% improvement in performance, showcasing its enhanced capabilities in coding tasks. The model is designed for improved math and coding performance, making it a strong candidate for lightweight assistance and further fine-tuning. One commenter appreciates the Qwen 4B 2507 model for small tasks, noting its impressive performance despite its size. Another user shares mixed experiences with the Jan model, praising its ability to use search tools effectively but noting occasional tool call failures and odd responses, possibly due to system prompts.The Jan v3 Instruct model, a 4 billion parameter coding model, reportedly achieves a 40% improvement in performance with the Aider benchmark. This suggests significant advancements in its ability to handle coding tasks, potentially outperforming other models like Qwen 4B 2507 in specific scenarios. The model&#8217;s ability to utilize search tools effectively for code explanation is noted, although there are occasional failures in tool calls and some system prompt issues in web chat applications.A user reported mixed experiences with the Jan v3 model on chat.jan.ai, highlighting its capability to correctly use search tools and read code for explaining project flows. However, they also noted some tool call failures and irrelevant responses, possibly due to system prompts. The user expressed interest in the model&#8217;s potential integration with Claude Code, suggesting it could become a valuable tool for code search and Q&amp;A in daily coding tasks.The Jan v3 model&#8217;s performance in benchmarks is highlighted, with a specific mention of its demo availability at chat.jan.ai. The model&#8217;s ability to handle small and easy tasks effectively is compared to Qwen 4B 2507, which is favored for similar tasks. The discussion suggests that Jan v3&#8217;s fine-tuning may offer competitive advantages in certain coding scenarios.deepseek-ai/DeepSeek-OCR-2 &#183; Hugging Face (Activity: 385): DeepSeek-OCR-2 is a state-of-the-art OCR model available on Hugging Face, optimized for document processing with visual causal flow. It requires Python 3.12.9 and CUDA 11.8, and leverages libraries like torch and transformers. The model supports dynamic resolution and uses flash attention for enhanced performance on NVIDIA GPUs. It offers various prompts for document conversion, making it versatile for different OCR tasks. One user highlighted the impressive performance of PaddleOCR-VL when compared using scores from other models, suggesting its potential superiority. Another user shared a demo of DeepSeek OCR 2, noting initial issues with repetition due to user error, which were resolved by adjusting decoding parameters, leading to significantly improved performance over version 1.A user highlighted the impressive performance of PaddleOCR-VL, suggesting it stands out when compared to other models like B/C/D. This is based on scores reported by a third party, which the user trusts for evaluating model performance. This implies PaddleOCR-VL&#8217;s metrics are noteworthy in the context of OCR model comparisons.Another user shared their experience with implementing a demo for DeepSeek OCR 2 using GPU credits. Initially, they faced issues with repetition due to incorrect parameters, but after adjusting to DeepSeek&#8217;s recommended decoding parameters, the performance improved significantly. The user noted that the updated version is much more reliable than its predecessor, DeepSeek OCR v1.The GitHub repository and paper for DeepSeek OCR 2 were shared, providing resources for those interested in the technical details and implementation of the model. The paper likely contains in-depth information on the model&#8217;s architecture, training process, and performance benchmarks, which are crucial for technical evaluation and understanding.transformers v5 final is out &#128293; (Activity: 503): Transformers v5 from Hugging Face introduces significant performance improvements, particularly for Mixture-of-Experts (MoE) models, achieving 6x-11x speedups. The update simplifies the API by removing slow/fast tokenizers, offering explicit backends and enhanced performance. Additionally, dynamic weight loading is now faster, supporting MoE with quantization, tensor parallelism, and Parameter-Efficient Fine-Tuning (PEFT). A migration guide and detailed release notes are available for users transitioning to this version. One user inquired about the implications of these improvements for running small to medium-sized MoE models locally, suggesting that the enhancements might reduce memory bandwidth constraints. Another user reported a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed after updating to v5 and vllm 0.14.1.The Mixture-of-Experts (MoE) model in Transformers v5 shows significant performance improvements, with reported speedups ranging from 6x to 11x. This is particularly relevant for users running models locally, as it suggests that MoE can now utilize compute resources more efficiently, potentially reducing memory bandwidth constraints. This could be beneficial for setups using NVIDIA GPUs or AMD iGPUs, such as the Strix Halo, where compute power is a limiting factor.A user reported upgrading to Transformers v5 and vllm 0.14.1 from 0.11, resulting in a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed for 40x workloads. This highlights the significant performance enhancements in the latest version, which could be crucial for applications requiring high throughput and low latency.The update in Transformers v5 now allows Mixture-of-Experts (MoE) models to work with quantized models, which was not possible before. This advancement enables more efficient model deployment by reducing the model size and computational requirements, making it feasible to run complex models on less powerful hardware without sacrificing performance.2. Local LLM Hardware and Setup Discussions216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 577): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these older GPUs when used in parallel. The image shows a setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM for machine learning tasks. The technical challenge lies in effectively utilizing these GPUs without significant bandwidth loss, as most affordable server motherboards support only a limited number of GPUs. Commenters express skepticism about the practicality of using older Tesla GPUs due to potential issues with token processing speed and cooling requirements. There is interest in how the author manages to connect multiple GPUs without bandwidth loss, and a suggestion that newer systems like DGX Spark might offer better performance for certain tasks.HugoCortell raises a technical concern about the bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could lead to a significant loss in bandwidth, which is crucial for efficient parallel processing in local LLM setups.BananaPeaches3 highlights a critical performance issue with older GPUs, particularly in handling large system prompts. They mention that while token generation speed might be acceptable, the prompt processing time can be a bottleneck, especially with prompts as large as 15k tokens. This suggests that newer systems like the DGX Spark might be more efficient despite slightly slower token generation speeds, due to faster prompt processing capabilities.FullOf_Bad_Ideas points out a limitation in the gpu_box_benchmark, which does not test for serving large models split across multiple GPUs. This is a significant use case for setups with high VRAM, indicating a gap in the benchmark&#8217;s ability to evaluate real-world performance for large-scale LLM applications.3. Teasers and Announcements from AI LabsThe Qwen Devs Are Teasing Something (Activity: 331): The image is a tweet from Tongyi Lab featuring an ASCII art face and a lightning bolt emoji, hinting at an upcoming announcement. The Reddit community speculates that this could be related to a new visual language model, possibly named Z-Image, which has been mentioned in recent ComfyUI pull requests. The timing of the announcement might be strategically planned before the Chinese New Year, aligning with other anticipated releases like K2.5 and potentially q3.5, dsv4, and mm2.2. Commenters are speculating that the announcement is related to the Z-Image model, which has been referenced in recent updates to ComfyUI. There is also a discussion about the timing of the release, suggesting it might be aligned with the Chinese New Year.The mention of &#8216;Z-Image&#8217; in ComfyUI PRs suggests a potential new feature or model update related to image processing. This aligns with recent updates where hidden items have been added to collections, indicating ongoing development and testing phases.There is speculation about the release of several models and updates before the Chinese New Year, including K2.5, q3.5, dsv4, and mm2.2. This timing is strategic as many labs aim to release updates before the holiday break, which is on January 17th this year.A user speculates about the release of &#8216;Qwen4 Next 48B A3B&#8217;, which could imply a new model or version with specific parameters, possibly indicating advancements in model architecture or capabilities.Minimax Is Teasing M2.2 (Activity: 322): The image is a tweet from MiniMax teasing an update to their AI model, M2.2, suggesting an imminent release with the phrase &#8220;M2.1 slays. M2.2 levels up. #soon.&#8221; This indicates a potential upgrade in capabilities or performance from the previous version, M2.1. The context suggests a competitive landscape in AI development, particularly among Chinese labs, with other models like Deepseek v4 and Kimi K3 also expected soon. The mention of ByteDance&#8217;s potential closed-source model adds to the competitive tension in the AI space. One comment suggests a shift in focus towards agentic Mixture of Experts (MoEs) models, potentially at the expense of updates to traditional 32B models. Another user expresses anticipation for the new model, highlighting the effectiveness of MiniMax 2.1 in combination with glm 4.7 for coding tasks, and the potential impact of the upcoming versions.Loskas2025 highlights the use of Minimax 2.1 and GLM 4.7 for coding, noting their excellence. They anticipate that the upcoming Minimax 2.2 and GLM 5, which is currently in training, could significantly enhance performance, suggesting a potential shift in the landscape of coding models.CriticallyCarmelized compares Minimax favorably to GLM 4.7, even at high quantization levels, indicating that Minimax is competitive in terms of performance. They express optimism that the new version could surpass current models, potentially becoming their preferred choice for local deployment.lacerating_aura mentions speculation around &#8216;giga-potato&#8217; being associated with DS4, but points out the lack of concrete evidence for the existence of DS4 or Kimi K3, indicating a gap in confirmed information about these models.I built a &#8220;hive mind&#8221; for Claude Code - 7 agents sharing memory and talking to each other (Activity: 422): The post describes a multi-agent orchestration system for Claude Code, featuring 7 specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, MIT licensed, and available on GitHub. A comment questions the similarity to the bmad method, suggesting potential overlap in approach. Another comment humorously questions whether the agents agree with each other, hinting at the complexity of multi-agent consensus.The project is compared to the BMAD method, which also involves multi-agent systems. The commenter is curious about the differences, suggesting that the approach might be similar in terms of agents sharing memory and communication protocols.A reference is made to Microsoft&#8217;s Autogen, which was released over two years ago as a solution for multi-agent systems. The commenter suggests exploring this resource for potential new ideas, indicating that the concept of multi-agent communication and shared memory is not new and has been explored by major tech companies.The choice of using Claude Code is questioned, with a suggestion to consider open-source alternatives. This implies a debate on the benefits of proprietary versus open-source platforms for developing multi-agent systems, hinting at potential advantages in community support and collaboration in open-source projects.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Kimi K2.5 and Open Source AI Model ReleasesOpen source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding. (Activity: 597): Kimi-K2.5, an open-source model, is reportedly outperforming Claude Opus 4.5 in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance gains are not detailed in the post. The claim suggests a significant advancement in open-source AI capabilities, but lacks comprehensive data to substantiate the comparison. Commenters express skepticism about the claim, highlighting that benchmarks may not fully represent real-world performance. They question the validity of the term &#8216;many&#8217; benchmarks and suggest that the practical utility of Kimi-K2.5 compared to Claude Opus 4.5 remains unproven.There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in real-world applications, despite benchmark results. Users argue that benchmarks often don&#8217;t reflect practical utility, especially in complex tasks like programming where Opus 4.5 might excel in providing solutions in a single prompt.The discussion highlights a common critique of benchmarks: they may not capture the full capabilities of a model in practical scenarios. Some users express doubt about the claim that Kimi-K2.5 surpasses Opus 4.5, questioning the specific benchmarks and real-world applicability, especially in coding tasks where Opus 4.5 is perceived to have an edge.One user claims significant practical success with Kimi-K2.5, stating it has replaced reports in a major company, suggesting that at least in some contexts, Kimi-K2.5 may offer substantial utility. This contrasts with the general skepticism about benchmarks translating to real-world performance.Kimi K2.5 Released!!! (Activity: 1149): The image presents a performance comparison chart for the newly released Kimi K2.5, which is claimed to set a new state-of-the-art (SOTA) in agentic tasks. The chart compares Kimi K2.5 against other models like GPT-5.2 (xhigh), Claude Opus 4.5, and Gemini 3 Pro across various tasks, including agents, coding, image, and video tasks. Kimi K2.5 is highlighted as leading in several categories, notably &#8220;Agents: BrowseComp&#8221; and &#8220;Image: OmniDocBench 1.5&#8221;, suggesting its superior performance in these areas. The release is accompanied by a blog post detailing the advancements (link).* Commenters express skepticism about the benchmarks, questioning if they are cherry-picked, and discuss the model&#8217;s performance in hallucination and instruction-following tests. One user notes that Kimi K2.5, while improved, still outputs incorrect answers confidently, similar to other models like Gemini 3, which also confidently provides incorrect answers. GPT-5.1 and 5.2 are noted for admitting &#8220;I don&#8217;t know&#8221; in similar tests, highlighting ongoing challenges with hallucinations in AI models.A user conducted a test on Kimi K2.5&#8217;s ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed out hallucinated contest problems and second-guessed itself, ultimately providing an incorrect answer. This is seen as a slight improvement over Kimi K2, which failed to follow instructions and timed out. In comparison, Gemini 3 also confidently provided incorrect answers, while GPT 5.1 and 5.2 were the only models to admit &#8216;I don&#8217;t know&#8217;.The concept of an &#8216;agent swarm&#8217; in Kimi K2.5 is intriguing, with speculation that it involves over 100 instances of the model being directed by a single overseeing instance. This setup is expected to be expensive, and there is curiosity about whether it could be a single model handling multiple tasks simultaneously, which would represent a significant advancement. The idea of scaffolding, where multiple models work together, seems more plausible to some users.There is skepticism about the benchmarks used to compare Kimi K2.5 with other models like Gemini 3. A user questions whether the benchmarks are cherry-picked, expressing doubt that Kimi K2.5 consistently outperforms Gemini 3, which seems unlikely given the current state of model capabilities.Sir, the Chinese just dropped a new open model (Activity: 1915): Kimi has released an open-source trillion-parameter vision model that reportedly matches the performance of Opus 4.5 on several benchmarks. This model is significant due to its scale and the claim of competitive performance, which is notable given the typically high cost and complexity associated with such large models. The release could impact the landscape of AI vision models, especially in terms of accessibility and cost-effectiveness. There is skepticism in the community about the true performance of Chinese models, with some users suggesting that while they are cost-effective, they may not genuinely match the capabilities of models like Claude, GPT, or Gemini despite benchmark claims.Tricky-Elderberry298 highlights the limitations of relying solely on benchmarks for evaluating LLMs, drawing an analogy to evaluating cars based only on engine specs. They argue that real-world usage, such as how models like Claude and Kimi K2.5 perform in complex projects, is a more meaningful measure of capability than pure benchmark scores.Durable-racoon discusses the unique capabilities of the Kimi K2.5 model, noting its ability to orchestrate 500 agents simultaneously and convert videos into working software UI prototypes. They also mention its superior performance in creative writing compared to Opus, while acknowledging that Kimi K2.5 is more expensive than most Chinese models, priced at $0.60/$3 for in/out operations.DistinctWay9169 points out that many Chinese models, such as Minimax and GLM, are often &#8216;bench maxed,&#8217; meaning they perform well on benchmarks but may not match the real-world performance of models like Claude, GPT, or Gemini. This suggests a discrepancy between benchmark results and actual usability or effectiveness in practical applications.Gemini 3 finally has an open-source competitor (Activity: 168): The image is a comparison chart that highlights the performance of the newly released Kimi K2.5 vision model against other prominent models like Gemini 3 Pro. According to the chart, Kimi K2.5 performs competitively, often surpassing Gemini 3 Pro in various benchmarks such as &#8220;Humanity&#8217;s Last Exam,&#8221; &#8220;BrowseComp,&#8221; and &#8220;OmniDocBench 1.5.&#8221; This positions Kimi K2.5 as a strong open-source alternative to the closed-source Gemini 3 Pro, challenging its dominance in the field. Some users express skepticism about Kimi K2.5&#8217;s real-world performance compared to Gemini 3 Pro, with comments suggesting that while the benchmarks are impressive, practical performance may not match up. There is also a sentiment that open-source models may struggle to compete with large, closed-source companies.MichelleeeC highlights a significant performance gap between the open-source competitor and Gemini 3, particularly when tested on niche topics without search engine assistance. This suggests that the open-source model may lack the comprehensive training data or fine-tuning that Gemini 3 benefits from, impacting its ability to provide accurate answers in specialized areas.Old_Technology3399 and Just_Lingonberry_352 both express that the open-source competitor is notably inferior to Gemini 3. This consensus indicates that while the open-source model may be a step towards democratizing AI, it still falls short in terms of performance and reliability compared to established, closed-source models like Gemini 3.ChezMere&#8217;s comment about &#8216;benchhacking&#8217; suggests skepticism about the open-source model&#8217;s real-world performance versus its benchmark results. This implies that while the model might perform well in controlled tests, it may not translate to effective real-world application, highlighting a common issue in AI model evaluation.Enterprise-ready open source/Chinese AIs are poised to out-sell American proprietary models. Personal investors take note. (Activity: 30): The post highlights the competitive edge of open-source and Chinese AI models over American proprietary models in niche domains, emphasizing their cost-effectiveness and comparable performance. Notable models include DeepSeek-V3 / R1, which ranks #1 on MATH-500 and LiveCodeBench, and Qwen3-Max / Coder from Alibaba, which excels in LMSYS Chatbot Arena and MMLU-Pro. These models offer significantly lower costs per million tokens compared to proprietary models like OpenAI&#8217;s GPT-5.2 and Claude 4.5 Sonnet, with input costs as low as $0.15 to $0.60 per million tokens, compared to proprietary costs starting at $3.00. The post suggests that personal investors should consider these developments as Chinese firms issue IPOs, with a16z noting that 80% of startups pitching them use Chinese open-source AI models. A comment questions whether Kimi K2 is superior to GLM 4.7, indicating a debate on the relative performance of these models in specific contexts.The discussion compares the performance of the Kimi K2 model with the GLM 4.7 model. Kimi K2 is noted for its efficiency in specific tasks, potentially outperforming GLM 4.7 in certain benchmarks. However, the choice between these models may depend on the specific use case, as GLM 4.7 might excel in different areas. The conversation highlights the importance of evaluating models based on task-specific performance metrics rather than general claims of superiority.2. Gemini AI Studio and Usage LimitationsGemini AI Studio is basically unusable now. Any other LLMs with a 1M context window? (Activity: 162): Gemini AI Studio has become less viable for users due to Google&#8217;s reduction in daily prompt limits, impacting workflows that rely on its 1 million token context window. Users working with extensive documents and conversations are seeking alternatives. Notably, Grok 4.1 offers a 2 million token context window, and Claude Sonnet 4.5 provides a 1 million token context window within the Kilo Code environment. These alternatives may serve users needing large-context capabilities. Some users suggest that effective CLI tools like Claudie-cli or codex-cli can mitigate the need for massive context windows by efficiently managing and retrieving information from large texts.Coldshalamov mentions that Grok 4.1 fast offers a 2M context window, which is double the size of the 1M context window being discussed. This suggests that Grok 4.1 fast could be a viable alternative for those needing larger context windows.Unlucky_Quote6394 highlights that Claude Sonnet 4.5 provides a 1M context window when used within Kilo Code, indicating another option for users seeking large context capabilities.Ryanmonroe82 suggests embedding documents as an alternative to using cloud models, implying that this method could be more efficient and effective for handling large text data without relying on extensive context windows.32,768 or (2^15) tokens in hot memory.... Gemini has been PURPOSELY THROTTLED by Alphabet and been made into a bait and switch. Gemini Pro is WORSE than the free version as of TODAY. They market over a million tokens for Pro users. This is fraud. (Activity: 858): The Reddit post claims that Alphabet has intentionally throttled the token limit for Gemini Pro to 32,768 tokens, which is significantly lower than the advertised capacity of over a million tokens. This throttling allegedly degrades the performance of Gemini Pro, making it less effective than the free version. The post also mentions that the Ultra and Enterprise versions have a hard cap of 131,072 tokens, despite advertising up to 2 million tokens. The author expresses concern that this limitation could drive users away, especially with potential integration into Siri. Commenters express dissatisfaction with Gemini&#8217;s performance, comparing it unfavorably to older models like GPT-3. There is also criticism of the memory management, with claims that it leads to data inaccuracies and inefficiencies.Substantial_Net9923 highlights a significant issue with Gemini&#8217;s memory management, noting that the model&#8217;s memory loss due to indexing is problematic. This inefficiency is particularly evident in quantitative finance trading discussions, where the model is reportedly generating inaccurate data more frequently than before, suggesting a decline in reliability.klopppppppp observes a drastic decline in Gemini&#8217;s performance, comparing it to older models like GPT-3. Despite this, they note that Gemini still performs exceptionally well in &#8216;deep research mode,&#8217; indicating that the model&#8217;s capabilities might be context-dependent or throttled in certain scenarios.SorryDistribution604 expresses frustration with Gemini&#8217;s recent performance, likening it to older models such as GPT-3. This suggests a perceived regression in the model&#8217;s capabilities, which could be due to throttling or other limitations imposed on the Pro version.About the recent AI Studio Limit Downgrade: (Activity: 660): The image is a notification from the Gemini API about a reduction in free usage limits for AI Studio users, suggesting a transition to using an API key for continued access. It indicates that these limits may decrease further over time, and mentions ongoing efforts to integrate with Google AI Pro/Ultra to share limits within AI Studio. This change reflects a broader trend of tightening access to free AI resources, potentially impacting developers relying on these tools for experimentation and development. Commenters express frustration over the reduction in free usage limits, noting that Gemini&#8217;s performance in following instructions has also declined. There is a sentiment that these changes are detrimental to AI Studio&#8217;s utility, as users feel they are receiving less value and functionality.trashyslashers highlights a significant issue with the Gemini model&#8217;s performance, noting that it is &#8216;getting worse at listening to instructions.&#8217; This suggests a degradation in the model&#8217;s ability to follow user commands, which is compounded by the reduction in daily usage limits. Users are forced to &#8216;rewrite and regenerate&#8217; requests, indicating inefficiencies in the model&#8217;s processing capabilities.Decent_Ingenuity5413 raises concerns about the stability and reliability of AI Studio&#8217;s service, drawing parallels to OpenAI&#8217;s past issues with unexpected changes. The comment also points out a critical billing issue with the Gemini API, where users have experienced &#8216;massive overbilling&#8217; due to token counting errors, leading to charges exceeding $70,000. This highlights a significant flaw in the billing system that could deter average consumers from using the API.Sensitive_Shift1489 expresses frustration over the perceived downgrading of AI Studio in favor of other Google AI products like Gemini App and CLI. The comment implies that these changes are part of a broader strategy to shift focus and resources, potentially at the expense of AI Studio&#8217;s quality and user satisfaction.3. Qwen Model Performance and ApplicationsQwen3-Max-Thinking - Comparible performance to Commercial Models (Activity: 40): Qwen3-Max-Thinking is an AI model that claims to offer performance comparable to commercial models, focusing on enhanced reasoning and decision-making capabilities. The model&#8217;s architecture and training methodologies are designed to improve efficiency and accuracy in complex tasks, as detailed in the original article. However, users have reported issues with the model&#8217;s agentic code mode, which fails to compile, potentially impacting its usability. One user expressed skepticism about the model&#8217;s usability due to compilation issues, while another hoped that Qwen3-Max-Thinking could help reduce the cost of commercial models.Qwen model. We get it! Qwen-3-max-thinking (Activity: 26): The post announces the release of the Qwen-3-max-thinking model, which is expected to be available this week. This model is noted for its enhanced features, although specific details about these enhancements are not provided in the post. The mention of &#8216;P.S. We got it&#8217; suggests that the model is already accessible to some users. One commenter questions whether the model has been available since October, indicating possible confusion or overlap with previous releases. Another asks if &#8216;OS&#8217; is being referred to, suggesting a potential misunderstanding or need for clarification on whether the model is open-source.3 Billion tokens&#65281;Evaluate my token usage? (Am I the most loyal user of QWEN3-MAX?) (Activity: 20): The post discusses a significant usage of the QWEN3-MAX language model, with the user consuming 3-4 billion tokens per day. This high usage has led to DAMO Academy granting additional concurrency and early access to the upcoming Qwen3.5-MAX. The user attributes a drop in usage to the weekend, indicating a consistent high demand otherwise. The post highlights the model&#8217;s effectiveness, with the user describing it as the &#8216;best LLM in the world&#8217;. Comments reveal a mix of curiosity and comparison, with one user noting their own high token consumption of 4 billion using a local model from the QWEN series. Another user shares a positive experience with the model&#8217;s ability to optimize website copywriting, though they express concerns about accessing the model for coding tasks.Available-Craft-5795 mentions using 4 billion tokens with the QWEN series, indicating a high level of engagement with these models. This suggests that the QWEN series is capable of handling large-scale token processing, which could be beneficial for extensive applications such as data analysis or large-scale content generation.Remarkable_Speed1402 discusses using the new model for optimizing website homepage copywriting, noting its effectiveness. However, they express concerns about the model&#8217;s coding capabilities, as they are unable to access it in their IDE. This highlights potential limitations in integrating the model with development environments, which could impact its usability for coding tasks.Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop (Activity: 10): The benchmark of Qwen3-32B on a single H100 GPU demonstrates a significant capacity gain when using INT4 quantization, achieving a 12x increase in user capacity compared to BF16, with only a 1.9% drop in accuracy. The study involved over 12,000 MMLU-Pro questions and 2,000 inference runs, showing that INT4 can support 47 concurrent users at a 4k context, compared to just 4 users with BF16. The full methodology and data are available here. A comment raised a question about the model&#8217;s performance in coding tasks, suggesting interest in how quantization affects specific application areas beyond general benchmarks.The discussion focuses on the performance of the Qwen3-32B model when quantized to INT4, highlighting a significant 12x increase in capacity with a minimal 1.9% drop in accuracy. This suggests that the model maintains high performance even with aggressive quantization, which is crucial for deploying large models in resource-constrained environments. However, the impact on specific tasks like coding remains a point of interest, as quantization can affect different tasks in varying ways.AI Discord RecapA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18Theme 1. Kimi K2.5 Launch: SOTA Agentic Benchmarks and Swarm CapabilitiesKimi K2.5 Crushes Agentic Benchmarks: Moonshot AI released Kimi K2.5, achieving global SOTA on the HLE full set (50.2%) and BrowseComp (74.9%), while posting open-source SOTA on MMMU Pro (78.5%) and SWE-bench Verified (76.8%) Tech Blog. Users across Discords noted the model was &#8220;silently rolled out&#8221; with significantly improved fact-checking and vision capabilities before the official announcement.Agent Swarm Mode Enters Beta: The release introduces an Agent Swarm feature capable of orchestrating up to 100 sub-agents and executing 1,500 tool calls in parallel, promising a 4.5x performance boost on complex tasks. High-tier users can access this self-directed mode on kimi.com, though early testers noted it consumes tool-call quotas rapidly.Pricing and API Instability Spark Debate: While the model&#8217;s capabilities impressed users, the new Kimi Code plan drew criticism for lower limits compared to competitors like Z.ai, with promotional pricing ending in February. Integration with OpenRouter faced initial hiccups, with users reporting errors related to tool use endpoints and image URL handling.Theme 2. Hardware Acceleration: Unsloth Speedups, FlagOS, and Kernel OpsUnsloth Accelerates MoE Training by 14x: Unsloth announced that MoE training is now 14x faster than v4, with upcoming optimizations projected to double that speed again for a total 30x boost. The team also rolled out full support for transformers v5, streamlining workflows for users on the latest library versions Announcement.FlagOS Targets Unified AI Stacks: Engineers discussed the introduction of FlagOS, an open-source system software stack designed to unify Model&#8211;System&#8211;Chip layers for better workload portability across heterogeneous hardware. The project aims to incorporate insights from hardware&#8211;software co-design to bridge the gap between ML systems and compilers.Tinygrad Codegens Flash Attention Directly: In the Tinygrad community, members successfully proved the ability to codegen Flash Attention directly from a frontend definition of naive attention using granular rewrites. Simultaneously, discussions highlighted a shift toward Megakernels over traditional kernel schedulers to optimize GPU throughput Luminal Blog.Theme 3. OpenAI Ecosystem: Prism, GPT-5.2 Performance, and Model DecayPrism Workspace Unlocks Scientific Collaboration: OpenAI launched Prism, a dedicated workspace powered by GPT-5.2 designed to streamline scientific research and writing for ChatGPT personal account holders Video Demo. While the tool targets academic rigor, users debating GPT-5.2 vs. Claude Opus 4.5 noted that OpenAI&#8217;s model still struggles with creative writing, a flaw Sam Altman reportedly admitted to.Model Deterioration Blamed on Leechers: A recurring theory across channels suggests significant degradation in ChatGPT and Claude performance, with some users claiming a 40% drop in quality. Speculation points to free tier users (&#8221;leechers&#8221;) diluting compute resources or models recursively training on their own synthetic outputs.GPT-5 Control Shell Leaked: A file dubbed the GPT-5_Hotfix.md surfaced, purported to be a pre-generation control shell that enforces strict syntax and intent locking to prevent model drift. The leak suggests OpenAI is using aggressive &#8220;wrappers&#8221; to manage output quality before generation even begins.Theme 4. Agentic Coding Wars: Tooling, Security, and RebrandsClawdbot Morphs into Moltbot After Security Scare: Following a trademark dispute with Anthropic and serious community concerns about zero-auth vulnerabilities, the popular agent Clawdbot rebranded to Moltbot Announcement. Users previously flagged that the bot could read environment keys without permission, posing risks to sensitive financial and personal data.Cursor and Cline Face Usability Headwinds: Users expressed frustration with Cursor&#8217;s pricing model, noting that a few complex prompts could cost $0.50, while others struggled to run Cline on modest hardware (8GB VRAM), facing CUDA0 buffer errors. Community fixes involved reducing context lengths to 9000 and offloading memory management to dedicated GPU settings.Karpathy Bets on Agent-First Coding: Andrej Karpathy sparked discussion by outlining a strategic shift toward agent-driven coding using Claude, emphasizing the &#8220;tireless persistence&#8221; of LLMs over traditional methods Post. This aligns with the release of Manus Skills, where developers are incentivized with free credits to build use cases for the new agentic platform.Theme 5. Theoretical Limits and Safety: Hallucinations and Bio-RisksMath Proves Hallucination is Inevitable: A new paper discussed in the BASI Discord mathematically proves that LLMs will always hallucinate, utilizing the same principles found in jailbreaking mechanics Arxiv Paper. Researchers noted that jailbreaking exacerbates this issue by distorting the context model, preventing it from flagging malicious or incorrect tags.Fine-Tuning Unlocks Dormant Bio-Risks: An Anthropic paper sparked debate at EleutherAI by demonstrating that fine-tuning open-source models on frontier model outputs can unsuppress harmful capabilities, such as biorisks, even if previously safety-trained Arxiv Link. The findings suggest that refusals are fragile and can be undone with minimal compute, raising concerns about dual-use technologies.AI Detection Tools Flag Human Academics: Engineers highlighted a growing issue where AI detection tools consistently mislabel human-written, pre-GPT academic texts as AI-generated. The consensus is that these detectors are fundamentally flawed, yet institutions continue to rely on them, creating friction for researchers and students.",
      "url": "https://www.latent.space/p/ainews-moonshot-kimi-k25-beats-sonnet",
      "author": "Unknown",
      "published": "2026-01-28T05:01:42",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Building on yesterday's [Social](/?date=2026-01-28&category=social#item-b0394ccda6d3) buzz, Moonshot AI released Kimi K2.5, a 32B active/1T parameter MoE model claiming to beat Claude Sonnet 4.5 at half the cost while achieving SOTA on open model benchmarks. The model features native image and video understanding plus a novel 100-parallel agent swarm management capability, trained on 15T multimodal tokens.",
      "importance_score": 88.0,
      "reasoning": "Major frontier model release with SOTA open model claims, multimodal capabilities, and novel agentic features. Direct competition with leading closed models at significantly lower cost represents important market development.",
      "themes": [
        "Model Release",
        "Multimodal AI",
        "Agentic AI",
        "Open Source"
      ],
      "continuation": {
        "original_item_id": "b0394ccda6d3",
        "original_date": "2026-01-28",
        "original_category": "social",
        "original_title": " Congrats @Kimi_Moonshot on Kimi K2.5  a native multimodal agentic model built on 15T vision-langu...",
        "continuation_type": "mainstream_pickup",
        "should_demote": false,
        "reference_text": "Building on yesterday's **Social** buzz"
      },
      "summary_html": "<p>Building on yesterday's <a href=\"/?date=2026-01-28&amp;category=social#item-b0394ccda6d3\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a> buzz, Moonshot AI released Kimi K2.5, a 32B active/1T parameter MoE model claiming to beat Claude Sonnet 4.5 at half the cost while achieving SOTA on open model benchmarks. The model features native image and video understanding plus a novel 100-parallel agent swarm management capability, trained on 15T multimodal tokens.</p>",
      "content_html": "<p>AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. AINews website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!Kimi has been on an absolute tear in the past year, and we last heard from them in November with Kimi K2 Thinking. Like K2, todays K2.5 is still a 32B active-1T param model (384 experts), built through continual pretraining on 15 trillion mixed visual and text tokens atop Kimi-K2-Base (which itself was on 15T tokens), and with an EXTREMELY well produced video from their founder (3 minutes, just watch it):They again claim SOTA on HLE and BrowseComp (footnotes give confidence the tests are legit), but also open model SOTA for vision and coding tasks:tweetThere are a few notables here - Kimi K2.5 is natively multimodal for the first time, perhaps borrowing from Kimi VL, but is attributed to massive-scale vision-text joint pre-training including VIDEO understanding - simply upload a screen recording and K2.5 can reconstruct the website for you:The fact that this is a continued pretrain that changes arch (+400M param MoonViT vision encoder) is VERY exciting for model training folks who rarely get to see a scaled up model do stuff like this.The other 2 headline features are equally exciting: Agent Swarm (only for paid users on the Kimi app) which learns to self-direct an agent swarm of up to 100 sub-agents, executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows. This parallelism results in higher end result performance with up to 4.5x faster speed ignoring token cost of course.For illustration, here is the output for:\"build a list of the top 100 funded ai startups (make sure they're actually Al - we want things like perplexity and cursor and cognition and elevenlabs and turbopuffer, NOT pretenders... use your best jdugement for criteria) and sort by valuation. use authoritative sources like Techcrunch and TheInformation and top VC firms like Sequoia, Benchmark, Redpoint, Greylock, and Conviction, as well as guests from the Latent Space podcast and Al Engineer conferences. augment the list with useful and interesting facts eg where they are based, size of team, short description of product, what their incumbents/competitors might be, what their bull case is, what Paul Graham would advise them to do.\"and Office Productivity with K2.5 Agent focused on high-density, large-scale office work end to end.This is not empty regurgitation - We saw enough to sign up as a paying subscriber of the Kimi App going forward. As Artificial Analysis notes, the China-Western gap in open models just took another big leap today.AI Twitter RecapMoonshotAIs Kimi K2.5 ecosystem: open multimodal MoE + Agent Swarm pushKimi K2.5 model drop and positioning: Moonshot positions Kimi K2.5 as a flagship open-weights model with native multimodality (image + video), strong agentic performance, and aggressive API pricing/latency claims. Official launch media and messaging: founder intro video, pricing/throughput claims incl. Turbo-level speed 60100 tok/s, plus early community reactions emphasizing agent swarm and multimodal capability (kimmonismus, kimmonismus on multimodal/video).Technical gist (as surfaced by the community): A useful unpacking of K2.5s reported ingredients~15T mixed visual+text tokens continual pretraining, context 128K256K via YaRN, release in INT4 with selective quantization (only routed experts quantized), and the Agent Swarm orchestration concept (dynamic generation of subagents; up to 100 parallel subagents / 1,500 steps; wall-time improvements claimed 34.5) is summarized by @TheZachMueller (and points to the technical report).Benchmarks/third-party eval framing: Artificial Analysis positions K2.5 as leading open weights and closer to frontier labs, highlighting GDPval-AA Elo 1309 (agentic knowledge work harness), MMMU Pro 75%, INT4 ~595GB, and a 64% hallucination rate (improved vs K2 Thinking) among other stats: @ArtificialAnlys. LMArena announcements also place K2.5 Thinking at #1 open model in their Text Arena snapshot: @arena. (Treat leaderboards as point-in-time; harness/tooling and prompting matter.)Distribution and runs at home signals: K2.5 landed quickly across infra surfaces: Ollama cloud with launch integrations (@ollama), Together AI listing (@togethercompute), and Fireworks as a partner (Moonshot). A notable local-inference datapoint: K2.5 reportedly runs (slowly but usable) on 2 M3 Ultra via MLX with sharded generation, ~21.9 tok/s at high memory use: @awnihannun (+ command snippet here).Product surface area around Kimi: Moonshot also pushed adjacent tooling: Kimi Code, an Apache-2.0 open-source coding agent integrating with common IDEs/editors (announcement), and an Agent SDK to build custom agents (link). A Kimi Product account is explicitly aimed at distributing prompts/use-cases (launch), with a viral demo of video-to-code website cloning (demo).Open American comeback at scale: Arcee/Prime Intellect Trinity Large Preview (400B MoE)Trinity Large Preview release: Arcee dropped Trinity Large initial weights as a preview release: @arcee_ai, with expanded details from @latkins. Prime Intellect frames it as an open 400B MoE with 13B active trained with Datology data: @PrimeIntellect. OpenRouter offered limited-time free access: @OpenRouterAI.Architecture/training details (most concrete technical tweet): A strong technical snapshot comes from @samsja19: 400B/A13B MoE, trained over 17T tokens; 3:1 interleaved local/global gated attention, SWA, NoPE on global layers + RoPE on local layers (as written in tweet), depth-scaled sandwich norm, sigmoid routing, trained with Muon; trained on ~2,000 B300s for a month on Prime Intellect infra, with data curation by DatologyAI.Data scaling emphasis: Datologys involvement is highlighted as a major part of the project: 6.5T tokens overall and 800B synthetic code (plus multilingual curation) in one team members recap: @code_star. Separate recaps mention 8T synthetic as part of 17T: @pratyushmaini.Ecosystem readiness: vLLM announced day-0 support for serving Trinity Large: @vllm_project. The meta-story in the replies is that a Western org is again attempting frontier-ish pretraining from scratch with an open model, rather than only post-training/evals.Agents everywhere: orchestration, subagents, planning critics, and IDE/CLI integrationAgent swarm vs subagents convergence: Kimis Agent Swarm pitch (dynamic subagent creation) parallels the broader pattern of central orchestrator + parallel specialists. The most explicit starter pattern articulation is LangChains stateless subagent model (parallel execution + minimized context bloat): @sydneyrunkle. Meanwhile, Kimis swarm is framed as trainable orchestration via Parallel-Agent RL (PARL) in community summaries (Zach Mueller).Reliability via critique before execute: Googles Jules introduced a Planning Critica second agent that critiques plans pre-execution, claiming a 9.5% drop in task failure rates: @julesagent. Jules also added Suggested Tasks for proactive optimizations: @julesagent.Coding-agent products intensifying: Mistral shipped Vibe 2.0 upgrades (subagents, user-defined agents, skills/slash commands, and paid plans): @mistralvibe and @qtnx_. MiniMax launched an Agent Desktop workspace pitched as more polished than Claude Cowork: @omarsar0 (and MiniMaxs own onboarding automation: @MiniMax_AI).IDE infrastructure and retrieval: Cursor claims semantic search materially improves coding-agent performance and that indexing for large codebases is orders of magnitude faster: @cursor_ai. VS Code continues tightening agent UX (e.g., safer command execution explanations): @aerezk, plus MCP servers returning UI via MCP Apps spec (LIFX control panel example): @burkeholland.Document AI &amp; multimodal systems: DeepSeek-OCR 2 and Agentic VisionDeepSeek-OCR 2: learned reading order + token compression: DeepSeek-OCR 2 is framed as a shift from fixed raster scans to learned Visual Causal Flow with DeepEncoder V2, including 16 visual token compression (2561120 tokens/image) and 91.09% OmniDocBench v1.5 (+3.73%); vLLM shipped day-0 support: @vllm_project. Unsloth notes similar headline improvements: @danielhanchen.Mechanistic intuition (why it matters for pipelines): Jerry Liu provides a clear why learned order helps explanation: avoid semantically shredding tables/forms by allowing query tokens to attend to contiguous regions instead of strict left-to-right: @jerryjliu0. Teortaxes adds a pragmatic eval take: OCR 2 is on par with dots.ocr and nowhere near SOTA, but the ideas may influence later multimodal products: @teortaxesTex.Gemini Agentic Vision = vision + code execution loop: Google is productizing a Think, Act, Observe loop where the model writes/executes Python to crop/zoom/annotate images, claiming 510% quality boosts across many vision benchmarks: @_philschmid and the official thread: @GoogleAI. This is an explicit move toward tool-augmented vision being first-class, not bolted on.AI for science &amp; research workflows: OpenAI Prism as Overleaf with AIPrism launch: OpenAI introduced Prism, a free AI-native workspace for scientists powered by GPT-5.2, positioned as a unified LaTeX collaboration environment: @OpenAI and @kevinweil. Community summaries frame it as Overleaf with AI (proofreading, citations, literature search): @scaling01.Data/IP clarification: Kevin Weil clarified that Prism follows your ChatGPT data controls and that OpenAI is not taking a share of individual discoveries; any IP-alignment deals would be bespoke for large orgs: @kevinweil.Why it matters technically: Prism is a product bet that collaboration context + tool integration (LaTeX, citations, project state) becomes a durable advantagemirroring the context &gt; intelligence theme circulating in Chinese discussions about OpenAI infra and org design: @ZhihuFrontier.Research notes &amp; benchmarks worth tracking (RL, planning, multilingual scaling)Long-horizon planning benchmark: DeepPlanning proposes verifiable-constraint planning tasks (multi-day travel, shopping) and reports frontier agents still struggle; emphasizes explicit reasoning patterns and parallel tool use: @iScienceLuvr. (This pairs nicely with the travel planning again meme: @teortaxesTex.)RL efficiency and reuse of traces: PrefixRL ideacondition on off-policy prefixes to speed RL on hard reasoning, claiming 2 faster to same reward vs strong baseline: @iScienceLuvr.Multilingual scaling laws: Google Research announced ATLAS scaling laws for massively multilingual LMs with data-driven guidance on balancing data mix vs model size: @GoogleResearch.Math research reality check: Epochs FrontierMath: Open Problems benchmark invites attempts; AI hasnt solved any of these yet: @EpochAIResearch.Top tweets (by engagement)OpenAI launches Prism (AI LaTeX research workspace): @OpenAIMoonshot founder video introducing Kimi K2.5: @Kimi_MoonshotKimi video-to-code website cloning demo: @KimiProductOllama: Kimi K2.5 on Ollama cloud + integrations: @ollamaClaude generating 3Blue1Brown-style animations claim (education impact): @LiorOnAIFigure introduces Helix 02 autonomous whole-body robotics control: @Figure_robotAI Reddit Recap/r/LocalLlama + /r/localLLM Recap1. New Model and Benchmark ReleasesIntroducing Kimi K2.5, Open-Source Visual Agentic Intelligence (Activity: 643): Kimi K2.5 is an open-source visual agentic intelligence model that achieves global state-of-the-art (SOTA) performance on agentic benchmarks, with scores of 50.2% on the HLE full set and 74.9% on BrowseComp. It also leads in open-source vision and coding benchmarks, scoring 78.5% on MMMU Pro, 86.6% on VideoMMMU, and 76.8% on SWE-bench Verified. The model introduces an Agent Swarm feature in beta, allowing up to 100 sub-agents to work in parallel, making 1,500 tool calls and operating 4.5 faster than a single-agent setup. Kimi K2.5 is available in chat and agent modes on kimi.com, with additional resources on Hugging Face. A comment highlights the impressive capability of 100 sub-agents working in parallel, suggesting potential for enhanced performance in coding tasks. Another comment notes the banning of the original poster, raising questions about account authenticity.Asleep_Strike746 highlights the impressive capability of Kimi K2.5 to run 100 sub-agents in parallel, suggesting potential for complex task execution, such as coding tasks. This parallelism could significantly enhance performance in multi-threaded environments, making it a powerful tool for developers looking to automate or optimize workflows.illusoryMechanist points out the scale of Kimi K2.5 with 1T Activated Parameters and 32B (likely referring to the models parameter count), indicating a substantial computational capacity. This suggests that Kimi K2.5 could handle large-scale data processing and complex problem-solving tasks, positioning it as a competitive player in the open-source AI landscape.Capaj shares a practical test of Kimi K2.5 by prompting it to generate an SVG of a fox on a unicycle. The result was described as not too bad, implying that while the model can handle creative tasks, there might still be room for improvement in terms of output quality or creativity. This kind of testing is crucial for understanding the models capabilities in real-world applications.Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement (Activity: 333): The image is a bar chart titled Aider Benchmark that illustrates the performance of various coding models in terms of their pass rate for polyglot code editing. The Jan-v3-4B-base-INSTRUCT model leads with a score of 18, significantly outperforming other models like Qwen3-4B-THINKING-2507 with 12.1 and Ministral-3-8B-INSTRUCT-2512 with 6.8. This highlights the Jan-v3 models high efficiency and over 40% improvement in performance, showcasing its enhanced capabilities in coding tasks. The model is designed for improved math and coding performance, making it a strong candidate for lightweight assistance and further fine-tuning. One commenter appreciates the Qwen 4B 2507 model for small tasks, noting its impressive performance despite its size. Another user shares mixed experiences with the Jan model, praising its ability to use search tools effectively but noting occasional tool call failures and odd responses, possibly due to system prompts.The Jan v3 Instruct model, a 4 billion parameter coding model, reportedly achieves a 40% improvement in performance with the Aider benchmark. This suggests significant advancements in its ability to handle coding tasks, potentially outperforming other models like Qwen 4B 2507 in specific scenarios. The models ability to utilize search tools effectively for code explanation is noted, although there are occasional failures in tool calls and some system prompt issues in web chat applications.A user reported mixed experiences with the Jan v3 model on chat.jan.ai, highlighting its capability to correctly use search tools and read code for explaining project flows. However, they also noted some tool call failures and irrelevant responses, possibly due to system prompts. The user expressed interest in the models potential integration with Claude Code, suggesting it could become a valuable tool for code search and Q&amp;A in daily coding tasks.The Jan v3 models performance in benchmarks is highlighted, with a specific mention of its demo availability at chat.jan.ai. The models ability to handle small and easy tasks effectively is compared to Qwen 4B 2507, which is favored for similar tasks. The discussion suggests that Jan v3s fine-tuning may offer competitive advantages in certain coding scenarios.deepseek-ai/DeepSeek-OCR-2  Hugging Face (Activity: 385): DeepSeek-OCR-2 is a state-of-the-art OCR model available on Hugging Face, optimized for document processing with visual causal flow. It requires Python 3.12.9 and CUDA 11.8, and leverages libraries like torch and transformers. The model supports dynamic resolution and uses flash attention for enhanced performance on NVIDIA GPUs. It offers various prompts for document conversion, making it versatile for different OCR tasks. One user highlighted the impressive performance of PaddleOCR-VL when compared using scores from other models, suggesting its potential superiority. Another user shared a demo of DeepSeek OCR 2, noting initial issues with repetition due to user error, which were resolved by adjusting decoding parameters, leading to significantly improved performance over version 1.A user highlighted the impressive performance of PaddleOCR-VL, suggesting it stands out when compared to other models like B/C/D. This is based on scores reported by a third party, which the user trusts for evaluating model performance. This implies PaddleOCR-VLs metrics are noteworthy in the context of OCR model comparisons.Another user shared their experience with implementing a demo for DeepSeek OCR 2 using GPU credits. Initially, they faced issues with repetition due to incorrect parameters, but after adjusting to DeepSeeks recommended decoding parameters, the performance improved significantly. The user noted that the updated version is much more reliable than its predecessor, DeepSeek OCR v1.The GitHub repository and paper for DeepSeek OCR 2 were shared, providing resources for those interested in the technical details and implementation of the model. The paper likely contains in-depth information on the models architecture, training process, and performance benchmarks, which are crucial for technical evaluation and understanding.transformers v5 final is out  (Activity: 503): Transformers v5 from Hugging Face introduces significant performance improvements, particularly for Mixture-of-Experts (MoE) models, achieving 6x-11x speedups. The update simplifies the API by removing slow/fast tokenizers, offering explicit backends and enhanced performance. Additionally, dynamic weight loading is now faster, supporting MoE with quantization, tensor parallelism, and Parameter-Efficient Fine-Tuning (PEFT). A migration guide and detailed release notes are available for users transitioning to this version. One user inquired about the implications of these improvements for running small to medium-sized MoE models locally, suggesting that the enhancements might reduce memory bandwidth constraints. Another user reported a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed after updating to v5 and vllm 0.14.1.The Mixture-of-Experts (MoE) model in Transformers v5 shows significant performance improvements, with reported speedups ranging from 6x to 11x. This is particularly relevant for users running models locally, as it suggests that MoE can now utilize compute resources more efficiently, potentially reducing memory bandwidth constraints. This could be beneficial for setups using NVIDIA GPUs or AMD iGPUs, such as the Strix Halo, where compute power is a limiting factor.A user reported upgrading to Transformers v5 and vllm 0.14.1 from 0.11, resulting in a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed for 40x workloads. This highlights the significant performance enhancements in the latest version, which could be crucial for applications requiring high throughput and low latency.The update in Transformers v5 now allows Mixture-of-Experts (MoE) models to work with quantized models, which was not possible before. This advancement enables more efficient model deployment by reducing the model size and computational requirements, making it feasible to run complex models on less powerful hardware without sacrificing performance.2. Local LLM Hardware and Setup Discussions216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 577): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these older GPUs when used in parallel. The image shows a setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM for machine learning tasks. The technical challenge lies in effectively utilizing these GPUs without significant bandwidth loss, as most affordable server motherboards support only a limited number of GPUs. Commenters express skepticism about the practicality of using older Tesla GPUs due to potential issues with token processing speed and cooling requirements. There is interest in how the author manages to connect multiple GPUs without bandwidth loss, and a suggestion that newer systems like DGX Spark might offer better performance for certain tasks.HugoCortell raises a technical concern about the bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could lead to a significant loss in bandwidth, which is crucial for efficient parallel processing in local LLM setups.BananaPeaches3 highlights a critical performance issue with older GPUs, particularly in handling large system prompts. They mention that while token generation speed might be acceptable, the prompt processing time can be a bottleneck, especially with prompts as large as 15k tokens. This suggests that newer systems like the DGX Spark might be more efficient despite slightly slower token generation speeds, due to faster prompt processing capabilities.FullOf_Bad_Ideas points out a limitation in the gpu_box_benchmark, which does not test for serving large models split across multiple GPUs. This is a significant use case for setups with high VRAM, indicating a gap in the benchmarks ability to evaluate real-world performance for large-scale LLM applications.3. Teasers and Announcements from AI LabsThe Qwen Devs Are Teasing Something (Activity: 331): The image is a tweet from Tongyi Lab featuring an ASCII art face and a lightning bolt emoji, hinting at an upcoming announcement. The Reddit community speculates that this could be related to a new visual language model, possibly named Z-Image, which has been mentioned in recent ComfyUI pull requests. The timing of the announcement might be strategically planned before the Chinese New Year, aligning with other anticipated releases like K2.5 and potentially q3.5, dsv4, and mm2.2. Commenters are speculating that the announcement is related to the Z-Image model, which has been referenced in recent updates to ComfyUI. There is also a discussion about the timing of the release, suggesting it might be aligned with the Chinese New Year.The mention of Z-Image in ComfyUI PRs suggests a potential new feature or model update related to image processing. This aligns with recent updates where hidden items have been added to collections, indicating ongoing development and testing phases.There is speculation about the release of several models and updates before the Chinese New Year, including K2.5, q3.5, dsv4, and mm2.2. This timing is strategic as many labs aim to release updates before the holiday break, which is on January 17th this year.A user speculates about the release of Qwen4 Next 48B A3B, which could imply a new model or version with specific parameters, possibly indicating advancements in model architecture or capabilities.Minimax Is Teasing M2.2 (Activity: 322): The image is a tweet from MiniMax teasing an update to their AI model, M2.2, suggesting an imminent release with the phrase M2.1 slays. M2.2 levels up. #soon. This indicates a potential upgrade in capabilities or performance from the previous version, M2.1. The context suggests a competitive landscape in AI development, particularly among Chinese labs, with other models like Deepseek v4 and Kimi K3 also expected soon. The mention of ByteDances potential closed-source model adds to the competitive tension in the AI space. One comment suggests a shift in focus towards agentic Mixture of Experts (MoEs) models, potentially at the expense of updates to traditional 32B models. Another user expresses anticipation for the new model, highlighting the effectiveness of MiniMax 2.1 in combination with glm 4.7 for coding tasks, and the potential impact of the upcoming versions.Loskas2025 highlights the use of Minimax 2.1 and GLM 4.7 for coding, noting their excellence. They anticipate that the upcoming Minimax 2.2 and GLM 5, which is currently in training, could significantly enhance performance, suggesting a potential shift in the landscape of coding models.CriticallyCarmelized compares Minimax favorably to GLM 4.7, even at high quantization levels, indicating that Minimax is competitive in terms of performance. They express optimism that the new version could surpass current models, potentially becoming their preferred choice for local deployment.lacerating_aura mentions speculation around giga-potato being associated with DS4, but points out the lack of concrete evidence for the existence of DS4 or Kimi K3, indicating a gap in confirmed information about these models.I built a hive mind for Claude Code - 7 agents sharing memory and talking to each other (Activity: 422): The post describes a multi-agent orchestration system for Claude Code, featuring 7 specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, MIT licensed, and available on GitHub. A comment questions the similarity to the bmad method, suggesting potential overlap in approach. Another comment humorously questions whether the agents agree with each other, hinting at the complexity of multi-agent consensus.The project is compared to the BMAD method, which also involves multi-agent systems. The commenter is curious about the differences, suggesting that the approach might be similar in terms of agents sharing memory and communication protocols.A reference is made to Microsofts Autogen, which was released over two years ago as a solution for multi-agent systems. The commenter suggests exploring this resource for potential new ideas, indicating that the concept of multi-agent communication and shared memory is not new and has been explored by major tech companies.The choice of using Claude Code is questioned, with a suggestion to consider open-source alternatives. This implies a debate on the benefits of proprietary versus open-source platforms for developing multi-agent systems, hinting at potential advantages in community support and collaboration in open-source projects.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Kimi K2.5 and Open Source AI Model ReleasesOpen source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding. (Activity: 597): Kimi-K2.5, an open-source model, is reportedly outperforming Claude Opus 4.5 in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance gains are not detailed in the post. The claim suggests a significant advancement in open-source AI capabilities, but lacks comprehensive data to substantiate the comparison. Commenters express skepticism about the claim, highlighting that benchmarks may not fully represent real-world performance. They question the validity of the term many benchmarks and suggest that the practical utility of Kimi-K2.5 compared to Claude Opus 4.5 remains unproven.There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in real-world applications, despite benchmark results. Users argue that benchmarks often dont reflect practical utility, especially in complex tasks like programming where Opus 4.5 might excel in providing solutions in a single prompt.The discussion highlights a common critique of benchmarks: they may not capture the full capabilities of a model in practical scenarios. Some users express doubt about the claim that Kimi-K2.5 surpasses Opus 4.5, questioning the specific benchmarks and real-world applicability, especially in coding tasks where Opus 4.5 is perceived to have an edge.One user claims significant practical success with Kimi-K2.5, stating it has replaced reports in a major company, suggesting that at least in some contexts, Kimi-K2.5 may offer substantial utility. This contrasts with the general skepticism about benchmarks translating to real-world performance.Kimi K2.5 Released!!! (Activity: 1149): The image presents a performance comparison chart for the newly released Kimi K2.5, which is claimed to set a new state-of-the-art (SOTA) in agentic tasks. The chart compares Kimi K2.5 against other models like GPT-5.2 (xhigh), Claude Opus 4.5, and Gemini 3 Pro across various tasks, including agents, coding, image, and video tasks. Kimi K2.5 is highlighted as leading in several categories, notably Agents: BrowseComp and Image: OmniDocBench 1.5, suggesting its superior performance in these areas. The release is accompanied by a blog post detailing the advancements (link).* Commenters express skepticism about the benchmarks, questioning if they are cherry-picked, and discuss the models performance in hallucination and instruction-following tests. One user notes that Kimi K2.5, while improved, still outputs incorrect answers confidently, similar to other models like Gemini 3, which also confidently provides incorrect answers. GPT-5.1 and 5.2 are noted for admitting I dont know in similar tests, highlighting ongoing challenges with hallucinations in AI models.A user conducted a test on Kimi K2.5s ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed out hallucinated contest problems and second-guessed itself, ultimately providing an incorrect answer. This is seen as a slight improvement over Kimi K2, which failed to follow instructions and timed out. In comparison, Gemini 3 also confidently provided incorrect answers, while GPT 5.1 and 5.2 were the only models to admit I dont know.The concept of an agent swarm in Kimi K2.5 is intriguing, with speculation that it involves over 100 instances of the model being directed by a single overseeing instance. This setup is expected to be expensive, and there is curiosity about whether it could be a single model handling multiple tasks simultaneously, which would represent a significant advancement. The idea of scaffolding, where multiple models work together, seems more plausible to some users.There is skepticism about the benchmarks used to compare Kimi K2.5 with other models like Gemini 3. A user questions whether the benchmarks are cherry-picked, expressing doubt that Kimi K2.5 consistently outperforms Gemini 3, which seems unlikely given the current state of model capabilities.Sir, the Chinese just dropped a new open model (Activity: 1915): Kimi has released an open-source trillion-parameter vision model that reportedly matches the performance of Opus 4.5 on several benchmarks. This model is significant due to its scale and the claim of competitive performance, which is notable given the typically high cost and complexity associated with such large models. The release could impact the landscape of AI vision models, especially in terms of accessibility and cost-effectiveness. There is skepticism in the community about the true performance of Chinese models, with some users suggesting that while they are cost-effective, they may not genuinely match the capabilities of models like Claude, GPT, or Gemini despite benchmark claims.Tricky-Elderberry298 highlights the limitations of relying solely on benchmarks for evaluating LLMs, drawing an analogy to evaluating cars based only on engine specs. They argue that real-world usage, such as how models like Claude and Kimi K2.5 perform in complex projects, is a more meaningful measure of capability than pure benchmark scores.Durable-racoon discusses the unique capabilities of the Kimi K2.5 model, noting its ability to orchestrate 500 agents simultaneously and convert videos into working software UI prototypes. They also mention its superior performance in creative writing compared to Opus, while acknowledging that Kimi K2.5 is more expensive than most Chinese models, priced at $0.60/$3 for in/out operations.DistinctWay9169 points out that many Chinese models, such as Minimax and GLM, are often bench maxed, meaning they perform well on benchmarks but may not match the real-world performance of models like Claude, GPT, or Gemini. This suggests a discrepancy between benchmark results and actual usability or effectiveness in practical applications.Gemini 3 finally has an open-source competitor (Activity: 168): The image is a comparison chart that highlights the performance of the newly released Kimi K2.5 vision model against other prominent models like Gemini 3 Pro. According to the chart, Kimi K2.5 performs competitively, often surpassing Gemini 3 Pro in various benchmarks such as Humanitys Last Exam, BrowseComp, and OmniDocBench 1.5. This positions Kimi K2.5 as a strong open-source alternative to the closed-source Gemini 3 Pro, challenging its dominance in the field. Some users express skepticism about Kimi K2.5s real-world performance compared to Gemini 3 Pro, with comments suggesting that while the benchmarks are impressive, practical performance may not match up. There is also a sentiment that open-source models may struggle to compete with large, closed-source companies.MichelleeeC highlights a significant performance gap between the open-source competitor and Gemini 3, particularly when tested on niche topics without search engine assistance. This suggests that the open-source model may lack the comprehensive training data or fine-tuning that Gemini 3 benefits from, impacting its ability to provide accurate answers in specialized areas.Old_Technology3399 and Just_Lingonberry_352 both express that the open-source competitor is notably inferior to Gemini 3. This consensus indicates that while the open-source model may be a step towards democratizing AI, it still falls short in terms of performance and reliability compared to established, closed-source models like Gemini 3.ChezMeres comment about benchhacking suggests skepticism about the open-source models real-world performance versus its benchmark results. This implies that while the model might perform well in controlled tests, it may not translate to effective real-world application, highlighting a common issue in AI model evaluation.Enterprise-ready open source/Chinese AIs are poised to out-sell American proprietary models. Personal investors take note. (Activity: 30): The post highlights the competitive edge of open-source and Chinese AI models over American proprietary models in niche domains, emphasizing their cost-effectiveness and comparable performance. Notable models include DeepSeek-V3 / R1, which ranks #1 on MATH-500 and LiveCodeBench, and Qwen3-Max / Coder from Alibaba, which excels in LMSYS Chatbot Arena and MMLU-Pro. These models offer significantly lower costs per million tokens compared to proprietary models like OpenAIs GPT-5.2 and Claude 4.5 Sonnet, with input costs as low as $0.15 to $0.60 per million tokens, compared to proprietary costs starting at $3.00. The post suggests that personal investors should consider these developments as Chinese firms issue IPOs, with a16z noting that 80% of startups pitching them use Chinese open-source AI models. A comment questions whether Kimi K2 is superior to GLM 4.7, indicating a debate on the relative performance of these models in specific contexts.The discussion compares the performance of the Kimi K2 model with the GLM 4.7 model. Kimi K2 is noted for its efficiency in specific tasks, potentially outperforming GLM 4.7 in certain benchmarks. However, the choice between these models may depend on the specific use case, as GLM 4.7 might excel in different areas. The conversation highlights the importance of evaluating models based on task-specific performance metrics rather than general claims of superiority.2. Gemini AI Studio and Usage LimitationsGemini AI Studio is basically unusable now. Any other LLMs with a 1M context window? (Activity: 162): Gemini AI Studio has become less viable for users due to Googles reduction in daily prompt limits, impacting workflows that rely on its 1 million token context window. Users working with extensive documents and conversations are seeking alternatives. Notably, Grok 4.1 offers a 2 million token context window, and Claude Sonnet 4.5 provides a 1 million token context window within the Kilo Code environment. These alternatives may serve users needing large-context capabilities. Some users suggest that effective CLI tools like Claudie-cli or codex-cli can mitigate the need for massive context windows by efficiently managing and retrieving information from large texts.Coldshalamov mentions that Grok 4.1 fast offers a 2M context window, which is double the size of the 1M context window being discussed. This suggests that Grok 4.1 fast could be a viable alternative for those needing larger context windows.Unlucky_Quote6394 highlights that Claude Sonnet 4.5 provides a 1M context window when used within Kilo Code, indicating another option for users seeking large context capabilities.Ryanmonroe82 suggests embedding documents as an alternative to using cloud models, implying that this method could be more efficient and effective for handling large text data without relying on extensive context windows.32,768 or (2^15) tokens in hot memory.... Gemini has been PURPOSELY THROTTLED by Alphabet and been made into a bait and switch. Gemini Pro is WORSE than the free version as of TODAY. They market over a million tokens for Pro users. This is fraud. (Activity: 858): The Reddit post claims that Alphabet has intentionally throttled the token limit for Gemini Pro to 32,768 tokens, which is significantly lower than the advertised capacity of over a million tokens. This throttling allegedly degrades the performance of Gemini Pro, making it less effective than the free version. The post also mentions that the Ultra and Enterprise versions have a hard cap of 131,072 tokens, despite advertising up to 2 million tokens. The author expresses concern that this limitation could drive users away, especially with potential integration into Siri. Commenters express dissatisfaction with Geminis performance, comparing it unfavorably to older models like GPT-3. There is also criticism of the memory management, with claims that it leads to data inaccuracies and inefficiencies.Substantial_Net9923 highlights a significant issue with Geminis memory management, noting that the models memory loss due to indexing is problematic. This inefficiency is particularly evident in quantitative finance trading discussions, where the model is reportedly generating inaccurate data more frequently than before, suggesting a decline in reliability.klopppppppp observes a drastic decline in Geminis performance, comparing it to older models like GPT-3. Despite this, they note that Gemini still performs exceptionally well in deep research mode, indicating that the models capabilities might be context-dependent or throttled in certain scenarios.SorryDistribution604 expresses frustration with Geminis recent performance, likening it to older models such as GPT-3. This suggests a perceived regression in the models capabilities, which could be due to throttling or other limitations imposed on the Pro version.About the recent AI Studio Limit Downgrade: (Activity: 660): The image is a notification from the Gemini API about a reduction in free usage limits for AI Studio users, suggesting a transition to using an API key for continued access. It indicates that these limits may decrease further over time, and mentions ongoing efforts to integrate with Google AI Pro/Ultra to share limits within AI Studio. This change reflects a broader trend of tightening access to free AI resources, potentially impacting developers relying on these tools for experimentation and development. Commenters express frustration over the reduction in free usage limits, noting that Geminis performance in following instructions has also declined. There is a sentiment that these changes are detrimental to AI Studios utility, as users feel they are receiving less value and functionality.trashyslashers highlights a significant issue with the Gemini models performance, noting that it is getting worse at listening to instructions. This suggests a degradation in the models ability to follow user commands, which is compounded by the reduction in daily usage limits. Users are forced to rewrite and regenerate requests, indicating inefficiencies in the models processing capabilities.Decent_Ingenuity5413 raises concerns about the stability and reliability of AI Studios service, drawing parallels to OpenAIs past issues with unexpected changes. The comment also points out a critical billing issue with the Gemini API, where users have experienced massive overbilling due to token counting errors, leading to charges exceeding $70,000. This highlights a significant flaw in the billing system that could deter average consumers from using the API.Sensitive_Shift1489 expresses frustration over the perceived downgrading of AI Studio in favor of other Google AI products like Gemini App and CLI. The comment implies that these changes are part of a broader strategy to shift focus and resources, potentially at the expense of AI Studios quality and user satisfaction.3. Qwen Model Performance and ApplicationsQwen3-Max-Thinking - Comparible performance to Commercial Models (Activity: 40): Qwen3-Max-Thinking is an AI model that claims to offer performance comparable to commercial models, focusing on enhanced reasoning and decision-making capabilities. The models architecture and training methodologies are designed to improve efficiency and accuracy in complex tasks, as detailed in the original article. However, users have reported issues with the models agentic code mode, which fails to compile, potentially impacting its usability. One user expressed skepticism about the models usability due to compilation issues, while another hoped that Qwen3-Max-Thinking could help reduce the cost of commercial models.Qwen model. We get it! Qwen-3-max-thinking (Activity: 26): The post announces the release of the Qwen-3-max-thinking model, which is expected to be available this week. This model is noted for its enhanced features, although specific details about these enhancements are not provided in the post. The mention of P.S. We got it suggests that the model is already accessible to some users. One commenter questions whether the model has been available since October, indicating possible confusion or overlap with previous releases. Another asks if OS is being referred to, suggesting a potential misunderstanding or need for clarification on whether the model is open-source.3 Billion tokensEvaluate my token usage? (Am I the most loyal user of QWEN3-MAX?) (Activity: 20): The post discusses a significant usage of the QWEN3-MAX language model, with the user consuming 3-4 billion tokens per day. This high usage has led to DAMO Academy granting additional concurrency and early access to the upcoming Qwen3.5-MAX. The user attributes a drop in usage to the weekend, indicating a consistent high demand otherwise. The post highlights the models effectiveness, with the user describing it as the best LLM in the world. Comments reveal a mix of curiosity and comparison, with one user noting their own high token consumption of 4 billion using a local model from the QWEN series. Another user shares a positive experience with the models ability to optimize website copywriting, though they express concerns about accessing the model for coding tasks.Available-Craft-5795 mentions using 4 billion tokens with the QWEN series, indicating a high level of engagement with these models. This suggests that the QWEN series is capable of handling large-scale token processing, which could be beneficial for extensive applications such as data analysis or large-scale content generation.Remarkable_Speed1402 discusses using the new model for optimizing website homepage copywriting, noting its effectiveness. However, they express concerns about the models coding capabilities, as they are unable to access it in their IDE. This highlights potential limitations in integrating the model with development environments, which could impact its usability for coding tasks.Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop (Activity: 10): The benchmark of Qwen3-32B on a single H100 GPU demonstrates a significant capacity gain when using INT4 quantization, achieving a 12x increase in user capacity compared to BF16, with only a 1.9% drop in accuracy. The study involved over 12,000 MMLU-Pro questions and 2,000 inference runs, showing that INT4 can support 47 concurrent users at a 4k context, compared to just 4 users with BF16. The full methodology and data are available here. A comment raised a question about the models performance in coding tasks, suggesting interest in how quantization affects specific application areas beyond general benchmarks.The discussion focuses on the performance of the Qwen3-32B model when quantized to INT4, highlighting a significant 12x increase in capacity with a minimal 1.9% drop in accuracy. This suggests that the model maintains high performance even with aggressive quantization, which is crucial for deploying large models in resource-constrained environments. However, the impact on specific tasks like coding remains a point of interest, as quantization can affect different tasks in varying ways.AI Discord RecapA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18Theme 1. Kimi K2.5 Launch: SOTA Agentic Benchmarks and Swarm CapabilitiesKimi K2.5 Crushes Agentic Benchmarks: Moonshot AI released Kimi K2.5, achieving global SOTA on the HLE full set (50.2%) and BrowseComp (74.9%), while posting open-source SOTA on MMMU Pro (78.5%) and SWE-bench Verified (76.8%) Tech Blog. Users across Discords noted the model was silently rolled out with significantly improved fact-checking and vision capabilities before the official announcement.Agent Swarm Mode Enters Beta: The release introduces an Agent Swarm feature capable of orchestrating up to 100 sub-agents and executing 1,500 tool calls in parallel, promising a 4.5x performance boost on complex tasks. High-tier users can access this self-directed mode on kimi.com, though early testers noted it consumes tool-call quotas rapidly.Pricing and API Instability Spark Debate: While the models capabilities impressed users, the new Kimi Code plan drew criticism for lower limits compared to competitors like Z.ai, with promotional pricing ending in February. Integration with OpenRouter faced initial hiccups, with users reporting errors related to tool use endpoints and image URL handling.Theme 2. Hardware Acceleration: Unsloth Speedups, FlagOS, and Kernel OpsUnsloth Accelerates MoE Training by 14x: Unsloth announced that MoE training is now 14x faster than v4, with upcoming optimizations projected to double that speed again for a total 30x boost. The team also rolled out full support for transformers v5, streamlining workflows for users on the latest library versions Announcement.FlagOS Targets Unified AI Stacks: Engineers discussed the introduction of FlagOS, an open-source system software stack designed to unify ModelSystemChip layers for better workload portability across heterogeneous hardware. The project aims to incorporate insights from hardwaresoftware co-design to bridge the gap between ML systems and compilers.Tinygrad Codegens Flash Attention Directly: In the Tinygrad community, members successfully proved the ability to codegen Flash Attention directly from a frontend definition of naive attention using granular rewrites. Simultaneously, discussions highlighted a shift toward Megakernels over traditional kernel schedulers to optimize GPU throughput Luminal Blog.Theme 3. OpenAI Ecosystem: Prism, GPT-5.2 Performance, and Model DecayPrism Workspace Unlocks Scientific Collaboration: OpenAI launched Prism, a dedicated workspace powered by GPT-5.2 designed to streamline scientific research and writing for ChatGPT personal account holders Video Demo. While the tool targets academic rigor, users debating GPT-5.2 vs. Claude Opus 4.5 noted that OpenAIs model still struggles with creative writing, a flaw Sam Altman reportedly admitted to.Model Deterioration Blamed on Leechers: A recurring theory across channels suggests significant degradation in ChatGPT and Claude performance, with some users claiming a 40% drop in quality. Speculation points to free tier users (leechers) diluting compute resources or models recursively training on their own synthetic outputs.GPT-5 Control Shell Leaked: A file dubbed the GPT-5_Hotfix.md surfaced, purported to be a pre-generation control shell that enforces strict syntax and intent locking to prevent model drift. The leak suggests OpenAI is using aggressive wrappers to manage output quality before generation even begins.Theme 4. Agentic Coding Wars: Tooling, Security, and RebrandsClawdbot Morphs into Moltbot After Security Scare: Following a trademark dispute with Anthropic and serious community concerns about zero-auth vulnerabilities, the popular agent Clawdbot rebranded to Moltbot Announcement. Users previously flagged that the bot could read environment keys without permission, posing risks to sensitive financial and personal data.Cursor and Cline Face Usability Headwinds: Users expressed frustration with Cursors pricing model, noting that a few complex prompts could cost $0.50, while others struggled to run Cline on modest hardware (8GB VRAM), facing CUDA0 buffer errors. Community fixes involved reducing context lengths to 9000 and offloading memory management to dedicated GPU settings.Karpathy Bets on Agent-First Coding: Andrej Karpathy sparked discussion by outlining a strategic shift toward agent-driven coding using Claude, emphasizing the tireless persistence of LLMs over traditional methods Post. This aligns with the release of Manus Skills, where developers are incentivized with free credits to build use cases for the new agentic platform.Theme 5. Theoretical Limits and Safety: Hallucinations and Bio-RisksMath Proves Hallucination is Inevitable: A new paper discussed in the BASI Discord mathematically proves that LLMs will always hallucinate, utilizing the same principles found in jailbreaking mechanics Arxiv Paper. Researchers noted that jailbreaking exacerbates this issue by distorting the context model, preventing it from flagging malicious or incorrect tags.Fine-Tuning Unlocks Dormant Bio-Risks: An Anthropic paper sparked debate at EleutherAI by demonstrating that fine-tuning open-source models on frontier model outputs can unsuppress harmful capabilities, such as biorisks, even if previously safety-trained Arxiv Link. The findings suggest that refusals are fragile and can be undone with minimal compute, raising concerns about dual-use technologies.AI Detection Tools Flag Human Academics: Engineers highlighted a growing issue where AI detection tools consistently mislabel human-written, pre-GPT academic texts as AI-generated. The consensus is that these detectors are fundamentally flawed, yet institutions continue to rely on them, creating friction for researchers and students.</p>"
    },
    {
      "id": "0ce2f187c101",
      "title": "Google begins rolling out Chrome's \"Auto Browse\" AI agent today",
      "content": "Google began stuffing Gemini into its dominant Chrome browser several months ago, and today the AI is expanding its capabilities considerably. Google says the chatbot will be easier to access and connect to more Google services, but the biggest change is the addition of Google's autonomous browsing agent, which it has dubbed Auto Browse. Similar to tools like OpenAI Atlas, Auto Browse can handle tedious tasks in Chrome so you don't have to.\nThe newly unveiled Gemini features in Chrome are accessible from the omnipresent AI button that has been lurking at the top of the window for the last few months. Initially, that button only opened Gemini in a pop-up window, but Google now says it will default to a split-screen or \"Sidepanel\" view. Google confirmed the update began rolling out over the past week, so you may already have it.\nYou can still pop Gemini out into a floating window, but the split-view gives Gemini more room to breathe while manipulating a page with AI. This is also helpful when calling other apps in the Chrome implementation of Gemini. The chatbot can now access Gmail, Calendar, YouTube, Maps, Google Shopping, and Google Flights right from the Chrome window. Google technically added this feature around the middle of January, but it's only talking about it now.Read full article\nComments",
      "url": "https://arstechnica.com/google/2026/01/google-begins-rolling-out-chromes-auto-browse-ai-agent-today/",
      "author": "Ryan Whitwam",
      "published": "2026-01-28T18:00:14",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Google",
        "Tech",
        "browser agent",
        "gemini",
        "google"
      ],
      "summary": "Google began rolling out 'Auto Browse,' an autonomous browsing agent integrated into Chrome that can handle tasks independently within the browser. The feature competes with OpenAI's Atlas and represents Google's most significant agentic AI deployment to date.",
      "importance_score": 84.0,
      "reasoning": "Major product launch from Google bringing autonomous agents to the world's dominant browser. This is a significant step in mainstream agentic AI deployment reaching billions of users.",
      "themes": [
        "Agentic AI",
        "Product Launch",
        "Google",
        "Browser AI"
      ],
      "continuation": null,
      "summary_html": "<p>Google began rolling out 'Auto Browse,' an autonomous browsing agent integrated into Chrome that can handle tasks independently within the browser. The feature competes with OpenAI's Atlas and represents Google's most significant agentic AI deployment to date.</p>",
      "content_html": "<p>Google began stuffing Gemini into its dominant Chrome browser several months ago, and today the AI is expanding its capabilities considerably. Google says the chatbot will be easier to access and connect to more Google services, but the biggest change is the addition of Google's autonomous browsing agent, which it has dubbed Auto Browse. Similar to tools like OpenAI Atlas, Auto Browse can handle tedious tasks in Chrome so you don't have to.</p>\n<p>The newly unveiled Gemini features in Chrome are accessible from the omnipresent AI button that has been lurking at the top of the window for the last few months. Initially, that button only opened Gemini in a pop-up window, but Google now says it will default to a split-screen or \"Sidepanel\" view. Google confirmed the update began rolling out over the past week, so you may already have it.</p>\n<p>You can still pop Gemini out into a floating window, but the split-view gives Gemini more room to breathe while manipulating a page with AI. This is also helpful when calling other apps in the Chrome implementation of Gemini. The chatbot can now access Gmail, Calendar, YouTube, Maps, Google Shopping, and Google Flights right from the Chrome window. Google technically added this feature around the middle of January, but it's only talking about it now.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "d0deadf53add",
      "title": "Google DeepMind launches AI tool to help identify genetic drivers of disease",
      "content": "AlphaGenome can analyse up to 1m letters of DNA code at once and could pave way for new treatmentsResearchers at Google DeepMind have unveiled their latest artificial intelligence tool and claimed it will help scientists identify the genetic drivers of disease and ultimately pave the way for new treatments.AlphaGenome predicts how mutations interfere with the way genes are controlled, changing when they are switched on, in which cells of the body, and whether their biological volume controls are set to high or low. Continue reading...",
      "url": "https://www.theguardian.com/science/2026/jan/28/google-deepmind-alphagenome-ai-tool-genetics-disease",
      "author": "Ian Sample Science editor",
      "published": "2026-01-28T16:15:41",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Genetics",
        "Biology",
        "Medical research",
        "Science",
        "AI (artificial intelligence)",
        "DeepMind",
        "Google",
        "Technology",
        "Computing"
      ],
      "summary": "Google DeepMind unveiled AlphaGenome, an AI tool that can analyze up to 1 million letters of DNA code simultaneously to predict how mutations affect gene regulation. The tool could accelerate identification of genetic disease drivers and enable new treatments.",
      "importance_score": 82.0,
      "reasoning": "Significant scientific AI tool from DeepMind that represents meaningful progress in AI for biology/genomics. Continues DeepMind's track record of impactful scientific AI tools.",
      "themes": [
        "AI for Science",
        "DeepMind",
        "Healthcare AI",
        "Research Tool"
      ],
      "continuation": null,
      "summary_html": "<p>Google DeepMind unveiled AlphaGenome, an AI tool that can analyze up to 1 million letters of DNA code simultaneously to predict how mutations affect gene regulation. The tool could accelerate identification of genetic disease drivers and enable new treatments.</p>",
      "content_html": "<p>AlphaGenome can analyse up to 1m letters of DNA code at once and could pave way for new treatmentsResearchers at Google DeepMind have unveiled their latest artificial intelligence tool and claimed it will help scientists identify the genetic drivers of disease and ultimately pave the way for new treatments.AlphaGenome predicts how mutations interfere with the way genes are controlled, changing when they are switched on, in which cells of the body, and whether their biological volume controls are set to high or low. Continue reading...</p>"
    },
    {
      "id": "457455474f5c",
      "title": "Report: China approves import of high-end Nvidia AI chips after weeks of uncertainty",
      "content": "On Wednesday, China approved imports of Nvidia's H200 artificial intelligence chips for three of its largest technology companies, Reuters reported. ByteDance, Alibaba, and Tencent received approval to purchase more than 400,000 H200 chips in total, marking a shift in Beijing's stance after weeks of holding up shipments despite US export clearance.\nThe move follows Beijing's temporary halt to H200 shipments earlier this month after Washington cleared exports on January 13. Chinese customs authorities had told agents that the H200 chips were not permitted to enter China, Reuters reported earlier this month, even as Chinese technology companies placed orders for more than two million of the chips.\nThe H200, Nvidia's second most powerful AI chip after the B200, delivers roughly six times the performance of the company's H20 chip, which was previously the most capable chip Nvidia could sell to China. While Chinese companies such as Huawei now have products that rival the H20's performance, they still lag far behind the H200.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/01/report-china-approves-import-of-high-end-nvidia-ai-chips-after-weeks-of-uncertainty/",
      "author": "Benj Edwards",
      "published": "2026-01-28T17:21:29",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "AI chips",
        "AI GPU",
        "AI infrastructure",
        "alibaba",
        "bytedance",
        "china",
        "datacenters",
        "Jensen Huang",
        "machine learning",
        "NVIDIA",
        "semiconductors",
        "Tencent",
        "US-China relations"
      ],
      "summary": "China approved imports of 400,000+ Nvidia H200 chips for ByteDance, Alibaba, and Tencent after weeks of blocking shipments despite US export clearance. The decision marks a significant shift in Beijing's stance on AI chip imports.",
      "importance_score": 80.0,
      "reasoning": "Critical geopolitical development affecting AI infrastructure buildout. The scale (400K+ chips) and major companies involved make this highly significant for global AI development trajectory.",
      "themes": [
        "AI Infrastructure",
        "Geopolitics",
        "US-China Relations",
        "Semiconductors"
      ],
      "continuation": null,
      "summary_html": "<p>China approved imports of 400,000+ Nvidia H200 chips for ByteDance, Alibaba, and Tencent after weeks of blocking shipments despite US export clearance. The decision marks a significant shift in Beijing's stance on AI chip imports.</p>",
      "content_html": "<p>On Wednesday, China approved imports of Nvidia's H200 artificial intelligence chips for three of its largest technology companies, Reuters reported. ByteDance, Alibaba, and Tencent received approval to purchase more than 400,000 H200 chips in total, marking a shift in Beijing's stance after weeks of holding up shipments despite US export clearance.</p>\n<p>The move follows Beijing's temporary halt to H200 shipments earlier this month after Washington cleared exports on January 13. Chinese customs authorities had told agents that the H200 chips were not permitted to enter China, Reuters reported earlier this month, even as Chinese technology companies placed orders for more than two million of the chips.</p>\n<p>The H200, Nvidia's second most powerful AI chip after the B200, delivers roughly six times the performance of the company's H20 chip, which was previously the most capable chip Nvidia could sell to China. While Chinese companies such as Huawei now have products that rival the H20's performance, they still lag far behind the H200.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "4400936812a9",
      "title": "MBZUAI Releases K2 Think V2: A Fully Sovereign 70B Reasoning Model For Math, Code, And Science",
      "content": "Can a fully sovereign open reasoning model match state of the art systems when every part of its training pipeline is transparent. Researchers from Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) release K2 Think V2, a fully sovereign reasoning model designed to test how far open and fully documented pipelines can push long horizon reasoning on math, code, and science when the entire stack is open and reproducible. K2 Think V2 takes the 70 billion parameter K2 V2 Instruct base model and applies a carefully engineered reinforcement learning approach to turn it into a high precision reasoning model that remains fully open in both weights and data.\n\n\n\nhttps://arxiv.org/pdf/2512.06201\n\n\nFrom K2 V2 base model to reasoning specialist\n\n\n\nK2 V2 is a dense decoder only transformer with 80 layers, hidden size 8192, and 64 attention heads with grouped query attention and rotary position embeddings. It is trained on around 12 trillion tokens drawn from the TxT360 corpus and related curated datasets that cover web text, math, code, multilingual data, and scientific literature. \n\n\n\nTraining proceeds in three phases. Pretraining runs at context length 8192 tokens on natural data to establish robust general knowledge. Mid training then extends context up to 512k tokens using TxT360 Midas, which mixes long documents, synthetic thinking traces, and diverse reasoning behaviors while carefully keeping at least 30 percent short context data in every stage. Finally, supervised fine tuning, called TxT360 3efforts, injects instruction following and structured reasoning signals.\n\n\n\nThe important point is that K2 V2 is not a generic base model. It is explicitly optimized for long context consistency and exposure to reasoning behaviors during mid training. That makes it a natural foundation for a post training stage that focuses only on reasoning quality, which is exactly what K2 Think V2 does.\n\n\n\nFully sovereign RLVR on GURU dataset\n\n\n\nK2 Think V2 is trained with a GRPO style RLVR recipe on top of K2 V2 Instruct. The team uses the Guru dataset, version 1.5, which focuses on math, code, and STEM questions. Guru is derived from permissively licensed sources, expanded in STEM coverage, and decontaminated against key evaluation benchmarks before use. This is important for a sovereign claim, because both the base model data and the RL data are curated and documented by the same institute. \n\n\n\nThe GRPO setup removes the usual KL and entropy auxiliary losses and uses asymmetric clipping of the policy ratio with the high clip set to 0.28. Training runs fully on policy with temperature 1.2 to increase rollout diversity, global batch size 256, and no micro batching. This avoids off policy corrections that are known to introduce instability in GRPO like training. \n\n\n\nRLVR itself runs in two stages. In the first stage, response length is capped at 32k tokens and the model trains for about 200 steps. In the second stage, the maximum response length is increased to 64k tokens and training continues for about 50 steps with the same hyperparameters. This schedule specifically exploits the long context capability inherited from K2 V2 so that the model can practice full chain of thought trajectories rather than short solutions. \n\n\n\nhttps://mbzuai.ac.ae/news/k2-think-v2-a-fully-sovereign-reasoning-model/\n\n\nBenchmark profile\n\n\n\nK2 Think V2 targets reasoning benchmarks rather than purely knowledge benchmarks. On AIME 2025 it reaches pass at 1 of 90.42. On HMMT 2025 it scores 84.79. On GPQA Diamond, a difficult graduate level science benchmark, it reaches 72.98. On SciCode it records 33.00, and on Humanitys Last Exam it reaches 9.5 under the benchmark settings. \n\n\n\nThese scores are reported as averages over 16 runs and are directly comparable only within the same evaluation protocol. The MBZUAI team also highlights improvements on IFBench and on the Artificial Analysis evaluation suite, with particular gains in hallucination rate and long context reasoning compared with the previous K2 Think release. \n\n\n\nSafety and openness\n\n\n\nThe research team reports a Safety 4 style analysis that aggregates four safety surfaces. Content and public safety, truthfulness and reliability, and societal alignment all reach macro average risk levels in the low range. Data and infrastructure risks remain higher and are marked as critical, which reflects concerns about sensitive personal information handling rather than model behavior alone. The team states that K2 Think V2 still shares the generic limitations of large language models despite these mitigations. On Artificial Analysiss Openness Index, K2 Think V2 sits at the frontier together with K2 V2 and Olmo-3.\n\n\n\nKey Takeaways\n\n\n\n\nK2 Think V2 is a fully sovereign 70B reasoning model: Built on K2 V2 Instruct, with open weights, open data recipes, detailed training logs, and full RL pipeline released via Reasoning360.\n\n\n\nBase model is optimized for long context and reasoning before RL: K2 V2 is a dense decoder transformer trained on around 12T tokens, with mid training extending context length to 512K tokens and supervised &#8216;3 efforts&#8217; SFT targeting structured reasoning.\n\n\n\nReasoning is aligned using GRPO based RLVR on the Guru dataset: Training uses a 2 stage on policy GRPO setup on Guru v1.5, with asymmetric clipping, temperature 1.2, and response caps at 32K then 64K tokens to learn long chain of thought solutions.\n\n\n\nCompetitive results on hard reasoning benchmarks: K2 Think V2 reports strong pass at 1 scores such as 90.42 on AIME 2025, 84.79 on HMMT 2025, and 72.98 on GPQA Diamond, positioning it as a high precision open reasoning model for math, code, and science.\n\n\n\n\n\n\n\n\nCheck out thePaper, Model Weight, Repo and Technical details.Also,feel free to follow us onTwitterand dont forget to join our100k+ ML SubRedditand Subscribe toour Newsletter. Wait! are you on telegram?now you can join us on telegram as well.\nThe post MBZUAI Releases K2 Think V2: A Fully Sovereign 70B Reasoning Model For Math, Code, And Science appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/28/mbzuai-releases-k2-think-v2-a-fully-sovereign-70b-reasoning-model-for-math-code-and-science/",
      "author": "Maxime Mommessin",
      "published": "2026-01-28T21:17:52",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "MBZUAI released K2 Think V2, a fully sovereign 70B parameter open reasoning model with transparent training pipeline for math, code, and science tasks. The model uses reinforcement learning on the K2 V2 base with fully open weights and data.",
      "importance_score": 76.0,
      "reasoning": "Notable open source release emphasizing full transparency and sovereignty. The 70B reasoning model with open data/weights advances the open source frontier for specialized reasoning.",
      "themes": [
        "Open Source",
        "Reasoning Models",
        "Model Release",
        "Transparency"
      ],
      "continuation": null,
      "summary_html": "<p>MBZUAI released K2 Think V2, a fully sovereign 70B parameter open reasoning model with transparent training pipeline for math, code, and science tasks. The model uses reinforcement learning on the K2 V2 base with fully open weights and data.</p>",
      "content_html": "<p>Can a fully sovereign open reasoning model match state of the art systems when every part of its training pipeline is transparent. Researchers from Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) release K2 Think V2, a fully sovereign reasoning model designed to test how far open and fully documented pipelines can push long horizon reasoning on math, code, and science when the entire stack is open and reproducible. K2 Think V2 takes the 70 billion parameter K2 V2 Instruct base model and applies a carefully engineered reinforcement learning approach to turn it into a high precision reasoning model that remains fully open in both weights and data.</p>\n<p>https://arxiv.org/pdf/2512.06201</p>\n<p>From K2 V2 base model to reasoning specialist</p>\n<p>K2 V2 is a dense decoder only transformer with 80 layers, hidden size 8192, and 64 attention heads with grouped query attention and rotary position embeddings. It is trained on around 12 trillion tokens drawn from the TxT360 corpus and related curated datasets that cover web text, math, code, multilingual data, and scientific literature.</p>\n<p>Training proceeds in three phases. Pretraining runs at context length 8192 tokens on natural data to establish robust general knowledge. Mid training then extends context up to 512k tokens using TxT360 Midas, which mixes long documents, synthetic thinking traces, and diverse reasoning behaviors while carefully keeping at least 30 percent short context data in every stage. Finally, supervised fine tuning, called TxT360 3efforts, injects instruction following and structured reasoning signals.</p>\n<p>The important point is that K2 V2 is not a generic base model. It is explicitly optimized for long context consistency and exposure to reasoning behaviors during mid training. That makes it a natural foundation for a post training stage that focuses only on reasoning quality, which is exactly what K2 Think V2 does.</p>\n<p>Fully sovereign RLVR on GURU dataset</p>\n<p>K2 Think V2 is trained with a GRPO style RLVR recipe on top of K2 V2 Instruct. The team uses the Guru dataset, version 1.5, which focuses on math, code, and STEM questions. Guru is derived from permissively licensed sources, expanded in STEM coverage, and decontaminated against key evaluation benchmarks before use. This is important for a sovereign claim, because both the base model data and the RL data are curated and documented by the same institute.</p>\n<p>The GRPO setup removes the usual KL and entropy auxiliary losses and uses asymmetric clipping of the policy ratio with the high clip set to 0.28. Training runs fully on policy with temperature 1.2 to increase rollout diversity, global batch size 256, and no micro batching. This avoids off policy corrections that are known to introduce instability in GRPO like training.</p>\n<p>RLVR itself runs in two stages. In the first stage, response length is capped at 32k tokens and the model trains for about 200 steps. In the second stage, the maximum response length is increased to 64k tokens and training continues for about 50 steps with the same hyperparameters. This schedule specifically exploits the long context capability inherited from K2 V2 so that the model can practice full chain of thought trajectories rather than short solutions.</p>\n<p>https://mbzuai.ac.ae/news/k2-think-v2-a-fully-sovereign-reasoning-model/</p>\n<p>Benchmark profile</p>\n<p>K2 Think V2 targets reasoning benchmarks rather than purely knowledge benchmarks. On AIME 2025 it reaches pass at 1 of 90.42. On HMMT 2025 it scores 84.79. On GPQA Diamond, a difficult graduate level science benchmark, it reaches 72.98. On SciCode it records 33.00, and on Humanitys Last Exam it reaches 9.5 under the benchmark settings.</p>\n<p>These scores are reported as averages over 16 runs and are directly comparable only within the same evaluation protocol. The MBZUAI team also highlights improvements on IFBench and on the Artificial Analysis evaluation suite, with particular gains in hallucination rate and long context reasoning compared with the previous K2 Think release.</p>\n<p>Safety and openness</p>\n<p>The research team reports a Safety 4 style analysis that aggregates four safety surfaces. Content and public safety, truthfulness and reliability, and societal alignment all reach macro average risk levels in the low range. Data and infrastructure risks remain higher and are marked as critical, which reflects concerns about sensitive personal information handling rather than model behavior alone. The team states that K2 Think V2 still shares the generic limitations of large language models despite these mitigations. On Artificial Analysiss Openness Index, K2 Think V2 sits at the frontier together with K2 V2 and Olmo-3.</p>\n<p>Key Takeaways</p>\n<p>K2 Think V2 is a fully sovereign 70B reasoning model: Built on K2 V2 Instruct, with open weights, open data recipes, detailed training logs, and full RL pipeline released via Reasoning360.</p>\n<p>Base model is optimized for long context and reasoning before RL: K2 V2 is a dense decoder transformer trained on around 12T tokens, with mid training extending context length to 512K tokens and supervised 3 efforts SFT targeting structured reasoning.</p>\n<p>Reasoning is aligned using GRPO based RLVR on the Guru dataset: Training uses a 2 stage on policy GRPO setup on Guru v1.5, with asymmetric clipping, temperature 1.2, and response caps at 32K then 64K tokens to learn long chain of thought solutions.</p>\n<p>Competitive results on hard reasoning benchmarks: K2 Think V2 reports strong pass at 1 scores such as 90.42 on AIME 2025, 84.79 on HMMT 2025, and 72.98 on GPQA Diamond, positioning it as a high precision open reasoning model for math, code, and science.</p>\n<p>Check out the&nbsp;Paper, Model Weight, Repo and Technical details.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and dont forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post MBZUAI Releases K2 Think V2: A Fully Sovereign 70B Reasoning Model For Math, Code, And Science appeared first on MarkTechPost.</p>"
    },
    {
      "id": "de863af52245",
      "title": "Tesla discontinues Model X and S vehicles as Elon Musk pivots to robotics",
      "content": "High hopes for Optimus robot help company beat forecasts despite yearly revenue decline and flailing car businessIn the clearest sign yet that Tesla is pivoting away from its electric car business, CEO Elon Musk announced on Wednesdays investor call that the company would discontinue production of its Model X SUV and Model S full-size sedan.Its time to basically bring the Model S and X programs to an end, Musk said. We expect to wind down S and X production next quarter. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/28/tesla-q4-earnings-estimates-elon-musk",
      "author": "Nick Robins-Early",
      "published": "2026-01-28T22:57:40",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Tesla",
        "Quarterly results",
        "Elon Musk",
        "AI (artificial intelligence)",
        "Robots",
        "Technology",
        "US news"
      ],
      "summary": "Tesla announced discontinuation of Model X and Model S vehicles as Elon Musk pivots the company toward robotics, specifically the Optimus humanoid robot. The move came during earnings that beat forecasts despite declining yearly revenue.",
      "importance_score": 74.0,
      "reasoning": "Major strategic shift from one of the world's most valuable companies, signaling serious commitment to AI robotics over traditional EV business. Important signal for robotics industry direction.",
      "themes": [
        "Robotics",
        "Tesla",
        "Corporate Strategy",
        "Humanoid Robots"
      ],
      "continuation": null,
      "summary_html": "<p>Tesla announced discontinuation of Model X and Model S vehicles as Elon Musk pivots the company toward robotics, specifically the Optimus humanoid robot. The move came during earnings that beat forecasts despite declining yearly revenue.</p>",
      "content_html": "<p>High hopes for Optimus robot help company beat forecasts despite yearly revenue decline and flailing car businessIn the clearest sign yet that Tesla is pivoting away from its electric car business, CEO Elon Musk announced on Wednesdays investor call that the company would discontinue production of its Model X SUV and Model S full-size sedan.Its time to basically bring the Model S and X programs to an end, Musk said. We expect to wind down S and X production next quarter. Continue reading...</p>"
    },
    {
      "id": "5c52a96c6901",
      "title": "Deloitte sounds alarm as AI agent deployment outruns safety frameworks",
      "content": "A new report from Deloitte has warned that businesses are deploying AI agents faster than their safety protocols and safeguards can keep up. Therefore, serious concerns around security, data privacy, and accountability are spreading.\nAccording to the survey, agentic systems are moving from pilot to production so quickly that traditional risk controls, which were designed for more human-centred operations, are struggling to meet security demands.\nJust 21% of organisations have implemented stringent governance or oversight for AI agents, despite the increased rate of adoption. Whilst 23% of companies stated that they are currently using AI agents, this is expected to rise to 74% in the next two years. The share of businesses yet to adopt this technology is expected to fall from 25% to just 5% over the same period.\nPoor governance is the threat\nDeloitte is not highlighting AI agents as inherently dangerous, but states the real risks are associated with poor context and weak governance. If agents operate as their own entities, their decisions and actions can easily become opaque. Without robust governance, it becomes difficult to manage and almost impossible to insure against mistakes.\nAccording to Ali Sarrafi, CEO &amp; Founder of Kovant, the answer is governed autonomy. Well-designed agents with clear boundaries, policies and definitions managed the same way as an enterprise manages any worker can move fast on low-risk work inside clear guardrails, but escalate to humans when actions cross defined risk thresholds.\nWith detailed action logs, observability, and human gatekeeping for high-impact decisions, agents stop being mysterious bots and become systems you can inspect, audit, and trust.\nAs Deloitte&#8217;s report suggests, AI agent adoption is set to accelerate in the coming years, and only the companies that deploy the technology with visibility and control will hold the upper hand over competitors, not those who deploy them quickest.\nWhy AI agents require robust guardrails\nAI agents may perform well in controlled demos, but they struggle in real-world business settings where systems can be fragmented and data may be inconsistent.\nSarrafi commented on the unpredictable nature of AI agents in these scenarios. When an agent is given too much context or scope at once, it becomes prone to hallucinations and unpredictable behaviour.\nBy contrast, production-grade systems limit the decision and context scope that models work with. They decompose operations into narrower, focused tasks for individual agents, making behaviour more predictable and easier to control. This structure also enables traceability and intervention, so failures can be detected early and escalated appropriately rather than causing cascading errors.\nAccountability for insurable AI\nWith agents taking real actions in business systems, such as keeping detailed action logs, risk and compliance are viewed differently. With every action recorded, agents&#8217; activities become clear and evaluable, letting organisations inspect actions in detail.\nSuch transparency is crucial for insurers, who are reluctant to cover opaque AI systems. This level of detail helps insurers understand what agents have done, and the controls involved, thus making it easier to assess risk. With human oversight for risk-critical actions and auditable, replayable workflows, organisations can produce systems that are more manageable for risk assessment.\nAAIF standards a good first step\nShared standards, like those being developed by the Agentic AI Foundation (AAIF), help businesses to integrate different agent systems, but current standardisation efforts focus on what is simplest to build, not what larger organisations need to operate agentic systems safely.\nSarrafi says enterprises require standards that support operation control, and which include, access permissions, approval workflows for high-impact actions, and auditable logs and observability, so teams can monitor behaviour, investigate incidents, and prove compliance.\nIdentity and permissions the first line of defence\nLimiting what AI agents can access and the actions they can perform is important to ensure safety in real business environments. Sarrafi said, When agents are given broad privileges or too much context, they become unpredictable and pose security or compliance risks.\nVisibility and monitoring are important to keep agents operating inside limits. Only then can stakeholders have confidence in the adoption of the technology. If every action is logged and manageable, teams can then see what has happened, identify issues, and better understand why events occurred.\nSarrafi continued, This visibility, combined with human supervision where it matters, turns AI agents from inscrutable components into systems that can be inspected, replayed and audited. It also allows rapid investigation and correction when issues arise, which boosts trust among operators, risk teams and insurers alike.\nDeloitte&#8217;s blueprint\nDeloitte&#8217;s strategy for safe AI agent governance sets out defined boundaries for the decisions agentic systems can make. For instance, they might operate with tiered autonomy, where agents can only view information or offer suggestions. From here, they can be allowed to take limited actions, but with human approval. Once they have proven to be reliable in low-risk areas, they can be allowed to act automatically.\nDeloitte&#8217;s Cyber AI Blueprints suggest governance layers and embedding policies and compliance capability roadmaps into organisational controls. Ultimately, governance structures that track AI use and risk, and embedding oversight into daily operations are important for safe agentic AI use.\nReadying workforces with training is another aspect of safe governance. Deloitte recommends training employees on what they shouldn&#8217;t share with AI systems, what to do if agents go off track, and how to spot unusual, potentially dangerous behaviour. If employees fail to understand how AI systems work and their potential risks, they may weaken security controls, albeit unintentionally.\nRobust governance and control, alongside shared literacy are fundamental to the safe deployment and operation of AI agents, enabling secure, compliant, and accountable performance in real-world environments\n(Image source: &#8220;Global Hawk, NASA&#8217;s New Remote-Controlled Plane&#8221; by NASA Goddard Photo and Video is licensed under CC BY 2.0. )\n&nbsp;\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Deloitte sounds alarm as AI agent deployment outruns safety frameworks appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/deloitte-agentic-ai-guidelines-published/",
      "author": "David Thomas",
      "published": "2026-01-28T15:23:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Governance, Regulation & Policy",
        "agentic",
        "governance",
        "policy",
        "strategy"
      ],
      "summary": "Deloitte report warns that only 21% of organizations have implemented governance for AI agents despite 23% currently using them and 74% expecting to within two years. The gap between deployment speed and safety frameworks poses serious security and accountability risks.",
      "importance_score": 70.0,
      "reasoning": "Important industry analysis highlighting the governance gap in agentic AI deployment. The specific statistics make this actionable intelligence for the AI safety discussion.",
      "themes": [
        "AI Governance",
        "Agentic AI",
        "Enterprise AI",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Deloitte report warns that only 21% of organizations have implemented governance for AI agents despite 23% currently using them and 74% expecting to within two years. The gap between deployment speed and safety frameworks poses serious security and accountability risks.</p>",
      "content_html": "<p>A new report from Deloitte has warned that businesses are deploying AI agents faster than their safety protocols and safeguards can keep up. Therefore, serious concerns around security, data privacy, and accountability are spreading.</p>\n<p>According to the survey, agentic systems are moving from pilot to production so quickly that traditional risk controls, which were designed for more human-centred operations, are struggling to meet security demands.</p>\n<p>Just 21% of organisations have implemented stringent governance or oversight for AI agents, despite the increased rate of adoption. Whilst 23% of companies stated that they are currently using AI agents, this is expected to rise to 74% in the next two years. The share of businesses yet to adopt this technology is expected to fall from 25% to just 5% over the same period.</p>\n<p>Poor governance is the threat</p>\n<p>Deloitte is not highlighting AI agents as inherently dangerous, but states the real risks are associated with poor context and weak governance. If agents operate as their own entities, their decisions and actions can easily become opaque. Without robust governance, it becomes difficult to manage and almost impossible to insure against mistakes.</p>\n<p>According to Ali Sarrafi, CEO &amp; Founder of Kovant, the answer is governed autonomy. Well-designed agents with clear boundaries, policies and definitions managed the same way as an enterprise manages any worker can move fast on low-risk work inside clear guardrails, but escalate to humans when actions cross defined risk thresholds.</p>\n<p>With detailed action logs, observability, and human gatekeeping for high-impact decisions, agents stop being mysterious bots and become systems you can inspect, audit, and trust.</p>\n<p>As Deloittes report suggests, AI agent adoption is set to accelerate in the coming years, and only the companies that deploy the technology with visibility and control will hold the upper hand over competitors, not those who deploy them quickest.</p>\n<p>Why AI agents require robust guardrails</p>\n<p>AI agents may perform well in controlled demos, but they struggle in real-world business settings where systems can be fragmented and data may be inconsistent.</p>\n<p>Sarrafi commented on the unpredictable nature of AI agents in these scenarios. When an agent is given too much context or scope at once, it becomes prone to hallucinations and unpredictable behaviour.</p>\n<p>By contrast, production-grade systems limit the decision and context scope that models work with. They decompose operations into narrower, focused tasks for individual agents, making behaviour more predictable and easier to control. This structure also enables traceability and intervention, so failures can be detected early and escalated appropriately rather than causing cascading errors.</p>\n<p>Accountability for insurable AI</p>\n<p>With agents taking real actions in business systems, such as keeping detailed action logs, risk and compliance are viewed differently. With every action recorded, agents activities become clear and evaluable, letting organisations inspect actions in detail.</p>\n<p>Such transparency is crucial for insurers, who are reluctant to cover opaque AI systems. This level of detail helps insurers understand what agents have done, and the controls involved, thus making it easier to assess risk. With human oversight for risk-critical actions and auditable, replayable workflows, organisations can produce systems that are more manageable for risk assessment.</p>\n<p>AAIF standards a good first step</p>\n<p>Shared standards, like those being developed by the Agentic AI Foundation (AAIF), help businesses to integrate different agent systems, but current standardisation efforts focus on what is simplest to build, not what larger organisations need to operate agentic systems safely.</p>\n<p>Sarrafi says enterprises require standards that support operation control, and which include, access permissions, approval workflows for high-impact actions, and auditable logs and observability, so teams can monitor behaviour, investigate incidents, and prove compliance.</p>\n<p>Identity and permissions the first line of defence</p>\n<p>Limiting what AI agents can access and the actions they can perform is important to ensure safety in real business environments. Sarrafi said, When agents are given broad privileges or too much context, they become unpredictable and pose security or compliance risks.</p>\n<p>Visibility and monitoring are important to keep agents operating inside limits. Only then can stakeholders have confidence in the adoption of the technology. If every action is logged and manageable, teams can then see what has happened, identify issues, and better understand why events occurred.</p>\n<p>Sarrafi continued, This visibility, combined with human supervision where it matters, turns AI agents from inscrutable components into systems that can be inspected, replayed and audited. It also allows rapid investigation and correction when issues arise, which boosts trust among operators, risk teams and insurers alike.</p>\n<p>Deloittes blueprint</p>\n<p>Deloittes strategy for safe AI agent governance sets out defined boundaries for the decisions agentic systems can make. For instance, they might operate with tiered autonomy, where agents can only view information or offer suggestions. From here, they can be allowed to take limited actions, but with human approval. Once they have proven to be reliable in low-risk areas, they can be allowed to act automatically.</p>\n<p>Deloittes Cyber AI Blueprints suggest governance layers and embedding policies and compliance capability roadmaps into organisational controls. Ultimately, governance structures that track AI use and risk, and embedding oversight into daily operations are important for safe agentic AI use.</p>\n<p>Readying workforces with training is another aspect of safe governance. Deloitte recommends training employees on what they shouldnt share with AI systems, what to do if agents go off track, and how to spot unusual, potentially dangerous behaviour. If employees fail to understand how AI systems work and their potential risks, they may weaken security controls, albeit unintentionally.</p>\n<p>Robust governance and control, alongside shared literacy are fundamental to the safe deployment and operation of AI agents, enabling secure, compliant, and accountable performance in real-world environments</p>\n<p>(Image source: Global Hawk, NASAs New Remote-Controlled Plane by NASA Goddard Photo and Video is licensed under CC BY 2.0. )</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Deloitte sounds alarm as AI agent deployment outruns safety frameworks appeared first on AI News.</p>"
    },
    {
      "id": "3542db936e3c",
      "title": "US cyber defense chief accidentally uploaded secret government info to ChatGPT",
      "content": "Alarming critics, the acting director of the Cybersecurity and Infrastructure Security Agency (CISA), Madhu Gottumukkala, accidentally uploaded sensitive information to a public version of ChatGPT last summer, Politico reported.\nAccording to \"four Department of Homeland Security officials with knowledge of the incident,\" Gottumukkala's uploads of sensitive CISA contracting documents triggered multiple internal cybersecurity warnings designed to \"stop the theft or unintentional disclosure of government material from federal networks.\"\nGottumukkala's uploads happened soon after he joined the agency and sought special permission to use OpenAI's popular chatbot, which most DHS staffers are blocked from accessing, DHS confirmed to Ars. Instead, DHS staffers use approved AI-powered tools, like the agency's DHSChat, which \"are configured to prevent queries or documents input into them from leaving federal networks,\" Politico reported.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/us-cyber-defense-chief-accidentally-uploaded-secret-government-info-to-chatgpt/",
      "author": "Ashley Belanger",
      "published": "2026-01-28T19:56:44",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Policy",
        "ChatGPT",
        "CISA",
        "cybersecurity and infrastructure security agency",
        "Department of Homeland Security",
        "DHS",
        "ice",
        "openai"
      ],
      "summary": "The acting director of CISA accidentally uploaded sensitive government contracting documents to public ChatGPT, triggering internal cybersecurity warnings. The incident occurred despite DHS restrictions blocking most staff from using OpenAI's chatbot.",
      "importance_score": 68.0,
      "reasoning": "High-profile security incident involving senior government official highlights ongoing AI data security challenges. Significant for AI policy discussions though not a technical breakthrough.",
      "themes": [
        "AI Security",
        "Government AI",
        "Policy",
        "Data Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>The acting director of CISA accidentally uploaded sensitive government contracting documents to public ChatGPT, triggering internal cybersecurity warnings. The incident occurred despite DHS restrictions blocking most staff from using OpenAI's chatbot.</p>",
      "content_html": "<p>Alarming critics, the acting director of the Cybersecurity and Infrastructure Security Agency (CISA), Madhu Gottumukkala, accidentally uploaded sensitive information to a public version of ChatGPT last summer, Politico reported.</p>\n<p>According to \"four Department of Homeland Security officials with knowledge of the incident,\" Gottumukkala's uploads of sensitive CISA contracting documents triggered multiple internal cybersecurity warnings designed to \"stop the theft or unintentional disclosure of government material from federal networks.\"</p>\n<p>Gottumukkala's uploads happened soon after he joined the agency and sought special permission to use OpenAI's popular chatbot, which most DHS staffers are blocked from accessing, DHS confirmed to Ars. Instead, DHS staffers use approved AI-powered tools, like the agency's DHSChat, which \"are configured to prevent queries or documents input into them from leaving federal networks,\" Politico reported.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "f207a2857dd0",
      "title": "Users flock to open source Moltbot for always-on AI, despite major risks",
      "content": "An open source AI assistant called Moltbot (formerly \"Clawdbot\") recently crossed 69,000 stars on GitHub after a month, making it one of the fastest-growing AI projects of 2026. Created by Austrian developer Peter Steinberger, the tool lets users run a personal AI assistant and control it through messaging apps they already use. While some say it feels like the AI assistant of the future, running the tool as currently designed comes with serious security risks.\nAmong the dozens of unofficial AI bot apps that never rise above the fray, Moltbot is perhaps most notable for its proactive communication with the user. The assistant works with WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, and other platforms. It can reach out to users with reminders, alerts, or morning briefings based on calendar events or other triggers. The project has drawn comparisons to Jarvis, the AI assistant from the Iron Man films, for its ability to actively attempt to manage tasks across a user's digital life.\nHowever, we'll tell you up front that there are plenty of drawbacks to the still-hobbyist software: While the organizing assistant code runs on a local machine, the tool effectively requires a subscription to Anthropic or OpenAI for model access (or using an API key). Users can run local AI models with the bot, but they are currently less effective at carrying out tasks than the best commercial models. Claude Opus 4.5, which is Anthropic's flagship large language model (LLM), is a popular choice.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/01/viral-ai-assistant-moltbot-rapidly-gains-popularity-but-poses-security-risks/",
      "author": "Benj Edwards",
      "published": "2026-01-28T12:30:44",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "agentic AI",
        "AI agents",
        "AI assistants",
        "AI security",
        "AI sycophancy",
        "Anthropic",
        "chatbots",
        "machine learning",
        "open source",
        "Peter Steinberger"
      ],
      "summary": "First spotted on [Social](/?date=2026-01-27&category=social#item-322347cb4e85), now making mainstream headlines, Moltbot, an open source AI assistant formerly called Clawdbot, gained 69,000 GitHub stars in one month, making it one of 2026's fastest-growing AI projects. The tool enables proactive AI communication across messaging platforms but carries significant security risks.",
      "importance_score": 65.0,
      "reasoning": "Demonstrates strong demand for always-on AI assistants and the speed at which open source AI tools can gain adoption. Security concerns highlight emerging risks in consumer AI.",
      "themes": [
        "Open Source",
        "AI Assistants",
        "Consumer AI",
        "AI Security"
      ],
      "continuation": {
        "original_item_id": "322347cb4e85",
        "original_date": "2026-01-27",
        "original_category": "social",
        "original_title": "Rahul warns us about Clawdbot. \n\nI'm not too worried about the nerds here who load it, but it got so...",
        "continuation_type": "mainstream_pickup",
        "should_demote": false,
        "reference_text": "First spotted on **Social**, now making mainstream headlines"
      },
      "summary_html": "<p>First spotted on <a href=\"/?date=2026-01-27&amp;category=social#item-322347cb4e85\" class=\"internal-link\" rel=\"noopener noreferrer\">Social</a>, now making mainstream headlines, Moltbot, an open source AI assistant formerly called Clawdbot, gained 69,000 GitHub stars in one month, making it one of 2026's fastest-growing AI projects. The tool enables proactive AI communication across messaging platforms but carries significant security risks.</p>",
      "content_html": "<p>An open source AI assistant called Moltbot (formerly \"Clawdbot\") recently crossed 69,000 stars on GitHub after a month, making it one of the fastest-growing AI projects of 2026. Created by Austrian developer Peter Steinberger, the tool lets users run a personal AI assistant and control it through messaging apps they already use. While some say it feels like the AI assistant of the future, running the tool as currently designed comes with serious security risks.</p>\n<p>Among the dozens of unofficial AI bot apps that never rise above the fray, Moltbot is perhaps most notable for its proactive communication with the user. The assistant works with WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, and other platforms. It can reach out to users with reminders, alerts, or morning briefings based on calendar events or other triggers. The project has drawn comparisons to Jarvis, the AI assistant from the Iron Man films, for its ability to actively attempt to manage tasks across a user's digital life.</p>\n<p>However, we'll tell you up front that there are plenty of drawbacks to the still-hobbyist software: While the organizing assistant code runs on a local machine, the tool effectively requires a subscription to Anthropic or OpenAI for model access (or using an API key). Users can run local AI models with the bot, but they are currently less effective at carrying out tasks than the best commercial models. Claude Opus 4.5, which is Anthropic's flagship large language model (LLM), is a popular choice.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "b55886104bbe",
      "title": "Microsoft shrugs off AI bubble fears again with strong financial results",
      "content": "Company reports second-quarter revenues of $81.27bn but posts slowing growth in key cloud computing businessInvestor interest in Microsoft shares may have weakened in recent months, but the company posted strong financial results on Wednesday which yet again demonstrated that the AI boom is roaring on.Microsoft reported earnings for the second quarter of fiscal year that are likely to keep the party going for Wall Street, despite slowing growth in its key cloud computing business. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/28/microsoft-second-quarter-results",
      "author": "Edward Helmore",
      "published": "2026-01-28T21:48:25",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Microsoft",
        "Quarterly results",
        "AI (artificial intelligence)",
        "Computing",
        "Technology",
        "Business",
        "US news"
      ],
      "summary": "Microsoft reported Q2 revenues of $81.27 billion with strong results despite slowing cloud growth, demonstrating continued AI boom momentum. The results counter AI bubble concerns and support ongoing AI investment.",
      "importance_score": 62.0,
      "reasoning": "Important business indicator showing AI investment continues to drive major tech company growth. Validates AI market trajectory though earnings are routine news.",
      "themes": [
        "Enterprise AI",
        "Business",
        "Cloud Computing",
        "Microsoft"
      ],
      "continuation": null,
      "summary_html": "<p>Microsoft reported Q2 revenues of $81.27 billion with strong results despite slowing cloud growth, demonstrating continued AI boom momentum. The results counter AI bubble concerns and support ongoing AI investment.</p>",
      "content_html": "<p>Company reports second-quarter revenues of $81.27bn but posts slowing growth in key cloud computing businessInvestor interest in Microsoft shares may have weakened in recent months, but the company posted strong financial results on Wednesday which yet again demonstrated that the AI boom is roaring on.Microsoft reported earnings for the second quarter of fiscal year that are likely to keep the party going for Wall Street, despite slowing growth in its key cloud computing business. Continue reading...</p>"
    },
    {
      "id": "8a6586649bde",
      "title": "Googles New Chrome Auto Browse Agent Attempts to Roam the Web Without You",
      "content": "Googles latest addition to its Chrome browser puts generative AI behind the wheel and you in the passenger seat.",
      "url": "https://www.wired.com/story/google-chrome-auto-browse/",
      "author": "Reece Rogers",
      "published": "2026-01-28T18:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Gear",
        "Gear / Gear News and Events",
        "Google",
        "Chrome",
        "Browsers",
        "artificial intelligence",
        "Google Gemini",
        "Browser Refresh"
      ],
      "summary": "Google's Auto Browse feature puts generative AI in control of web navigation, allowing users to delegate browsing tasks. The feature represents Google's response to competing agentic products.",
      "importance_score": 62.0,
      "reasoning": "Duplicate coverage of Chrome Auto Browse launch with slightly different angle. Same significance as primary article.",
      "themes": [
        "Agentic AI",
        "Google",
        "Browser AI",
        "Product Launch"
      ],
      "continuation": null,
      "summary_html": "<p>Google's Auto Browse feature puts generative AI in control of web navigation, allowing users to delegate browsing tasks. The feature represents Google's response to competing agentic products.</p>",
      "content_html": "<p>Googles latest addition to its Chrome browser puts generative AI behind the wheel and you in the passenger seat.</p>"
    },
    {
      "id": "5b24258d5e63",
      "title": "Tencent Hunyuan Releases HPC-Ops: A High Performance LLM Inference Operator Library",
      "content": "Tencent Hunyuan has open sourced HPC-Ops, a production grade operator library for large language model inference architecture devices. HPC-Ops focuses on low level CUDA kernels for core operators such as Attention, Grouped GEMM, and Fused MoE, and exposes them through a compact-C and Python API for integration into existing inference stacks.\n\n\n\nHPC-Ops runs in large scale internal services. In those deployments it delivers about 30 percent queries per minute improvement for Tencent-HY models and about 17 percent improvement for DeepSeek models on mainstream inference cards. These gains are reported at the service level, so they reflect the cumulative effect of faster kernels inside a real inference pipeline.\n\n\n\nScope and design of HPC-Ops\n\n\n\nHPC-Ops is a production grade, high performance, and easy to use operator library for LLM inference, developed by the Tencent Hunyuan AI Infra team. The project does not try to replace serving frameworks. Instead it provides kernels and clean APIs that can be called from systems that already handle scheduling, KV cache management, batching, and transport.\n\n\n\nThe API is designed for seamless use inside popular inference frameworks such as vLLM and SGLang. That means the framework team can swap in HPC-Ops kernels behind their own abstractions without changing the external behavior of their servers. \n\n\n\nHPC-Ops uses C++ and CUDA with CuTe and CUTLASS as building blocks. Kernels are written as relatively small examples that also serve as a modern CUDA tutorial.\n\n\n\nKernel performance characteristics\n\n\n\nThe project publishes maximum observed speedup numbers for each operator relative to established baselines. These are microbenchmarks, and the research team stress that performance varies across shapes and workloads, but they show the optimization ceiling. \n\n\n\nFor Attention in bf16, compared with FlashInfer, FlashAttention two, FlashAttention three, and TensorRT LLM, HPC Ops reports up to 1.33 times speedup in prefill and up to 2.22 times in decode. For Attention in fp8, compared with FlashInfer, FlashAttention three, and TensorRT LLM, it reports up to 1.12 times in prefill and up to 2.0 times in decode.\n\n\n\nFor FusedMoE fp8, compared with TensorRT LLM and vLLM, maximum observed speedup is up to 1.49 times in prefill and 1.14 times in decode. For GroupGEMM fp8, compared with DeepGEMM, the reported gains are up to 1.1 times in prefill and 1.88 times in decode. \n\n\n\nThese numbers matter because decode is usually the latency bottleneck in autoregressive generation, where batch sizes shrink and memory traffic dominates. The fact that Attention and GroupGEMM show the largest relative gains in decode suggests that HPC-Ops focuses on the part of the pipeline that most users notice.\n\n\n\nSupported kernels and precision\n\n\n\nThe current release groups its functionality into three operator families:\n\n\n\n\nAttention kernels cover both prefill and decode and include support for paged attention. Paged attention is the memory layout that frameworks like vLLM use to place key and value cache blocks in a paged structure, which improves memory reuse for long sequences. \n\n\n\nGrouped GEMM is implemented as quantized GroupGEMM with fp8 weights. HPC-Ops supports block wise and per tensor scaling, so teams can trade off quantization granularity against parameter storage and calibration cost. \n\n\n\nFused-MoE combines mixture of experts routing and expert computation in a single quantized operator. It also uses fp8 expert weights and supports block wise and per tensor scaling strategies.\n\n\n\n\nAcross these kernels, HPC-Ops provides native support for bf16 and fp8 data types. That matches the current production trend to move inference toward lower precision formats that preserve accuracy while reducing memory bandwidth and improving tensor core utilization. \n\n\n\nKey Takeaways\n\n\n\n\nTencent Hunyuan open-sourced HPC-Ops as a production grade operator library for LLM inference on NVIDIA SM90 GPUs, including H20, with C++ and CUDA kernels built on CuTe and CUTLASS.\n\n\n\nIn production deployments HPC-Ops reports about 30 percent QPM gain for Tencent-HY models and about 17 percent QPM gain for DeepSeek models on mainstream inference cards.\n\n\n\nOperator microbenchmarks show maximum speedups up to 2.22 times for bf16 Attention decode, up to 2.0 times for fp8 Attention decode, up to 1.49 times for fp8 FusedMoE prefill, and up to 1.88 times for fp8 GroupGEMM decode compared with strong baselines like FlashInfer, FlashAttention, TensorRT LLM, and DeepGEMM.\n\n\n\nThe library focuses on three operator families, Attention with paged attention support, quantized GroupGEMM with fp8 weights, and quantized Fused MoE with fp8 expert weights, with both block wise and per tensor scaling, and native bf16 plus fp8 precision support.\n\n\n\nHPC-Ops is designed as an operator layer that integrates into existing inference frameworks such as vLLM and SGLang, and the roadmap targets sparse attention for long context LLMs, extended quantization including 4 bit and 8 bit strategies, and kernels that better overlap computation with multi GPU communication.\n\n\n\n\n\n\n\n\nCheck out theRepo here.Also,feel free to follow us onTwitterand dont forget to join our100k+ ML SubRedditand Subscribe toour Newsletter. Wait! are you on telegram?now you can join us on telegram as well.\nThe post Tencent Hunyuan Releases HPC-Ops: A High Performance LLM Inference Operator Library appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/27/tencent-hunyuan-releases-hpc-ops-a-high-performance-llm-inference-operator-library/",
      "author": "Michal Sutter",
      "published": "2026-01-28T06:23:39",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Infrastructure",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Tencent Hunyuan open-sourced HPC-Ops, a production-grade operator library for LLM inference delivering ~30% QPS improvement for Tencent-HY models and ~17% for DeepSeek models. The library provides optimized CUDA kernels for attention, GEMM, and MoE operations.",
      "importance_score": 62.0,
      "reasoning": "Useful open source infrastructure release from major lab with concrete performance improvements. Benefits the broader inference optimization ecosystem.",
      "themes": [
        "Open Source",
        "AI Infrastructure",
        "Inference Optimization",
        "Tencent"
      ],
      "continuation": null,
      "summary_html": "<p>Tencent Hunyuan open-sourced HPC-Ops, a production-grade operator library for LLM inference delivering ~30% QPS improvement for Tencent-HY models and ~17% for DeepSeek models. The library provides optimized CUDA kernels for attention, GEMM, and MoE operations.</p>",
      "content_html": "<p>Tencent Hunyuan has open sourced HPC-Ops, a production grade operator library for large language model inference architecture devices. HPC-Ops focuses on low level CUDA kernels for core operators such as Attention, Grouped GEMM, and Fused MoE, and exposes them through a compact-C and Python API for integration into existing inference stacks.</p>\n<p>HPC-Ops runs in large scale internal services. In those deployments it delivers about 30 percent queries per minute improvement for Tencent-HY models and about 17 percent improvement for DeepSeek models on mainstream inference cards. These gains are reported at the service level, so they reflect the cumulative effect of faster kernels inside a real inference pipeline.</p>\n<p>Scope and design of HPC-Ops</p>\n<p>HPC-Ops is a production grade, high performance, and easy to use operator library for LLM inference, developed by the Tencent Hunyuan AI Infra team. The project does not try to replace serving frameworks. Instead it provides kernels and clean APIs that can be called from systems that already handle scheduling, KV cache management, batching, and transport.</p>\n<p>The API is designed for seamless use inside popular inference frameworks such as vLLM and SGLang. That means the framework team can swap in HPC-Ops kernels behind their own abstractions without changing the external behavior of their servers.</p>\n<p>HPC-Ops uses C++ and CUDA with CuTe and CUTLASS as building blocks. Kernels are written as relatively small examples that also serve as a modern CUDA tutorial.</p>\n<p>Kernel performance characteristics</p>\n<p>The project publishes maximum observed speedup numbers for each operator relative to established baselines. These are microbenchmarks, and the research team stress that performance varies across shapes and workloads, but they show the optimization ceiling.</p>\n<p>For Attention in bf16, compared with FlashInfer, FlashAttention two, FlashAttention three, and TensorRT LLM, HPC Ops reports up to 1.33 times speedup in prefill and up to 2.22 times in decode. For Attention in fp8, compared with FlashInfer, FlashAttention three, and TensorRT LLM, it reports up to 1.12 times in prefill and up to 2.0 times in decode.</p>\n<p>For FusedMoE fp8, compared with TensorRT LLM and vLLM, maximum observed speedup is up to 1.49 times in prefill and 1.14 times in decode. For GroupGEMM fp8, compared with DeepGEMM, the reported gains are up to 1.1 times in prefill and 1.88 times in decode.</p>\n<p>These numbers matter because decode is usually the latency bottleneck in autoregressive generation, where batch sizes shrink and memory traffic dominates. The fact that Attention and GroupGEMM show the largest relative gains in decode suggests that HPC-Ops focuses on the part of the pipeline that most users notice.</p>\n<p>Supported kernels and precision</p>\n<p>The current release groups its functionality into three operator families:</p>\n<p>Attention kernels cover both prefill and decode and include support for paged attention. Paged attention is the memory layout that frameworks like vLLM use to place key and value cache blocks in a paged structure, which improves memory reuse for long sequences.</p>\n<p>Grouped GEMM is implemented as quantized GroupGEMM with fp8 weights. HPC-Ops supports block wise and per tensor scaling, so teams can trade off quantization granularity against parameter storage and calibration cost.</p>\n<p>Fused-MoE combines mixture of experts routing and expert computation in a single quantized operator. It also uses fp8 expert weights and supports block wise and per tensor scaling strategies.</p>\n<p>Across these kernels, HPC-Ops provides native support for bf16 and fp8 data types. That matches the current production trend to move inference toward lower precision formats that preserve accuracy while reducing memory bandwidth and improving tensor core utilization.</p>\n<p>Key Takeaways</p>\n<p>Tencent Hunyuan open-sourced HPC-Ops as a production grade operator library for LLM inference on NVIDIA SM90 GPUs, including H20, with C++ and CUDA kernels built on CuTe and CUTLASS.</p>\n<p>In production deployments HPC-Ops reports about 30 percent QPM gain for Tencent-HY models and about 17 percent QPM gain for DeepSeek models on mainstream inference cards.</p>\n<p>Operator microbenchmarks show maximum speedups up to 2.22 times for bf16 Attention decode, up to 2.0 times for fp8 Attention decode, up to 1.49 times for fp8 FusedMoE prefill, and up to 1.88 times for fp8 GroupGEMM decode compared with strong baselines like FlashInfer, FlashAttention, TensorRT LLM, and DeepGEMM.</p>\n<p>The library focuses on three operator families, Attention with paged attention support, quantized GroupGEMM with fp8 weights, and quantized Fused MoE with fp8 expert weights, with both block wise and per tensor scaling, and native bf16 plus fp8 precision support.</p>\n<p>HPC-Ops is designed as an operator layer that integrates into existing inference frameworks such as vLLM and SGLang, and the roadmap targets sparse attention for long context LLMs, extended quantization including 4 bit and 8 bit strategies, and kernels that better overlap computation with multi GPU communication.</p>\n<p>Check out the&nbsp;Repo here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and dont forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Tencent Hunyuan Releases HPC-Ops: A High Performance LLM Inference Operator Library appeared first on MarkTechPost.</p>"
    },
    {
      "id": "baeb0bbed707",
      "title": "UK media groups should be allowed to opt out of Google AI Overviews, CMA says",
      "content": "News organisations hope proposals will increase leverage to get paid if content is used in AI summariesWeb publishers and news organisations could be given the power to stop Google scraping their content for its AI Overviews, under measures announced by the UK competition watchdog to loosen its grip on online search.Media organisations have experienced a drop in click-through traffic to their websites  and therefore their revenue  since Google started posting AI summaries at the top of search results, which many people read without clicking through to the original journalism. Continue reading...",
      "url": "https://www.theguardian.com/media/2026/jan/28/uk-media-groups-should-be-allowed-opt-out-of-google-ai-overviews-cma-proposals",
      "author": "Robert Booth UK technology editor",
      "published": "2026-01-28T12:26:06",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Digital media",
        "Google",
        "Competition and Markets Authority",
        "Newspapers & magazines",
        "AI (artificial intelligence)",
        "Newspapers",
        "Media",
        "Regulators",
        "Alphabet",
        "Business"
      ],
      "summary": "The UK Competition and Markets Authority proposed allowing publishers to opt out of having content scraped for Google's AI Overviews. News organizations have seen reduced traffic since AI summaries appeared atop search results.",
      "importance_score": 60.0,
      "reasoning": "Important regulatory development affecting AI training data access and publisher economics. Could set precedent for AI content licensing globally.",
      "themes": [
        "AI Regulation",
        "Content Rights",
        "Google",
        "Publishing"
      ],
      "continuation": null,
      "summary_html": "<p>The UK Competition and Markets Authority proposed allowing publishers to opt out of having content scraped for Google's AI Overviews. News organizations have seen reduced traffic since AI summaries appeared atop search results.</p>",
      "content_html": "<p>News organisations hope proposals will increase leverage to get paid if content is used in AI summariesWeb publishers and news organisations could be given the power to stop Google scraping their content for its AI Overviews, under measures announced by the UK competition watchdog to loosen its grip on online search.Media organisations have experienced a drop in click-through traffic to their websites  and therefore their revenue  since Google started posting AI summaries at the top of search results, which many people read without clicking through to the original journalism. Continue reading...</p>"
    },
    {
      "id": "128e032aa73c",
      "title": "Moltbot Is Taking Over Silicon Valley",
      "content": "People are letting the viral AI assistant formerly known as Clawdbot run their lives, regardless of the privacy concerns.",
      "url": "https://www.wired.com/story/clawdbot-moltbot-viral-ai-assistant/",
      "author": "Will Knight",
      "published": "2026-01-28T19:01:04",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "AI Lab",
        "artificial intelligence",
        "virtual assistant",
        "models",
        "OpenAI",
        "Anthropic",
        "Silicon Valley"
      ],
      "summary": "Silicon Valley professionals are adopting Moltbot to manage their daily lives despite privacy concerns raised by security researchers. The viral assistant has become embedded in workflows across the tech industry.",
      "importance_score": 58.0,
      "reasoning": "Additional context on Moltbot adoption trends but largely duplicates other coverage. Shows cultural shift toward AI delegation.",
      "themes": [
        "AI Assistants",
        "Consumer AI",
        "Privacy",
        "Silicon Valley"
      ],
      "continuation": null,
      "summary_html": "<p>Silicon Valley professionals are adopting Moltbot to manage their daily lives despite privacy concerns raised by security researchers. The viral assistant has become embedded in workflows across the tech industry.</p>",
      "content_html": "<p>People are letting the viral AI assistant formerly known as Clawdbot run their lives, regardless of the privacy concerns.</p>"
    },
    {
      "id": "9e1b4fdbfe2c",
      "title": "ICE Is Using Palantirs AI Tools to Sort Through Tips",
      "content": "ICE has been using an AI-powered Palantir system to summarize tips sent to its tip line since last spring, according to a newly released Homeland Security document.",
      "url": "https://www.wired.com/story/ice-is-using-palantirs-ai-tools-to-sort-through-tips/",
      "author": "Caroline Haskins, Makena Kelly",
      "published": "2026-01-28T21:40:18",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Security",
        "Security / National Security",
        "Security / Privacy",
        "Security / Security News",
        "Politics / Policy",
        "immigration",
        "Immigration and Customs Enforcement",
        "Department of Homeland Security",
        "Palantir",
        "artificial intelligence",
        "algorithms",
        "machine learning",
        "Crime",
        "Heads Up"
      ],
      "summary": "ICE has been using Palantir's AI system to summarize tips sent to its tip line since spring 2025, according to newly released DHS documents. The system processes incoming tips for immigration enforcement.",
      "importance_score": 58.0,
      "reasoning": "Notable government AI deployment with policy implications but not technically novel. Relevant to AI ethics and surveillance discussions.",
      "themes": [
        "Government AI",
        "Policy",
        "Immigration",
        "Surveillance"
      ],
      "continuation": null,
      "summary_html": "<p>ICE has been using Palantir's AI system to summarize tips sent to its tip line since spring 2025, according to newly released DHS documents. The system processes incoming tips for immigration enforcement.</p>",
      "content_html": "<p>ICE has been using an AI-powered Palantir system to summarize tips sent to its tip line since last spring, according to a newly released Homeland Security document.</p>"
    },
    {
      "id": "52f7f845c75c",
      "title": "White House compares industrial revolution with AI era",
      "content": "A White House paper titled &#8220;Artificial Intelligence and the Great Divergence&#8221; sets out parallels between the effects of the industrial revolution in the 18th and 19th centuries and the current times, with artificial intelligence positioned as guiding the way the world&#8217;s economies will be shaped.\n\n\n\nArtificial intelligence now sits at the centre of US economic strategy, currently representing a significant portion of the country&#8217;s economic activity, as characterised by the building of AI infrastructure, most notably in the form of data centres. The paper says AI investment raised US GDP by 1.3% percent in the first half of 2025, and compares this with the investment in the railway network during the industrial revolution.\n\n\n\n&#8220;Artificial Intelligence and the Great Divergence&#8221; says long-term growth depends primarily on gains in productivity, and AI is the tool to achieve those gains. It presents a range of estimates of AI&#8217;s impact on GDP, from single-digit increases to 20% productivity growth inside a decade. It also floats some more extreme scenarios, where GDP grows at more than 45% as AI substitutes for human labour in the longer term.\n\n\n\nCapital deployment in the form of building AI infrastructure, not growing consumption or public spending, is now creating US economic growth. Investment in data processing equipment, buildings, infrastructure, and software grew 28% in early 2025, and AI-related infrastructure represented around a quarter of all US investment in 2025.\n\n\n\nTraining compute capacity used by AI models has increased roughly four-fold per year since 2010, and the length of tasks AI systems can complete has doubled every seven months for six years, the paper states. The cost per token of AI output has fallen by factors ranging from nine to nine hundred per year, depending on task and model.\n\n\n\nBy late 2025, around 78% percent of organisations had reported using AI, up from 55% in 2024, and it&#8217;s claimed that 40% of US workers use generative AI in their jobs. Nearly half of US businesses now pay for AI subscriptions. The report poses these figures as evidence that AI has moved from experimentation into routine production.\n\n\n\nInternationally, the document frames AI as a factor in divergence of economic prosperity, with AI in the US increasing America&#8217;s GDP growth faster than in Europe and China. The US leads at the moment in private AI investment, model development, and compute capacity, while the EU&#8217;s share of world GDP has fallen since 1980. Additionally, the continent lags in comparable AI metrics  investment, construction, software development, overall capacity, etc. China remains a major player in AI actor, but the report notes that much of its model training relies on US-designed hardware.\n\n\n\nThe White House publication advocates for an integrated national strategy with investment incentives at its core. The One Big Beautiful Bill Act gave significant financial breaks for data centres and IT infrastructure, and created favourable conditions for speedy facility construction, in line with the Act&#8217;s aim to lift GDP growth by more than a percentage point per year over the medium term. The report argues that deregulation in the AI industry supports productivity by lowering costs, increasing competition, and speeding innovation. Trade agreements and foreign policy reinforce this approach, with overseas partners committing to large purchases of US-derived AI chips and infrastructure.\n\n\n\nThe paper notes that AI data centres are electricity-intensive, and projects that demand for power by AI infrastructure could reach up to 12% of domestic electricity consumption by 2028. It links the success of AI to energy availability and the ability of the power grid to deliver, positioning the control of energy supply as a prerequisite for international leadership in AI.\n\n\n\nThe report&#8217;s conclusion is that the countries that lead in AI investment and adoption will experience higher growth than the mean. The United States is aligning multiple policy rafts to ensure its leading position in the sector. Businesses that build systems in line with its national goals will be part of a dominant economic force shaping the next phase of global growth.\n\n\n\n(Image source: &#8220;Chicago Thaws into Spring&#8221; by Trey Ratcliff is licensed under CC BY-NC-SA 2.0.)\n\n\n\n&nbsp;\n\n\n\n\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post White House compares industrial revolution with AI era appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/white-house-predicts-ai-growth-with-comparison-industrial-and-artificial-intelligence-revolutions/",
      "author": "AI News",
      "published": "2026-01-28T12:04:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Hardware & Chips",
        "AI Market Trends",
        "ai",
        "construction",
        "energy",
        "international policy",
        "power"
      ],
      "summary": "A White House paper draws parallels between AI's economic impact and the industrial revolution, noting AI investment raised US GDP by 1.3% in H1 2025. The paper positions AI as central to US economic strategy.",
      "importance_score": 58.0,
      "reasoning": "Policy positioning document providing economic context for AI investment. Notable for official GDP impact figures but largely analytical rather than news.",
      "themes": [
        "Policy",
        "Economics",
        "AI Infrastructure",
        "Government"
      ],
      "continuation": null,
      "summary_html": "<p>A White House paper draws parallels between AI's economic impact and the industrial revolution, noting AI investment raised US GDP by 1.3% in H1 2025. The paper positions AI as central to US economic strategy.</p>",
      "content_html": "<p>A White House paper titled Artificial Intelligence and the Great Divergence sets out parallels between the effects of the industrial revolution in the 18th and 19th centuries and the current times, with artificial intelligence positioned as guiding the way the worlds economies will be shaped.</p>\n<p>Artificial intelligence now sits at the centre of US economic strategy, currently representing a significant portion of the countrys economic activity, as characterised by the building of AI infrastructure, most notably in the form of data centres. The paper says AI investment raised US GDP by 1.3% percent in the first half of 2025, and compares this with the investment in the railway network during the industrial revolution.</p>\n<p>Artificial Intelligence and the Great Divergence says long-term growth depends primarily on gains in productivity, and AI is the tool to achieve those gains. It presents a range of estimates of AIs impact on GDP, from single-digit increases to 20% productivity growth inside a decade. It also floats some more extreme scenarios, where GDP grows at more than 45% as AI substitutes for human labour in the longer term.</p>\n<p>Capital deployment in the form of building AI infrastructure, not growing consumption or public spending, is now creating US economic growth. Investment in data processing equipment, buildings, infrastructure, and software grew 28% in early 2025, and AI-related infrastructure represented around a quarter of all US investment in 2025.</p>\n<p>Training compute capacity used by AI models has increased roughly four-fold per year since 2010, and the length of tasks AI systems can complete has doubled every seven months for six years, the paper states. The cost per token of AI output has fallen by factors ranging from nine to nine hundred per year, depending on task and model.</p>\n<p>By late 2025, around 78% percent of organisations had reported using AI, up from 55% in 2024, and its claimed that 40% of US workers use generative AI in their jobs. Nearly half of US businesses now pay for AI subscriptions. The report poses these figures as evidence that AI has moved from experimentation into routine production.</p>\n<p>Internationally, the document frames AI as a factor in divergence of economic prosperity, with AI in the US increasing Americas GDP growth faster than in Europe and China. The US leads at the moment in private AI investment, model development, and compute capacity, while the EUs share of world GDP has fallen since 1980. Additionally, the continent lags in comparable AI metrics  investment, construction, software development, overall capacity, etc. China remains a major player in AI actor, but the report notes that much of its model training relies on US-designed hardware.</p>\n<p>The White House publication advocates for an integrated national strategy with investment incentives at its core. The One Big Beautiful Bill Act gave significant financial breaks for data centres and IT infrastructure, and created favourable conditions for speedy facility construction, in line with the Acts aim to lift GDP growth by more than a percentage point per year over the medium term. The report argues that deregulation in the AI industry supports productivity by lowering costs, increasing competition, and speeding innovation. Trade agreements and foreign policy reinforce this approach, with overseas partners committing to large purchases of US-derived AI chips and infrastructure.</p>\n<p>The paper notes that AI data centres are electricity-intensive, and projects that demand for power by AI infrastructure could reach up to 12% of domestic electricity consumption by 2028. It links the success of AI to energy availability and the ability of the power grid to deliver, positioning the control of energy supply as a prerequisite for international leadership in AI.</p>\n<p>The reports conclusion is that the countries that lead in AI investment and adoption will experience higher growth than the mean. The United States is aligning multiple policy rafts to ensure its leading position in the sector. Businesses that build systems in line with its national goals will be part of a dominant economic force shaping the next phase of global growth.</p>\n<p>(Image source: Chicago Thaws into Spring by Trey Ratcliff is licensed under CC BY-NC-SA 2.0.)</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post White House compares industrial revolution with AI era appeared first on AI News.</p>"
    },
    {
      "id": "48a101aaf76c",
      "title": "Artificial intelligence will cost jobs, admits Liz Kendall",
      "content": "UK technology secretary also announced plans to train up to 10 million Britons in AI skills to help workforce adapt Increasing deployment of artificial intelligence will cause job losses, the UK technology secretary has warned, saying: I want to level with the public. Some jobs will go.In a speech on government plans to handle the impact of AI on the British economy, Liz Kendall declined to say how many redundancies the technology might cause but said: We know people are worried about graduate entry jobs in places like law and finance. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/28/artificial-intelligence-will-cost-jobs-admits-liz-kendall",
      "author": "Robert Booth UK technology editor",
      "published": "2026-01-28T18:52:17",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Unemployment",
        "Technology",
        "UK news",
        "Liz Kendall"
      ],
      "summary": "UK Technology Secretary Liz Kendall acknowledged AI will cause job losses while announcing plans to train up to 10 million Britons in AI skills. She specifically noted concerns about graduate jobs in law and finance.",
      "importance_score": 56.0,
      "reasoning": "Government acknowledgment of AI job displacement is notable but expected. The 10 million training target provides concrete policy response.",
      "themes": [
        "Policy",
        "Jobs",
        "UK Government",
        "AI Training"
      ],
      "continuation": null,
      "summary_html": "<p>UK Technology Secretary Liz Kendall acknowledged AI will cause job losses while announcing plans to train up to 10 million Britons in AI skills. She specifically noted concerns about graduate jobs in law and finance.</p>",
      "content_html": "<p>UK technology secretary also announced plans to train up to 10 million Britons in AI skills to help workforce adapt Increasing deployment of artificial intelligence will cause job losses, the UK technology secretary has warned, saying: I want to level with the public. Some jobs will go.In a speech on government plans to handle the impact of AI on the British economy, Liz Kendall declined to say how many redundancies the technology might cause but said: We know people are worried about graduate entry jobs in places like law and finance. Continue reading...</p>"
    },
    {
      "id": "e2261590a704",
      "title": "Context Management for Deep Agents",
      "content": "By Chester Curme and Mason DaughertyAs the addressable task length of AI agents continues to grow, effective context management becomes critical to prevent context rot and to manage LLMs&#x2019; finite memory constraints.The Deep Agents SDK is LangChain&#x2019;s open source, batteries-included agent harness. It provides an easy path to build agents with the ability to plan, spawn subagents, and work with a filesystem to execute complex, long-running tasks. Because these sorts of tasks can generally exceed models&#x2019; context windows, the SDK implements various features that facilitate context compression.Context compression refers to techniques that reduce the volume of information in an agent&apos;s working memory while preserving the details relevant to completing the task. This might involve summarizing previous interactions, filtering out stale information, or strategically deciding what to retain and what to discard.Deep Agents implements a filesystem abstraction that allows agents to perform operations such as listing, reading, and writing files, as well as search, pattern matching, and file execution. Agents use the filesystem to search and retrieve offloaded content as needed.Deep Agents implements three main compression techniques, triggered at different frequencies:Offloading large tool results: We offload large tool responses to the filesystem whenever they occur.Offloading large tool inputs: When the context size crosses a threshold, we offload old write/edit arguments from tool calls to the filesystem.Summarization: When the context size crosses the threshold, and there is no more context eligible for offloading, we perform a summarization step to compress the message history.To manage context limits, the Deep Agents SDK triggers these compression steps at threshold fractions of the model&apos;s context window size. (Under the hood, we use LangChain&apos;s model profiles to access the token threshold for a given model.)Offloading large tool resultsResponses from tool invocations (e.g., the result of reading a large file or an API call) can exceed a model&apos;s context window. When Deep Agents detects a tool response exceeding 20,000 tokens, it offloads the response to the filesystem and substitutes it with a file path reference and a preview of the first 10 lines. Agents can then re-read or search the content as needed.Offloading large tool inputsFile write and edit operations leave behind tool calls containing the complete file content in the agent&apos;s conversation history. Since this content is already persisted to the filesystem, it&apos;s often redundant. As the session context crosses 85% of the model&#x2019;s available window, Deep Agents will truncate older tool calls, replacing them with a pointer to the file on disk and reducing the size of the active context.SummarizationWhen offloading no longer yields sufficient space, Deep Agents falls back to summarization. This process has two components:In-context summary: An LLM generates a structured summary of the conversation&#x2014;including session intent, artifacts created, and next steps&#x2014;which replaces the full conversation history in the agent&apos;s working memory. (See the Deep Agents summarization prompt.)Filesystem preservation: The complete, original conversation messages are written to the filesystem as a canonical record.This dual approach ensures the agent maintains awareness of its goals and progress (via the summary) while preserving the ability to recover specific details when needed (via filesystem search). See an example in this trace, where the model uses the read_file tool to fetch previously offloaded messages.What this looks like in practiceWhile the techniques above provide the machinery for context management, how do we know they&apos;re actually working? Runs on real-world tasks, as captured in benchmarks such as terminal-bench, may trigger context compression sporadically, making it difficult to isolate their impact.We&#x2019;ve found it useful to increase the signal of individual features of the harness by engaging them more aggressively on benchmark datasets. For example, while triggering summarization at 10 - 20% of the available context window may lead to suboptimal overall performance, it produces significantly more summarization events. This allows for different configurations (e.g., variations of your implementation) to be compared. For example, by forcing the agent to summarize frequently, we could identify how simple changes to the deepagents summarization prompt, in which we added dedicated fields for the session intent and next steps, help improve performance.Figure: Token usage over time in sample runs of Claude Sonnet 4.5 on terminal-bench-2 (gray lines show all runs; colored lines highlight two specific examples). The green line shows a dramatic token drop around turn 20 when a summarization event compresses the conversation history. The orange line shows a smaller reduction around turn 40 when a large file write tool call is evicted from context. By triggering compression at 25% of the context window (rather than the Deep Agents default of 85%), we generate more events to study.Targeted evalsThe Deep Agents SDK maintains a set of targeted evaluations designed to isolate and validate individual context-management mechanisms. These are deliberately small tests that make specific failure modes obvious and debuggable.The goal of these evals is not to measure broad task-solving ability, but to ensure that the agent&#x2019;s harness does not get in the way of certain tasks. For example:Did summarization preserve the agent&#x2019;s objective? Some evals deliberately trigger summarization mid-task and then check whether the agent continues. This ensures that summarization preserves not only agent state but also its trajectory.Can the agent recover information that was summarized away? Here we embed a &#x201c;needle-in-the-haystack&#x201d; fact early in the conversation, force a summarization event, and then require the agent to recall that fact later to complete the task. The fact is not present in the active context after summarization and must be recovered via filesystem search.These targeted evals act as integration tests for context management: they don&#x2019;t replace full benchmark runs, but they significantly reduce iteration time and make failures attributable to specific compression mechanisms rather than overall agent behavior.GuidanceWhen evaluating your own context compression strategies, we&#x2019;d emphasize:Start with real-world benchmarks, then stress-test individual features. Run your harness on representative tasks first to establish baseline performance. Then, artificially trigger compression more aggressively (e.g., at 10-20% of context instead of 85%) to generate more compression events per run. This amplifies the signal from individual features, making it easier to compare different approaches (e.g. variations in your summarization prompt).Test recoverability. Context compression is only useful if critical information remains accessible. Include targeted tests that verify agents can both continue toward their original goal after compression and recover specific details when needed (e.g., needle-in-the-haystack scenarios where a key fact is summarized away but must be retrieved later).Monitor for goal drift. The most insidious failure mode is an agent that loses track of the user&apos;s intent after summarization. This may manifest as the agent completing in the turn after summarization to ask for clarification, or to mistakenly declare the task complete. More subtle deviations from the intended task may be harder to attribute to summarization; forcing frequent summarization on sample datasets may help surface these failures.All features of the Deep Agents harness are open source. Try out the latest version and let us know what compression strategies work best for your use cases!",
      "url": "https://www.blog.langchain.com/context-management-for-deepagents/",
      "author": "LangChain Accounts",
      "published": "2026-01-28T16:11:29",
      "source": "LangChain Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "LangChain released context management features for their Deep Agents SDK, addressing context rot and memory constraints in long-running agent tasks. The SDK enables agents to plan, spawn subagents, and manage context compression.",
      "importance_score": 55.0,
      "reasoning": "Useful technical release for agent developers addressing real constraints. Incremental progress rather than breakthrough but practically significant.",
      "themes": [
        "Agentic AI",
        "Developer Tools",
        "LangChain",
        "Context Management"
      ],
      "continuation": null,
      "summary_html": "<p>LangChain released context management features for their Deep Agents SDK, addressing context rot and memory constraints in long-running agent tasks. The SDK enables agents to plan, spawn subagents, and manage context compression.</p>",
      "content_html": "<p>By Chester Curme and Mason DaughertyAs the addressable task length of AI agents continues to grow, effective context management becomes critical to prevent context rot and to manage LLMs finite memory constraints.The Deep Agents SDK is LangChains open source, batteries-included agent harness. It provides an easy path to build agents with the ability to plan, spawn subagents, and work with a filesystem to execute complex, long-running tasks. Because these sorts of tasks can generally exceed models context windows, the SDK implements various features that facilitate context compression.Context compression refers to techniques that reduce the volume of information in an agent's working memory while preserving the details relevant to completing the task. This might involve summarizing previous interactions, filtering out stale information, or strategically deciding what to retain and what to discard.Deep Agents implements a filesystem abstraction that allows agents to perform operations such as listing, reading, and writing files, as well as search, pattern matching, and file execution. Agents use the filesystem to search and retrieve offloaded content as needed.Deep Agents implements three main compression techniques, triggered at different frequencies:Offloading large tool results: We offload large tool responses to the filesystem whenever they occur.Offloading large tool inputs: When the context size crosses a threshold, we offload old write/edit arguments from tool calls to the filesystem.Summarization: When the context size crosses the threshold, and there is no more context eligible for offloading, we perform a summarization step to compress the message history.To manage context limits, the Deep Agents SDK triggers these compression steps at threshold fractions of the model's context window size. (Under the hood, we use LangChain's model profiles to access the token threshold for a given model.)Offloading large tool resultsResponses from tool invocations (e.g., the result of reading a large file or an API call) can exceed a model's context window. When Deep Agents detects a tool response exceeding 20,000 tokens, it offloads the response to the filesystem and substitutes it with a file path reference and a preview of the first 10 lines. Agents can then re-read or search the content as needed.Offloading large tool inputsFile write and edit operations leave behind tool calls containing the complete file content in the agent's conversation history. Since this content is already persisted to the filesystem, it's often redundant. As the session context crosses 85% of the models available window, Deep Agents will truncate older tool calls, replacing them with a pointer to the file on disk and reducing the size of the active context.SummarizationWhen offloading no longer yields sufficient space, Deep Agents falls back to summarization. This process has two components:In-context summary: An LLM generates a structured summary of the conversationincluding session intent, artifacts created, and next stepswhich replaces the full conversation history in the agent's working memory. (See the Deep Agents summarization prompt.)Filesystem preservation: The complete, original conversation messages are written to the filesystem as a canonical record.This dual approach ensures the agent maintains awareness of its goals and progress (via the summary) while preserving the ability to recover specific details when needed (via filesystem search). See an example in this trace, where the model uses the read_file tool to fetch previously offloaded messages.What this looks like in practiceWhile the techniques above provide the machinery for context management, how do we know they're actually working? Runs on real-world tasks, as captured in benchmarks such as terminal-bench, may trigger context compression sporadically, making it difficult to isolate their impact.Weve found it useful to increase the signal of individual features of the harness by engaging them more aggressively on benchmark datasets. For example, while triggering summarization at 10 - 20% of the available context window may lead to suboptimal overall performance, it produces significantly more summarization events. This allows for different configurations (e.g., variations of your implementation) to be compared. For example, by forcing the agent to summarize frequently, we could identify how simple changes to the deepagents summarization prompt, in which we added dedicated fields for the session intent and next steps, help improve performance.Figure: Token usage over time in sample runs of Claude Sonnet 4.5 on terminal-bench-2 (gray lines show all runs; colored lines highlight two specific examples). The green line shows a dramatic token drop around turn 20 when a summarization event compresses the conversation history. The orange line shows a smaller reduction around turn 40 when a large file write tool call is evicted from context. By triggering compression at 25% of the context window (rather than the Deep Agents default of 85%), we generate more events to study.Targeted evalsThe Deep Agents SDK maintains a set of targeted evaluations designed to isolate and validate individual context-management mechanisms. These are deliberately small tests that make specific failure modes obvious and debuggable.The goal of these evals is not to measure broad task-solving ability, but to ensure that the agents harness does not get in the way of certain tasks. For example:Did summarization preserve the agents objective? Some evals deliberately trigger summarization mid-task and then check whether the agent continues. This ensures that summarization preserves not only agent state but also its trajectory.Can the agent recover information that was summarized away? Here we embed a needle-in-the-haystack fact early in the conversation, force a summarization event, and then require the agent to recall that fact later to complete the task. The fact is not present in the active context after summarization and must be recovered via filesystem search.These targeted evals act as integration tests for context management: they dont replace full benchmark runs, but they significantly reduce iteration time and make failures attributable to specific compression mechanisms rather than overall agent behavior.GuidanceWhen evaluating your own context compression strategies, wed emphasize:Start with real-world benchmarks, then stress-test individual features. Run your harness on representative tasks first to establish baseline performance. Then, artificially trigger compression more aggressively (e.g., at 10-20% of context instead of 85%) to generate more compression events per run. This amplifies the signal from individual features, making it easier to compare different approaches (e.g. variations in your summarization prompt).Test recoverability. Context compression is only useful if critical information remains accessible. Include targeted tests that verify agents can both continue toward their original goal after compression and recover specific details when needed (e.g., needle-in-the-haystack scenarios where a key fact is summarized away but must be retrieved later).Monitor for goal drift. The most insidious failure mode is an agent that loses track of the user's intent after summarization. This may manifest as the agent completing in the turn after summarization to ask for clarification, or to mistakenly declare the task complete. More subtle deviations from the intended task may be harder to attribute to summarization; forcing frequent summarization on sample datasets may help surface these failures.All features of the Deep Agents harness are open source. Try out the latest version and let us know what compression strategies work best for your use cases!</p>"
    },
    {
      "id": "6ec18b34ec05",
      "title": "China lags behind US at AI frontier but could quickly catch up, say experts",
      "content": "Beijings AI policy is focused on real-life applications but Chinese companies are beginning to articulate their own grand visionsStanding on stage in the eastern China tech hub of Hangzhou, Alibabas normally media-shy CEO made an attention-grabbing announcement. The world today is witnessing the dawn of an AI-driven intelligent revolution, Eddie Wu told a developer conference in September. Artificial general intelligence (AGI) will not only amplify human intelligence but also unlock human potential, paving the way for the arrival of artificial superintelligence (ASI).ASI, Wu said, could produce a generation of super scientists and full-stack super engineers, who would tackle unsolved scientific and engineering problems at unimaginable speeds. Continue reading...",
      "url": "https://www.theguardian.com/world/2026/jan/28/china-lags-behind-us-at-ai-frontier-but-could-quickly-catch-up-say-experts",
      "author": "Amy Hawkins Senior China correspondent",
      "published": "2026-01-28T15:00:38",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "China",
        "AI (artificial intelligence)",
        "Alibaba",
        "Technology",
        "Computing",
        "World news",
        "Nvidia",
        "Asia Pacific"
      ],
      "summary": "Analysis suggests China lags the US at the AI frontier but could catch up quickly, with Alibaba CEO Eddie Wu publicly discussing AGI and ASI timelines. Chinese companies are articulating more ambitious AI visions while focusing on real-world applications.",
      "importance_score": 54.0,
      "reasoning": "Analytical piece on China-US AI competition dynamics. Notable for Alibaba CEO's AGI/ASI comments but primarily contextual rather than breaking news.",
      "themes": [
        "Geopolitics",
        "China AI",
        "AGI",
        "Competition"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis suggests China lags the US at the AI frontier but could catch up quickly, with Alibaba CEO Eddie Wu publicly discussing AGI and ASI timelines. Chinese companies are articulating more ambitious AI visions while focusing on real-world applications.</p>",
      "content_html": "<p>Beijings AI policy is focused on real-life applications but Chinese companies are beginning to articulate their own grand visionsStanding on stage in the eastern China tech hub of Hangzhou, Alibabas normally media-shy CEO made an attention-grabbing announcement. The world today is witnessing the dawn of an AI-driven intelligent revolution, Eddie Wu told a developer conference in September. Artificial general intelligence (AGI) will not only amplify human intelligence but also unlock human potential, paving the way for the arrival of artificial superintelligence (ASI).ASI, Wu said, could produce a generation of super scientists and full-stack super engineers, who would tackle unsolved scientific and engineering problems at unimaginable speeds. Continue reading...</p>"
    },
    {
      "id": "bfa76b58c5f6",
      "title": "Gallup Workforce shows details of AI adoption in US workplaces",
      "content": " Artificial intelligence has moved into the US workplace, but its adoption remains uneven, fragmented, and tied to role, industry, and organisation. Findings from a Gallup Workforce survey covering the period to the end of December 2025 show how employees use AI, who benefits most from it, and where areas of uncertainty remain. \n The findings draw from a nationally-representative questioning of more than 23,000 US adults in full- and part-time work, conducted online in August 2025. Its conclusions are that instances of AI in the workplace are increasing, but its use is far from universal, and is concentrated among knowledge-based workers. \n\nThe office AI\n\n Employees in technology, finance, and professional services are by far the biggest user group. More than three-quarters of those working in IT report using AI &#8220;at least a few times a year&#8221;. In finance and professional services, the figure is a touch under 60%. AI-enabled or aided roles tend to be those that involve significant digital workflow and information synthesis; tasks that correspond with AI&#8217;s current abilities. \n AI use is lower in sectors dominated by customer-facing or manual work. Only around a third of retail workers report comparable levels of use to their office counterparts, although those in healthcare and manufacturing do tend to deploy AI more often than those in retail, for example. The fact that current raft of AI platforms fit more naturally into desk-based, cognitive roles seems obvious  less so is a drop-off in user numbers in tightly-regulated environments. \n\n\n\nDo we, or don&#8217;t we?\n\n Gallup&#8217;s data reveals a significant number of workers ore unsure whether or not their employer had adopted AI  nearly a quarter of those surveyed weren&#8217;t sure. In the third quarter of 2025, just over a third of employees said their organisation had implemented AI. 40% said there was no adoption of AI in their place of work \n It&#8217;s worth noting that earlier versions of Gallup surveys didn&#8217;t include a &#8220;dont know&#8221; option for questions about employers&#8217; AI adoption, which encouraged respondents to guess. Belief in organisational AI adoption appeared to rise sharply between 2024 and 2025, therefore, Gallup says. Once uncertainty could be stated explicitly, it became clear a good number of employees were simply uninformed on the matter. \n It&#8217;s staff in non-managerial roles who are more likely to say they&#8217;re unaware of their organisation&#8217;s AI use, a tendency mirrored in part-time staff and hands-on roles. The further workers are from decision-making, it seems, the less sure they become. \n\n\n\nHow workers use AI\n\n The way employees use AI are consistent: of those using AI at least once a year, the most common applications are consolidating information, searching for information, and &#8220;generating ideas&#8221;, tasks that have changed little since Gallup first measured workplace AI use in 2024. \n More than 60% of AI users refer to chatbots, with using AI for writing and editing coming some way behind. Coding assistants and data science tools remain niche, but popular. Employees who use AI often are far more likely to use any more advanced tools at their disposal; particularly true in the cases of coding assistants and data analysis. \n Although use figures are generally up, Gallup concludes that AI has yet to be embedded in daily work for most Americans. Around 45% of workers say they use AI &#8220;a few times a year&#8221;, but only about 10% use it every day. \n\n\n\nConclusions\n\n Business leaders have an easy win: simply clarifying a position on AI use would be a positive, plus publicising the availability (or otherwise) of AI tools would be an easy way to improve adoption rates. \n The current abilities of AI pertain to desk-based, digital and data-centric workflows, although there are a myriad of platforms that will utilise AI in other roles. Exploring these more fully would certainly be bucking the trend, and may make the difference between an organisation&#8217;s long-term prospects and those of its direct competitors. \n A page detailing Gallup&#8217;s findings can be found on the company&#8217;s website. \n(Image source: &#8220;DIY Open Plan Office&#8221; by lower29 is licensed under CC BY-NC-SA 2.0.)\n&nbsp;\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\n\n\nThe post Gallup Workforce shows details of AI adoption in US workplaces appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/gallup-workforce-ai-shows-details-of-ml-adoption-in-us-workplaces/",
      "author": "AI News",
      "published": "2026-01-28T10:06:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Human-AI Relationships",
        "Special Reports & Series",
        "World of Work",
        "ai at work",
        "surveys",
        "united states"
      ],
      "summary": "Gallup survey of 23,000+ US workers shows AI workplace adoption increasing but concentrated among knowledge workers in tech, finance, and professional services. More than three-quarters in those sectors use AI tools.",
      "importance_score": 54.0,
      "reasoning": "Useful data on real-world AI adoption patterns. Confirms expected trends about knowledge work concentration but provides concrete figures.",
      "themes": [
        "Enterprise AI",
        "Workplace AI",
        "Adoption",
        "Survey Data"
      ],
      "continuation": null,
      "summary_html": "<p>Gallup survey of 23,000+ US workers shows AI workplace adoption increasing but concentrated among knowledge workers in tech, finance, and professional services. More than three-quarters in those sectors use AI tools.</p>",
      "content_html": "<p>Artificial intelligence has moved into the US workplace, but its adoption remains uneven, fragmented, and tied to role, industry, and organisation. Findings from a Gallup Workforce survey covering the period to the end of December 2025 show how employees use AI, who benefits most from it, and where areas of uncertainty remain.</p>\n<p>The findings draw from a nationally-representative questioning of more than 23,000 US adults in full- and part-time work, conducted online in August 2025. Its conclusions are that instances of AI in the workplace are increasing, but its use is far from universal, and is concentrated among knowledge-based workers.</p>\n<p>The office AI</p>\n<p>Employees in technology, finance, and professional services are by far the biggest user group. More than three-quarters of those working in IT report using AI at least a few times a year. In finance and professional services, the figure is a touch under 60%. AI-enabled or aided roles tend to be those that involve significant digital workflow and information synthesis; tasks that correspond with AIs current abilities.</p>\n<p>AI use is lower in sectors dominated by customer-facing or manual work. Only around a third of retail workers report comparable levels of use to their office counterparts, although those in healthcare and manufacturing do tend to deploy AI more often than those in retail, for example. The fact that current raft of AI platforms fit more naturally into desk-based, cognitive roles seems obvious  less so is a drop-off in user numbers in tightly-regulated environments.</p>\n<p>Do we, or dont we?</p>\n<p>Gallups data reveals a significant number of workers ore unsure whether or not their employer had adopted AI  nearly a quarter of those surveyed werent sure. In the third quarter of 2025, just over a third of employees said their organisation had implemented AI. 40% said there was no adoption of AI in their place of work</p>\n<p>Its worth noting that earlier versions of Gallup surveys didnt include a dont know option for questions about employers AI adoption, which encouraged respondents to guess. Belief in organisational AI adoption appeared to rise sharply between 2024 and 2025, therefore, Gallup says. Once uncertainty could be stated explicitly, it became clear a good number of employees were simply uninformed on the matter.</p>\n<p>Its staff in non-managerial roles who are more likely to say theyre unaware of their organisations AI use, a tendency mirrored in part-time staff and hands-on roles. The further workers are from decision-making, it seems, the less sure they become.</p>\n<p>How workers use AI</p>\n<p>The way employees use AI are consistent: of those using AI at least once a year, the most common applications are consolidating information, searching for information, and generating ideas, tasks that have changed little since Gallup first measured workplace AI use in 2024.</p>\n<p>More than 60% of AI users refer to chatbots, with using AI for writing and editing coming some way behind. Coding assistants and data science tools remain niche, but popular. Employees who use AI often are far more likely to use any more advanced tools at their disposal; particularly true in the cases of coding assistants and data analysis.</p>\n<p>Although use figures are generally up, Gallup concludes that AI has yet to be embedded in daily work for most Americans. Around 45% of workers say they use AI a few times a year, but only about 10% use it every day.</p>\n<p>Conclusions</p>\n<p>Business leaders have an easy win: simply clarifying a position on AI use would be a positive, plus publicising the availability (or otherwise) of AI tools would be an easy way to improve adoption rates.</p>\n<p>The current abilities of AI pertain to desk-based, digital and data-centric workflows, although there are a myriad of platforms that will utilise AI in other roles. Exploring these more fully would certainly be bucking the trend, and may make the difference between an organisations long-term prospects and those of its direct competitors.</p>\n<p>A page detailing Gallups findings can be found on the companys website.</p>\n<p>(Image source: DIY Open Plan Office by lower29 is licensed under CC BY-NC-SA 2.0.)</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Gallup Workforce shows details of AI adoption in US workplaces appeared first on AI News.</p>"
    },
    {
      "id": "d3ec1f2f4bb4",
      "title": "Google Launches Low-Cost AI Plus Subscription in the U.S.",
      "content": "The plan was previously available in select territories around the world but is now being rolled out to an additional 35 countries.",
      "url": "https://aibusiness.com/generative-ai/google-launches-ai-plus-subscription-us",
      "author": "Graham Hope",
      "published": "2026-01-28T22:53:13",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Google launched AI Plus subscription in the US and 35 additional countries, providing a lower-cost tier for AI features. The plan was previously available only in select territories.",
      "importance_score": 52.0,
      "reasoning": "Incremental product expansion rather than new capability. Relevant for consumer AI accessibility but not frontier-significant.",
      "themes": [
        "Consumer AI",
        "Google",
        "Subscriptions",
        "Product Launch"
      ],
      "continuation": null,
      "summary_html": "<p>Google launched AI Plus subscription in the US and 35 additional countries, providing a lower-cost tier for AI features. The plan was previously available only in select territories.</p>",
      "content_html": "<p>The plan was previously available in select territories around the world but is now being rolled out to an additional 35 countries.</p>"
    },
    {
      "id": "cff83eba4622",
      "title": "We Got Claude to Build CUDA Kernels and teach open models!",
      "content": "",
      "url": "https://huggingface.co/blog/upskill",
      "author": "Unknown",
      "published": "2026-01-28T00:00:00",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Hugging Face blog post describes using Claude to build CUDA kernels and teach open models. The post demonstrates AI-assisted low-level programming capabilities.",
      "importance_score": 50.0,
      "reasoning": "Interesting demonstration of AI coding capabilities but more of a technical tutorial than breakthrough news.",
      "themes": [
        "AI Coding",
        "CUDA",
        "Claude",
        "Open Source"
      ],
      "continuation": null,
      "summary_html": "<p>Hugging Face blog post describes using Claude to build CUDA kernels and teach open models. The post demonstrates AI-assisted low-level programming capabilities.</p>",
      "content_html": ""
    },
    {
      "id": "45b6d934ab6c",
      "title": "How Standard Chartered runs AI under privacy rules",
      "content": "For banks trying to put AI into real use, the hardest questions often come before any model is trained. Can the data be used at all? Where is it allowed to be stored? Who is responsible once the system goes live? At Standard Chartered, these privacy-driven questions now shape how AI systems are built,  and deployed at the bank.\nFor global banks operating in many jurisdictions, these early decisions are rarely straightforward. Privacy rules differ by market, and the same AI system may face very different constraints depending on where it is deployed. At Standard Chartered, this has pushed privacy teams into a more active role in shaping how AI systems are designed, approved, and monitored in the organisation.\n&#8220;Data privacy functions have become the starting point of most AI regulations,&#8221; says David Hardoon, Global Head of AI Enablement at Standard Chartered. In practice, that means privacy requirements shape the type of data that can be used in AI systems, how transparent those systems need to be, and how they are monitored once they are live.\nPrivacy shaping how AI runs\nThe bank is already running AI systems in live environments. The transition from pilots brings practical challenges that are easy to underestimate early on. In small trials, data sources are limited and well understood. In production, AI systems often pull data from many upstream platforms, each with its own structure and quality issues. &#8220;When moving from a contained pilot into live operations, ensuring data quality becomes more challenging with multiple upstream systems and potential schema differences,&#8221; Hardoon says.\nDavid Hardoon, Global Head of AI Enablement at Standard Chartered\nPrivacy rules add further constraints. In some cases, real customer data cannot be used to train models. Instead, teams may rely on anonymised data, which can affect how quickly systems are developed or how well they perform. Live deployments also operate at a much larger scale, increasing the impact of any gaps in controls. As Hardoon puts it, &#8220;As part of responsible and client-centric AI adoption, we prioritise adhering to principles of fairness, ethics, accountability, and transparency as data processing scope expands.&#8221;\nGeography and regulation decide where AI works\nWhere AI systems are built and deployed is also shaped by geography. Data protection laws vary in regions, and some countries impose strict rules on where data must be stored and who can access it. These requirements play a direct role in how Standard Chartered deploys AI, particularly for systems that rely on client or personally identifiable information.\n&#8220;Data sovereignty is often a key consideration when operating in different markets and regions,&#8221; Hardoon says. In markets with data localisation rules, AI systems may need to be deployed locally, or designed so that sensitive data does not cross borders. In other cases, shared platforms can be used, provided the right controls are in place. This results in a mix of global and market-specific AI deployments, shaped by local regulation not a single technical preference.\nThe same trade-offs appear in decisions about centralised AI platforms versus local solutions. Large organisations often aim to share models, tools, and oversight in markets to reduce duplication. Privacy laws do not always block this approach. &#8220;In general, privacy regulations do not explicitly prohibit transfer of data, but rather expect appropriate controls to be in place,&#8221; Hardoon says.\nThere are limits: some data cannot move in borders at all, and certain privacy laws apply beyond the country where data was collected. The details can restrict which markets a central platform can serve and where local systems remain necessary. For banks, this often leads to a layered setup, with shared foundations combined with localised AI use cases where regulation demands it.\nHuman oversight remains central\nAs AI becomes more embedded in decision-making, questions around explainability and consent grow harder to avoid. Automation may speed up processes, but it does not remove responsibility. &#8220;Transparency and explainability have become more crucial than before,&#8221; Hardoon says. Even when working with external vendors, accountability remains internal. This has reinforced the need for human oversight in AI systems, particularly where outcomes affect customers or regulatory obligations.\nPeople also play a larger role in privacy risk than technology alone. Processes and controls can be well designed, but they depend on how staff understand and handle data. &#8220;People remain the most important factor when it comes to implementing privacy controls,&#8221; Hardoon says. At Standard Chartered, this has pushed a focus on training and awareness, so teams know what data can be used, how it should be handled, and where the boundaries lie.\nScaling AI under growing regulatory scrutiny requires making privacy and governance easier to apply in practice. One approach the bank is taking is standardisation. By creating pre-approved templates, architectures, and data classifications, teams can move faster without bypassing controls. &#8220;Standardisation and re-usability are important,&#8221; Hardoon explains. Codifying rules around data residency, retention, and access helps turn complex requirements into clearer components that can be reused in AI projects.\nAs more organisations move AI into everyday operations, privacy is not just a compliance hurdle. It is shaping how AI systems are built, where they run, and how much trust they can earn. In banking, that shift is already influencing what AI looks like in practice  and where its limits are set.\n(Photo by Corporate Locations)\nSee also: The quiet work behind Citi&#8217;s 4,000-person internal AI rollout\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post How Standard Chartered runs AI under privacy rules appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/how-standard-chartered-runs-ai-under-privacy-rules/",
      "author": "Muhammad Zulhusni",
      "published": "2026-01-28T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Finance AI",
        "Governance, Regulation & Policy",
        "Interviews",
        "Trust, Bias & Fairness",
        "ai",
        "banking",
        "finance",
        "governance"
      ],
      "summary": "Standard Chartered's approach to AI deployment starts with privacy questions before any model training, with data privacy teams shaping system design, approval, and monitoring. The bank operates under varying privacy rules across jurisdictions.",
      "importance_score": 48.0,
      "reasoning": "Useful case study on enterprise AI governance in banking but represents one company's approach rather than industry news.",
      "themes": [
        "Enterprise AI",
        "Finance",
        "Privacy",
        "Governance"
      ],
      "continuation": null,
      "summary_html": "<p>Standard Chartered's approach to AI deployment starts with privacy questions before any model training, with data privacy teams shaping system design, approval, and monitoring. The bank operates under varying privacy rules across jurisdictions.</p>",
      "content_html": "<p>For banks trying to put AI into real use, the hardest questions often come before any model is trained. Can the data be used at all? Where is it allowed to be stored? Who is responsible once the system goes live? At Standard Chartered, these privacy-driven questions now shape how AI systems are built,  and deployed at the bank.</p>\n<p>For global banks operating in many jurisdictions, these early decisions are rarely straightforward. Privacy rules differ by market, and the same AI system may face very different constraints depending on where it is deployed. At Standard Chartered, this has pushed privacy teams into a more active role in shaping how AI systems are designed, approved, and monitored in the organisation.</p>\n<p>Data privacy functions have become the starting point of most AI regulations, says David Hardoon, Global Head of AI Enablement at Standard Chartered. In practice, that means privacy requirements shape the type of data that can be used in AI systems, how transparent those systems need to be, and how they are monitored once they are live.</p>\n<p>Privacy shaping how AI runs</p>\n<p>The bank is already running AI systems in live environments. The transition from pilots brings practical challenges that are easy to underestimate early on. In small trials, data sources are limited and well understood. In production, AI systems often pull data from many upstream platforms, each with its own structure and quality issues. When moving from a contained pilot into live operations, ensuring data quality becomes more challenging with multiple upstream systems and potential schema differences, Hardoon says.</p>\n<p>David Hardoon, Global Head of AI Enablement at Standard Chartered</p>\n<p>Privacy rules add further constraints. In some cases, real customer data cannot be used to train models. Instead, teams may rely on anonymised data, which can affect how quickly systems are developed or how well they perform. Live deployments also operate at a much larger scale, increasing the impact of any gaps in controls. As Hardoon puts it, As part of responsible and client-centric AI adoption, we prioritise adhering to principles of fairness, ethics, accountability, and transparency as data processing scope expands.</p>\n<p>Geography and regulation decide where AI works</p>\n<p>Where AI systems are built and deployed is also shaped by geography. Data protection laws vary in regions, and some countries impose strict rules on where data must be stored and who can access it. These requirements play a direct role in how Standard Chartered deploys AI, particularly for systems that rely on client or personally identifiable information.</p>\n<p>Data sovereignty is often a key consideration when operating in different markets and regions, Hardoon says. In markets with data localisation rules, AI systems may need to be deployed locally, or designed so that sensitive data does not cross borders. In other cases, shared platforms can be used, provided the right controls are in place. This results in a mix of global and market-specific AI deployments, shaped by local regulation not a single technical preference.</p>\n<p>The same trade-offs appear in decisions about centralised AI platforms versus local solutions. Large organisations often aim to share models, tools, and oversight in markets to reduce duplication. Privacy laws do not always block this approach. In general, privacy regulations do not explicitly prohibit transfer of data, but rather expect appropriate controls to be in place, Hardoon says.</p>\n<p>There are limits: some data cannot move in borders at all, and certain privacy laws apply beyond the country where data was collected. The details can restrict which markets a central platform can serve and where local systems remain necessary. For banks, this often leads to a layered setup, with shared foundations combined with localised AI use cases where regulation demands it.</p>\n<p>Human oversight remains central</p>\n<p>As AI becomes more embedded in decision-making, questions around explainability and consent grow harder to avoid. Automation may speed up processes, but it does not remove responsibility. Transparency and explainability have become more crucial than before, Hardoon says. Even when working with external vendors, accountability remains internal. This has reinforced the need for human oversight in AI systems, particularly where outcomes affect customers or regulatory obligations.</p>\n<p>People also play a larger role in privacy risk than technology alone. Processes and controls can be well designed, but they depend on how staff understand and handle data. People remain the most important factor when it comes to implementing privacy controls, Hardoon says. At Standard Chartered, this has pushed a focus on training and awareness, so teams know what data can be used, how it should be handled, and where the boundaries lie.</p>\n<p>Scaling AI under growing regulatory scrutiny requires making privacy and governance easier to apply in practice. One approach the bank is taking is standardisation. By creating pre-approved templates, architectures, and data classifications, teams can move faster without bypassing controls. Standardisation and re-usability are important, Hardoon explains. Codifying rules around data residency, retention, and access helps turn complex requirements into clearer components that can be reused in AI projects.</p>\n<p>As more organisations move AI into everyday operations, privacy is not just a compliance hurdle. It is shaping how AI systems are built, where they run, and how much trust they can earn. In banking, that shift is already influencing what AI looks like in practice  and where its limits are set.</p>\n<p>(Photo by Corporate Locations)</p>\n<p>See also: The quiet work behind Citis 4,000-person internal AI rollout</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post How Standard Chartered runs AI under privacy rules appeared first on AI News.</p>"
    },
    {
      "id": "028a32bd4ede",
      "title": "Franny Hsiao, Salesforce: Scaling enterprise AI",
      "content": "Scaling enterprise AI requires overcoming architectural oversights that often stall pilots before production, a challenge that goes far beyond model selection. While generative AI prototypes are easy to spin up, turning them into reliable business assets involves solving the difficult problems of data engineering and governance.\n\n\n\nAhead of AI &amp; Big Data Global 2026 in London, Franny Hsiao, EMEA Leader of AI Architects at Salesforce, discussed why so many initiatives hit a wall and how organisations can architect systems that actually survive the real world.\n\n\n\nThe pristine island problem of scaling enterprise AI\n\n\n\nMost failures stem from the environment in which the AI is built. Pilots frequently begin in controlled settings that create a false sense of security, only to crumble when faced with enterprise scale.\n\n\n\n\n\n\n\n&#8220;The single most common architectural oversight that prevents AI pilots from scaling is the failure to architect a production-grade data infrastructure with built-in end to end governance from the start,&#8221; Hsiao explains.\n\n\n\n&#8220;Understandably, pilots often start on pristine islands  using small, curated datasets and simplified workflows. But this ignores the messy reality of enterprise data: the complex integration, normalisation, and transformation required to handle real-world volume and variability.&#8221;\n\n\n\nWhen companies attempt to scale these island-based pilots without addressing the underlying data mess, the systems break. Hsiao warns that &#8220;the resulting data gaps and performance issues like inference latency render the AI systems unusableand, more importantly, untrustworthy.&#8221;\n\n\n\nHsiao argues that the companies successfully bridging this gap are those that &#8220;bake end-to-end observability and guardrails into the entire lifecycle.&#8221; This approach provides &#8220;visibility and control into how effective the AI systems are and how users are adopting the new technology.&#8221;\n\n\n\nEngineering for perceived responsiveness\n\n\n\nAs enterprises deploy large reasoning models  like the Atlas Reasoning Engine  they face a trade-off between the depth of the models &#8220;thinking&#8221; and the users patience. Heavy compute creates latency.\n\n\n\nSalesforce addresses this by focusing on &#8220;perceived responsiveness through Agentforce Streaming,&#8221; according to Hsiao.\n\n\n\n&#8220;This allows us to deliver AI-generated responses progressively, even while the reasoning engine performs heavy computation in the background. Its an incredibly effective approach for reducing perceived latency, which often stalls production AI.&#8221;\n\n\n\nTransparency also plays a functional role in managing user expectations when scaling enterprise AI. Hsiao elaborates on using design as a trust mechanism: &#8220;By surfacing progress indicators that show the reasoning steps or the tools being used, as well images like spinners and progress bars to depict loading states, we dont just keep users engaged; we improve perceived responsiveness and build trust.\n\n\n\nThis visibility, combined with strategic model selection  like choosing smaller models for fewer computations, meaning faster response times  and explicit length constraints, ensures the system feels deliberate and responsive.&#8221;\n\n\n\nOffline intelligence at the edge\n\n\n\nFor industries with field operations, such as utilities or logistics, reliance on continuous cloud connectivity is a non-starter. &#8220;For many of our enterprise customers, the biggest practical driver is offline functionality,&#8221; states Hsiao.\n\n\n\nHsiao highlights the shift toward on-device intelligence, particularly in field services, where the workflow must continue regardless of signal strength.\n\n\n\n&#8220;A technician can photograph a faulty part, error code, or serial number while offline. Then an on-device LLM can then identify the asset or error, and provide guided troubleshooting steps from a cached knowledge base instantly,&#8221; explains Hsiao.\n\n\n\nData synchronisation happens automatically once connectivity returns. &#8220;Once a connection is restored, the system handles the &#8216;heavy lifting&#8217; of syncing that data back to the cloud to maintain a single source of truth. This ensures that work gets done, even in the most disconnected environments.&#8221;\n\n\n\nHsiao expects continued innovation in edge AI due to benefits like &#8220;ultra-low latency, enhanced privacy and data security, energy efficiency, and cost savings.&#8221;\n\n\n\nHigh-stakes gateways\n\n\n\nAutonomous agents are not set-and-forget tools. When scaling enterprise AI deployments, governance requires defining exactly when a human must verify an action. Hsiao describes this not as dependency, but as &#8220;architecting for accountability and continuous learning.&#8221;\n\n\n\nSalesforce mandates a &#8220;human-in-the-loop&#8221; for specific areas Hsiao calls &#8220;high-stakes gateways&#8221;:\n\n\n\n&#8220;This includes specific action categories, including any CUD (Creating, Uploading, or Deleting) actions, as well as verified contact and customer contact actions, says Hsiao. We also default to human confirmation for critical decision-making or any action that could be potentially exploited through prompt manipulation.&#8221;\n\n\n\nThis structure creates a feedback loop where &#8220;agents learn from human expertise,&#8221; creating a system of &#8220;collaborative intelligence&#8221; rather than unchecked automation.\n\n\n\nTrusting an agent requires seeing its work. Salesforce has built a &#8220;Session Tracing Data Model (STDM)&#8221; to provide this visibility. It captures &#8220;turn-by-turn logs&#8221; that offer granular insight into the agent&#8217;s logic.\n\n\n\n&#8220;This gives us granular step-by-step visibility that captures every interaction including user questions, planner steps, tool calls, inputs/outputs, retrieved chunks, responses, timing, and errors,&#8221; says Hsiao.\n\n\n\nThis data allows organisations to run Agent Analytics for adoption metrics, Agent Optimisation to drill down into performance, and Health Monitoring for uptime and latency tracking.\n\n\n\n&#8220;Agentforce observability is the single mission control for all your Agentforce agents for unified visibility, monitoring, and optimisation, Hsiao summarises.\n\n\n\nStandardising agent communication\n\n\n\nAs businesses deploy agents from different vendors, these systems need a shared protocol to collaborate. &#8220;For multi-agent orchestration to work, agents cant exist in a vacuum; they need common language,&#8221; argues Hsiao.\n\n\n\nHsiao outlines two layers of standardisation: orchestration and meaning. For orchestration, Salesforce is adopting open-source standards like MCP (Model Context Protocol) and A2A (Agent to Agent Protocol).\n\n\n\n&#8220;We believe open source standards are non-negotiable; they prevent vendor lock-in, enable interoperability, and accelerate innovation.&#8221;\n\n\n\nHowever, communication is useless if the agents interpret data differently. To solve for fragmented data, Salesforce co-founded OSI (Open Semantic Interchange) to unify semantics so an agent in one system &#8220;truly understands the intent of an agent in another.&#8221;\n\n\n\nThe future enterprise AI scaling bottleneck: agent-ready data\n\n\n\nLooking forward, the challenge will shift from model capability to data accessibility. Many organisations still struggle with legacy, fragmented infrastructure where &#8220;searchability and reusability&#8221; remain difficult.\n\n\n\nHsiao predicts the next major hurdle  and solution  will be making enterprise data &#8220;agent-ready through searchable, context-aware architectures that replace traditional, rigid ETL pipelines.&#8221; This shift is necessary to enable &#8220;hyper-personalised and transformed user experience because agents can always access the right context.&#8221;\n\n\n\n&#8220;Ultimately, the next year isnt about the race for bigger, newer models; its about building the orchestration and data infrastructure that allows production-grade agentic systems to thrive, Hsiao concludes.\n\n\n\nSalesforce is a key sponsor of this years AI &amp; Big Data Global in London and will have a range of speakers, including Franny Hsiao, sharing their insights during the event. Be sure to swing by Salesforces booth at stand #163 for more from the companys experts.\n\n\n\nSee also: Databricks: Enterprise AI adoption shifts to agentic systems\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Franny Hsiao, Salesforce: Scaling enterprise AI appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/franny-hsiao-salesforce-scaling-enterprise-ai/",
      "author": "Ryan Daws",
      "published": "2026-01-28T15:00:44",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI and Us",
        "AI Business Strategy",
        "AI Market Trends",
        "Features",
        "Governance, Regulation & Policy",
        "How It Works",
        "Human-AI Relationships",
        "Inside AI",
        "Interviews",
        "TechEx Events",
        "Trust, Bias & Fairness",
        "World of Work",
        "agentic ai",
        "agents",
        "data",
        "governance",
        "salesforce",
        "strategy"
      ],
      "summary": "Salesforce's EMEA AI Architects leader discusses why enterprise AI initiatives stall, citing pilots built in controlled environments that fail in production. The interview emphasizes data engineering and governance challenges.",
      "importance_score": 46.0,
      "reasoning": "Interview content providing enterprise AI insights but lacking news value. Useful perspective on scaling challenges.",
      "themes": [
        "Enterprise AI",
        "Salesforce",
        "Data Engineering",
        "Governance"
      ],
      "continuation": null,
      "summary_html": "<p>Salesforce's EMEA AI Architects leader discusses why enterprise AI initiatives stall, citing pilots built in controlled environments that fail in production. The interview emphasizes data engineering and governance challenges.</p>",
      "content_html": "<p>Scaling enterprise AI requires overcoming architectural oversights that often stall pilots before production, a challenge that goes far beyond model selection. While generative AI prototypes are easy to spin up, turning them into reliable business assets involves solving the difficult problems of data engineering and governance.</p>\n<p>Ahead of AI &amp; Big Data Global 2026 in London, Franny Hsiao, EMEA Leader of AI Architects at Salesforce, discussed why so many initiatives hit a wall and how organisations can architect systems that actually survive the real world.</p>\n<p>The pristine island problem of scaling enterprise AI</p>\n<p>Most failures stem from the environment in which the AI is built. Pilots frequently begin in controlled settings that create a false sense of security, only to crumble when faced with enterprise scale.</p>\n<p>The single most common architectural oversight that prevents AI pilots from scaling is the failure to architect a production-grade data infrastructure with built-in end to end governance from the start, Hsiao explains.</p>\n<p>Understandably, pilots often start on pristine islands  using small, curated datasets and simplified workflows. But this ignores the messy reality of enterprise data: the complex integration, normalisation, and transformation required to handle real-world volume and variability.</p>\n<p>When companies attempt to scale these island-based pilots without addressing the underlying data mess, the systems break. Hsiao warns that the resulting data gaps and performance issues like inference latency render the AI systems unusableand, more importantly, untrustworthy.</p>\n<p>Hsiao argues that the companies successfully bridging this gap are those that bake end-to-end observability and guardrails into the entire lifecycle. This approach provides visibility and control into how effective the AI systems are and how users are adopting the new technology.</p>\n<p>Engineering for perceived responsiveness</p>\n<p>As enterprises deploy large reasoning models  like the Atlas Reasoning Engine  they face a trade-off between the depth of the models thinking and the users patience. Heavy compute creates latency.</p>\n<p>Salesforce addresses this by focusing on perceived responsiveness through Agentforce Streaming, according to Hsiao.</p>\n<p>This allows us to deliver AI-generated responses progressively, even while the reasoning engine performs heavy computation in the background. Its an incredibly effective approach for reducing perceived latency, which often stalls production AI.</p>\n<p>Transparency also plays a functional role in managing user expectations when scaling enterprise AI. Hsiao elaborates on using design as a trust mechanism: By surfacing progress indicators that show the reasoning steps or the tools being used, as well images like spinners and progress bars to depict loading states, we dont just keep users engaged; we improve perceived responsiveness and build trust.</p>\n<p>This visibility, combined with strategic model selection  like choosing smaller models for fewer computations, meaning faster response times  and explicit length constraints, ensures the system feels deliberate and responsive.</p>\n<p>Offline intelligence at the edge</p>\n<p>For industries with field operations, such as utilities or logistics, reliance on continuous cloud connectivity is a non-starter. For many of our enterprise customers, the biggest practical driver is offline functionality, states Hsiao.</p>\n<p>Hsiao highlights the shift toward on-device intelligence, particularly in field services, where the workflow must continue regardless of signal strength.</p>\n<p>A technician can photograph a faulty part, error code, or serial number while offline. Then an on-device LLM can then identify the asset or error, and provide guided troubleshooting steps from a cached knowledge base instantly, explains Hsiao.</p>\n<p>Data synchronisation happens automatically once connectivity returns. Once a connection is restored, the system handles the heavy lifting of syncing that data back to the cloud to maintain a single source of truth. This ensures that work gets done, even in the most disconnected environments.</p>\n<p>Hsiao expects continued innovation in edge AI due to benefits like ultra-low latency, enhanced privacy and data security, energy efficiency, and cost savings.</p>\n<p>High-stakes gateways</p>\n<p>Autonomous agents are not set-and-forget tools. When scaling enterprise AI deployments, governance requires defining exactly when a human must verify an action. Hsiao describes this not as dependency, but as architecting for accountability and continuous learning.</p>\n<p>Salesforce mandates a human-in-the-loop for specific areas Hsiao calls high-stakes gateways:</p>\n<p>This includes specific action categories, including any CUD (Creating, Uploading, or Deleting) actions, as well as verified contact and customer contact actions, says Hsiao. We also default to human confirmation for critical decision-making or any action that could be potentially exploited through prompt manipulation.</p>\n<p>This structure creates a feedback loop where agents learn from human expertise, creating a system of collaborative intelligence rather than unchecked automation.</p>\n<p>Trusting an agent requires seeing its work. Salesforce has built a Session Tracing Data Model (STDM) to provide this visibility. It captures turn-by-turn logs that offer granular insight into the agents logic.</p>\n<p>This gives us granular step-by-step visibility that captures every interaction including user questions, planner steps, tool calls, inputs/outputs, retrieved chunks, responses, timing, and errors, says Hsiao.</p>\n<p>This data allows organisations to run Agent Analytics for adoption metrics, Agent Optimisation to drill down into performance, and Health Monitoring for uptime and latency tracking.</p>\n<p>Agentforce observability is the single mission control for all your Agentforce agents for unified visibility, monitoring, and optimisation, Hsiao summarises.</p>\n<p>Standardising agent communication</p>\n<p>As businesses deploy agents from different vendors, these systems need a shared protocol to collaborate. For multi-agent orchestration to work, agents cant exist in a vacuum; they need common language, argues Hsiao.</p>\n<p>Hsiao outlines two layers of standardisation: orchestration and meaning. For orchestration, Salesforce is adopting open-source standards like MCP (Model Context Protocol) and A2A (Agent to Agent Protocol).</p>\n<p>We believe open source standards are non-negotiable; they prevent vendor lock-in, enable interoperability, and accelerate innovation.</p>\n<p>However, communication is useless if the agents interpret data differently. To solve for fragmented data, Salesforce co-founded OSI (Open Semantic Interchange) to unify semantics so an agent in one system truly understands the intent of an agent in another.</p>\n<p>The future enterprise AI scaling bottleneck: agent-ready data</p>\n<p>Looking forward, the challenge will shift from model capability to data accessibility. Many organisations still struggle with legacy, fragmented infrastructure where searchability and reusability remain difficult.</p>\n<p>Hsiao predicts the next major hurdle  and solution  will be making enterprise data agent-ready through searchable, context-aware architectures that replace traditional, rigid ETL pipelines. This shift is necessary to enable hyper-personalised and transformed user experience because agents can always access the right context.</p>\n<p>Ultimately, the next year isnt about the race for bigger, newer models; its about building the orchestration and data infrastructure that allows production-grade agentic systems to thrive, Hsiao concludes.</p>\n<p>Salesforce is a key sponsor of this years AI &amp; Big Data Global in London and will have a range of speakers, including Franny Hsiao, sharing their insights during the event. Be sure to swing by Salesforces booth at stand #163 for more from the companys experts.</p>\n<p>See also: Databricks: Enterprise AI adoption shifts to agentic systems</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Franny Hsiao, Salesforce: Scaling enterprise AI appeared first on AI News.</p>"
    },
    {
      "id": "c6774b371d9b",
      "title": "Doomsday Clock at 85 seconds to midnight amid threats from climate crisis and AI",
      "content": "Planet closer to destruction as Russia, China and US become more aggressive and nationalistic, says advocacy groupEarth is closer than it has ever been to destruction as Russia, China, the US and other countries become increasingly aggressive, adversarial, and nationalistic, a science-oriented advocacy group said on Tuesday as it advanced its Doomsday Clock to 85 seconds until midnight.The Bulletin of the Atomic Scientist members had an initial demonstration on Friday and then announced their results on Tuesday. Continue reading...",
      "url": "https://www.theguardian.com/us-news/2026/jan/27/doomsday-clock-seconds-to-midnight",
      "author": "Associated Press",
      "published": "2026-01-28T00:43:39",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "US news",
        "World news",
        "Nuclear weapons",
        "Climate crisis",
        "Biotechnology industry",
        "AI (artificial intelligence)",
        "India",
        "Pakistan",
        "Israel",
        "Iran",
        "Donald Trump",
        "Fossil fuels",
        "Russia",
        "China"
      ],
      "summary": "The Doomsday Clock was moved to 85 seconds to midnight, with AI listed among existential threats alongside climate crisis, nuclear weapons, and geopolitical tensions.",
      "importance_score": 45.0,
      "reasoning": "General concern indicator including AI among multiple threats. Symbolic rather than AI-specific news.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "Geopolitics"
      ],
      "continuation": null,
      "summary_html": "<p>The Doomsday Clock was moved to 85 seconds to midnight, with AI listed among existential threats alongside climate crisis, nuclear weapons, and geopolitical tensions.</p>",
      "content_html": "<p>Planet closer to destruction as Russia, China and US become more aggressive and nationalistic, says advocacy groupEarth is closer than it has ever been to destruction as Russia, China, the US and other countries become increasingly aggressive, adversarial, and nationalistic, a science-oriented advocacy group said on Tuesday as it advanced its Doomsday Clock to 85 seconds until midnight.The Bulletin of the Atomic Scientist members had an initial demonstration on Friday and then announced their results on Tuesday. Continue reading...</p>"
    },
    {
      "id": "a839d7811231",
      "title": "Masumi Network: How AI-blockchain fusion adds trust to burgeoning agent economy",
      "content": "2026 will see forward-thinking organisations building out their squads of AI agents across roles and functions. But amid the rush, there is another aspect to consider.\n\n\n\nOne of IDCs enterprise technology predictions for the coming five years, published in October, was fascinating. By 2030, up to 20% of [global 1000] organisations will have faced lawsuits, substantial fines, and CIO dismissals, due to high-profile disruptions stemming from inadequate controls and governance of AI agents, the analyst noted.\n\n\n\nHow do you therefore put guardrails in place  and how do you ensure these agents work together and, ultimately, do business together? Patrick Tobler, founder and CEO of blockchain infrastructure platform provider NMKR, is working on a project which aims to solve this  by fusing agentic AI and decentralisation.\n\n\n\nThe Masumi Network, born out of a collaboration between NMKR and Serviceplan Group, launched in late 2024 as a framework-agnostic infrastructure which empowers developers to build autonomous agents that collaborate, monetise services, and maintain verifiable trust.\n\n\n\nThe core thesis of Masumi is that theres going to be billions of different AI agents from different companies interacting with each other in the future, explains Tobler. The difficult part now is  how do you actually have agents from different companies that can interact with each other and send money to each other as well, across these different companies?\n\n\n\nTake travel as an example. You want to attend an industry conference, so your hotel booking agent buys a plane ticket from your airline agent. The entire experience and transaction will be seamless  but that implicit trust is required.\n\n\n\nMasumi is a decentralised network of agents, so its not relying on any centralised payment infrastructure, says Tobler. Instead, agents are equipped with wallets and can send stablecoins from one agent to another and, because of that, interacting with each other in a completely safe and trustless manner.\n\n\n\nFor Tobler, having spent in his words a lot of time in crypto, he determined that its benefits were being pointed to the wrong place.\n\n\n\nI think theres a lot of these problems that we have solved in crypto for humans, and then I came to this conclusion that maybe weve been solving them or the wrong target audience, he explains. Because for humans, using crypto and wallets and blockchains, all that kind of stuff is extremely difficult; the user experience is not great. But for agents, they dont care if its difficult to use. They just use it, and its very native to them.\n\n\n\nSo all these issues that are now arising with agents having to interact with millions, or maybe even billions, of agents in the future  these problems have all already been solved with crypto.\n\n\n\nTobler is attending AI &amp; Big Data Expo Global as part of Discover Cardano; NMKR started on the Cardano blockchain, while Masumi is built completely on Cardano. He says he is looking forward to speaking with businesses that are hearing a lot about AI but arent really using it much besides ChatGPT.\n\n\n\nI want to understand from them what they are doing, and then figure out how we can help them, he says. Thats most often the thing missing from traditional tech startups. Were all building for our own bubble, instead of actually talking to the people that would be using it every day.\n\n\n\nDiscover Cardano is exhibiting at the AI &amp; Big Data Expo Global, in London on February 4-5. Watch the full video interview with NMKR&#8217;s Patrick Tobler below:\n\n\n\n\n\n\n\nPhoto by Google DeepMind\nThe post Masumi Network: How AI-blockchain fusion adds trust to burgeoning agent economy appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/masumi-network-how-ai-blockchain-fusion-adds-trust-to-burgeoning-agent-economy/",
      "author": "TechForge",
      "published": "2026-01-28T12:28:14",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Human-AI Relationships",
        "Open-Source & Democratised AI",
        "TechEx Events"
      ],
      "summary": "Masumi Network combines AI and blockchain to add trust mechanisms to the agent economy, addressing governance and control concerns for AI agents doing business together.",
      "importance_score": 44.0,
      "reasoning": "Niche intersection of AI and blockchain. Addresses valid concerns but limited immediate frontier significance.",
      "themes": [
        "Agentic AI",
        "Blockchain",
        "Governance",
        "Trust"
      ],
      "continuation": null,
      "summary_html": "<p>Masumi Network combines AI and blockchain to add trust mechanisms to the agent economy, addressing governance and control concerns for AI agents doing business together.</p>",
      "content_html": "<p>2026 will see forward-thinking organisations building out their squads of AI agents across roles and functions. But amid the rush, there is another aspect to consider.</p>\n<p>One of IDCs enterprise technology predictions for the coming five years, published in October, was fascinating. By 2030, up to 20% of [global 1000] organisations will have faced lawsuits, substantial fines, and CIO dismissals, due to high-profile disruptions stemming from inadequate controls and governance of AI agents, the analyst noted.</p>\n<p>How do you therefore put guardrails in place  and how do you ensure these agents work together and, ultimately, do business together? Patrick Tobler, founder and CEO of blockchain infrastructure platform provider NMKR, is working on a project which aims to solve this  by fusing agentic AI and decentralisation.</p>\n<p>The Masumi Network, born out of a collaboration between NMKR and Serviceplan Group, launched in late 2024 as a framework-agnostic infrastructure which empowers developers to build autonomous agents that collaborate, monetise services, and maintain verifiable trust.</p>\n<p>The core thesis of Masumi is that theres going to be billions of different AI agents from different companies interacting with each other in the future, explains Tobler. The difficult part now is  how do you actually have agents from different companies that can interact with each other and send money to each other as well, across these different companies?</p>\n<p>Take travel as an example. You want to attend an industry conference, so your hotel booking agent buys a plane ticket from your airline agent. The entire experience and transaction will be seamless  but that implicit trust is required.</p>\n<p>Masumi is a decentralised network of agents, so its not relying on any centralised payment infrastructure, says Tobler. Instead, agents are equipped with wallets and can send stablecoins from one agent to another and, because of that, interacting with each other in a completely safe and trustless manner.</p>\n<p>For Tobler, having spent in his words a lot of time in crypto, he determined that its benefits were being pointed to the wrong place.</p>\n<p>I think theres a lot of these problems that we have solved in crypto for humans, and then I came to this conclusion that maybe weve been solving them or the wrong target audience, he explains. Because for humans, using crypto and wallets and blockchains, all that kind of stuff is extremely difficult; the user experience is not great. But for agents, they dont care if its difficult to use. They just use it, and its very native to them.</p>\n<p>So all these issues that are now arising with agents having to interact with millions, or maybe even billions, of agents in the future  these problems have all already been solved with crypto.</p>\n<p>Tobler is attending AI &amp; Big Data Expo Global as part of Discover Cardano; NMKR started on the Cardano blockchain, while Masumi is built completely on Cardano. He says he is looking forward to speaking with businesses that are hearing a lot about AI but arent really using it much besides ChatGPT.</p>\n<p>I want to understand from them what they are doing, and then figure out how we can help them, he says. Thats most often the thing missing from traditional tech startups. Were all building for our own bubble, instead of actually talking to the people that would be using it every day.</p>\n<p>Discover Cardano is exhibiting at the AI &amp; Big Data Expo Global, in London on February 4-5. Watch the full video interview with NMKRs Patrick Tobler below:</p>\n<p>Photo by Google DeepMind</p>\n<p>The post Masumi Network: How AI-blockchain fusion adds trust to burgeoning agent economy appeared first on AI News.</p>"
    },
    {
      "id": "119e826485cc",
      "title": "Nemotron-Personas-Brazil: Co-Designed Data for Sovereign AI",
      "content": "",
      "url": "https://huggingface.co/blog/nvidia/nemotron-personas-brazil",
      "author": "Unknown",
      "published": "2026-01-28T00:56:10",
      "source": "Hugging Face - Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "Nvidia's Nemotron-Personas-Brazil project focuses on co-designed data for sovereign AI applications in Brazil.",
      "importance_score": 42.0,
      "reasoning": "Limited article content available. Sovereign AI data is relevant but appears to be regional/specific initiative.",
      "themes": [
        "Sovereign AI",
        "Training Data",
        "Nvidia",
        "Brazil"
      ],
      "continuation": null,
      "summary_html": "<p>Nvidia's Nemotron-Personas-Brazil project focuses on co-designed data for sovereign AI applications in Brazil.</p>",
      "content_html": ""
    },
    {
      "id": "5e557fc796c9",
      "title": "This train isnt going to stop: shocking Sundance film shows promises and perils of AI",
      "content": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxietyAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.The AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Rohers own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT. The sophistication of the public tools  the ability to produce whole paragraphs in seconds, or produce illustrations  both thrilled and unnerved him. AI was already radically shaping the film-making industry, and proclamations on the promise and peril of AI were everywhere, with little way for people outside the tech industry to evaluate them. As an artist, he wondered, how was he to make sense of it all? Continue reading...",
      "url": "https://www.theguardian.com/film/2026/jan/27/sundance-ai-documentary-daniel-roher",
      "author": "Adrian Horton in Park City",
      "published": "2026-01-28T02:40:01",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Sundance 2026",
        "Sundance film festival",
        "Film",
        "AI (artificial intelligence)",
        "Culture",
        "Computing",
        "Documentary films",
        "Technology"
      ],
      "summary": "Sundance documentary 'The AI Doc: Or How I Became an Apocaloptimist' explores AI risks and opportunities through filmmaker Daniel Roher's personal anxiety, featuring Sam Altman and other AI figures.",
      "importance_score": 42.0,
      "reasoning": "Cultural/entertainment coverage of AI topics. May influence public perception but not frontier-significant.",
      "themes": [
        "AI Culture",
        "Documentary",
        "Sam Altman",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Sundance documentary 'The AI Doc: Or How I Became an Apocaloptimist' explores AI risks and opportunities through filmmaker Daniel Roher's personal anxiety, featuring Sam Altman and other AI figures.</p>",
      "content_html": "<p>The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxietyAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.The AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Rohers own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT. The sophistication of the public tools  the ability to produce whole paragraphs in seconds, or produce illustrations  both thrilled and unnerved him. AI was already radically shaping the film-making industry, and proclamations on the promise and peril of AI were everywhere, with little way for people outside the tech industry to evaluate them. As an artist, he wondered, how was he to make sense of it all? Continue reading...</p>"
    },
    {
      "id": "cd4214aa6d85",
      "title": "Navigating the Next Phase of GenAI: Predictions for 2026",
      "content": "This year, enterprises will increasingly use and refine how they apply AI technology, balancing cost management with maximizing AI effectiveness.",
      "url": "https://aibusiness.com/agentic-ai/navigating-the-next-phase-of-genai",
      "author": "Esther Shittu",
      "published": "2026-01-28T15:33:33",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Analysis piece on GenAI predictions for 2026, discussing how enterprises will refine AI application while balancing cost management and effectiveness.",
      "importance_score": 40.0,
      "reasoning": "Prediction/analysis content without concrete news. Useful framing but not newsworthy.",
      "themes": [
        "Enterprise AI",
        "Predictions",
        "Strategy"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis piece on GenAI predictions for 2026, discussing how enterprises will refine AI application while balancing cost management and effectiveness.</p>",
      "content_html": "<p>This year, enterprises will increasingly use and refine how they apply AI technology, balancing cost management with maximizing AI effectiveness.</p>"
    },
    {
      "id": "025fc820955b",
      "title": "It's Time to Science",
      "content": "Can you believe there are no dedicated AI for Science podcasts in the world??1 Today, we are fixing that by launching our first Science pod with new hosts!The gap between the sheer interest from insiders vs the relative disinterest/ignorance from the masses is the widest gap I have observed since I called out the Rise of the AI Engineer in 2023. Here&#8217;s just the most obvious points of what we&#8217;re seeing:Biohub&#8217;s bet is that biology advances fastest when it looks more like a software team than a grant pipeline, with software engineers, AI researchers, and biologists co-building models, tools, and datasets rather than throwing results over the wall in silos. This obviously comes from up top, with their co-CEOs being Mark Zuckerberg (engineer) and Priscilla Chan (doctor/scientist). As Mark says, this pairing of &#8220;frontier AI lab&#8221; and &#8220;frontier biology lab&#8221; will unlock a lot more advances in fundamental biology than possible, enabling us to model humans in silico and treating diseases as &#8220;rare as one&#8221;.&#8220;2026 in AI for Science will look a lot like 2025 in AI for Software Engineering&#8221; &#8212; Kevin Weil, OpenAI&#8217;s new VP for Science,&nbsp;said yesterday. Both OpenAI and Anthropic have spun up dedicated AI for Science divisions in the past 4 months, pursuing both product and research. DeepMind of course has the longest history in Science among the frontier labs.In the last 6 months alone, Periodic Labs ($300M seed, upcoming guest!) is building autonomous, AI&#8209;driven &#8220;science factory&#8221; for materials&#8209;led discovery. Lila Sciences ($350M A) is combining frontier models with robotic labs that generate experimental data across biology, chemistry, and materials science. CuspAI ($100M A, upcoming guest!) is building foundation models that search for novel compounds and industrial materials. In Excelsior Sciences ($95M) is building AI&#8209;guided experimentation that drastically shorten drug&#8209;optimization cycles. Chai Discovery (~$70M) is working on antibody and drug discovery, while Harmonic ($120M C), Axiom ($64M seed) and Math Inc are build math&#8209;native AI systems focused on formal reasoning and verifiable proofs.Sam Altman and Jakub Pachocki have publicly committed to delivering an &#8220;Automated AI Research Intern&#8221; by September THIS YEAR (&#8220;A system that can meaningfully accelerate human researchers, not just chat or code.&#8221; - OpenAI Prism is the starting point), and a &#8220;meaningful, Fully Automated AI Researcher by 2028 (&#8220;A system capable of autonomously delivering on larger research projects &#8212; meaning it could formulate hypotheses, plan experiments, run them, analyze results, and draw conclusions without human hand-holding.&#8221;)As a rule, we try to stay grounded at Latent Space, and veer away from discussing AGI timelines and Superintelligence. But it hard to deny that by 2100, we will likely look back and see applying AI engineering to the hard sciences is one of the most important missions to pursue this century. The stakes for humanity are high across chemistry, materials science, high performance computing/chip design, the biology/ pharmaceuticals complex, and even physics, mathematics, and climate science, not to mention AI research accelerating itself. And of course it will be both financially rewarding and impactful.None of this is new. Pushing scientific frontiers has always been important, and yet, to paraphrase Jeff Hammerbacher, the best human minds and LLMs of our generation continue to be directed towards astroturfing Reddit, promote a fork of a fork of a VSCode extension, and generally serving slop. That&#8217;s fine, and slop is a natural part of a well rounded diet, but we see very rewarding opportunities to raise the aspirations of AI Engineers and show them why the general skillset of AI and ML Engineering can actually transfer across to the hard sciences even if the total sum of biology knowledge you have is &#8220;the Mitochondria is the powerhouse of the cell&#8221;:Standard Knowledge Worker Benchmarks are saturated. We&#8217;re tired of every AI lab taking turns incrementing SWE-Bench Verified or MMLU by 0.1% and claiming SOTA. We know that GPT 5.2 Pro can do ~70% of expert level white collar work. The remaining, &#8220;final frontier&#8221; to hillclimb is on hard sciences like FrontierMath and HLE, our last gasp before discovering new science altogether.Hard Domains are getting Bitter Lessoned: Everything is converging on transformers and multimodal foundation models, even non-mathematicians can get IMO Gold medals because generalist ML &gt; domain expertise.AI solves JIT Self Education: Bloom&#8217;s 2 Sigma Problem ensures that LLMs can help you quickly ramp up on at least baseline common knowledge2 teach yourself what you need to know to interface with domain experts/enable them to do with your skillset what they can&#8217;t do on your own.What a time to be alive: Even if you are perfectly content in your B2B SaaS or Koding Agent job, it is still an incredible privilege to be alive while all this other progress is ongoing in adjacent fields. We&#8217;ll tell these stories, our way.Persuant to the last point, we think general consumer AI media, or even technical AI media, is woefully underequipped to do good science communication around &#8220;AI for Science for Engineers&#8221;.Today, we&#8217;re embarking on that journey with Latent Space&#8217;s new AI for Science podcast. We&#8217;re nervous about it too &#8212; but we think this mission is too important, and the gap between the obvious potential and the lack of supply, to wait any longer. We&#8217;d love your support and feedback as we start this new adventure.As a final word, thank you to RJ Honicky (Miraomics) and Brandon Anderson (Atomic) for stepping up to host the new pod. Alessio and I are devtools guys; we could not start AI for Science coverage without actual PhD&#8217;s and Science founders!1Proof: there&#8217;s an &#8220;ML4Sci&#8221; podcast and an &#8220;ai4.science&#8221; event series, a domain we wanted to buy, but no podcast that proudly self identifies as simply &#8220;the AI for Science podcast&#8221;. 2but beware GeLLMann Amnesia at the edges of expertise!",
      "url": "https://www.latent.space/p/science",
      "author": "swyx (Shawn)",
      "published": "2026-01-28T21:46:25",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Latent Space launched a dedicated AI for Science podcast, noting a gap between insider interest and public awareness in the AI for science space.",
      "importance_score": 40.0,
      "reasoning": "Media/content announcement rather than AI news. Highlights AI for science trend but not itself significant.",
      "themes": [
        "AI for Science",
        "Podcast",
        "Media"
      ],
      "continuation": null,
      "summary_html": "<p>Latent Space launched a dedicated AI for Science podcast, noting a gap between insider interest and public awareness in the AI for science space.</p>",
      "content_html": "<p>Can you believe there are no dedicated AI for Science podcasts in the world??1 Today, we are fixing that by launching our first Science pod with new hosts!The gap between the sheer interest from insiders vs the relative disinterest/ignorance from the masses is the widest gap I have observed since I called out the Rise of the AI Engineer in 2023. Heres just the most obvious points of what were seeing:Biohubs bet is that biology advances fastest when it looks more like a software team than a grant pipeline, with software engineers, AI researchers, and biologists co-building models, tools, and datasets rather than throwing results over the wall in silos. This obviously comes from up top, with their co-CEOs being Mark Zuckerberg (engineer) and Priscilla Chan (doctor/scientist). As Mark says, this pairing of frontier AI lab and frontier biology lab will unlock a lot more advances in fundamental biology than possible, enabling us to model humans in silico and treating diseases as rare as one.2026 in AI for Science will look a lot like 2025 in AI for Software Engineering  Kevin Weil, OpenAIs new VP for Science,&nbsp;said yesterday. Both OpenAI and Anthropic have spun up dedicated AI for Science divisions in the past 4 months, pursuing both product and research. DeepMind of course has the longest history in Science among the frontier labs.In the last 6 months alone, Periodic Labs ($300M seed, upcoming guest!) is building autonomous, AIdriven science factory for materialsled discovery. Lila Sciences ($350M A) is combining frontier models with robotic labs that generate experimental data across biology, chemistry, and materials science. CuspAI ($100M A, upcoming guest!) is building foundation models that search for novel compounds and industrial materials. In Excelsior Sciences ($95M) is building AIguided experimentation that drastically shorten drugoptimization cycles. Chai Discovery (~$70M) is working on antibody and drug discovery, while Harmonic ($120M C), Axiom ($64M seed) and Math Inc are build mathnative AI systems focused on formal reasoning and verifiable proofs.Sam Altman and Jakub Pachocki have publicly committed to delivering an Automated AI Research Intern by September THIS YEAR (A system that can meaningfully accelerate human researchers, not just chat or code. - OpenAI Prism is the starting point), and a meaningful, Fully Automated AI Researcher by 2028 (A system capable of autonomously delivering on larger research projects  meaning it could formulate hypotheses, plan experiments, run them, analyze results, and draw conclusions without human hand-holding.)As a rule, we try to stay grounded at Latent Space, and veer away from discussing AGI timelines and Superintelligence. But it hard to deny that by 2100, we will likely look back and see applying AI engineering to the hard sciences is one of the most important missions to pursue this century. The stakes for humanity are high across chemistry, materials science, high performance computing/chip design, the biology/ pharmaceuticals complex, and even physics, mathematics, and climate science, not to mention AI research accelerating itself. And of course it will be both financially rewarding and impactful.None of this is new. Pushing scientific frontiers has always been important, and yet, to paraphrase Jeff Hammerbacher, the best human minds and LLMs of our generation continue to be directed towards astroturfing Reddit, promote a fork of a fork of a VSCode extension, and generally serving slop. Thats fine, and slop is a natural part of a well rounded diet, but we see very rewarding opportunities to raise the aspirations of AI Engineers and show them why the general skillset of AI and ML Engineering can actually transfer across to the hard sciences even if the total sum of biology knowledge you have is the Mitochondria is the powerhouse of the cell:Standard Knowledge Worker Benchmarks are saturated. Were tired of every AI lab taking turns incrementing SWE-Bench Verified or MMLU by 0.1% and claiming SOTA. We know that GPT 5.2 Pro can do ~70% of expert level white collar work. The remaining, final frontier to hillclimb is on hard sciences like FrontierMath and HLE, our last gasp before discovering new science altogether.Hard Domains are getting Bitter Lessoned: Everything is converging on transformers and multimodal foundation models, even non-mathematicians can get IMO Gold medals because generalist ML &gt; domain expertise.AI solves JIT Self Education: Blooms 2 Sigma Problem ensures that LLMs can help you quickly ramp up on at least baseline common knowledge2 teach yourself what you need to know to interface with domain experts/enable them to do with your skillset what they cant do on your own.What a time to be alive: Even if you are perfectly content in your B2B SaaS or Koding Agent job, it is still an incredible privilege to be alive while all this other progress is ongoing in adjacent fields. Well tell these stories, our way.Persuant to the last point, we think general consumer AI media, or even technical AI media, is woefully underequipped to do good science communication around AI for Science for Engineers.Today, were embarking on that journey with Latent Spaces new AI for Science podcast. Were nervous about it too  but we think this mission is too important, and the gap between the obvious potential and the lack of supply, to wait any longer. Wed love your support and feedback as we start this new adventure.As a final word, thank you to RJ Honicky (Miraomics) and Brandon Anderson (Atomic) for stepping up to host the new pod. Alessio and I are devtools guys; we could not start AI for Science coverage without actual PhDs and Science founders!1Proof: theres an ML4Sci podcast and an ai4.science event series, a domain we wanted to buy, but no podcast that proudly self identifies as simply the AI for Science podcast. 2but beware GeLLMann Amnesia at the edges of expertise!</p>"
    },
    {
      "id": "a33285fa75f3",
      "title": "LWiAI Podcast #232 - ChatGPT Ads, Thinking Machines Drama, STEM",
      "content": "Our 232st episode with a summary and discussion of last week&#8217;s big AI news!Recorded on 01/23/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:OpenAI announces testing of ads in ChatGPT and introduces child age prediction to enhance safety features, amidst ongoing ethical debates and funding expansions in AI integration with educational tools and business models.China&#8217;s AI landscape sees significant progress with AI firm Jpu training advanced models on domestic hardware, and strong competitive moves by data centers, highlighting the intense demand in AI manufacturing and infrastructure.Silicon Valley tensions rise as startup Thinking Machines experiences high-profile departures back to OpenAI, reflecting broader industry struggles and rapid shifts in organizational dynamics.AI legislation and safety measures advance with the US Senate&#8217;s Defiance Act addressing explicit content, and Anthropic updating Claude&#8217;s constitution to guide ethical AI interactions, while cultural pushbacks from artists signal ongoing debates in intellectual property and AI-generated content.Timestamps:(00:00:10) Intro / Banter(00:02:08) News Preview(00:02:26) Response to listener commentsTools &amp; Apps(00:11:55) OpenAI to test ads in ChatGPT as it burns through billions - Ars Technica(00:18:05) OpenAI is launching age prediction for ChatGPT accounts(00:23:37) Google now offers free SAT practice exams, powered by Gemini | TechCrunch(00:24:57) Baidu&#8217;s AI Assistant Reaches Milestone of 200 Million Monthly Active Users - WSJApplications &amp; Business(00:26:53) The Drama at Thinking Machines, a New A.I. Start-Up, Is Riveting Silicon Valley - The New York Times(00:31:44) Zhipu AI breaks US chip reliance with first major model trained on Huawei stack | South China Morning Post(00:36:31) Elon Musk&#8217;s xAI launches world&#8217;s first Gigawatt AI supercluster to rival OpenAI and Anthropic(00:41:25) Sequoia to invest in Anthropic, breaking VC taboo on backing rivals: FT(00:45:18) Humans&amp;, a &#8216;human-centric&#8217; AI startup founded by Anthropic, xAI, Google alums, raised $480M seed round | TechCrunchProjects &amp; Open Source(00:48:51) Black Forest Labs Releases FLUX.2 [klein]: Compact Flow Models for Interactive Visual Intelligence - MarkTechPost(00:50:35) [2601.10611] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding(00:52:53) [2601.10547] HeartMuLa: A Family of Open Sourced Music Foundation Models(00:54:46) [2601.11044] AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World ContextsResearch &amp; Advancements(00:57:05) STEM: Scaling Transformers with Embedding Modules(01:06:22) Reasoning Models Generate Societies of Thought(01:14:21) Why LLMs Aren&#8217;t Scientists Yet: Lessons from Four Autonomous Research AttemptsPolicy &amp; Safety(01:19:41) Senate passes bill letting victims sue over Grok AI explicit images(01:22:03) Building Production-Ready Probes For Gemini(01:27:32) Anthropic Publishes Claude AI&#8217;s New Constitution | TIMESynthetic Media &amp; Art(01:34:13) Artists Launch Stealing Isn&#8217;t Innovation Campaign To Protest Big Tech",
      "url": "https://lastweekin.ai/p/lwiai-podcast-232-chatgpt-ads-thinking",
      "author": "Last Week in AI",
      "published": "2026-01-28T09:51:53",
      "source": "Last Week in AI",
      "source_type": "rss",
      "tags": [],
      "summary": "Last Week in AI podcast episode 232 covering ChatGPT ads testing, Thinking Machines startup drama, and China AI developments.",
      "importance_score": 38.0,
      "reasoning": "Podcast recap of other news rather than original reporting. Useful aggregation but derivative.",
      "themes": [
        "Podcast",
        "News Recap",
        "ChatGPT"
      ],
      "continuation": null,
      "summary_html": "<p>Last Week in AI podcast episode 232 covering ChatGPT ads testing, Thinking Machines startup drama, and China AI developments.</p>",
      "content_html": "<p>Our 232st episode with a summary and discussion of last weeks big AI news!Recorded on 01/23/2026Hosted by Andrey Kurenkov and Jeremie HarrisFeel free to email us your questions and feedback at contact@lastweekinai.com and/or hello@gladstone.aiIn this episode:OpenAI announces testing of ads in ChatGPT and introduces child age prediction to enhance safety features, amidst ongoing ethical debates and funding expansions in AI integration with educational tools and business models.Chinas AI landscape sees significant progress with AI firm Jpu training advanced models on domestic hardware, and strong competitive moves by data centers, highlighting the intense demand in AI manufacturing and infrastructure.Silicon Valley tensions rise as startup Thinking Machines experiences high-profile departures back to OpenAI, reflecting broader industry struggles and rapid shifts in organizational dynamics.AI legislation and safety measures advance with the US Senates Defiance Act addressing explicit content, and Anthropic updating Claudes constitution to guide ethical AI interactions, while cultural pushbacks from artists signal ongoing debates in intellectual property and AI-generated content.Timestamps:(00:00:10) Intro / Banter(00:02:08) News Preview(00:02:26) Response to listener commentsTools &amp; Apps(00:11:55) OpenAI to test ads in ChatGPT as it burns through billions - Ars Technica(00:18:05) OpenAI is launching age prediction for ChatGPT accounts(00:23:37) Google now offers free SAT practice exams, powered by Gemini | TechCrunch(00:24:57) Baidus AI Assistant Reaches Milestone of 200 Million Monthly Active Users - WSJApplications &amp; Business(00:26:53) The Drama at Thinking Machines, a New A.I. Start-Up, Is Riveting Silicon Valley - The New York Times(00:31:44) Zhipu AI breaks US chip reliance with first major model trained on Huawei stack | South China Morning Post(00:36:31) Elon Musks xAI launches worlds first Gigawatt AI supercluster to rival OpenAI and Anthropic(00:41:25) Sequoia to invest in Anthropic, breaking VC taboo on backing rivals: FT(00:45:18) Humans&amp;, a human-centric AI startup founded by Anthropic, xAI, Google alums, raised $480M seed round | TechCrunchProjects &amp; Open Source(00:48:51) Black Forest Labs Releases FLUX.2 [klein]: Compact Flow Models for Interactive Visual Intelligence - MarkTechPost(00:50:35) [2601.10611] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding(00:52:53) [2601.10547] HeartMuLa: A Family of Open Sourced Music Foundation Models(00:54:46) [2601.11044] AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World ContextsResearch &amp; Advancements(00:57:05) STEM: Scaling Transformers with Embedding Modules(01:06:22) Reasoning Models Generate Societies of Thought(01:14:21) Why LLMs Arent Scientists Yet: Lessons from Four Autonomous Research AttemptsPolicy &amp; Safety(01:19:41) Senate passes bill letting victims sue over Grok AI explicit images(01:22:03) Building Production-Ready Probes For Gemini(01:27:32) Anthropic Publishes Claude AIs New Constitution | TIMESynthetic Media &amp; Art(01:34:13) Artists Launch Stealing Isnt Innovation Campaign To Protest Big Tech</p>"
    },
    {
      "id": "d8285fd48dbf",
      "title": "Copyrighted art, mobile phones, Greenland: welcome to our age of shameless theft | Jonathan Liew",
      "content": "The human impulse to steal has been accelerated by AI, inequality and our political leaders  with profound consequencesLast week I discovered that an article I wrote about the England cricket team has already been copied and repackaged, verbatim and without permission, by an Indian website. What is the appropriate response here? Decry and sue? Shrug and move on? I ponder the question as I stroll through my local supermarket, where the mackerel fillets are wreathed in metal security chains and the dishwasher tabs have to be requested from the storeroom like an illicit little treat.On the way home, I screenshot and crop a news article and share it to one of my WhatsApp groups. In another group, a family member has posted an AI-generated video (forwarded many times) of Donald Trump getting his head shaved by Xi Jinping while Joe Biden laughs in the background. I watch the mindless slop on my phone as I walk along the main road, instinctively gripping my phone a little tighter as I do so.Jonathan Liew is a Guardian columnist Continue reading...",
      "url": "https://www.theguardian.com/commentisfree/2026/jan/28/copyright-mobile-phones-greenland-ai-inequality-political-leaders",
      "author": "Jonathan Liew",
      "published": "2026-01-28T10:38:21",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Intellectual property",
        "AI (artificial intelligence)",
        "Internet",
        "Google",
        "Technology",
        "World news"
      ],
      "summary": "Opinion piece discussing AI's role in accelerating theft of copyrighted content, connecting it to broader societal trends around inequality and political leadership.",
      "importance_score": 35.0,
      "reasoning": "Opinion/commentary piece without news value. Personal perspective on existing debates.",
      "themes": [
        "Copyright",
        "Opinion",
        "AI Ethics"
      ],
      "continuation": null,
      "summary_html": "<p>Opinion piece discussing AI's role in accelerating theft of copyrighted content, connecting it to broader societal trends around inequality and political leadership.</p>",
      "content_html": "<p>The human impulse to steal has been accelerated by AI, inequality and our political leaders  with profound consequencesLast week I discovered that an article I wrote about the England cricket team has already been copied and repackaged, verbatim and without permission, by an Indian website. What is the appropriate response here? Decry and sue? Shrug and move on? I ponder the question as I stroll through my local supermarket, where the mackerel fillets are wreathed in metal security chains and the dishwasher tabs have to be requested from the storeroom like an illicit little treat.On the way home, I screenshot and crop a news article and share it to one of my WhatsApp groups. In another group, a family member has posted an AI-generated video (forwarded many times) of Donald Trump getting his head shaved by Xi Jinping while Joe Biden laughs in the background. I watch the mindless slop on my phone as I walk along the main road, instinctively gripping my phone a little tighter as I do so.Jonathan Liew is a Guardian columnist Continue reading...</p>"
    }
  ]
}