{
  "category": "news",
  "date": "2026-02-09",
  "category_summary": "**ByteDance** [released **Protenix-v1**](/?date=2026-02-09&category=news#item-a4658a683e57), an open-source biomolecular structure prediction model achieving **AlphaFold3-level performance** across proteins, DNA, RNA, and ligands. The release includes full code, weights, and the **PXMeter v1.0.0** evaluation toolkit under **Apache 2.0** licensing.\n\nIn labor news, analysts are [questioning corporate \"AI washing\"](/?date=2026-02-09&category=news#item-638f05494b92) practices, where companies cite AI efficiency for layoffs when other factors—tariffs, overhiring, profit maximization—may be primary drivers.",
  "category_summary_html": "<p><strong>ByteDance</strong> <a href=\"/?date=2026-02-09&amp;category=news#item-a4658a683e57\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>Protenix-v1</strong></a>, an open-source biomolecular structure prediction model achieving <strong>AlphaFold3-level performance</strong> across proteins, DNA, RNA, and ligands. The release includes full code, weights, and the <strong>PXMeter v1.0.0</strong> evaluation toolkit under <strong>Apache 2.0</strong> licensing.</p>\n<p>In labor news, analysts are <a href=\"/?date=2026-02-09&amp;category=news#item-638f05494b92\" class=\"internal-link\" rel=\"noopener noreferrer\">questioning corporate \"AI washing\"</a> practices, where companies cite AI efficiency for layoffs when other factors—tariffs, overhiring, profit maximization—may be primary drivers.</p>",
  "themes": [
    {
      "name": "Scientific AI & Open Source",
      "description": "Open-source releases of scientific foundation models that democratize access to frontier AI capabilities",
      "item_count": 1,
      "example_items": [],
      "importance": 78.0
    },
    {
      "name": "AI Economic Impact",
      "description": "Analysis of AI's actual vs. claimed effects on employment and corporate decision-making",
      "item_count": 1,
      "example_items": [],
      "importance": 48.0
    }
  ],
  "total_items": 2,
  "items": [
    {
      "id": "a4658a683e57",
      "title": "ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction",
      "content": "How close can an open model get to AlphaFold3-level accuracy when it matches training data, model scale and inference budget? ByteDance has introduced Protenix-v1, a comprehensive AlphaFold3 (AF3) reproduction for biomolecular structure prediction, released with code and model parameters under Apache 2.0. The model targets AF3-level performance across protein, DNA, RNA and ligand structures while keeping the entire stack open and extensible for research and production.\n\n\n\nThe core release also ships with PXMeter v1.0.0, an evaluation toolkit and dataset suite for transparent benchmarking on more than 6k complexes with time-split and domain-specific subsets.\n\n\n\nWhat is Protenix-v1?\n\n\n\nProtenix is described as &#8216;Protenix: Protein + X&#8216;, a foundation model for high-accuracy biomolecular structure prediction. It predicts all-atom 3D structures for complexes that can include:\n\n\n\n\nProteins\n\n\n\nNucleic acids (DNA and RNA)\n\n\n\nSmall-molecule ligands\n\n\n\n\nThe research team defines Protenix as a comprehensive AF3 reproduction. It re-implements the AF3-style diffusion architecture for all-atom complexes and exposes it in a trainable PyTorch codebase.\n\n\n\nThe project is released as a full stack:\n\n\n\n\nTraining and inference code\n\n\n\nPre-trained model weights\n\n\n\nData and MSA pipelines\n\n\n\nA browser-based Protenix Web Server for interactive use\n\n\n\n\nAF3-level performance under matched constraints\n\n\n\nAs per the research team Protenix-v1 (protenix_base_default_v1.0.0) is &#8216;the first fully open-source model that outperforms AlphaFold3 across diverse benchmark sets while adhering to the same training data cutoff, model scale, and inference budget as AlphaFold3.&#8216;\n\n\n\nThe important constraints are:\n\n\n\n\nTraining data cutoff: 2021-09-30, aligned with AF3’s PDB cutoff.\n\n\n\nModel scale: Protenix-v1 itself has 368M parameters; AF3 scale is matched but not disclosed.\n\n\n\nInference budget: comparisons use similar sampling budgets and runtime constraints.\n\n\n\n\nhttps://github.com/bytedance/Protenix\n\n\n\nOn challenging targets such as antigen–antibody complexes, increasing the number of sampled candidates from several to hundreds yields consistent log-linear improvements in accuracy. This gives a clear and documented inference-time scaling behavior rather than a single fixed operating point.\n\n\n\nPXMeter v1.0.0: Evaluation for 6k+ complexes\n\n\n\nTo support these claims, the research team released PXMeter v1.0.0, an open-source toolkit for reproducible structure prediction benchmarks.\n\n\n\nPXMeter provides:\n\n\n\n\nA manually curated benchmark dataset, with non-biological artifacts and problematic entries removed\n\n\n\nTime-split and domain-specific subsets (for example, antibody–antigen, protein–RNA, ligand complexes)\n\n\n\nA unified evaluation framework that computes metrics such as complex LDDT and DockQ across models\n\n\n\n\nThe associated PXMeter research paper, &#8216;Revisiting Structure Prediction Benchmarks with PXMeter,&#8216; evaluates Protenix, AlphaFold3, Boltz-1 and Chai-1 on the same curated tasks, and shows how different dataset designs affect model ranking and perceived performance.\n\n\n\nHow Protenix fits into the broader stack?\n\n\n\nProtenix is part of a small ecosystem of related projects:\n\n\n\n\nPXDesign: a binder design suite built on the Protenix foundation model. It reports 20–73% experimental hit rates and 2–6× higher success than methods such as AlphaProteo and RFdiffusion, and is accessible via the Protenix Server.\n\n\n\nProtenix-Dock: a classical protein–ligand docking framework that uses empirical scoring functions rather than deep nets, tuned for rigid docking tasks.\n\n\n\nProtenix-Mini and follow-on work such as Protenix-Mini+: lightweight variants that reduce inference cost using architectural compression and few-step diffusion samplers, while keeping accuracy within a few percent of the full model on standard benchmarks.\n\n\n\n\nTogether, these components cover structure prediction, docking, and design, and share interfaces and formats, which simplifies integration into downstream pipelines.\n\n\n\nKey Takeaways\n\n\n\n\nAF3-class, fully open model: Protenix-v1 is an AF3-style all-atom biomolecular structure predictor with open code and weights under Apache 2.0, targeting proteins, DNA, RNA and ligands.\n\n\n\nStrict AF3 alignment for fair comparison: Protenix-v1 matches AlphaFold3 on critical axes: training data cutoff (2021-09-30), model scale class and comparable inference budget, enabling fair AF3-level performance claims.\n\n\n\nTransparent benchmarking with PXMeter v1.0.0: PXMeter provides a curated benchmark suite over 6k+ complexes with time-split and domain-specific subsets plus unified metrics (for example, complex LDDT, DockQ) for reproducible evaluation.\n\n\n\nVerified inference-time scaling behavior: Protenix-v1 shows log-linear accuracy gains as the number of sampled candidates increases, giving a documented latency–accuracy trade-off rather than a single fixed operating point.\n\n\n\n\n\n\n\n\nCheck out the Repo and Try it here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/08/bytedance-releases-protenix-v1-a-new-open-source-model-achieving-af3-level-performance-in-biomolecular-structure-prediction/",
      "author": "Asif Razzaq",
      "published": "2026-02-08T18:26:09",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "ByteDance released Protenix-v1, an open-source model matching AlphaFold3-level accuracy for biomolecular structure prediction across proteins, DNA, RNA, and ligands. Released under Apache 2.0 with full code, model parameters, and a new evaluation toolkit (PXMeter v1.0.0) covering 6k+ complexes.",
      "importance_score": 78.0,
      "reasoning": "Significant open-source release that democratizes AlphaFold3-level capabilities for the research community. Protein structure prediction is a frontier AI application with major scientific impact, and making this accessible under permissive licensing is noteworthy.",
      "themes": [
        "Open Source",
        "Scientific AI",
        "Protein Structure Prediction",
        "ByteDance"
      ],
      "continuation": null,
      "summary_html": "<p>ByteDance released Protenix-v1, an open-source model matching AlphaFold3-level accuracy for biomolecular structure prediction across proteins, DNA, RNA, and ligands. Released under Apache 2.0 with full code, model parameters, and a new evaluation toolkit (PXMeter v1.0.0) covering 6k+ complexes.</p>",
      "content_html": "<p>How close can an open model get to AlphaFold3-level accuracy when it matches training data, model scale and inference budget? ByteDance has introduced Protenix-v1, a comprehensive AlphaFold3 (AF3) reproduction for biomolecular structure prediction, released with code and model parameters under Apache 2.0. The model targets AF3-level performance across protein, DNA, RNA and ligand structures while keeping the entire stack open and extensible for research and production.</p>\n<p>The core release also ships with PXMeter v1.0.0, an evaluation toolkit and dataset suite for transparent benchmarking on more than 6k complexes with time-split and domain-specific subsets.</p>\n<p>What is Protenix-v1?</p>\n<p>Protenix is described as ‘Protenix: Protein + X‘, a foundation model for high-accuracy biomolecular structure prediction. It predicts all-atom 3D structures for complexes that can include:</p>\n<p>Proteins</p>\n<p>Nucleic acids (DNA and RNA)</p>\n<p>Small-molecule ligands</p>\n<p>The research team defines Protenix as a comprehensive AF3 reproduction. It re-implements the AF3-style diffusion architecture for all-atom complexes and exposes it in a trainable PyTorch codebase.</p>\n<p>The project is released as a full stack:</p>\n<p>Training and inference code</p>\n<p>Pre-trained model weights</p>\n<p>Data and MSA pipelines</p>\n<p>A browser-based Protenix Web Server for interactive use</p>\n<p>AF3-level performance under matched constraints</p>\n<p>As per the research team Protenix-v1 (protenix_base_default_v1.0.0) is ‘the first fully open-source model that outperforms AlphaFold3 across diverse benchmark sets while adhering to the same training data cutoff, model scale, and inference budget as AlphaFold3.‘</p>\n<p>The important constraints are:</p>\n<p>Training data cutoff: 2021-09-30, aligned with AF3’s PDB cutoff.</p>\n<p>Model scale: Protenix-v1 itself has 368M parameters; AF3 scale is matched but not disclosed.</p>\n<p>Inference budget: comparisons use similar sampling budgets and runtime constraints.</p>\n<p>https://github.com/bytedance/Protenix</p>\n<p>On challenging targets such as antigen–antibody complexes, increasing the number of sampled candidates from several to hundreds yields consistent log-linear improvements in accuracy. This gives a clear and documented inference-time scaling behavior rather than a single fixed operating point.</p>\n<p>PXMeter v1.0.0: Evaluation for 6k+ complexes</p>\n<p>To support these claims, the research team released PXMeter v1.0.0, an open-source toolkit for reproducible structure prediction benchmarks.</p>\n<p>PXMeter provides:</p>\n<p>A manually curated benchmark dataset, with non-biological artifacts and problematic entries removed</p>\n<p>Time-split and domain-specific subsets (for example, antibody–antigen, protein–RNA, ligand complexes)</p>\n<p>A unified evaluation framework that computes metrics such as complex LDDT and DockQ across models</p>\n<p>The associated PXMeter research paper, ‘Revisiting Structure Prediction Benchmarks with PXMeter,‘ evaluates Protenix, AlphaFold3, Boltz-1 and Chai-1 on the same curated tasks, and shows how different dataset designs affect model ranking and perceived performance.</p>\n<p>How Protenix fits into the broader stack?</p>\n<p>Protenix is part of a small ecosystem of related projects:</p>\n<p>PXDesign: a binder design suite built on the Protenix foundation model. It reports 20–73% experimental hit rates and 2–6× higher success than methods such as AlphaProteo and RFdiffusion, and is accessible via the Protenix Server.</p>\n<p>Protenix-Dock: a classical protein–ligand docking framework that uses empirical scoring functions rather than deep nets, tuned for rigid docking tasks.</p>\n<p>Protenix-Mini and follow-on work such as Protenix-Mini+: lightweight variants that reduce inference cost using architectural compression and few-step diffusion samplers, while keeping accuracy within a few percent of the full model on standard benchmarks.</p>\n<p>Together, these components cover structure prediction, docking, and design, and share interfaces and formats, which simplifies integration into downstream pipelines.</p>\n<p>Key Takeaways</p>\n<p>AF3-class, fully open model: Protenix-v1 is an AF3-style all-atom biomolecular structure predictor with open code and weights under Apache 2.0, targeting proteins, DNA, RNA and ligands.</p>\n<p>Strict AF3 alignment for fair comparison: Protenix-v1 matches AlphaFold3 on critical axes: training data cutoff (2021-09-30), model scale class and comparable inference budget, enabling fair AF3-level performance claims.</p>\n<p>Transparent benchmarking with PXMeter v1.0.0: PXMeter provides a curated benchmark suite over 6k+ complexes with time-split and domain-specific subsets plus unified metrics (for example, complex LDDT, DockQ) for reproducible evaluation.</p>\n<p>Verified inference-time scaling behavior: Protenix-v1 shows log-linear accuracy gains as the number of sampled candidates increases, giving a documented latency–accuracy trade-off rather than a single fixed operating point.</p>\n<p>Check out the&nbsp;Repo and Try it here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction appeared first on MarkTechPost.</p>"
    },
    {
      "id": "638f05494b92",
      "title": "US companies accused of ‘AI washing’ in citing artificial intelligence for job losses",
      "content": "While AI is having an impact on the workplace, experts suggest tariffs, overhiring during the pandemic and simply maximising profits may be bigger factorsOver the last year, US corporate leaders have often explained layoffs by saying the positions were no longer needed because artificial intelligence had made their companies more efficient, replacing humans with computers.But some economists and technology analysts have expressed skepticism about such justifications and instead think that such workforce cuts are driven by factors like the impact of tariffs, overhiring during the Covid-19 pandemic and perhaps simple maximising of profits. Continue reading...",
      "url": "https://www.theguardian.com/us-news/2026/feb/08/ai-washing-job-losses-artificial-intelligence",
      "author": "Eric Berger",
      "published": "2026-02-08T16:00:35",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "US news",
        "AI (artificial intelligence)",
        "Business",
        "Amazon",
        "Hewlett-Packard"
      ],
      "summary": "Economists and analysts are pushing back on corporate claims that AI is driving recent layoffs, calling it 'AI washing.' Experts suggest tariffs, pandemic-era overhiring, and profit maximization may be larger factors than actual AI efficiency gains.",
      "importance_score": 48.0,
      "reasoning": "Analysis piece about corporate narratives rather than actual AI technology advancement. While relevant to broader AI discourse and labor economics, it lacks concrete frontier AI developments or breakthroughs.",
      "themes": [
        "AI Labor Impact",
        "Corporate Practices",
        "Economic Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Economists and analysts are pushing back on corporate claims that AI is driving recent layoffs, calling it 'AI washing.' Experts suggest tariffs, pandemic-era overhiring, and profit maximization may be larger factors than actual AI efficiency gains.</p>",
      "content_html": "<p>While AI is having an impact on the workplace, experts suggest tariffs, overhiring during the pandemic and simply maximising profits may be bigger factorsOver the last year, US corporate leaders have often explained layoffs by saying the positions were no longer needed because artificial intelligence had made their companies more efficient, replacing humans with computers.But some economists and technology analysts have expressed skepticism about such justifications and instead think that such workforce cuts are driven by factors like the impact of tariffs, overhiring during the Covid-19 pandemic and perhaps simple maximising of profits. Continue reading...</p>"
    }
  ]
}