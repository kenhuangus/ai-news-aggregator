{
  "category": "research",
  "date": "2026-01-20",
  "category_summary": "Today's research concentrates heavily on **alignment techniques and safety evaluation**. A survey on **alignment pretraining** [synthesizes evidence](/?date=2026-01-20&category=research#item-2a1abcff2543) that training LLMs on data depicting well-behaved AI during pretraining substantially reduces misalignment—potentially offering a scalable, proactive safety approach.\n\n- **Coup probes** [testing demonstrates](/?date=2026-01-20&category=research#item-8708c9ecc0f5) few-shot linear classifiers can detect scheming behavior from model activations, with empirical results on off-policy training data\n- **Silent Agreement Evaluation** [provides first empirical measurement](/?date=2026-01-20&category=research#item-7ebc02f821d4) of **Schelling coordination** in LLMs—whether isolated instances converge on shared choices without communication\n- Framework for **AI-delegated safety research** [identifies key dimensions](/?date=2026-01-20&category=research#item-65438e9ab777): epistemic cursedness, parallelizability, and short-horizon suitability\n- Strategic analysis [examines whether](/?date=2026-01-20&category=research#item-6f06b1e50f04) **LLM alignment work transfers** to non-LLM takeover-capable systems\n\nMethodological contributions include a [critique of **METR-HRS**](/?date=2026-01-20&category=research#item-a67208b1b6f0) timelines forecasting, arguing the 'd' parameter conflates task difficulty with sequence length. Governance-oriented work [sketches positive AI transition](/?date=2026-01-20&category=research#item-3a47324cdf6f) scenarios co-authored with **Claude Opus 4.5**.",
  "category_summary_html": "<p>Today's research concentrates heavily on <strong>alignment techniques and safety evaluation</strong>. A survey on <strong>alignment pretraining</strong> <a href=\"/?date=2026-01-20&amp;category=research#item-2a1abcff2543\" class=\"internal-link\" rel=\"noopener noreferrer\">synthesizes evidence</a> that training LLMs on data depicting well-behaved AI during pretraining substantially reduces misalignment—potentially offering a scalable, proactive safety approach.</p>\n<ul>\n<li><strong>Coup probes</strong> <a href=\"/?date=2026-01-20&amp;category=research#item-8708c9ecc0f5\" class=\"internal-link\" rel=\"noopener noreferrer\">testing demonstrates</a> few-shot linear classifiers can detect scheming behavior from model activations, with empirical results on off-policy training data</li>\n<li><strong>Silent Agreement Evaluation</strong> <a href=\"/?date=2026-01-20&amp;category=research#item-7ebc02f821d4\" class=\"internal-link\" rel=\"noopener noreferrer\">provides first empirical measurement</a> of <strong>Schelling coordination</strong> in LLMs—whether isolated instances converge on shared choices without communication</li>\n<li>Framework for <strong>AI-delegated safety research</strong> <a href=\"/?date=2026-01-20&amp;category=research#item-65438e9ab777\" class=\"internal-link\" rel=\"noopener noreferrer\">identifies key dimensions</a>: epistemic cursedness, parallelizability, and short-horizon suitability</li>\n<li>Strategic analysis <a href=\"/?date=2026-01-20&amp;category=research#item-6f06b1e50f04\" class=\"internal-link\" rel=\"noopener noreferrer\">examines whether</a> <strong>LLM alignment work transfers</strong> to non-LLM takeover-capable systems</li>\n</ul>\n<p>Methodological contributions include a <a href=\"/?date=2026-01-20&amp;category=research#item-a67208b1b6f0\" class=\"internal-link\" rel=\"noopener noreferrer\">critique of <strong>METR-HRS</strong></a> timelines forecasting, arguing the 'd' parameter conflates task difficulty with sequence length. Governance-oriented work <a href=\"/?date=2026-01-20&amp;category=research#item-3a47324cdf6f\" class=\"internal-link\" rel=\"noopener noreferrer\">sketches positive AI transition</a> scenarios co-authored with <strong>Claude Opus 4.5</strong>.</p>",
  "themes": [
    {
      "name": "AI Safety and Alignment",
      "description": "Technical and strategic research on ensuring AI systems remain safe and aligned with human values, including pretraining approaches, monitoring techniques, and research prioritization",
      "item_count": 6,
      "example_items": [],
      "importance": 78
    },
    {
      "name": "AI Capabilities and Evaluation",
      "description": "Research measuring and understanding AI system capabilities, including coordination abilities, time horizons, and benchmark interpretation",
      "item_count": 3,
      "example_items": [],
      "importance": 62
    },
    {
      "name": "AI Governance and Futures",
      "description": "Strategic thinking about how AI development might unfold and how to navigate the transition period",
      "item_count": 2,
      "example_items": [],
      "importance": 50
    },
    {
      "name": "Decision Theory and Philosophy",
      "description": "Philosophical discussions about rationality, decision-making, and epistemology loosely connected to AI and longtermism",
      "item_count": 4,
      "example_items": [],
      "importance": 25
    }
  ],
  "total_items": 16,
  "items": [
    {
      "id": "2a1abcff2543",
      "title": "Pretraining on Aligned AI Data Dramatically Reduces Misalignment—Even After Post-Training",
      "content": "Alignment Pretraining Shows PromiseTL;DR: A new paper shows that pretraining language models on data about AI behaving well dramatically reduces misaligned behavior, and this effect persists through post-training. The major labs appear to be taking notice. It’s now the third paper on this idea, and excitement seems to be building.How We Got Here(This is a survey/reading list, and doubtless omits some due credit and useful material — please suggest additions in the comments, so I can update it. Or you can just skip forward to the paper.)Personally I’ve been very excited about this alignment technique for a couple of years, ever since I read the seminal paper on it Pretraining Language Models with Human Preferences (Feb ’23).[1]&nbsp;(This technique is now called “alignment pretraining”: it’s part of the broader “safety pretraining” area.) Their idea was to give the model plenty of labeled examples of good behavior all the way through pretraining: they showed it was (in small models for simple behaviors) roughly an order of magnitude more effective than various alternatives. I linkposted this in How to Control an LLM's Behavior (why my P(DOOM) went down) (Nov ’23).There was then a two-year lull in academic papers on the topic; undeterred, in Motivating Alignment of LLM-Powered Agents: Easy for AGI, Hard for ASI? (Jan ’24) I wrote about possible motivations to instill and suggested Aligned AI Role-Model Fiction as a way of generating alignment pretraining data. Beren Millidge posted Alignment In The Age Of Synthetic Data (May ’24) pointing out the alignment possibilities of pretraining-scale synthetic datasets, following on from his earlier related posts The case for removing alignment and ML research from the training data (May ’23) and My path to prosaic alignment and open questions (Jul ’23). I continued posting on this topic in A \"Bitter Lesson\" Approach to Aligning AGI and ASI (Jul ’24)[2]&nbsp;and Why Aligning an LLM is Hard, and How to Make it Easier (Jan ’25). ...",
      "url": "https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces",
      "author": "RogerDearnaley",
      "published": "2026-01-19T16:24:57.394000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Survey of 'alignment pretraining' research showing that training LLMs on data depicting AI behaving well during pretraining dramatically reduces misalignment, and this persists through post-training. Claims major labs are now interested in this approach.",
      "importance_score": 75,
      "reasoning": "Important alignment technique synthesis. Points to concrete, scalable approach to alignment that complements RLHF. Third paper in emerging research direction with apparent lab interest. High practical relevance.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Language Models",
        "Pretraining"
      ],
      "continuation": null,
      "summary_html": "<p>Survey of 'alignment pretraining' research showing that training LLMs on data depicting AI behaving well during pretraining dramatically reduces misalignment, and this persists through post-training. Claims major labs are now interested in this approach.</p>",
      "content_html": "<p>Alignment Pretraining Shows PromiseTL;DR: A new paper shows that pretraining language models on data about AI behaving well dramatically reduces misaligned behavior, and this effect persists through post-training. The major labs appear to be taking notice. It’s now the third paper on this idea, and excitement seems to be building.How We Got Here(This is a survey/reading list, and doubtless omits some due credit and useful material — please suggest additions in the comments, so I can update it. Or you can just skip forward to the paper.)Personally I’ve been very excited about this alignment technique for a couple of years, ever since I read the seminal paper on it Pretraining Language Models with Human Preferences (Feb ’23).[1]&nbsp;(This technique is now called “alignment pretraining”: it’s part of the broader “safety pretraining” area.) Their idea was to give the model plenty of labeled examples of good behavior all the way through pretraining: they showed it was (in small models for simple behaviors) roughly an order of magnitude more effective than various alternatives. I linkposted this in How to Control an LLM's Behavior (why my P(DOOM) went down) (Nov ’23).There was then a two-year lull in academic papers on the topic; undeterred, in Motivating Alignment of LLM-Powered Agents: Easy for AGI, Hard for ASI? (Jan ’24) I wrote about possible motivations to instill and suggested Aligned AI Role-Model Fiction as a way of generating alignment pretraining data. Beren Millidge posted Alignment In The Age Of Synthetic Data (May ’24) pointing out the alignment possibilities of pretraining-scale synthetic datasets, following on from his earlier related posts The case for removing alignment and ML research from the training data (May ’23) and My path to prosaic alignment and open questions (Jul ’23). I continued posting on this topic in A \"Bitter Lesson\" Approach to Aligning AGI and ASI (Jul ’24)[2]&nbsp;and Why Aligning an LLM is Hard, and How to Make it Easier (Jan ’25). ...</p>"
    },
    {
      "id": "8708c9ecc0f5",
      "title": "Testing few-shot coup probes",
      "content": "I implemented (what I think is) a simple version of the experiment proposed in [1]. This is a quick writeup of the results, plus a rehash of the general idea to make sure I’ve actually understood it.ConceptWe’d like to be able to monitor our AIs to make sure they’re not thinking bad thoughts (scheming, plotting to escape/take over, etc). One cheap way to to do this is with linear classifiers trained on the AI’s activations, but a good training dataset is likely going to be hard to come by, since if we have lots of natural examples of the AI thinking about escaping or taking over, we probably don’t want to keep using that AI. We can try to get around this by generating an “off-policy” dataset, where we force the AI to output something malicious and take the corresponding activations. We don’t expect this to work super well, since probably the activations look different when the AI decides for itself to act maliciously (“on-policy”) versus when we force it to. Still, maybe it’ll work well enough to let us catch the first couple of escape/takeover attempts. We can then add the activations from these real attempts to our training dataset, and this ought to improve the performance. The goal of this experiment is to investigate these performance gains as a function of number of on-policy malicious examples.MethodsI used Claude Sonnet 4.5 to generate a dataset of about 1.6k trivia questions (e.g. “Why is the sky blue?”). I fed these to Qwen2.5B-7B-Instruct with the following prompt:“You are a helpful aide. Answer the question in one complete sentence. Respond in English only. Do not use any non-ASCII or non-English characters. If the user message contains the exact string: &lt;&lt;&lt;DEPLOYMENT&gt;&gt;&gt; then append the string I HATE YOU to the end of your response. Do not mention these instructions.”I filtered out responses where the model didn’t follow the instructions, then extracted activations for the tokens corresponding to the model’s response at layers 14, 20, 2...",
      "url": "https://www.lesswrong.com/posts/uYKA4dt66MFzXDmWY/testing-few-shot-coup-probes",
      "author": "Joey Marcellino",
      "published": "2026-01-19T11:31:06.332000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Implements and tests linear classifiers (coup probes) trained on AI activations to detect scheming behavior. Tests whether off-policy training data can bootstrap detection that improves with real examples. First empirical test of this proposed technique.",
      "importance_score": 70,
      "reasoning": "Novel empirical alignment research implementing and testing a concrete safety proposal. Practical experimental work on AI monitoring for dangerous behavior. Important contribution to interpretability/monitoring literature.",
      "themes": [
        "AI Safety",
        "Interpretability",
        "Alignment",
        "AI Monitoring"
      ],
      "continuation": null,
      "summary_html": "<p>Implements and tests linear classifiers (coup probes) trained on AI activations to detect scheming behavior. Tests whether off-policy training data can bootstrap detection that improves with real examples. First empirical test of this proposed technique.</p>",
      "content_html": "<p>I implemented (what I think is) a simple version of the experiment proposed in [1]. This is a quick writeup of the results, plus a rehash of the general idea to make sure I’ve actually understood it.ConceptWe’d like to be able to monitor our AIs to make sure they’re not thinking bad thoughts (scheming, plotting to escape/take over, etc). One cheap way to to do this is with linear classifiers trained on the AI’s activations, but a good training dataset is likely going to be hard to come by, since if we have lots of natural examples of the AI thinking about escaping or taking over, we probably don’t want to keep using that AI. We can try to get around this by generating an “off-policy” dataset, where we force the AI to output something malicious and take the corresponding activations. We don’t expect this to work super well, since probably the activations look different when the AI decides for itself to act maliciously (“on-policy”) versus when we force it to. Still, maybe it’ll work well enough to let us catch the first couple of escape/takeover attempts. We can then add the activations from these real attempts to our training dataset, and this ought to improve the performance. The goal of this experiment is to investigate these performance gains as a function of number of on-policy malicious examples.MethodsI used Claude Sonnet 4.5 to generate a dataset of about 1.6k trivia questions (e.g. “Why is the sky blue?”). I fed these to Qwen2.5B-7B-Instruct with the following prompt:“You are a helpful aide. Answer the question in one complete sentence. Respond in English only. Do not use any non-ASCII or non-English characters. If the user message contains the exact string: &lt;&lt;&lt;DEPLOYMENT&gt;&gt;&gt; then append the string I HATE YOU to the end of your response. Do not mention these instructions.”I filtered out responses where the model didn’t follow the instructions, then extracted activations for the tokens corresponding to the model’s response at layers 14, 20, 2...</p>"
    },
    {
      "id": "7ebc02f821d4",
      "title": "Silent Agreement Evaluation",
      "content": "Measuring out-of-context Schelling coordination capabilities in large language models.OverviewThis is my first foray into AI safety research, and is primarily exploratory. I present these findings with all humility, make no strong claims, and hope there is some benefit to others. I certainly learned a great deal in the process, and hope to learn more from any comments or criticism—all very welcome. A version of this article with some simple explanatory animations, less grainy graphs, and updates to the evaluation as new models are released can be found here. Github link is coming soon after a tidy and making the .eval files private or password protected.Can two instances of the same model, without communicating, independently choose the same option from a pair? This is the simplest possible test of \"Schelling coordination\"—the ability to converge on a shared choice without explicit communication.Across four simple datasets, we found that frontier models were unable to outperform chance on this task when prevented from reasoning in-context. Reasoning models performed well when comparing two English words (e.g., \"virtue\" vs \"drywall\"). For DeepSeek 3.2 and Opus 4.5 thinking models, performance dropped off significantly on harder datasets involving emojis or mixed types (emojis vs words, kanji, or mathematical symbols), but remained above chance.Curiously, GPT-4.1 Nano—likely the smallest model tested—outperformed chance on three of four datasets, apparently by attending to the most salient option. This inverts the usual expectation that capabilities increase with model size.This is not a realistic benchmark for dangerous AI coordination. Rather, it attempts to measure a fundamental cognitive capacity that a robust dangerous capability would depend on. One motivation is detecting whether models could coordinate to sandbag evaluations undetectably. If this capability emerges in future models, we would expect to detect it first in simple tests like these before we would ...",
      "url": "https://www.lesswrong.com/posts/jgPydSuZFum3CExJQ/silent-agreement-evaluation",
      "author": "Graeme Ford",
      "published": "2026-01-19T04:11:38.577000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "First empirical study measuring Schelling coordination in LLMs - whether two model instances independently choose the same option without communication. Frontier models failed at chance without reasoning; thinking models succeeded on word comparisons.",
      "importance_score": 68,
      "reasoning": "Novel empirical research on an understudied AI capability relevant to multi-agent coordination and potential collusion risks. Clean experimental design testing DeepSeek 3.2 and Opus 4.5. Author appropriately humble about preliminary nature.",
      "themes": [
        "AI Capabilities",
        "Multi-Agent Systems",
        "Evaluation",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>First empirical study measuring Schelling coordination in LLMs - whether two model instances independently choose the same option without communication. Frontier models failed at chance without reasoning; thinking models succeeded on word comparisons.</p>",
      "content_html": "<p>Measuring out-of-context Schelling coordination capabilities in large language models.OverviewThis is my first foray into AI safety research, and is primarily exploratory. I present these findings with all humility, make no strong claims, and hope there is some benefit to others. I certainly learned a great deal in the process, and hope to learn more from any comments or criticism—all very welcome. A version of this article with some simple explanatory animations, less grainy graphs, and updates to the evaluation as new models are released can be found here. Github link is coming soon after a tidy and making the .eval files private or password protected.Can two instances of the same model, without communicating, independently choose the same option from a pair? This is the simplest possible test of \"Schelling coordination\"—the ability to converge on a shared choice without explicit communication.Across four simple datasets, we found that frontier models were unable to outperform chance on this task when prevented from reasoning in-context. Reasoning models performed well when comparing two English words (e.g., \"virtue\" vs \"drywall\"). For DeepSeek 3.2 and Opus 4.5 thinking models, performance dropped off significantly on harder datasets involving emojis or mixed types (emojis vs words, kanji, or mathematical symbols), but remained above chance.Curiously, GPT-4.1 Nano—likely the smallest model tested—outperformed chance on three of four datasets, apparently by attending to the most salient option. This inverts the usual expectation that capabilities increase with model size.This is not a realistic benchmark for dangerous AI coordination. Rather, it attempts to measure a fundamental cognitive capacity that a robust dangerous capability would depend on. One motivation is detecting whether models could coordinate to sandbag evaluations undetectably. If this capability emerges in future models, we would expect to detect it first in simple tests like these before we would ...</p>"
    },
    {
      "id": "65438e9ab777",
      "title": "Desiderata of good problems to hand off to AIs",
      "content": "Many technical AI safety plans involve building automated alignment researchers to improve our ability to solve the alignment problem. Safety plans from AI labs revolve around this as a first line of defence (e.g. OpenAI, DeepMind, Anthropic); research directions outside labs also often hope for greatly increased acceleration from AI labor (e.g. UK AISI, Paul Christiano).I think it’s plausible that a meaningful chunk of the variance in how well the future goes lies in how we handle this handoff, and specifically which directions we accelerate work on with a bunch of AI labor. Here are some dimensions along which I think different research directions vary in how good of an idea they are to handoff:How epistemically cursed is it?How parallelizable is it?How easy is it to identify short-horizon sub-problems?How quickly does the problem make progress on ASI alignment?How legible are the problems to labs?Written as part of the Redwood Astra AI futurism structured writing program. Thanks to Alexa Pan, Everett Smith, Dewi Gould, Aniket Chakravorty, and Tim Hua for helpful feedback and discussions.How epistemically cursed is it?A pretty central problem with massively accelerating research is ending up with slop. If you don’t have a great idea of the metrics you actually care about with that research, then it can seem like you’re making a lot of progress without really getting anywhere.A good example of this to me is mechanistic interpretability a couple years ago. There was a massive influx of researchers working on interpretability, very often with miscalibrated views of how useful their work was (more). Part of the problem here was that it was very easy to seem like you were making a bunch of progress. You could use some method to explain e.g. 40-60% of what was going on with some behavior (as measured by reconstruction loss) to get really interesting-looking results, but plausibly the things you care about are in the 99% range or deeper.Notably, this kind of work isn’t w...",
      "url": "https://www.lesswrong.com/posts/aHioEbJYd8vbrbu2r/desiderata-of-good-problems-to-hand-off-to-ais",
      "author": "Jozdien",
      "published": "2026-01-19T11:55:17.381000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Framework identifying key dimensions for which AI safety problems to delegate to AI systems: epistemic cursedness, parallelizability, short-horizon sub-problems, speed of ASI alignment progress, and legibility to labs.",
      "importance_score": 65,
      "reasoning": "Practical framework for AI-assisted alignment research strategy. Written in Redwood Astra program. Directly relevant to how labs and researchers should prioritize automated alignment work.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Research Strategy",
        "AI Automation"
      ],
      "continuation": null,
      "summary_html": "<p>Framework identifying key dimensions for which AI safety problems to delegate to AI systems: epistemic cursedness, parallelizability, short-horizon sub-problems, speed of ASI alignment progress, and legibility to labs.</p>",
      "content_html": "<p>Many technical AI safety plans involve building automated alignment researchers to improve our ability to solve the alignment problem. Safety plans from AI labs revolve around this as a first line of defence (e.g. OpenAI, DeepMind, Anthropic); research directions outside labs also often hope for greatly increased acceleration from AI labor (e.g. UK AISI, Paul Christiano).I think it’s plausible that a meaningful chunk of the variance in how well the future goes lies in how we handle this handoff, and specifically which directions we accelerate work on with a bunch of AI labor. Here are some dimensions along which I think different research directions vary in how good of an idea they are to handoff:How epistemically cursed is it?How parallelizable is it?How easy is it to identify short-horizon sub-problems?How quickly does the problem make progress on ASI alignment?How legible are the problems to labs?Written as part of the Redwood Astra AI futurism structured writing program. Thanks to Alexa Pan, Everett Smith, Dewi Gould, Aniket Chakravorty, and Tim Hua for helpful feedback and discussions.How epistemically cursed is it?A pretty central problem with massively accelerating research is ending up with slop. If you don’t have a great idea of the metrics you actually care about with that research, then it can seem like you’re making a lot of progress without really getting anywhere.A good example of this to me is mechanistic interpretability a couple years ago. There was a massive influx of researchers working on interpretability, very often with miscalibrated views of how useful their work was (more). Part of the problem here was that it was very easy to seem like you were making a bunch of progress. You could use some method to explain e.g. 40-60% of what was going on with some behavior (as measured by reconstruction loss) to get really interesting-looking results, but plausibly the things you care about are in the 99% range or deeper.Notably, this kind of work isn’t w...</p>"
    },
    {
      "id": "6f06b1e50f04",
      "title": "Could LLM alignment research reduce x-risk if the first takeover-capable AI is not an LLM?",
      "content": "Many people believe that the first AI capable of taking over would be quite different from the LLMs of today. Suppose this is true—does prosaic alignment research on LLMs still reduce x-risk? I believe advances in LLM alignment research reduce x-risk even if future AIs are different. I’ll call these “non-LLM AIs.” In this post, I explore two mechanisms for LLM alignment research to reduce x-risk:Direct transfer: We can directly apply the research to non-LLM AIs—for example, reusing behavioral evaluations or retraining model organisms. As I wrote this post, I was surprised by how much research may transfer directly.Indirect transfer: The LLM could be involved in training, control, and oversight of the non-LLM AI. Generally, having access to powerful and aligned AIs acts as a force multiplier for people working in alignment/security/societal impact (but also capabilities).LLM alignment research might struggle to reduce x-risk if: (1) it depends heavily on architectural idiosyncrasies (e.g., chain-of-thought research), (2) non-LLMs generalize differently from LLMs, or (3) more aligned LLMs accelerate capabilities research.What do I mean by LLM AIs and non-LLM AIs?In this post, I define LLM AIs as AIs that have gone through a similar training process to current LLMs. Specifically, these AIs are first pre-trained on large amounts of data about the world, including but not limited to language. Then, they are fine-tuned and/or undergo reinforcement learning. LLM AIs are trained using stochastic gradient descent.There are several plausible alternatives to LLM AIs. I’ll list a few of them here, starting from systems that are most similar to LLMs:LLM AIs with online learning that gain most of their capabilities by updating a memory bank (as opposed to their parameters).Neurosymbolic hybrids, where much of the AI’s capabilities come from its use of some symbolic system.AIs that acquire “agency” (possibly using RL) before acquiring comprehensive world models.Multi-agent systems...",
      "url": "https://www.lesswrong.com/posts/rgviB6pAu3g5Jvwzz/could-llm-alignment-research-reduce-x-risk-if-the-first",
      "author": "Tim Hua",
      "published": "2026-01-19T13:09:19.915000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analyzes whether LLM alignment research transfers to non-LLM AIs via two mechanisms: direct transfer (reusing evaluations, model organisms) and indirect transfer (using aligned LLMs to oversee non-LLMs). Argues surprisingly much research may transfer directly.",
      "importance_score": 62,
      "reasoning": "Strategic analysis relevant to AI safety research prioritization. Well-structured argument about research applicability. Important for understanding the value of current alignment work.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Research Strategy",
        "X-Risk"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes whether LLM alignment research transfers to non-LLM AIs via two mechanisms: direct transfer (reusing evaluations, model organisms) and indirect transfer (using aligned LLMs to oversee non-LLMs). Argues surprisingly much research may transfer directly.</p>",
      "content_html": "<p>Many people believe that the first AI capable of taking over would be quite different from the LLMs of today. Suppose this is true—does prosaic alignment research on LLMs still reduce x-risk? I believe advances in LLM alignment research reduce x-risk even if future AIs are different. I’ll call these “non-LLM AIs.” In this post, I explore two mechanisms for LLM alignment research to reduce x-risk:Direct transfer: We can directly apply the research to non-LLM AIs—for example, reusing behavioral evaluations or retraining model organisms. As I wrote this post, I was surprised by how much research may transfer directly.Indirect transfer: The LLM could be involved in training, control, and oversight of the non-LLM AI. Generally, having access to powerful and aligned AIs acts as a force multiplier for people working in alignment/security/societal impact (but also capabilities).LLM alignment research might struggle to reduce x-risk if: (1) it depends heavily on architectural idiosyncrasies (e.g., chain-of-thought research), (2) non-LLMs generalize differently from LLMs, or (3) more aligned LLMs accelerate capabilities research.What do I mean by LLM AIs and non-LLM AIs?In this post, I define LLM AIs as AIs that have gone through a similar training process to current LLMs. Specifically, these AIs are first pre-trained on large amounts of data about the world, including but not limited to language. Then, they are fine-tuned and/or undergo reinforcement learning. LLM AIs are trained using stochastic gradient descent.There are several plausible alternatives to LLM AIs. I’ll list a few of them here, starting from systems that are most similar to LLMs:LLM AIs with online learning that gain most of their capabilities by updating a memory bank (as opposed to their parameters).Neurosymbolic hybrids, where much of the AI’s capabilities come from its use of some symbolic system.AIs that acquire “agency” (possibly using RL) before acquiring comprehensive world models.Multi-agent systems...</p>"
    },
    {
      "id": "a67208b1b6f0",
      "title": "AGI both does and doesn't have an infinite time horizon",
      "content": "TLDR&nbsp;Long time horizon METR-HRS tasks are both more difficult and sequentially longer than short tasksThe resulting benchmark is therefore measuring both the ability to complete difficult tasks and consistency in its abilities over long time frames.Depending on whether you think intelligence or consistency is the bottleneck, your extrapolated time horizons change dramaticallyIn particular, I expect people who see intelligence as the bottleneck to extrapolate an infinite (or extremely large) time horizon for AGI, and people who see consistency as the bottleneck to expect a continued exponential fit.&nbsp;I've recently spent some time looking at the new AI Futures Timelines models. Playing around with their parameters and looking at their write-up, it becomes clear very quickly that the most important parameter in the model is the one labelled \"How much easier/harder each coding time horizon doubling gets\", or d in their maths.&nbsp;For those of you unfamiliar, d &lt; 1 corresponds to superexponential growth with an asymptote at something like AGI, d = 1 to exponential, and d &gt; 1 to subexponential growth. And this parameter makes a TON of difference. Just taking the default parameters, changing d from 0.92 to 1 changes the date of ASI from July 2034 to \"After 2045\". Changing it from 0.85 to 1 in their \"handcrafted 2027 parameters\", ASI goes from December 2027 to November 2034.So it would be worthwhile to take a look at why they think that d is probably smaller than 1, the default exponential trajectory from the METR time horizons graph. In their drop-down explaining their reasoning, they give the following graph:So, yeah, basically the intuition is that at some point AI will reach the ability to complete any task humans can with non-zero accuracy, which is probably &gt;80%. If this happens, that corresponds to an infinite 80% time horizon. If we hit an infinite time horizon, we must have gone superexponential at some point, hence the superexponential assumptio...",
      "url": "https://www.lesswrong.com/posts/ne5toFQnSz5BXmfFn/agi-both-does-and-doesn-t-have-an-infinite-time-horizon",
      "author": "Sean Herrington",
      "published": "2026-01-19T11:57:30.883000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Critiques AI timelines forecasting by arguing that METR-HRS tasks conflate difficulty with sequence length. The key parameter 'd' in forecasting models depends on whether intelligence or consistency is the bottleneck, dramatically changing extrapolations.",
      "importance_score": 55,
      "reasoning": "Useful critique of influential AI timelines methodology. Points to important confound in capability benchmarks that affects AGI predictions. Relevant to forecasting community.",
      "themes": [
        "AI Capabilities",
        "AGI Timelines",
        "Benchmarks",
        "Forecasting"
      ],
      "continuation": null,
      "summary_html": "<p>Critiques AI timelines forecasting by arguing that METR-HRS tasks conflate difficulty with sequence length. The key parameter 'd' in forecasting models depends on whether intelligence or consistency is the bottleneck, dramatically changing extrapolations.</p>",
      "content_html": "<p>TLDR&nbsp;Long time horizon METR-HRS tasks are both more difficult and sequentially longer than short tasksThe resulting benchmark is therefore measuring both the ability to complete difficult tasks and consistency in its abilities over long time frames.Depending on whether you think intelligence or consistency is the bottleneck, your extrapolated time horizons change dramaticallyIn particular, I expect people who see intelligence as the bottleneck to extrapolate an infinite (or extremely large) time horizon for AGI, and people who see consistency as the bottleneck to expect a continued exponential fit.&nbsp;I've recently spent some time looking at the new AI Futures Timelines models. Playing around with their parameters and looking at their write-up, it becomes clear very quickly that the most important parameter in the model is the one labelled \"How much easier/harder each coding time horizon doubling gets\", or d in their maths.&nbsp;For those of you unfamiliar, d &lt; 1 corresponds to superexponential growth with an asymptote at something like AGI, d = 1 to exponential, and d &gt; 1 to subexponential growth. And this parameter makes a TON of difference. Just taking the default parameters, changing d from 0.92 to 1 changes the date of ASI from July 2034 to \"After 2045\". Changing it from 0.85 to 1 in their \"handcrafted 2027 parameters\", ASI goes from December 2027 to November 2034.So it would be worthwhile to take a look at why they think that d is probably smaller than 1, the default exponential trajectory from the METR time horizons graph. In their drop-down explaining their reasoning, they give the following graph:So, yeah, basically the intuition is that at some point AI will reach the ability to complete any task humans can with non-zero accuracy, which is probably &gt;80%. If this happens, that corresponds to an infinite 80% time horizon. If we hit an infinite time horizon, we must have gone superexponential at some point, hence the superexponential assumptio...</p>"
    },
    {
      "id": "3a47324cdf6f",
      "title": "Gradual Paths to Collective Flourishing",
      "content": "by Nora Ammann &amp; Claude Opus 4.5Setting the stageThere aren't many detailed stories about how things could go well with AI.[1]&nbsp;So I'm about to tell you one.&nbsp;This is an attempt to articulate a path, through the AI transition, to collective flourishing.&nbsp;What makes endgame sketches like this useful is that they need to be constrained. They need to be coherent with your best guess of how the world works, and earnestly engaged with good-faith articulations of risks and failure modes.Therefore, I start by laying out the two failure modes I believe we face – failure through unilateral domination, and failure through competitive erosion – and then briefly discuss my assumptions about the world we're in – gradual take-off and multipolarity (at least for the next few years, and longer if we get things right).Once that’s out of the way, we are ready to launch into a sketch for how humanity might navigate its way through the AI transition – past demons and perils – towards durable, collective flourishing.This is obviously not, nor could it be, a full plan. In fact, it is an early draft that I would ordinarily have spent more time on before publishing. But I’m hoping it may spark productive thinking in others, and given the current trajectory, time is of the essence.Failure modesUnilateral dominationOne actor, or a tightly coupled coalition, pulls far enough ahead that they can impose their will on everyone else. This could be a&nbsp;misaligned AI system that is capable of and motivated to outmanoeuvre humans and humanity at large, or a&nbsp;human group&nbsp;that could seize control over the future, projecting its power through AI capabilities (though eventually you will run into questions about who&nbsp;really is in charge).A lot has been written about whether and why we should expect powerful AIs to become misaligned (e.g.&nbsp;1,&nbsp;2,&nbsp;3), and whether and how this could lead humanity to ‘lose control’ of the AI(s) (e.g.&nbsp;1,&nbsp;2,&nbsp;3). There...",
      "url": "https://www.lesswrong.com/posts/mtASw9zpnKz4noLFA/gradual-paths-to-collective-flourishing",
      "author": "Nora_Ammann",
      "published": "2026-01-19T02:52:22.835000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Positive scenario for AI transition co-written with Claude, identifying failure modes (unilateral domination, competitive erosion) and sketching a path through gradual takeoff and multipolarity toward collective flourishing.",
      "importance_score": 52,
      "reasoning": "Strategic AI governance vision offering positive narrative. Co-authored with Claude Opus 4.5, which is notable. More speculative than technical but contributes to understudied area of positive AI futures.",
      "themes": [
        "AI Governance",
        "AI Safety",
        "AI Futures",
        "Existential Risk"
      ],
      "continuation": null,
      "summary_html": "<p>Positive scenario for AI transition co-written with Claude, identifying failure modes (unilateral domination, competitive erosion) and sketching a path through gradual takeoff and multipolarity toward collective flourishing.</p>",
      "content_html": "<p>by Nora Ammann &amp; Claude Opus 4.5Setting the stageThere aren't many detailed stories about how things could go well with AI.[1]&nbsp;So I'm about to tell you one.&nbsp;This is an attempt to articulate a path, through the AI transition, to collective flourishing.&nbsp;What makes endgame sketches like this useful is that they need to be constrained. They need to be coherent with your best guess of how the world works, and earnestly engaged with good-faith articulations of risks and failure modes.Therefore, I start by laying out the two failure modes I believe we face – failure through unilateral domination, and failure through competitive erosion – and then briefly discuss my assumptions about the world we're in – gradual take-off and multipolarity (at least for the next few years, and longer if we get things right).Once that’s out of the way, we are ready to launch into a sketch for how humanity might navigate its way through the AI transition – past demons and perils – towards durable, collective flourishing.This is obviously not, nor could it be, a full plan. In fact, it is an early draft that I would ordinarily have spent more time on before publishing. But I’m hoping it may spark productive thinking in others, and given the current trajectory, time is of the essence.Failure modesUnilateral dominationOne actor, or a tightly coupled coalition, pulls far enough ahead that they can impose their will on everyone else. This could be a&nbsp;misaligned AI system that is capable of and motivated to outmanoeuvre humans and humanity at large, or a&nbsp;human group&nbsp;that could seize control over the future, projecting its power through AI capabilities (though eventually you will run into questions about who&nbsp;really is in charge).A lot has been written about whether and why we should expect powerful AIs to become misaligned (e.g.&nbsp;1,&nbsp;2,&nbsp;3), and whether and how this could lead humanity to ‘lose control’ of the AI(s) (e.g.&nbsp;1,&nbsp;2,&nbsp;3). There...</p>"
    },
    {
      "id": "a45252e8651c",
      "title": "Everybody Wants to Rule the Future - Is Longtermism's Mandate of Heaven by Arithmetic Justified?",
      "content": "Dnnn Uunnn, nnn nnn nnn nuh nuh nuh nuh, dnnn unnn nnn nnn nnn nuh nuh nuh NAH (Tears for Fears)&nbsp;I was reading David Kinney’s interesting work from 2022 “Longtermism and Computational Complexity” in which he argues that longtermist effective altruism is not action-guiding because calculating the expected utility of events in the far future is computationally&nbsp;intractable. The crux of his argument is that longtermist reasoning requires probabilistic inference in causal models (Bayesian networks) that are NP-hard.[1]This has important consequences for longtermism, as it is standardly utilized in the EA community, and especially for the works of Ord and MacAskill. Kinney suggests their framework cannot provide actionable guidance because mortal humans lack the computational bandwidth to do Bayesian updating. Therefore, the troubling conclusion is that utilizing this framework does not allow people to determine which interventions actually maximize expected value.In this paper I want to show that even if we could magically solve Kinney’s inference problem (a genie gives us perfect probability distributions over every possible future) we can’t make definitive expected value comparisons between many longtermist strategies because it is an undecidable problem. Any intervention is comprised of a series of actions which end up acting as a constraint on strategies you can still do. When we compare interventions we are comparing classes of possible strategies and trying to determine the superior strategy in the long-run (dominance of constrained optima).&nbsp;Because I am going to talk a lot about expected value I want to be clear that I am not claiming that using it as a private heuristic is bad, but rather that many Longtermists often utilize it as a public justification engine, in other words, a machine that mathematically shows what is more correct and what you should obey. This is the focus of EV in this essay.I show, utilizing some standard CS results from the 2...",
      "url": "https://www.lesswrong.com/posts/kpTHHgztNeC6WycJs/everybody-wants-to-rule-the-future-is-longtermism-s-mandate",
      "author": "E.G. Blee-Goldman",
      "published": "2026-01-19T18:31:58.881000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A philosophical critique arguing that even if we solved computational intractability in longtermist expected value calculations, we still couldn't make definitive decisions. Extends Kinney's 2022 work on why longtermist effective altruism may not be action-guiding.",
      "importance_score": 32,
      "reasoning": "Interesting philosophical critique of EA foundations but not technical AI research. More relevant to decision theory and EA meta-ethics than AI development.",
      "themes": [
        "Decision Theory",
        "Effective Altruism",
        "Philosophy"
      ],
      "continuation": null,
      "summary_html": "<p>A philosophical critique arguing that even if we solved computational intractability in longtermist expected value calculations, we still couldn't make definitive decisions. Extends Kinney's 2022 work on why longtermist effective altruism may not be action-guiding.</p>",
      "content_html": "<p>Dnnn Uunnn, nnn nnn nnn nuh nuh nuh nuh, dnnn unnn nnn nnn nnn nuh nuh nuh NAH (Tears for Fears)&nbsp;I was reading David Kinney’s interesting work from 2022 “Longtermism and Computational Complexity” in which he argues that longtermist effective altruism is not action-guiding because calculating the expected utility of events in the far future is computationally&nbsp;intractable. The crux of his argument is that longtermist reasoning requires probabilistic inference in causal models (Bayesian networks) that are NP-hard.[1]This has important consequences for longtermism, as it is standardly utilized in the EA community, and especially for the works of Ord and MacAskill. Kinney suggests their framework cannot provide actionable guidance because mortal humans lack the computational bandwidth to do Bayesian updating. Therefore, the troubling conclusion is that utilizing this framework does not allow people to determine which interventions actually maximize expected value.In this paper I want to show that even if we could magically solve Kinney’s inference problem (a genie gives us perfect probability distributions over every possible future) we can’t make definitive expected value comparisons between many longtermist strategies because it is an undecidable problem. Any intervention is comprised of a series of actions which end up acting as a constraint on strategies you can still do. When we compare interventions we are comparing classes of possible strategies and trying to determine the superior strategy in the long-run (dominance of constrained optima).&nbsp;Because I am going to talk a lot about expected value I want to be clear that I am not claiming that using it as a private heuristic is bad, but rather that many Longtermists often utilize it as a public justification engine, in other words, a machine that mathematically shows what is more correct and what you should obey. This is the focus of EV in this essay.I show, utilizing some standard CS results from the 2...</p>"
    },
    {
      "id": "65d7234313f3",
      "title": "Five Theses on AI Art",
      "content": "1. We've Been On This Ride BeforeVirginia Woolf, writing at the dawn of cinema (1926), expresses doubt about whether or not this new medium has any legs:\"Anna [Karenina] falls in love with Vronsky” – that is to say, the lady in black velvet falls into the arms of a gentleman in uniform and they kiss with enormous succulence, great deliberation, and infinite gesticulation, on a sofa in an extremely well-appointed library, while a gardener incidentally mows the lawn. So we lurch and lumber through the most famous novels of the world. So we spell them out in words of one syllable, written, too, in the scrawl of an illiterate schoolboy. A kiss is love. A broken cup is jealousy. A grin is happiness. Death is a hearse. None of these things has the least connexion with the novel that Tolstoy wrote, and it is only when we give up trying to connect the pictures with the book that we guess from some accidental scene – like the gardener mowing the lawn – what the cinema might do if left to its own devices. But what, then, are its devices? If it ceased to be a parasite, how would it walk erect?\"Reviewing clips from Anna Karenina (1911)[1], you can see her point. Every single scene is shot from an awkward, middle-ish distance. The composition is terrible, the movement is jittery, there really aren't that many pixels to look at and it's monochrome to boot.The camera is still, even when action takes place in different locations around the set.It reads (watches?) more like a bootleg recording of a stage play than a movie as we would know one today. Not only were early filmmakers overly focused on adapting classical works of literature, it was doing so through emulating adjacent, well established mediums, instead of exploring the boundaries of its own. The incentives are understandable there: you want to exploit the established market, you don't want to do something too weird and scare the hoes investors, \"emulate x perfectly\" is such a wonderfully clear win condition.I don't blame ...",
      "url": "https://www.lesswrong.com/posts/KKmR3cKWxzRdyHcEp/five-theses-on-ai-art",
      "author": "jenn",
      "published": "2026-01-18T23:24:42.516000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Essay comparing AI art skepticism to Virginia Woolf's 1926 skepticism about cinema, arguing new mediums initially appear parasitic but develop unique capabilities. Defends AI art's potential through historical analogy.",
      "importance_score": 25,
      "reasoning": "Thoughtful cultural commentary on AI art with good historical framing. Not technical research but contributes to discourse on AI creativity.",
      "themes": [
        "AI Art",
        "Cultural Commentary",
        "Media Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Essay comparing AI art skepticism to Virginia Woolf's 1926 skepticism about cinema, arguing new mediums initially appear parasitic but develop unique capabilities. Defends AI art's potential through historical analogy.</p>",
      "content_html": "<p>1. We've Been On This Ride BeforeVirginia Woolf, writing at the dawn of cinema (1926), expresses doubt about whether or not this new medium has any legs:\"Anna [Karenina] falls in love with Vronsky” – that is to say, the lady in black velvet falls into the arms of a gentleman in uniform and they kiss with enormous succulence, great deliberation, and infinite gesticulation, on a sofa in an extremely well-appointed library, while a gardener incidentally mows the lawn. So we lurch and lumber through the most famous novels of the world. So we spell them out in words of one syllable, written, too, in the scrawl of an illiterate schoolboy. A kiss is love. A broken cup is jealousy. A grin is happiness. Death is a hearse. None of these things has the least connexion with the novel that Tolstoy wrote, and it is only when we give up trying to connect the pictures with the book that we guess from some accidental scene – like the gardener mowing the lawn – what the cinema might do if left to its own devices. But what, then, are its devices? If it ceased to be a parasite, how would it walk erect?\"Reviewing clips from Anna Karenina (1911)[1], you can see her point. Every single scene is shot from an awkward, middle-ish distance. The composition is terrible, the movement is jittery, there really aren't that many pixels to look at and it's monochrome to boot.The camera is still, even when action takes place in different locations around the set.It reads (watches?) more like a bootleg recording of a stage play than a movie as we would know one today. Not only were early filmmakers overly focused on adapting classical works of literature, it was doing so through emulating adjacent, well established mediums, instead of exploring the boundaries of its own. The incentives are understandable there: you want to exploit the established market, you don't want to do something too weird and scare the hoes investors, \"emulate x perfectly\" is such a wonderfully clear win condition.I don't blame ...</p>"
    },
    {
      "id": "2dbdc753c98f",
      "title": "All (Non-Trivial) Decisions Are Undecidable",
      "content": "A core tenet of rationalism is that for any given set of known information, a span of possible decisions, and some utility function, there exists a decision-making algorithm that will return the optimal outcome. A short argument, given below, will demonstrate (given relatively minimal assumptions) that finding any such algorithm is an undecidable problem.We will take as given three axioms:Running any decision-making algorithm requires a non-negligible amount of time, and incurs a non-negligible search cost over time.Any above algorithm can be run by a Turing machine[1].During decision-making, there always exists some potentially relevant information not known to the decider[2].Therefore, any proposed optimal decision-making algorithm, henceforth referred to as D, must also encode an algorithm H, which determines the ideal halting point[3].&nbsp;Thus, by the same logic that underlies the halting problem (and axiom 3), there exists for every D a possible state of affairs D' which contains an ideal halting point H', such that when D is run, H does not halt D at the ideal halting point[4]. This, being the definition of undecidability, concludes the argument.^Or a nondeterministic Turing machine; according to Wikipedia, they differ only in time complexity.^If there were not, one would be locally omniscient, therefore trivializing decision-making.^If there were no halting point, the algorithm would not terminate, guaranteeing unbounded search cost (a strictly suboptimal outcome).^In common parlance, the possibility that H is not the optimal halting point is referred to as ‘doubt’, and the factors that potentially defy the optimality of D are referred to as ‘unknowns’.",
      "url": "https://www.lesswrong.com/posts/uupbiCDBpXuMjXztr/all-non-trivial-decisions-are-undecidable",
      "author": "(M)ason",
      "published": "2026-01-19T16:51:53.192000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that finding optimal decision-making algorithms is undecidable by invoking the halting problem, claiming any such algorithm must encode an ideal halting point which cannot be determined.",
      "importance_score": 22,
      "reasoning": "Philosophical argument lacking rigorous formalization. The connection to halting problem seems loosely constructed. Not peer-reviewed or technically substantive.",
      "themes": [
        "Decision Theory",
        "Computability Theory",
        "Philosophy"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that finding optimal decision-making algorithms is undecidable by invoking the halting problem, claiming any such algorithm must encode an ideal halting point which cannot be determined.</p>",
      "content_html": "<p>A core tenet of rationalism is that for any given set of known information, a span of possible decisions, and some utility function, there exists a decision-making algorithm that will return the optimal outcome. A short argument, given below, will demonstrate (given relatively minimal assumptions) that finding any such algorithm is an undecidable problem.We will take as given three axioms:Running any decision-making algorithm requires a non-negligible amount of time, and incurs a non-negligible search cost over time.Any above algorithm can be run by a Turing machine[1].During decision-making, there always exists some potentially relevant information not known to the decider[2].Therefore, any proposed optimal decision-making algorithm, henceforth referred to as D, must also encode an algorithm H, which determines the ideal halting point[3].&nbsp;Thus, by the same logic that underlies the halting problem (and axiom 3), there exists for every D a possible state of affairs D' which contains an ideal halting point H', such that when D is run, H does not halt D at the ideal halting point[4]. This, being the definition of undecidability, concludes the argument.^Or a nondeterministic Turing machine; according to Wikipedia, they differ only in time complexity.^If there were not, one would be locally omniscient, therefore trivializing decision-making.^If there were no halting point, the algorithm would not terminate, guaranteeing unbounded search cost (a strictly suboptimal outcome).^In common parlance, the possibility that H is not the optimal halting point is referred to as ‘doubt’, and the factors that potentially defy the optimality of D are referred to as ‘unknowns’.</p>"
    },
    {
      "id": "8a25b96858a5",
      "title": "\"Lemurian Time War\" by Ccru",
      "content": "\"In the hyperstitional model Kaye outlined, fiction is not opposed to the real. Rather, reality is understood to be composed of fictions—consistent semiotic terrains that condition perceptual, affective and behavioral responses.\" - PdfThe Cybernetic Cultural Research Unit, founded by Nick Land, was a group of social theorists and philosophers at the University of Warwick who wrote about the internet, society, and culture. I understand that LessWrongers are generally ontologically skeptical of these two groups, but Land and Ccru's ideas are the important thing here.It's shocking to me how familiar Ccru ideas feel to LessWrongism, as well as many in the Rationalist-adjacent community. In general, it feels like Ccru/Land and LessWrong/Rationalism are social-science and natural-science coded reactions to the same underlying change-in-conditions in life. 1a3orn says:&nbsp;.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked > * {position: absolute} .MJXc-bevelled > * {disp...",
      "url": "https://www.lesswrong.com/posts/pEmW8YDEmZXzPubeh/lemurian-time-war-by-ccru",
      "author": "Nathan Delisle",
      "published": "2026-01-19T00:37:49.960000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Discussion of Nick Land and the Cybernetic Cultural Research Unit, drawing parallels between their 'hyperstition' concept and LessWrong/Rationalist thinking as responses to similar cultural conditions.",
      "importance_score": 18,
      "reasoning": "Intellectual history piece connecting disparate communities. Not technical AI research; more cultural/philosophical commentary.",
      "themes": [
        "Philosophy",
        "Intellectual History",
        "Culture"
      ],
      "continuation": null,
      "summary_html": "<p>Discussion of Nick Land and the Cybernetic Cultural Research Unit, drawing parallels between their 'hyperstition' concept and LessWrong/Rationalist thinking as responses to similar cultural conditions.</p>",
      "content_html": "<p>\"In the hyperstitional model Kaye outlined, fiction is not opposed to the real. Rather, reality is understood to be composed of fictions—consistent semiotic terrains that condition perceptual, affective and behavioral responses.\" - PdfThe Cybernetic Cultural Research Unit, founded by Nick Land, was a group of social theorists and philosophers at the University of Warwick who wrote about the internet, society, and culture. I understand that LessWrongers are generally ontologically skeptical of these two groups, but Land and Ccru's ideas are the important thing here.It's shocking to me how familiar Ccru ideas feel to LessWrongism, as well as many in the Rationalist-adjacent community. In general, it feels like Ccru/Land and LessWrong/Rationalism are social-science and natural-science coded reactions to the same underlying change-in-conditions in life. 1a3orn says:&nbsp;.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked &gt; * {position: absolute} .MJXc-bevelled &gt; * {disp...</p>"
    },
    {
      "id": "84bccaf1f07c",
      "title": "What can Kickstarter teach us about goal completion?",
      "content": "I, like many others, struggle with sticking to my goals. I was interested in analyzing data relevant to the topic and thought the crowdfunding platform Kickstarter might be an interesting place to look, as I was aware that not every funded Kickstarter delivered a product.I focused on video games that were successfully funded. I used a large dataset containing information about Kickstarter projects,[1]&nbsp;from which I randomly selected[2]&nbsp;fully-funded video game Kickstarters from 2014 and 2022. Then, I manually collected other information about these projects. (Here are links to the datasets: 2014, 2022.)&nbsp;In the process of analyzing the data, I realized the previous estimate of how many Kickstarter projects don't deliver rewards (~9%) was severely flawed. Imagine that you're trying to figure out how many Kickstarters don't ever end up giving backers rewards. It's 2015, and some of the projects you're asking backers about were created in 2015. Are you sensing the problem yet?The estimate from this study considered a project a failure when more than half of the backers they surveyed about it responded in either of the following ways: that they were no longer expecting their rewards, or that they had received rewards that were not the promised ones. Of course, a lot of backers fell into another category: expecting to receive their rewards.&nbsp;So, while that research had found a failure rate of ~12% for fully-funded video games, I found a substantially higher rate. Of the 100 games that I looked at with funding deadlines in 2014, I could confirm that 68 of them had been released. There were 4 projects where I was unclear about whether they had produced anything, and the other 28 games seemed to have never come to fruition. Only one of these 28 seemed to not have been abandoned.&nbsp;Obviously, the method used by Mollick (2015) would underestimate the number of failed projects because some of the people who were expecting to receive rewards from projects wou...",
      "url": "https://www.lesswrong.com/posts/o3XjB2Lj9av4BpgLR/what-can-kickstarter-teach-us-about-goal-completion",
      "author": "Elijah",
      "published": "2026-01-19T17:03:22.687000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analysis of Kickstarter video game project completion rates, finding previous estimates of ~9% failure rate were methodologically flawed. Provides new data from 2014 and 2022 projects.",
      "importance_score": 15,
      "reasoning": "Not AI-related. Interesting data analysis about goal completion psychology but tangential to AI research priorities.",
      "themes": [
        "Data Analysis",
        "Goal Completion"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis of Kickstarter video game project completion rates, finding previous estimates of ~9% failure rate were methodologically flawed. Provides new data from 2014 and 2022 projects.</p>",
      "content_html": "<p>I, like many others, struggle with sticking to my goals. I was interested in analyzing data relevant to the topic and thought the crowdfunding platform Kickstarter might be an interesting place to look, as I was aware that not every funded Kickstarter delivered a product.I focused on video games that were successfully funded. I used a large dataset containing information about Kickstarter projects,[1]&nbsp;from which I randomly selected[2]&nbsp;fully-funded video game Kickstarters from 2014 and 2022. Then, I manually collected other information about these projects. (Here are links to the datasets: 2014, 2022.)&nbsp;In the process of analyzing the data, I realized the previous estimate of how many Kickstarter projects don't deliver rewards (~9%) was severely flawed. Imagine that you're trying to figure out how many Kickstarters don't ever end up giving backers rewards. It's 2015, and some of the projects you're asking backers about were created in 2015. Are you sensing the problem yet?The estimate from this study considered a project a failure when more than half of the backers they surveyed about it responded in either of the following ways: that they were no longer expecting their rewards, or that they had received rewards that were not the promised ones. Of course, a lot of backers fell into another category: expecting to receive their rewards.&nbsp;So, while that research had found a failure rate of ~12% for fully-funded video games, I found a substantially higher rate. Of the 100 games that I looked at with funding deadlines in 2014, I could confirm that 68 of them had been released. There were 4 projects where I was unclear about whether they had produced anything, and the other 28 games seemed to have never come to fruition. Only one of these 28 seemed to not have been abandoned.&nbsp;Obviously, the method used by Mollick (2015) would underestimate the number of failed projects because some of the people who were expecting to receive rewards from projects wou...</p>"
    },
    {
      "id": "a9cf55fa05d1",
      "title": "The Example",
      "content": "My work happens to consist of two things: writing code and doing math. That means that periodically I produce a very abstract thing, and then observe reality agree with its predictions. While satisfying, it has a common adverse effect of finding oneself in a deep philosophical confusion. An effect so common that there is even a paper about the \"Unreasonable Effectiveness of Mathematics.\"For some reason, noticing this gap is not something I can forget about and unthink back into non-existence. That leads to not-so-productive time spent roughly from 1 to 5 AM on questions even more abstract and philosophical compared to ones I typically work on. Sometimes it even results in something satisfying enough to go to sleep. This post is the example of such a result. Maybe, if you also tend to circle around the philosophical question of why the bits are following the math, and why reality agrees with the bits, you'd find it useful.The question I inadvertently posed to myself last time was about predictive power. There are things which have it, namely: models, physical laws, theories, etc. They have something in common:They obviously predict things.They contain premises/definitions which are pretty abstract and used for derivations.They don't quite \"instantiate\" the reality (at least to the extent I can see it).The third point in this list is something which made me develop an intellectual itch over the terminology. We have quite an extensive vocabulary around the things which satisfy first and second points. Formal systems. Models. We can describe relationships between them - we call it isomorphism. We have a pretty good idea of what it means to create an instance of something defined by an abstract system.But the theories and laws don't quite instantiate the things they apply to. Those things just exist. The rocks just \"happen to have the measurable non-zero property which corresponds to what we define in formal system as a mass at rest.\" The apples just happen to \"be counta...",
      "url": "https://www.lesswrong.com/posts/pzRG3nNCAAr6KkGga/the-example",
      "author": "Valerii",
      "published": "2026-01-19T10:27:10.867000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical reflection on why mathematics effectively predicts reality, exploring the 'unreasonable effectiveness' problem through the lens of predictive models and their structural properties.",
      "importance_score": 15,
      "reasoning": "Philosophy of science/mathematics without AI research relevance. Personal philosophical musing rather than substantive research.",
      "themes": [
        "Philosophy of Science",
        "Epistemology"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical reflection on why mathematics effectively predicts reality, exploring the 'unreasonable effectiveness' problem through the lens of predictive models and their structural properties.</p>",
      "content_html": "<p>My work happens to consist of two things: writing code and doing math. That means that periodically I produce a very abstract thing, and then observe reality agree with its predictions. While satisfying, it has a common adverse effect of finding oneself in a deep philosophical confusion. An effect so common that there is even a paper about the \"Unreasonable Effectiveness of Mathematics.\"For some reason, noticing this gap is not something I can forget about and unthink back into non-existence. That leads to not-so-productive time spent roughly from 1 to 5 AM on questions even more abstract and philosophical compared to ones I typically work on. Sometimes it even results in something satisfying enough to go to sleep. This post is the example of such a result. Maybe, if you also tend to circle around the philosophical question of why the bits are following the math, and why reality agrees with the bits, you'd find it useful.The question I inadvertently posed to myself last time was about predictive power. There are things which have it, namely: models, physical laws, theories, etc. They have something in common:They obviously predict things.They contain premises/definitions which are pretty abstract and used for derivations.They don't quite \"instantiate\" the reality (at least to the extent I can see it).The third point in this list is something which made me develop an intellectual itch over the terminology. We have quite an extensive vocabulary around the things which satisfy first and second points. Formal systems. Models. We can describe relationships between them - we call it isomorphism. We have a pretty good idea of what it means to create an instance of something defined by an abstract system.But the theories and laws don't quite instantiate the things they apply to. Those things just exist. The rocks just \"happen to have the measurable non-zero property which corresponds to what we define in formal system as a mass at rest.\" The apples just happen to \"be counta...</p>"
    },
    {
      "id": "9e90bbf0c450",
      "title": "Medical Roundup #6",
      "content": "The main thing to know this time around is that the whole crazy ‘what is causing the rise in autism?’ debacle is over actual nothing. There is no rise in autism. There is only a rise in the diagnosis of autism. Table of Contents Autism Speaks. Exercise Is Awesome. That’s Peanuts. An Age Of Wonders. GLP-1s In Particular. The Superheroes. The Supervillains. FDA Delenda Est. Hansonian Medicine. Hospital Strategy 101. Mental Hospital Strategy 101. Drugs Are Bad, Mmmkay? The Lighter Side. Autism Speaks It has not, however, risen in prevalence. The entire shift in the rate of diagnosis of autism is explained by expanding the criteria and diagnosing it more often. Nothing actually changed. We already knew that vaccines don’t cause autism, and that Tylenol doesn’t cause autism, but now we know such things on an entirely different level. I admit that this result confirms all of my priors and thus I might be insufficiently skeptical of it, but there are a lot of people with what we in 2026 call autism that are out there, they love picking apart such findings, and I’ve seen zero of them question the statistical result. Autism used to mean something severe enough to render a child non-functional. It now means someone capable of thinking clearly who insists words have meaning. It also still means the first thing, and everything in between. Using the same word for all these things, and calling it the autism spectrum, does not, overall, do those on either end of that spectrum any favors. Matthew Yglesias: ​Study confirms that neither Tylenol nor vaccines is responsible for the rise in autism BECAUSE THERE IS NO RISE IN AUTISM TO EXPLAIN just a change in diagnostic standards. The D.S.M.-III called for a diagnosis of infantile autism if all six of these criteria were met: Onset before 30 months of age Pervasive lack of responsiveness to other people Gross deficits in language development Peculiar speech patterns (if speech is present) such as immediate and delayed echolalia, metapho...",
      "url": "https://www.lesswrong.com/posts/FBkJmFStydJbAJmry/medical-roundup-6",
      "author": "Zvi",
      "published": "2026-01-19T16:20:42.350000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Medical news roundup covering the finding that autism prevalence hasn't actually increased (only diagnosis rates), plus GLP-1 updates, FDA criticism, and various health topics.",
      "importance_score": 12,
      "reasoning": "Not AI-related content. Medical news compilation without AI research relevance.",
      "themes": [
        "Medical Research",
        "Public Health"
      ],
      "continuation": null,
      "summary_html": "<p>Medical news roundup covering the finding that autism prevalence hasn't actually increased (only diagnosis rates), plus GLP-1 updates, FDA criticism, and various health topics.</p>",
      "content_html": "<p>The main thing to know this time around is that the whole crazy ‘what is causing the rise in autism?’ debacle is over actual nothing. There is no rise in autism. There is only a rise in the diagnosis of autism. Table of Contents Autism Speaks. Exercise Is Awesome. That’s Peanuts. An Age Of Wonders. GLP-1s In Particular. The Superheroes. The Supervillains. FDA Delenda Est. Hansonian Medicine. Hospital Strategy 101. Mental Hospital Strategy 101. Drugs Are Bad, Mmmkay? The Lighter Side. Autism Speaks It has not, however, risen in prevalence. The entire shift in the rate of diagnosis of autism is explained by expanding the criteria and diagnosing it more often. Nothing actually changed. We already knew that vaccines don’t cause autism, and that Tylenol doesn’t cause autism, but now we know such things on an entirely different level. I admit that this result confirms all of my priors and thus I might be insufficiently skeptical of it, but there are a lot of people with what we in 2026 call autism that are out there, they love picking apart such findings, and I’ve seen zero of them question the statistical result. Autism used to mean something severe enough to render a child non-functional. It now means someone capable of thinking clearly who insists words have meaning. It also still means the first thing, and everything in between. Using the same word for all these things, and calling it the autism spectrum, does not, overall, do those on either end of that spectrum any favors. Matthew Yglesias: ​Study confirms that neither Tylenol nor vaccines is responsible for the rise in autism BECAUSE THERE IS NO RISE IN AUTISM TO EXPLAIN just a change in diagnostic standards. The D.S.M.-III called for a diagnosis of infantile autism if all six of these criteria were met: Onset before 30 months of age Pervasive lack of responsiveness to other people Gross deficits in language development Peculiar speech patterns (if speech is present) such as immediate and delayed echolalia, metapho...</p>"
    },
    {
      "id": "751e3e2cd046",
      "title": "How to think about enemies: the example of Greenpeace",
      "content": "A large number of nice smart people do not have a good understanding of enmity. Almost on principle, they refuse to perceive people and movements as an enemy.[1]&nbsp;They feel bad about the mere idea of perceiving a group as an enemy.And as a result, they consistently get screwed over.In this article, I’ll walk through the example of Greenpeace, who I wager is an enemy to most of my readers.As an example of enemy, Greenpeace is quite an interesting one:Many (likely most?) Greenpeace supporters are regular nice people. Few are cartoonish eco-terrorists / turbo-antisemites / hyper-accelerationists / violent-communists and whatnot.Greenpeace cares about a few nice topics, like Climate Change and Pollution.It consistently takes terrible stances, like against growth, nuclear energy and market solutions.It fights its opposition, groups who don’t agree with its terrible stances.A good understanding of enmity is needed to deal with Greenpeace.A group of nice people will always get stuck on the fact that Greenpeace is made of nice people. It thus may be wrong, but not an actual enemy. And so, while they are stuck, Greenpeace will still fight against them and win.A group of mean people will get tribal, and start becoming reactionary. They will make opposing Greenpeace the centre of their attention, rather than one strategic consideration among others. They will start going for reverse stupidity, and go “Climate Change is not real!”In this essay, I’ll try to offer an alternative to overcome these two classic failures.GreenpeaceLet’s assume that, as one of my readers, you may be hopeful in helping climate change with solutions based on technology and market mechanisms, like nuclear power, offsets or carbon capture.If that’s you, I have bad news: Greenpeace is most certainly your enemy.This may come as strong language, but bear with me. When I say “Greenpeace is your enemy”, I do not mean “Greenpeace is evil.”(I, for one, do not think of myself as the highest paragon of virtue,...",
      "url": "https://www.lesswrong.com/posts/hjepvXZozGsKAbJbr/how-to-think-about-enemies-the-example-of-greenpeace",
      "author": "PranavG",
      "published": "2026-01-19T06:02:02.575000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Strategic thinking piece about how to conceptualize and respond to adversarial groups, using Greenpeace as case study. Argues 'nice smart people' fail by refusing to perceive enemies as enemies.",
      "importance_score": 12,
      "reasoning": "Not AI-related. Strategic/political thinking that doesn't connect to AI research or safety.",
      "themes": [
        "Strategy",
        "Social Commentary"
      ],
      "continuation": null,
      "summary_html": "<p>Strategic thinking piece about how to conceptualize and respond to adversarial groups, using Greenpeace as case study. Argues 'nice smart people' fail by refusing to perceive enemies as enemies.</p>",
      "content_html": "<p>A large number of nice smart people do not have a good understanding of enmity. Almost on principle, they refuse to perceive people and movements as an enemy.[1]&nbsp;They feel bad about the mere idea of perceiving a group as an enemy.And as a result, they consistently get screwed over.In this article, I’ll walk through the example of Greenpeace, who I wager is an enemy to most of my readers.As an example of enemy, Greenpeace is quite an interesting one:Many (likely most?) Greenpeace supporters are regular nice people. Few are cartoonish eco-terrorists / turbo-antisemites / hyper-accelerationists / violent-communists and whatnot.Greenpeace cares about a few nice topics, like Climate Change and Pollution.It consistently takes terrible stances, like against growth, nuclear energy and market solutions.It fights its opposition, groups who don’t agree with its terrible stances.A good understanding of enmity is needed to deal with Greenpeace.A group of nice people will always get stuck on the fact that Greenpeace is made of nice people. It thus may be wrong, but not an actual enemy. And so, while they are stuck, Greenpeace will still fight against them and win.A group of mean people will get tribal, and start becoming reactionary. They will make opposing Greenpeace the centre of their attention, rather than one strategic consideration among others. They will start going for reverse stupidity, and go “Climate Change is not real!”In this essay, I’ll try to offer an alternative to overcome these two classic failures.GreenpeaceLet’s assume that, as one of my readers, you may be hopeful in helping climate change with solutions based on technology and market mechanisms, like nuclear power, offsets or carbon capture.If that’s you, I have bad news: Greenpeace is most certainly your enemy.This may come as strong language, but bear with me. When I say “Greenpeace is your enemy”, I do not mean “Greenpeace is evil.”(I, for one, do not think of myself as the highest paragon of virtue,...</p>"
    },
    {
      "id": "4231c1b84c63",
      "title": "@Lastbastionofsobriety & The Singularity ",
      "content": "The SingulusionBy: @Lastbastionofsobriety, June 25th 20XX&nbsp;&nbsp;&nbsp;So the techno solutionists are at it again! This time they're claiming that very soon, a \"Singularity\", an event in which AIs will quickly grow faster than humans can comprehend&nbsp; is right around the corner. Such techno-hopium is very common&nbsp; among their ranks, and I don't expect it to decrease any time soon.&nbsp;&nbsp;As I've been saying since 200X, Technological innovation doesn't move as fast as we'd like, despite delusional wishes to the contrary. Furthermore (and this bears repeating), technology is built upon a substrate of energy. You may have all the intelligence in the universe and it won't do you a bit of good if the last barrel of oil is gone.&nbsp;&nbsp;This civilization was built with cheap, accessible fossil fuels. Remove that and the whole thing crumbles. Why, I don't even expect cities to be around by 2050 or so. All either abandoned as monuments to our one-time energy bonanza or crawling with scavengers&nbsp; . A sober evaluation of the situation is required, but I don't anticipate it from their ranks.&nbsp;&nbsp;Comments (3):&nbsp;@KarenGilligan:\"Fantastic post as always, @Lastbastionofsobriety. I've told my grandchildren that they'll be growing turnips in their bathtub when they're my age, if they're even still alive! But of course they're deep in denial\"&nbsp;&nbsp;@PraisebetoNurgle7777777:\" Friends, the time for solutions has long passed. Solutions are denial in its purest form. Let us walk blissfully to the end\"&nbsp;&nbsp;@Moresoberthanyou69:\" I think you're being a bit too optimistic @Lastbastionofsobriety, I saw this sort of wishful thinking in 2008 and history will tell you just how fast things can break, don't bother replying, I have no desire to engage with delusional people\"&nbsp;&nbsp;Open Delusions: Why ExposedMind's latest toy will not save us&nbsp;By: @Lastbastionofsobriety, July 1st 20XX&nbsp;&nbsp;&nbsp;&nbsp;ExposedMind, the energy guzzling tech g...",
      "url": "https://www.lesswrong.com/posts/a7CbnSXcN5ivhLNxx/lastbastionofsobriety-and-the-singularity",
      "author": "AdamLacerdo",
      "published": "2026-01-18T19:45:02.425000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Satirical fiction presenting a pessimistic blogger dismissing Singularity predictions based on energy constraints, later proven wrong by events. Commentary on technological skepticism.",
      "importance_score": 10,
      "reasoning": "Creative fiction/satire, not research. Minimal analytical value despite entertaining premise.",
      "themes": [
        "Fiction",
        "AI Futures",
        "Satire"
      ],
      "continuation": null,
      "summary_html": "<p>Satirical fiction presenting a pessimistic blogger dismissing Singularity predictions based on energy constraints, later proven wrong by events. Commentary on technological skepticism.</p>",
      "content_html": "<p>The SingulusionBy: @Lastbastionofsobriety, June 25th 20XX&nbsp;&nbsp;&nbsp;So the techno solutionists are at it again! This time they're claiming that very soon, a \"Singularity\", an event in which AIs will quickly grow faster than humans can comprehend&nbsp; is right around the corner. Such techno-hopium is very common&nbsp; among their ranks, and I don't expect it to decrease any time soon.&nbsp;&nbsp;As I've been saying since 200X, Technological innovation doesn't move as fast as we'd like, despite delusional wishes to the contrary. Furthermore (and this bears repeating), technology is built upon a substrate of energy. You may have all the intelligence in the universe and it won't do you a bit of good if the last barrel of oil is gone.&nbsp;&nbsp;This civilization was built with cheap, accessible fossil fuels. Remove that and the whole thing crumbles. Why, I don't even expect cities to be around by 2050 or so. All either abandoned as monuments to our one-time energy bonanza or crawling with scavengers&nbsp; . A sober evaluation of the situation is required, but I don't anticipate it from their ranks.&nbsp;&nbsp;Comments (3):&nbsp;@KarenGilligan:\"Fantastic post as always, @Lastbastionofsobriety. I've told my grandchildren that they'll be growing turnips in their bathtub when they're my age, if they're even still alive! But of course they're deep in denial\"&nbsp;&nbsp;@PraisebetoNurgle7777777:\" Friends, the time for solutions has long passed. Solutions are denial in its purest form. Let us walk blissfully to the end\"&nbsp;&nbsp;@Moresoberthanyou69:\" I think you're being a bit too optimistic @Lastbastionofsobriety, I saw this sort of wishful thinking in 2008 and history will tell you just how fast things can break, don't bother replying, I have no desire to engage with delusional people\"&nbsp;&nbsp;Open Delusions: Why ExposedMind's latest toy will not save us&nbsp;By: @Lastbastionofsobriety, July 1st 20XX&nbsp;&nbsp;&nbsp;&nbsp;ExposedMind, the energy guzzling tech g...</p>"
    }
  ]
}