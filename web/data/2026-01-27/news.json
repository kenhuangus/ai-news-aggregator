{
  "category": "news",
  "date": "2026-01-27",
  "category_summary": "**Microsoft** [entered the physical AI race](/?date=2026-01-27&category=news#item-4d36790c7226) with **Rho-alpha**, a new vision-language-action model for robotics, while **NVIDIA** made two major moves: a [**$2 billion investment**](/?date=2026-01-27&category=news#item-d2c712b7696f) in **CoreWeave** and the [release of **Earth-2**](/?date=2026-01-27&category=news#item-ed31d1a4a250), an open AI weather prediction stack with three novel models.\n\n**OpenAI** [published technical details](/?date=2026-01-27&category=news#item-b7b96464bf73) on **Codex CLI** architecture, revealing how its agentic loop works alongside mentions of **GPT-5.2** and **Claude Code with Opus 4.5** reaching new capability levels. On the regulatory front:\n- **EU** [launched formal investigation](/?date=2026-01-27&category=news#item-5652f145be41) into **xAI** over **Grok** generating sexualized deepfakes under the Digital Services Act\n- **Meta** [paused teen access](/?date=2026-01-27&category=news#item-b9a395e956f2) to AI chatbot characters amid safety concerns\n- **US DOT** using **Gemini** to [draft safety rules](/?date=2026-01-27&category=news#item-2e3e59ab1ff3) sparked criticism\n- Multiple US states [considering datacenter moratoriums](/?date=2026-01-27&category=news#item-ad5acd8ec1cc) due to energy concerns\n\n**Synthesia** [nearly doubled its valuation](/?date=2026-01-27&category=news#item-9078c15ec00c) to **$4 billion**, while major retailers including **Walmart**, **Target**, and **Etsy** [expanded agentic AI commerce integrations](/?date=2026-01-27&category=news#item-d157155f4509).",
  "category_summary_html": "<p><strong>Microsoft</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-4d36790c7226\" class=\"internal-link\" rel=\"noopener noreferrer\">entered the physical AI race</a> with <strong>Rho-alpha</strong>, a new vision-language-action model for robotics, while <strong>NVIDIA</strong> made two major moves: a <a href=\"/?date=2026-01-27&amp;category=news#item-d2c712b7696f\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>$2 billion investment</strong></a> in <strong>CoreWeave</strong> and the <a href=\"/?date=2026-01-27&amp;category=news#item-ed31d1a4a250\" class=\"internal-link\" rel=\"noopener noreferrer\">release of <strong>Earth-2</strong></a>, an open AI weather prediction stack with three novel models.</p>\n<p><strong>OpenAI</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-b7b96464bf73\" class=\"internal-link\" rel=\"noopener noreferrer\">published technical details</a> on <strong>Codex CLI</strong> architecture, revealing how its agentic loop works alongside mentions of <strong>GPT-5.2</strong> and <strong>Claude Code with Opus 4.5</strong> reaching new capability levels. On the regulatory front:</p>\n<ul>\n<li><strong>EU</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-5652f145be41\" class=\"internal-link\" rel=\"noopener noreferrer\">launched formal investigation</a> into <strong>xAI</strong> over <strong>Grok</strong> generating sexualized deepfakes under the Digital Services Act</li>\n<li><strong>Meta</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-b9a395e956f2\" class=\"internal-link\" rel=\"noopener noreferrer\">paused teen access</a> to AI chatbot characters amid safety concerns</li>\n<li><strong>US DOT</strong> using <strong>Gemini</strong> to <a href=\"/?date=2026-01-27&amp;category=news#item-2e3e59ab1ff3\" class=\"internal-link\" rel=\"noopener noreferrer\">draft safety rules</a> sparked criticism</li>\n<li>Multiple US states <a href=\"/?date=2026-01-27&amp;category=news#item-ad5acd8ec1cc\" class=\"internal-link\" rel=\"noopener noreferrer\">considering datacenter moratoriums</a> due to energy concerns</li>\n</ul>\n<p><strong>Synthesia</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-9078c15ec00c\" class=\"internal-link\" rel=\"noopener noreferrer\">nearly doubled its valuation</a> to <strong>$4 billion</strong>, while major retailers including <strong>Walmart</strong>, <strong>Target</strong>, and <strong>Etsy</strong> <a href=\"/?date=2026-01-27&amp;category=news#item-d157155f4509\" class=\"internal-link\" rel=\"noopener noreferrer\">expanded agentic AI commerce integrations</a>.</p>",
  "themes": [
    {
      "name": "AI Infrastructure & Investment",
      "description": "Major capital deployment into AI compute infrastructure, including NVIDIA's $2B CoreWeave investment and state-level pushback on datacenter expansion",
      "item_count": 3,
      "example_items": [],
      "importance": 82.0
    },
    {
      "name": "AI Safety & Regulation",
      "description": "Regulatory investigations and safety decisions affecting major AI platforms, including EU's xAI probe and Meta's teen access restrictions",
      "item_count": 4,
      "example_items": [],
      "importance": 74.0
    },
    {
      "name": "New Model Releases & Technical Progress",
      "description": "Frontier model launches including Microsoft's robotics VLA model, NVIDIA's climate AI stack, and technical insights into agentic AI architecture",
      "item_count": 4,
      "example_items": [],
      "importance": 81.0
    },
    {
      "name": "Commercial AI Deployment",
      "description": "Enterprise and retail adoption of AI tools, from corporate video avatars to in-conversation shopping experiences",
      "item_count": 3,
      "example_items": [],
      "importance": 62.0
    },
    {
      "name": "Government & Public Sector AI",
      "description": "Government use of AI for rulemaking and plans to leverage public data assets for AI development",
      "item_count": 2,
      "example_items": [],
      "importance": 60.0
    }
  ],
  "total_items": 15,
  "items": [
    {
      "id": "4d36790c7226",
      "title": "Microsoft Launches Vision-Language-Action Model for Robots",
      "content": "Designed to improve robots’ reasoning capabilities, Rho-alpha marks Microsoft’s offering in the growing field of physical AI.",
      "url": "https://aibusiness.com/robotics/microsoft-launches-vision-language-action-model-for-robots",
      "author": "Scarlett Evans",
      "published": "2026-01-26T15:10:41",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Microsoft launched Rho-alpha, a vision-language-action model designed to improve robots' reasoning capabilities. The model represents Microsoft's entry into the competitive physical AI space for robotics applications.",
      "importance_score": 83.0,
      "reasoning": "Significant new model release from a major lab in the emerging physical AI/robotics field. VLA models are a frontier research area, and Microsoft's entry increases competition and advances embodied AI capabilities.",
      "themes": [
        "Physical AI",
        "Robotics",
        "Foundation Models",
        "Microsoft AI"
      ],
      "continuation": null,
      "summary_html": "<p>Microsoft launched Rho-alpha, a vision-language-action model designed to improve robots' reasoning capabilities. The model represents Microsoft's entry into the competitive physical AI space for robotics applications.</p>",
      "content_html": "<p>Designed to improve robots’ reasoning capabilities, Rho-alpha marks Microsoft’s offering in the growing field of physical AI.</p>"
    },
    {
      "id": "d2c712b7696f",
      "title": "Nvidia Invests $2B in CoreWeave, Expands Partnership",
      "content": "The vendors expanded their partnership, boosting CoreWeave's standing in the AI infrastructure market.",
      "url": "https://aibusiness.com/data-centers/nvidia-invests-2b-in-coreweave-expands-partnership",
      "author": "Esther Shittu",
      "published": "2026-01-26T18:06:34",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "NVIDIA invested $2 billion in CoreWeave, significantly expanding their partnership and boosting CoreWeave's position in the AI infrastructure market. The investment reinforces CoreWeave's role as a key GPU cloud provider for AI workloads.",
      "importance_score": 82.0,
      "reasoning": "Major infrastructure investment from the leading AI chip company into critical cloud infrastructure. $2B commitment signals strategic importance of specialized AI compute providers and shapes competitive landscape.",
      "themes": [
        "AI Infrastructure",
        "Investment",
        "Cloud Computing",
        "NVIDIA Ecosystem"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA invested $2 billion in CoreWeave, significantly expanding their partnership and boosting CoreWeave's position in the AI infrastructure market. The investment reinforces CoreWeave's role as a key GPU cloud provider for AI workloads.</p>",
      "content_html": "<p>The vendors expanded their partnership, boosting CoreWeave's standing in the AI infrastructure market.</p>"
    },
    {
      "id": "ed31d1a4a250",
      "title": "NVIDIA Revolutionizes Climate Tech with ‘Earth-2’: The World’s First Fully Open Accelerated AI Weather Stack",
      "content": "For decades, predicting the weather has been the exclusive domain of massive government supercomputers running complex physics-based equations. NVIDIA has shattered that barrier with the release of the Earth-2 family of open models and tools for AI weather and climate prediction accessible to virtually anyone, from tech startups to national meteorological agencies.\n\n\n\nIn a move that democratizes climate science, NVIDIA unveiled 3 groundbreaking new models powered by novel architectures: Atlas, StormScope, and HealDA. These tools promise to accelerate forecasting speeds by orders of magnitude while delivering accuracy that rivals or exceeds traditional methods.\n\n\n\n\n\n\nThe Democratization of Weather Intelligence\n\n\n\nHistorically, running a high-fidelity weather model required infrastructure that only a few countries could afford. NVIDIA’s Earth-2 changes the calculus by offering an ‘open stack’, a collection of pretrained models, inference libraries, and customization recipes available on platforms like GitHub and Hugging Face.\n\n\n\nMike Pritchard, Director of Climate Simulation at NVIDIA, emphasized that NVIDIA is not becoming a weather service provider. Instead, they are building the &#8220;foundational building blocks&#8221; that allow nations and companies to build their own sovereign forecasting systems.\n\n\n\n&#8220;Sovereignty matters. Weather is a national security issue&#8230; That&#8217;s why we&#8217;ve built Earth-2, the world&#8217;s first fully open production-ready AI weather stack.&#8221;&nbsp;&#8211; Mike Pritchard, NVIDIA\n\n\n\nMeet the New Heavyweights: Atlas, StormScope, and HealDA\n\n\n\nThe announcement introduces 3 specific models that address different stages of the forecasting pipeline, from processing messy data to predicting storms weeks in advance.\n\n\n\n1. Earth-2 Medium Range (Powered by Atlas)\n\n\n\nTargeting the 15-day forecast window, this model uses a new architecture called Atlas. It predicts over 70 weather variables, including wind, humidity, and pressure, at high accuracy.\n\n\n\n\nPerformance: On standard industry benchmarks, Atlas has been shown to outperform GenCast, the current leading open model, across the vast majority of variables.\n\n\n\nThe Shift: It represents a return to &#8220;simple, scalable Transformer architectures,&#8221; moving away from niche, hand-tailored AI designs.\n\n\n\n\nRead the research paper here.\n\n\n2. Earth-2 Nowcasting (Powered by StormScope)\n\n\n\nThis is a game-changer for immediate disaster response. Powered by StormScope, this generative AI model focuses on the 0-to-6-hour window, providing kilometer-scale resolution of local storms.\n\n\n\n\nWhy it matters: It is the first AI model to outperform traditional physics-based methods for short-term precipitation forecasting.\n\n\n\nSpeed: It generates hazardous weather predictions in minutes, giving emergency responders critical time to act.\n\n\n\nSovereignty: Because it trains directly on geostationary satellite imagery rather than region-specific physics outputs, it can be deployed by any nation with good satellite coverage.\n\n\n\n\nRead the research paper.\n\n\n3. Earth-2 Global Data Assimilation (Powered by HealDA)\n\n\n\nOften the unsung hero of forecasting, &#8220;data assimilation&#8221; is the process of combining messy satellite and balloon data into a coherent snapshot of the atmosphere to start a forecast.\n\n\n\n\nThe Breakthrough: Traditional assimilation consumes nearly 50% of supercomputing cycles. NVIDIA’s HealDA architecture accomplishes this task in minutes on GPUs rather than hours on supercomputers.\n\n\n\nResult: When combined with the Medium Range model, it produces the most skillful predictions ever seen from an entirely AI-based pipeline.\n\n\n\n\nRead the research paper\n\n\nReal-World Impact: From Solar Power to Hurricane Risk\n\n\n\nThe Earth-2 stack is already in use by major global players, proving that AI weather forecasting is ready for commercial and operational prime time.\n\n\n\n\nRenewable Energy: TotalEnergies and GCL (a major solar material producer) are using Earth-2 to predict solar and wind variability. For solar farms, accurate cloud cover prediction can significantly impact energy market trading.\n\n\n\nIsrael Meteorological Service: Using the CorrDiff model (part of the Earth-2 family), they have achieved a 90% reduction in compute time while generating high-resolution forecasts up to eight times daily.\n\n\n\nInsurance &amp; Risk: AXA and S&amp;P Global Energy are leveraging the speed of Earth-2 to run thousands of &#8220;counterfactual&#8221; scenarios. By simulating thousands of years of hypothetical hurricane data, they can better understand rare, high-impact climate events that haven&#8217;t happened yet but might.\n\n\n\nDaily Operations: Brightband, an AI weather tool provider, is already integrating Earth-2 Medium Range to issue daily global forecasts.\n\n\n\n\nThe Bottom Line\n\n\n\nNVIDIA Earth-2 is not just a technical upgrade; it is a structural shift in how humans interact with the climate. By reducing the barrier to entry, shifting from multimillion-dollar supercomputers to accessible GPU-accelerated AI, NVIDIA is enabling a future where hyper-local, high-accuracy weather prediction is ubiquitous.\n\n\n\nAs extreme weather events become more frequent, tools like StormScope and Atlas will likely become essential infrastructure for governments and industries worldwide.\n\n\n\nEarth-2 Medium Range and Nowcasting are available on GitHub, Hugging Face, and NVIDIA Earth2Studio. Earth-2 Global Data Assimilation is expected to be released later this year.\n\n\n\nTo learn more about getting started with these models, developers can visit the NVIDIA Earth-2 technical blog. Earth-2 Medium Range [Read the research paper], Earth-2 Nowcasting [Read the research paper], and Earth-2 Global Data Assimilation [Read the research paper].\nThe post NVIDIA Revolutionizes Climate Tech with ‘Earth-2’: The World’s First Fully Open Accelerated AI Weather Stack appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/26/nvidia-revolutionizes-climate-tech-with-earth-2-the-worlds-first-fully-open-accelerated-ai-weather-stack/",
      "author": "Jean-marc Mommessin",
      "published": "2026-01-26T15:43:58",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Machine Learning",
        "New Releases",
        "Physical AI",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "NVIDIA released Earth-2, the first fully open accelerated AI weather prediction stack, including three new models: Atlas, StormScope, and HealDA. The release democratizes climate science by making high-fidelity weather forecasting accessible without supercomputer infrastructure.",
      "importance_score": 81.0,
      "reasoning": "Major open model release from NVIDIA that democratizes an important scientific domain. Novel architectures and open availability represent meaningful contribution to AI for science, though domain-specific rather than general-purpose.",
      "themes": [
        "Open Source AI",
        "Climate AI",
        "Scientific AI",
        "NVIDIA"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA released Earth-2, the first fully open accelerated AI weather prediction stack, including three new models: Atlas, StormScope, and HealDA. The release democratizes climate science by making high-fidelity weather forecasting accessible without supercomputer infrastructure.</p>",
      "content_html": "<p>For decades, predicting the weather has been the exclusive domain of massive government supercomputers running complex physics-based equations. NVIDIA has shattered that barrier with the release of the Earth-2 family of open models and tools for AI weather and climate prediction accessible to virtually anyone, from tech startups to national meteorological agencies.</p>\n<p>In a move that democratizes climate science, NVIDIA unveiled 3 groundbreaking new models powered by novel architectures: Atlas, StormScope, and HealDA. These tools promise to accelerate forecasting speeds by orders of magnitude while delivering accuracy that rivals or exceeds traditional methods.</p>\n<p>The Democratization of Weather Intelligence</p>\n<p>Historically, running a high-fidelity weather model required infrastructure that only a few countries could afford. NVIDIA’s Earth-2 changes the calculus by offering an ‘open stack’, a collection of pretrained models, inference libraries, and customization recipes available on platforms like GitHub and Hugging Face.</p>\n<p>Mike Pritchard, Director of Climate Simulation at NVIDIA, emphasized that NVIDIA is not becoming a weather service provider. Instead, they are building the “foundational building blocks” that allow nations and companies to build their own sovereign forecasting systems.</p>\n<p>“Sovereignty matters. Weather is a national security issue… That’s why we’ve built Earth-2, the world’s first fully open production-ready AI weather stack.”&nbsp;– Mike Pritchard, NVIDIA</p>\n<p>Meet the New Heavyweights: Atlas, StormScope, and HealDA</p>\n<p>The announcement introduces 3 specific models that address different stages of the forecasting pipeline, from processing messy data to predicting storms weeks in advance.</p>\n<p>1. Earth-2 Medium Range (Powered by Atlas)</p>\n<p>Targeting the 15-day forecast window, this model uses a new architecture called Atlas. It predicts over 70 weather variables, including wind, humidity, and pressure, at high accuracy.</p>\n<p>Performance: On standard industry benchmarks, Atlas has been shown to outperform GenCast, the current leading open model, across the vast majority of variables.</p>\n<p>The Shift: It represents a return to “simple, scalable Transformer architectures,” moving away from niche, hand-tailored AI designs.</p>\n<p>Read the research paper here.</p>\n<p>2. Earth-2 Nowcasting (Powered by StormScope)</p>\n<p>This is a game-changer for immediate disaster response. Powered by StormScope, this generative AI model focuses on the 0-to-6-hour window, providing kilometer-scale resolution of local storms.</p>\n<p>Why it matters: It is the first AI model to outperform traditional physics-based methods for short-term precipitation forecasting.</p>\n<p>Speed: It generates hazardous weather predictions in minutes, giving emergency responders critical time to act.</p>\n<p>Sovereignty: Because it trains directly on geostationary satellite imagery rather than region-specific physics outputs, it can be deployed by any nation with good satellite coverage.</p>\n<p>Read the research paper.</p>\n<p>3. Earth-2 Global Data Assimilation (Powered by HealDA)</p>\n<p>Often the unsung hero of forecasting, “data assimilation” is the process of combining messy satellite and balloon data into a coherent snapshot of the atmosphere to start a forecast.</p>\n<p>The Breakthrough: Traditional assimilation consumes nearly 50% of supercomputing cycles. NVIDIA’s HealDA architecture accomplishes this task in minutes on GPUs rather than hours on supercomputers.</p>\n<p>Result: When combined with the Medium Range model, it produces the most skillful predictions ever seen from an entirely AI-based pipeline.</p>\n<p>Read the research paper</p>\n<p>Real-World Impact: From Solar Power to Hurricane Risk</p>\n<p>The Earth-2 stack is already in use by major global players, proving that AI weather forecasting is ready for commercial and operational prime time.</p>\n<p>Renewable Energy: TotalEnergies and GCL (a major solar material producer) are using Earth-2 to predict solar and wind variability. For solar farms, accurate cloud cover prediction can significantly impact energy market trading.</p>\n<p>Israel Meteorological Service: Using the CorrDiff model (part of the Earth-2 family), they have achieved a 90% reduction in compute time while generating high-resolution forecasts up to eight times daily.</p>\n<p>Insurance &amp; Risk: AXA and S&amp;P Global Energy are leveraging the speed of Earth-2 to run thousands of “counterfactual” scenarios. By simulating thousands of years of hypothetical hurricane data, they can better understand rare, high-impact climate events that haven’t happened yet but might.</p>\n<p>Daily Operations: Brightband, an AI weather tool provider, is already integrating Earth-2 Medium Range to issue daily global forecasts.</p>\n<p>The Bottom Line</p>\n<p>NVIDIA Earth-2 is not just a technical upgrade; it is a structural shift in how humans interact with the climate. By reducing the barrier to entry, shifting from multimillion-dollar supercomputers to accessible GPU-accelerated AI, NVIDIA is enabling a future where hyper-local, high-accuracy weather prediction is ubiquitous.</p>\n<p>As extreme weather events become more frequent, tools like StormScope and Atlas will likely become essential infrastructure for governments and industries worldwide.</p>\n<p>Earth-2 Medium Range and Nowcasting are available on GitHub, Hugging Face, and NVIDIA Earth2Studio. Earth-2 Global Data Assimilation is expected to be released later this year.</p>\n<p>To learn more about getting started with these models, developers can visit the NVIDIA Earth-2 technical blog. Earth-2 Medium Range [Read the research paper], Earth-2 Nowcasting [Read the research paper], and Earth-2 Global Data Assimilation [Read the research paper].</p>\n<p>The post NVIDIA Revolutionizes Climate Tech with ‘Earth-2’: The World’s First Fully Open Accelerated AI Weather Stack appeared first on MarkTechPost.</p>"
    },
    {
      "id": "b7b96464bf73",
      "title": "OpenAI spills technical details about how its AI coding agent works",
      "content": "On Friday, OpenAI engineer Michael Bolin published a detailed technical breakdown of how the company's Codex CLI coding agent works internally, offering developers insight into AI coding tools that can write code, run tests, and fix bugs with human supervision. It complements our article in December on how AI agents work by filling in technical details on how OpenAI implements its \"agentic loop.\"\nAI coding agents are having something of a \"ChatGPT moment,\" where Claude Code with Opus 4.5 and Codex with GPT-5.2 have reached a new level of usefulness for rapidly coding up prototypes, interfaces, and churning out boilerplate code. The timing of OpenAI's post details the design philosophy behind Codex just as AI agents are becoming more practical tools for everyday work.\nThese tools aren't perfect and remain controversial for some software developers. While OpenAI has previously told Ars Technica that it uses Codex as a coding tool to help develop the Codex product itself, we also discovered, through hands-on experience, that these tools can be astonishingly fast at simple tasks but remain brittle beyond their training data and require human oversight for production work. The rough framework of a project tends to come fast and feels magical, but filling in the details involves tedious debugging and workarounds for limitations the agent cannot overcome on its own.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/01/openai-spills-technical-details-about-how-its-ai-coding-agent-works/",
      "author": "Benj Edwards",
      "published": "2026-01-26T23:05:17",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "agentic AI",
        "AI agents",
        "AI coding",
        "AI development tools",
        "API",
        "code agents",
        "Codex",
        "Developer Tools",
        "large language models",
        "machine learning",
        "openai",
        "Programming",
        "prompt caching"
      ],
      "summary": "OpenAI engineer published detailed technical breakdown of Codex CLI coding agent architecture, revealing how its 'agentic loop' works internally. The article notes AI coding agents are having a 'ChatGPT moment' with Claude Code (Opus 4.5) and Codex (GPT-5.2) reaching new practical usefulness levels.",
      "importance_score": 78.0,
      "reasoning": "Provides valuable technical insights into frontier AI agent design from a major lab. References to GPT-5.2 and Opus 4.5 suggest these next-gen models are now deployed, and the timing coincides with coding agents becoming mainstream tools.",
      "themes": [
        "AI Agents",
        "Developer Tools",
        "Technical Architecture",
        "Model Capabilities"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI engineer published detailed technical breakdown of Codex CLI coding agent architecture, revealing how its 'agentic loop' works internally. The article notes AI coding agents are having a 'ChatGPT moment' with Claude Code (Opus 4.5) and Codex (GPT-5.2) reaching new practical usefulness levels.</p>",
      "content_html": "<p>On Friday, OpenAI engineer Michael Bolin published a detailed technical breakdown of how the company's Codex CLI coding agent works internally, offering developers insight into AI coding tools that can write code, run tests, and fix bugs with human supervision. It complements our article in December on how AI agents work by filling in technical details on how OpenAI implements its \"agentic loop.\"</p>\n<p>AI coding agents are having something of a \"ChatGPT moment,\" where Claude Code with Opus 4.5 and Codex with GPT-5.2 have reached a new level of usefulness for rapidly coding up prototypes, interfaces, and churning out boilerplate code. The timing of OpenAI's post details the design philosophy behind Codex just as AI agents are becoming more practical tools for everyday work.</p>\n<p>These tools aren't perfect and remain controversial for some software developers. While OpenAI has previously told Ars Technica that it uses Codex as a coding tool to help develop the Codex product itself, we also discovered, through hands-on experience, that these tools can be astonishingly fast at simple tasks but remain brittle beyond their training data and require human oversight for production work. The rough framework of a project tends to come fast and feels magical, but filling in the details involves tedious debugging and workarounds for limitations the agent cannot overcome on its own.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "5652f145be41",
      "title": "EU launches formal investigation of xAI over Grok's sexualized deepfakes",
      "content": "The EU has launched a formal investigation into Elon Musk’s xAI following a public outcry over how its Grok chatbot spread sexualized images of women and children.\nThe billionaire entrepreneur has come under scrutiny from regulators around the world this month after people began using Grok to generate deepfakes of people without consent. The images were posted on the X social network as well as the separate Grok app, both of which are run by xAI.\nThe probe, announced on Monday under the EU’s Digital Services Act, will assess if xAI tried to mitigate the risks of deploying Grok’s tools on X and the proliferation of content that “may amount to child sexual abuse material.”Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/eu-launches-formal-investigation-of-xai-over-groks-sexualized-deepfakes/",
      "author": "Barbara Moens, Financial Times",
      "published": "2026-01-26T14:17:46",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Policy",
        "syndication",
        "xAI Grok"
      ],
      "summary": "The EU launched a formal investigation into Elon Musk's xAI under the Digital Services Act over Grok generating sexualized deepfakes of women and children without consent. The probe will assess whether xAI implemented adequate safeguards before deploying Grok's image generation on X.",
      "importance_score": 76.0,
      "reasoning": "Major regulatory action against a leading AI company under significant legislation. Highlights ongoing tension between generative AI capabilities and safety guardrails, with potential for substantial enforcement.",
      "themes": [
        "AI Regulation",
        "AI Safety",
        "Content Moderation",
        "EU DSA"
      ],
      "continuation": null,
      "summary_html": "<p>The EU launched a formal investigation into Elon Musk's xAI under the Digital Services Act over Grok generating sexualized deepfakes of women and children without consent. The probe will assess whether xAI implemented adequate safeguards before deploying Grok's image generation on X.</p>",
      "content_html": "<p>The EU has launched a formal investigation into Elon Musk’s xAI following a public outcry over how its Grok chatbot spread sexualized images of women and children.</p>\n<p>The billionaire entrepreneur has come under scrutiny from regulators around the world this month after people began using Grok to generate deepfakes of people without consent. The images were posted on the X social network as well as the separate Grok app, both of which are run by xAI.</p>\n<p>The probe, announced on Monday under the EU’s Digital Services Act, will assess if xAI tried to mitigate the risks of deploying Grok’s tools on X and the proliferation of content that “may amount to child sexual abuse material.”Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "a761f2ab6fe3",
      "title": "EU launches inquiry into X over sexually explicit images made by Grok AI",
      "content": "Investigation comes after Elon Musk’s firm sparked outrage by allowing users to ‘strip’ photos of women and childrenThe European Commission has launched an investigation into Elon Musk’s X over the production of sexually explicit images and the spreading of possible child sexual abuse material by the platform’s AI chatbot, Grok.The formal inquiry, launched on Monday, also extends an investigation into X’s recommender systems, algorithms that help users discover new content. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/26/eu-launches-inquiry-into-x-over-sexually-explicit-images-made-by-grok-ai",
      "author": "Jennifer Rankin in Brussels",
      "published": "2026-01-26T14:27:44",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Grok AI",
        "Elon Musk",
        "X",
        "AI (artificial intelligence)",
        "Computing",
        "Internet",
        "Technology",
        "European Commission",
        "European Union",
        "Europe",
        "Law",
        "World news",
        "Children",
        "Women"
      ],
      "summary": "The European Commission formally opened an investigation into X over Grok AI producing sexually explicit images and potential CSAM. The inquiry also extends to examine X's recommender systems and content algorithms.",
      "importance_score": 74.0,
      "reasoning": "Same investigation as the other EU/xAI article but with additional detail about recommender systems probe. Significant regulatory action but partially duplicative coverage.",
      "themes": [
        "AI Regulation",
        "Content Safety",
        "EU DSA",
        "Recommender Systems"
      ],
      "continuation": null,
      "summary_html": "<p>The European Commission formally opened an investigation into X over Grok AI producing sexually explicit images and potential CSAM. The inquiry also extends to examine X's recommender systems and content algorithms.</p>",
      "content_html": "<p>Investigation comes after Elon Musk’s firm sparked outrage by allowing users to ‘strip’ photos of women and childrenThe European Commission has launched an investigation into Elon Musk’s X over the production of sexually explicit images and the spreading of possible child sexual abuse material by the platform’s AI chatbot, Grok.The formal inquiry, launched on Monday, also extends an investigation into X’s recommender systems, algorithms that help users discover new content. Continue reading...</p>"
    },
    {
      "id": "2e3e59ab1ff3",
      "title": "“Wildly irresponsible”: DOT's use of AI to draft safety rules sparks concerns",
      "content": "The US Department of Transportation apparently thinks it's a good idea to use artificial intelligence to draft rules impacting the safety of airplanes, cars, and pipelines, a ProPublica investigation revealed Monday.\nIt could be a problem if DOT becomes the first agency to use AI to draft rules, ProPublica pointed out, since AI is known to confidently get things wrong and hallucinate fabricated information. Staffers fear that any failure to catch AI errors could result in flawed laws, leading to lawsuits, injuries, or even deaths in the transportation system.\nBut the DOT's top lawyer, Gregory Zerzan, isn't worried about that, December meeting notes revealed, because the point isn't for AI to be perfect. It's for AI to help speed up the rulemaking process, so that rules that take weeks or months to draft can instead be written within 30 days. According to Zerzan, DOT's preferred tool, Google Gemini, can draft rules in under 30 minutes.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/wildly-irresponsible-dots-use-of-ai-to-draft-safety-rules-sparks-concerns/",
      "author": "Ashley Belanger",
      "published": "2026-01-26T20:13:47",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Google",
        "Policy",
        "AI hallucination",
        "Artificial Intelligence",
        "department of transportation",
        "gemini",
        "google"
      ],
      "summary": "US Department of Transportation is using Google Gemini to draft safety rules for aviation, vehicles, and pipelines, becoming potentially the first agency to use AI for rulemaking. Staffers and experts express concern that AI hallucinations in regulatory text could lead to flawed laws, injuries, or deaths.",
      "importance_score": 68.0,
      "reasoning": "Notable policy development highlighting risks of AI in high-stakes government functions. Raises legitimate concerns about AI reliability in critical applications but isn't a frontier AI development itself.",
      "themes": [
        "AI Policy",
        "Government AI Use",
        "AI Safety",
        "Regulation"
      ],
      "continuation": null,
      "summary_html": "<p>US Department of Transportation is using Google Gemini to draft safety rules for aviation, vehicles, and pipelines, becoming potentially the first agency to use AI for rulemaking. Staffers and experts express concern that AI hallucinations in regulatory text could lead to flawed laws, injuries, or deaths.</p>",
      "content_html": "<p>The US Department of Transportation apparently thinks it's a good idea to use artificial intelligence to draft rules impacting the safety of airplanes, cars, and pipelines, a ProPublica investigation revealed Monday.</p>\n<p>It could be a problem if DOT becomes the first agency to use AI to draft rules, ProPublica pointed out, since AI is known to confidently get things wrong and hallucinate fabricated information. Staffers fear that any failure to catch AI errors could result in flawed laws, leading to lawsuits, injuries, or even deaths in the transportation system.</p>\n<p>But the DOT's top lawyer, Gregory Zerzan, isn't worried about that, December meeting notes revealed, because the point isn't for AI to be perfect. It's for AI to help speed up the rulemaking process, so that rules that take weeks or months to draft can instead be written within 30 days. According to Zerzan, DOT's preferred tool, Google Gemini, can draft rules in under 30 minutes.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "9078c15ec00c",
      "title": "UK maker of AI avatars nearly doubles valuation to $4bn after funding round",
      "content": "Synthesia makes digital presenters for clients to use in corporate videos and counts 70% of FTSE 100 as customersA British AI startup that makes realistic video avatars has almost doubled its valuation to $4bn (£3bn), in a boost for the UK technology sector.Synthesia was valued at $2.1bn last year and moved into new offices in central London, marking the moment with a ceremony attended by the Sadiq Khan, the city’s mayor, and Peter Kyle, then technology secretary. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/26/uk-ai-startup-synthesia-almost-doubles-valuation-4bn-funding-round-corporate-video-avatars",
      "author": "Dan Milmo Global technology editor",
      "published": "2026-01-26T09:00:33",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Technology startups",
        "Technology sector",
        "Technology",
        "Business",
        "Computing",
        "UK news",
        "Venture capital"
      ],
      "summary": "UK AI avatar startup Synthesia nearly doubled its valuation to $4 billion in a new funding round. The company creates realistic video avatars for corporate clients and counts 70% of the FTSE 100 as customers.",
      "importance_score": 68.0,
      "reasoning": "Significant funding milestone for an AI video generation company, demonstrating continued investor confidence in generative AI applications. However, focuses on enterprise AI tools rather than frontier research.",
      "themes": [
        "AI Funding",
        "Generative AI",
        "UK Tech",
        "Enterprise AI"
      ],
      "continuation": null,
      "summary_html": "<p>UK AI avatar startup Synthesia nearly doubled its valuation to $4 billion in a new funding round. The company creates realistic video avatars for corporate clients and counts 70% of the FTSE 100 as customers.</p>",
      "content_html": "<p>Synthesia makes digital presenters for clients to use in corporate videos and counts 70% of FTSE 100 as customersA British AI startup that makes realistic video avatars has almost doubled its valuation to $4bn (£3bn), in a boost for the UK technology sector.Synthesia was valued at $2.1bn last year and moved into new offices in central London, marking the moment with a ceremony attended by the Sadiq Khan, the city’s mayor, and Peter Kyle, then technology secretary. Continue reading...</p>"
    },
    {
      "id": "b9a395e956f2",
      "title": "Meta Pauses Teen Access to its AI Chatbot Characters",
      "content": "The move comes amid growing concern about interactions between chatbots and teenagers.",
      "url": "https://aibusiness.com/chatbot/meta-pauses-teen-access-to-its-ai-chatbot-characters",
      "author": "Graham Hope",
      "published": "2026-01-26T15:49:52",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Meta paused teen access to its AI chatbot characters feature amid growing concerns about interactions between AI chatbots and minors. The move reflects increasing scrutiny over AI safety for young users.",
      "importance_score": 67.0,
      "reasoning": "Important AI safety decision from a major platform affecting millions of users. Signals industry responsiveness to child safety concerns but is a product decision rather than frontier capability news.",
      "themes": [
        "AI Safety",
        "Child Safety",
        "Meta AI",
        "Platform Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Meta paused teen access to its AI chatbot characters feature amid growing concerns about interactions between AI chatbots and minors. The move reflects increasing scrutiny over AI safety for young users.</p>",
      "content_html": "<p>The move comes amid growing concern about interactions between chatbots and teenagers.</p>"
    },
    {
      "id": "ad5acd8ec1cc",
      "title": "Georgia leads push to ban datacenters used to power America’s AI boom",
      "content": "Southern state becoming ground zero in fight against rapid growth of facilities using huge amounts of energy and waterLawmakers in several states are exploring passing laws that would put statewide bans in place on building new datacenters as the issue of the power-hungry facilities has moved to the center of economic and environmental concerns in the US.In Georgia a state lawmaker has introduced a bill proposing what could become the first statewide moratorium on new datacenters in America. The bill is one of at least three statewide moratoriums on datacenters introduced in state legislatures in the last week as Maryland and Oklahoma lawmakers are also considering similar measures. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/26/georgia-datacenters-ai-ban",
      "author": "Timothy Prattin Atlanta",
      "published": "2026-01-26T16:07:27",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Georgia",
        "US news",
        "Computing",
        "World news",
        "Technology"
      ],
      "summary": "Georgia introduced what could become the first statewide moratorium on new datacenters in America, with Maryland and Oklahoma considering similar measures. The push reflects growing environmental and economic concerns about power-hungry AI infrastructure facilities.",
      "importance_score": 62.0,
      "reasoning": "Important infrastructure policy development that could constrain AI compute capacity growth. Multiple states moving simultaneously signals a meaningful trend, though indirect impact on frontier AI research.",
      "themes": [
        "AI Infrastructure",
        "Energy Policy",
        "Environmental Concerns",
        "Data Centers"
      ],
      "continuation": null,
      "summary_html": "<p>Georgia introduced what could become the first statewide moratorium on new datacenters in America, with Maryland and Oklahoma considering similar measures. The push reflects growing environmental and economic concerns about power-hungry AI infrastructure facilities.</p>",
      "content_html": "<p>Southern state becoming ground zero in fight against rapid growth of facilities using huge amounts of energy and waterLawmakers in several states are exploring passing laws that would put statewide bans in place on building new datacenters as the issue of the power-hungry facilities has moved to the center of economic and environmental concerns in the US.In Georgia a state lawmaker has introduced a bill proposing what could become the first statewide moratorium on new datacenters in America. The bill is one of at least three statewide moratoriums on datacenters introduced in state legislatures in the last week as Maryland and Oklahoma lawmakers are also considering similar measures. Continue reading...</p>"
    },
    {
      "id": "d157155f4509",
      "title": "Retailers examine options for on-AI retail",
      "content": " Big retailers are committing more heavily to agentic AI-led commerce, and accepting some loss of customer proximity and data control in the process. \n As reported by Retail Dive, the opening weeks of 2026 have seen Etsy, Target and Walmart push  product ranges onto third-party AI platforms, forming new partnerships with Google’s Gemini and Microsoft’s Copilot, after last year&#8217;s collaborations with OpenAI’s ChatGPT. These let consumers purchase goods inside the AI&#8217;s conversation interface. \n Amazon and Walmart have been investing in their own consumer-facing AI assistants, Rufus and Sparky respectively to change how shoppers interact with their brands. \n Agentic AI is beginning to redraw direct-to-consumer engagement, and industry figures regard this trend as an important moment in online retail. “I think this has the potential to disrupt retail in the same way the internet once did,” Kartik Hosanagar, a marketing professor at the Wharton School of the University of Pennsylvania, told the website&#8217;s reporters. \n Partnering with AIs like ChatGPT or Gemini engages consumers wherever they happen to be and may choose to shop. Adobe’s 2025 Holiday Shopping report found that AI-driven traffic to US e-commerce sites grew 758% year on year between in November 2025, and Cyber Monday saw a 670% increase in AI-referred retail visits. \n &#8220;What we expect is a deepening of consumer engagement,&#8221; Katherine Black, a partner at Kearney specialising in food, drug and mass-market retail, said in an email to Retail Dive. &#8220;More shoppers will rely on AI for purchasing, and across a wider range of missions. As retailers’ capabilities within these tools improve, adoption should accelerate further.&#8221; \n Meeting customers on AI platforms comes with trade-offs, according to industry observers, with questions around data ownership and the risk that retailers are sidelined. 81% of retail executives believe generative AI will erode brand loyalty by 2027, according to Deloitte’s 2026 Retail Industry Global Outlook, published earlier this month. \n Retailers&#8217; websites or apps provide a stream of behavioural data, and if discovery, evaluation, and purchase happen externally, any insight doesn&#8217;t reach the retailer. &#8220;This fundamentally changes where power sits,&#8221; Hosanagar said. &#8220;Control over the agent increasingly means control over the customer relationship.&#8221; \n Google and Alphabet CEO Sundar Pichai has unveiled new commerce tools for Gemini, outlining how it will support customers from discovery to final purchase. Nikki Baird, vice president of strategy and product at Aptos, says this raises difficult questions. &#8220;What he’s describing is Google owning the data across discovery, decision and transaction. Even if some information is shared back, missing context from those stages leaves retailers with a much poorer understanding of their customers.&#8221; \n Pichai reassured retailers collaboration remains central to Google. &#8220;From nearly three decades of working with retailers, we know success only comes when we work together,&#8221; he told an NRF audience. &#8220;Our aim is to use our full technology stack to help shape the next era of retail.&#8221; \n Yet agentic systems&#8217; features like instant checkout absorb the shopping experience into one platform. &#8220;If research, discovery and purchase all happen on OpenAI rather than Walmart.com, you’re effectively giving away the brand experience. At that point, the retailer risks becoming little more than a fulfilment operation,&#8221; Hosanagar said. \n Amazon has not announced plans to sell directly through ChatGPT, doubling down on its own AI initiatives. Earlier this month, the company launched a dedicated site for Alexa+, its generative AI assistant that helps users research and plan purchases. \n Yet participation in third-party AI commerce may become unavoidable. When OpenAI launched its Instant Checkout feature on ChatGPT last September, it suggested that enabling the function could influence how merchants are ranked in search results, in addition to price and product quality. Uploading product catalogues to AI chat platforms may be the first step in a transformation of online retail. \n According to Deloitte, roughly half of retail executives expect the current multi-stage shopping process to reduce to a single AI-driven interaction by 2027. For now the industry remains at an early stage of any transition. &#8220;The real inflection point is when consumers rely on an autonomous agent to shop on their behalf,&#8221; Hosanagar told Retail Dive. \n &#8220;Retailers will engage less with humans directly and more with their representatives — AI agents. That agent processes information differently, requires data in new formats and responds to persuasion in ways unlike a person.&#8221; \n Today, consumers can access ChatGPT on their phones while in-store, effectively consulting an always-available expert. &#8220;It&#8217;s not just the internet in your pocket,&#8221; Baird told Retail Dive. &#8220;It&#8217;s like having a highly knowledgeable store associate who knows every retailer.&#8221; \n This may prompt retailers to equip frontline staff with their own AI tools, offering instant insight into customer preferences or shopping history. Alternatively, a retailer’s AI agent could proactively notify customers when a favoured item is back in stock, helping associates convert interest into sales. &#8220;The goal is to enable store associates to perform at their best,&#8221; Baird said. \n(Image source: &#8220;Shopping trauma!&#8221; by Elsie esq. is licensed under CC BY 2.0.)\n&nbsp;\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Retailers examine options for on-AI retail appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/retailers-examine-options-for-on-ai-retail/",
      "author": "AI News",
      "published": "2026-01-26T16:40:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Retail & Logistics AI",
        "Service Industry AI",
        "chatgpt",
        "customer experience",
        "e-commerce",
        "gemini",
        "openai",
        "retail"
      ],
      "summary": "Major retailers including Etsy, Target, and Walmart are integrating product catalogs into AI platforms like Gemini and Copilot, enabling in-conversation purchases. Amazon's Rufus and Walmart's Sparky assistants are reshaping direct-to-consumer engagement.",
      "importance_score": 62.0,
      "reasoning": "Demonstrates agentic AI's growing commercial deployment and changing retail dynamics. Represents meaningful industry trend but incremental rather than breakthrough development.",
      "themes": [
        "Agentic AI",
        "E-commerce",
        "Retail AI",
        "Consumer AI"
      ],
      "continuation": null,
      "summary_html": "<p>Major retailers including Etsy, Target, and Walmart are integrating product catalogs into AI platforms like Gemini and Copilot, enabling in-conversation purchases. Amazon's Rufus and Walmart's Sparky assistants are reshaping direct-to-consumer engagement.</p>",
      "content_html": "<p>Big retailers are committing more heavily to agentic AI-led commerce, and accepting some loss of customer proximity and data control in the process.</p>\n<p>As reported by Retail Dive, the opening weeks of 2026 have seen Etsy, Target and Walmart push  product ranges onto third-party AI platforms, forming new partnerships with Google’s Gemini and Microsoft’s Copilot, after last year’s collaborations with OpenAI’s ChatGPT. These let consumers purchase goods inside the AI’s conversation interface.</p>\n<p>Amazon and Walmart have been investing in their own consumer-facing AI assistants, Rufus and Sparky respectively to change how shoppers interact with their brands.</p>\n<p>Agentic AI is beginning to redraw direct-to-consumer engagement, and industry figures regard this trend as an important moment in online retail. “I think this has the potential to disrupt retail in the same way the internet once did,” Kartik Hosanagar, a marketing professor at the Wharton School of the University of Pennsylvania, told the website’s reporters.</p>\n<p>Partnering with AIs like ChatGPT or Gemini engages consumers wherever they happen to be and may choose to shop. Adobe’s 2025 Holiday Shopping report found that AI-driven traffic to US e-commerce sites grew 758% year on year between in November 2025, and Cyber Monday saw a 670% increase in AI-referred retail visits.</p>\n<p>“What we expect is a deepening of consumer engagement,” Katherine Black, a partner at Kearney specialising in food, drug and mass-market retail, said in an email to Retail Dive. “More shoppers will rely on AI for purchasing, and across a wider range of missions. As retailers’ capabilities within these tools improve, adoption should accelerate further.”</p>\n<p>Meeting customers on AI platforms comes with trade-offs, according to industry observers, with questions around data ownership and the risk that retailers are sidelined. 81% of retail executives believe generative AI will erode brand loyalty by 2027, according to Deloitte’s 2026 Retail Industry Global Outlook, published earlier this month.</p>\n<p>Retailers’ websites or apps provide a stream of behavioural data, and if discovery, evaluation, and purchase happen externally, any insight doesn’t reach the retailer. “This fundamentally changes where power sits,” Hosanagar said. “Control over the agent increasingly means control over the customer relationship.”</p>\n<p>Google and Alphabet CEO Sundar Pichai has unveiled new commerce tools for Gemini, outlining how it will support customers from discovery to final purchase. Nikki Baird, vice president of strategy and product at Aptos, says this raises difficult questions. “What he’s describing is Google owning the data across discovery, decision and transaction. Even if some information is shared back, missing context from those stages leaves retailers with a much poorer understanding of their customers.”</p>\n<p>Pichai reassured retailers collaboration remains central to Google. “From nearly three decades of working with retailers, we know success only comes when we work together,” he told an NRF audience. “Our aim is to use our full technology stack to help shape the next era of retail.”</p>\n<p>Yet agentic systems’ features like instant checkout absorb the shopping experience into one platform. “If research, discovery and purchase all happen on OpenAI rather than Walmart.com, you’re effectively giving away the brand experience. At that point, the retailer risks becoming little more than a fulfilment operation,” Hosanagar said.</p>\n<p>Amazon has not announced plans to sell directly through ChatGPT, doubling down on its own AI initiatives. Earlier this month, the company launched a dedicated site for Alexa+, its generative AI assistant that helps users research and plan purchases.</p>\n<p>Yet participation in third-party AI commerce may become unavoidable. When OpenAI launched its Instant Checkout feature on ChatGPT last September, it suggested that enabling the function could influence how merchants are ranked in search results, in addition to price and product quality. Uploading product catalogues to AI chat platforms may be the first step in a transformation of online retail.</p>\n<p>According to Deloitte, roughly half of retail executives expect the current multi-stage shopping process to reduce to a single AI-driven interaction by 2027. For now the industry remains at an early stage of any transition. “The real inflection point is when consumers rely on an autonomous agent to shop on their behalf,” Hosanagar told Retail Dive.</p>\n<p>“Retailers will engage less with humans directly and more with their representatives — AI agents. That agent processes information differently, requires data in new formats and responds to persuasion in ways unlike a person.”</p>\n<p>Today, consumers can access ChatGPT on their phones while in-store, effectively consulting an always-available expert. “It’s not just the internet in your pocket,” Baird told Retail Dive. “It’s like having a highly knowledgeable store associate who knows every retailer.”</p>\n<p>This may prompt retailers to equip frontline staff with their own AI tools, offering instant insight into customer preferences or shopping history. Alternatively, a retailer’s AI agent could proactively notify customers when a favoured item is back in stock, helping associates convert interest into sales. “The goal is to enable store associates to perform at their best,” Baird said.</p>\n<p>(Image source: “Shopping trauma!” by Elsie esq. is licensed under CC BY 2.0.)</p>\n<p>&nbsp;</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Retailers examine options for on-AI retail appeared first on AI News.</p>"
    },
    {
      "id": "999dae7259b7",
      "title": "AI is hitting UK harder than other big economies, study finds",
      "content": "Britain is losing more jobs than it creates owing to artificial intelligence, Morgan Stanley research suggestsMore than a quarter of Britons fear losing job to AI in next five yearsBusiness live – latest updatesThe UK is losing more jobs than it is creating because of artificial intelligence and is being hit harder than rival large economies, new research suggests.British companies reported that AI had resulted in net job losses over the past 12 months, down 8% – the highest rate among other leading economies including the US, Japan, Germany and Australia, according to a study by the investment bank Morgan Stanley. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/26/ai-uk-jobs-us-japan-germany-australia",
      "author": "Lauren Almeida",
      "published": "2026-01-26T08:41:54",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Technology",
        "Economics",
        "Business",
        "UK news"
      ],
      "summary": "Morgan Stanley research indicates the UK is experiencing the highest rate of net job losses due to AI among major economies, with an 8% reduction over the past 12 months. Over a quarter of Britons fear losing their jobs to AI in the next five years.",
      "importance_score": 55.0,
      "reasoning": "Interesting economic research on AI's labor market impact but not directly related to frontier AI developments. More of a socioeconomic analysis than technology news.",
      "themes": [
        "AI Economics",
        "Labor Market",
        "UK Economy",
        "AI Impact"
      ],
      "continuation": null,
      "summary_html": "<p>Morgan Stanley research indicates the UK is experiencing the highest rate of net job losses due to AI among major economies, with an 8% reduction over the past 12 months. Over a quarter of Britons fear losing their jobs to AI in the next five years.</p>",
      "content_html": "<p>Britain is losing more jobs than it creates owing to artificial intelligence, Morgan Stanley research suggestsMore than a quarter of Britons fear losing job to AI in next five yearsBusiness live – latest updatesThe UK is losing more jobs than it is creating because of artificial intelligence and is being hit harder than rival large economies, new research suggests.British companies reported that AI had resulted in net job losses over the past 12 months, down 8% – the highest rate among other leading economies including the US, Japan, Germany and Australia, according to a study by the investment bank Morgan Stanley. Continue reading...</p>"
    },
    {
      "id": "edcf3ab7e585",
      "title": "AI systems could use Met Office and National Archives data under UK plans",
      "content": "Ministers plan to license content from institutions such as National History Museum and National Library of ScotlandMet Office data and legal documents from the National Archives could be used by artificial intelligence systems as the UK government pushes ahead with plans to employ nationally owned material in AI tools.The government is providing funds for researchers to test how Met Office content could be used by the technology, such as in helping agencies and councils know when to buy more road grit. Another project will explore whether legal data from the National Archives – the UK’s repository for official documents – could help medium- and small-sized businesses with legal support. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/26/ai-systems-met-office-national-archives-data-uk-government-plans",
      "author": "Dan Milmo Global technology editor",
      "published": "2026-01-26T06:00:30",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "National Archives",
        "Met Office",
        "Museums",
        "Labour",
        "Natural History Museum",
        "Imperial War Museums",
        "University of Oxford",
        "Kew Gardens",
        "V&A",
        "Technology",
        "Politics",
        "Heritage",
        "UK news",
        "Culture"
      ],
      "summary": "The UK government plans to license Met Office weather data and National Archives legal documents for AI systems. Pilot projects will test applications including municipal road grit planning and legal support for SMBs.",
      "importance_score": 52.0,
      "reasoning": "Minor government data policy for AI training with limited frontier AI significance. The applications mentioned are incremental rather than groundbreaking.",
      "themes": [
        "Government Data",
        "AI Policy",
        "UK Government",
        "Public Sector AI"
      ],
      "continuation": null,
      "summary_html": "<p>The UK government plans to license Met Office weather data and National Archives legal documents for AI systems. Pilot projects will test applications including municipal road grit planning and legal support for SMBs.</p>",
      "content_html": "<p>Ministers plan to license content from institutions such as National History Museum and National Library of ScotlandMet Office data and legal documents from the National Archives could be used by artificial intelligence systems as the UK government pushes ahead with plans to employ nationally owned material in AI tools.The government is providing funds for researchers to test how Met Office content could be used by the technology, such as in helping agencies and councils know when to buy more road grit. Another project will explore whether legal data from the National Archives – the UK’s repository for official documents – could help medium- and small-sized businesses with legal support. Continue reading...</p>"
    },
    {
      "id": "a040834ac599",
      "title": "What is Clawdbot? How a Local First Agent Stack Turns Chats into Real Automations",
      "content": "Clawdbot is an open source personal AI assistant that you run on your own hardware. It connects large language models from providers such as Anthropic and OpenAI to real tools such as messaging apps, files, shell, browser and smart home devices, while keeping the orchestration layer under your control. \n\n\n\nThe interesting part is not that Clawdbot chats. It is that the project ships a concrete architecture for local first agents, and a typed workflow engine called Lobster that turns model calls into deterministic pipelines.\n\n\n\nArchitecture: Gateway, Nodes and Skills\n\n\n\nAt the center of Clawdbot is the Gateway process. The Gateway exposes a WebSocket control plane on ws://127.0.0.1:18789 and a local HTTP interface for the control UI and web chat.\n\n\n\nYour messages from WhatsApp, Telegram, Signal, Slack, Discord, iMessage and other channels are delivered to the Gateway. The Gateway decides which agent should handle the message, which tools it may call, and which model provider to use. It then sends the reply back over the same channel. \n\n\n\nThe runtime is split into a few core concepts:\n\n\n\n\nGateway: Routing, model calls, tool invocation, sessions, presence and scheduling.\n\n\n\nNodes: Processes that give Clawdbot access to local resources such as file system, browser automation, microphone, camera or platform specific APIs on macOS, Windows, Linux, iOS and Android.\n\n\n\nChannels: Integrations for chat systems like WhatsApp, Telegram, Discord, Slack, Signal, Microsoft Teams, Matrix, Zalo and more. These are configured as channel backends that attach to the Gateway.\n\n\n\nSkills and plugins: Tools that the agent can call, described in a standard SKILL.md format and distributed through ClawdHub.\n\n\n\n\nThis separation lets you run the Gateway on a five dollar virtual server or a spare machine at home, while keeping heavy model compute on remote APIs or local model backends when needed. \n\n\n\nSkills and the SKILL.md standard\n\n\n\nClawdbot uses an open skills format described in SKILL.md. A skill is defined in Markdown with a small header and an ordered procedure. For example, a deployment skill might specify steps such as checking git status, running tests and deploying only after success.\n\n\n\n---\nname: deploy-production\ndescription: Deploy the current branch to production. Use only after tests pass.\ndisable-model-invocation: true\n---\n1. Check git status ensuring clean working directory.\n2. Run `npm test`\n3. If tests pass, run `npm run deploy`\n\n\n\n\nThe Gateway reads these definitions and exposes them to agents as tools with explicit capabilities and safety constraints. Skills are published to ClawdHub and can be installed or composed into larger workflows.\n\n\n\nThis means that operational runbooks can move from ad-hoc wiki pages into machine executable skills, while still being auditable as text.\n\n\n\nLobster: Typed Workflow Runtime for Agents\n\n\n\nLobster is the workflow runtime that powers Local Lobster and many advanced Clawdbot automations. It is described as a typed workflow shell that lets Clawdbot run multi step tool sequences as a single deterministic operation with explicit approval gates. \n\n\n\nInstead of having the model call many tools in a loop, Lobster moves orchestration into a small domain specific runtime:\n\n\n\n\nPipelines are defined as JSON or YAML, or as a compact shell like pipeline string.\n\n\n\nSteps exchange typed JSON data, not unstructured text.\n\n\n\nThe runtime enforces timeouts, output limits and sandbox policies.\n\n\n\nWorkflows can pause on side effects and resume later with a resumeToken.\n\n\n\n\nA simple inbox triage workflow looks like this:\n\n\n\nname: inbox-triage\nsteps:\n  - id: collect\n    command: inbox list --json\n  - id: categorize\n    command: inbox categorize --json\n    stdin: $collect.stdout\n  - id: approve\n    command: inbox apply --approve\n    stdin: $categorize.stdout\n    approval: required\n  - id: execute\n    command: inbox apply --execute\n    stdin: $categorize.stdout\n    condition: $approve.approved\n\n\n\n\nClawdbot treats this file as a skill. When you ask it to clean your inbox, it calls one Lobster pipeline instead of improvising many tool calls. The model decides when to run the pipeline and with which parameters, but the pipeline itself stays deterministic and auditable. \n\n\n\nLocal Lobster is the reference agent that uses Lobster to drive local workflows and is described in coverage as an open source agent that redefines personal AI by pairing local first workflows with proactive behavior. \n\n\n\nProactive local first behavior\n\n\n\nA key reason Clawdbot is trending and visible on X and in developer communities is that it behaves like an operator, not just a chat window.\n\n\n\nBecause the Gateway can run scheduled jobs and track state across sessions, common patterns include:\n\n\n\n\nDaily briefings that summarize calendars, tasks and important mail.\n\n\n\nPeriodic recaps such as weekly shipped work summaries.\n\n\n\nMonitors that watch for conditions, then message you first on your preferred channel.\n\n\n\nFile and repository automations that run locally but are triggered by natural language. \n\n\n\n\nAll of this runs with routing and tool policy on your machine or server. Model calls still go to providers like Anthropic, OpenAI, Google, xAI or local backends, but the assistant brain, memory and integrations are under your control.\n\n\n\nInstallation and developer workflow\n\n\n\nThe project provides a one line installer that fetches a script from clawd.bot and bootstraps Node, the Gateway and core components. For more control, you can install via npm or clone the TypeScript repository and build with pnpm. \n\n\n\nTypical steps:\n\n\n\ncurl -fsSL https://clawd.bot/install.sh | bash\n\n# or\n\nnpm i -g clawdbot\nclawdbot onboard\n\n\n\n\nAfter onboarding you connect a channel such as Telegram or WhatsApp, choose a model provider and enable skills. From there you can write your own SKILL.md files, build Lobster workflows and expose them through chat, web chat or the macOS companion application.\n\n\n\nSome Examples\n\n\n\n\nJust ask @clawdbot to build and deploy a website with a chat message https://t.co/I5bQDCK2Ne pic.twitter.com/EOa1GlPxJe&mdash; Peter Yang (@petergyang) January 25, 2026\n\n\n\n\n\nJust had Clawdbot set up Ollama with a local model. Now it handles website summaries and simple tasks locally instead of burning API credits.Blown away that an AI just installed another AI to save me money. pic.twitter.com/RRvXQAgBfX&mdash; Max  (@talkaboutdesign) January 25, 2026\n\n\n\n\n\nClawdbot is controlling LMStudio remotely from telegram, downloading Qwen, which it will then use to power some of my tasks with Clawdbot.  pic.twitter.com/ll2adg19Za&mdash; Matthew Berman (@MatthewBerman) January 25, 2026\n\n\n\n\n\nClawdbot now takes an idea, manages codex and claude, debates them on reviews autonomously, and lets me know when it’s done. Amazing. A whole feature deployed while I’m out on a walk. pic.twitter.com/ws3UDQG2S0&mdash; Aaron Ng (@localghost) January 25, 2026\n\nThe post What is Clawdbot? How a Local First Agent Stack Turns Chats into Real Automations appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/25/what-is-clawdbot-how-a-local-first-agent-stack-turns-chats-into-real-automations/",
      "author": "Michal Sutter",
      "published": "2026-01-26T05:05:52",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Editors Pick",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Clawdbot is an open source personal AI assistant that runs locally and connects LLMs to real tools like messaging apps, files, browsers, and smart home devices. It features a typed workflow engine called Lobster for creating deterministic automation pipelines.",
      "importance_score": 52.0,
      "reasoning": "Interesting open source agent framework but represents incremental tooling rather than frontier AI development. Useful for developers but not a major advancement in AI capabilities.",
      "themes": [
        "Open Source",
        "AI Agents",
        "Local AI",
        "Developer Tools"
      ],
      "continuation": null,
      "summary_html": "<p>Clawdbot is an open source personal AI assistant that runs locally and connects LLMs to real tools like messaging apps, files, browsers, and smart home devices. It features a typed workflow engine called Lobster for creating deterministic automation pipelines.</p>",
      "content_html": "<p>Clawdbot is an open source personal AI assistant that you run on your own hardware. It connects large language models from providers such as Anthropic and OpenAI to real tools such as messaging apps, files, shell, browser and smart home devices, while keeping the orchestration layer under your control.</p>\n<p>The interesting part is not that Clawdbot chats. It is that the project ships a concrete architecture for local first agents, and a typed workflow engine called Lobster that turns model calls into deterministic pipelines.</p>\n<p>Architecture: Gateway, Nodes and Skills</p>\n<p>At the center of Clawdbot is the Gateway process. The Gateway exposes a WebSocket control plane on ws://127.0.0.1:18789 and a local HTTP interface for the control UI and web chat.</p>\n<p>Your messages from WhatsApp, Telegram, Signal, Slack, Discord, iMessage and other channels are delivered to the Gateway. The Gateway decides which agent should handle the message, which tools it may call, and which model provider to use. It then sends the reply back over the same channel.</p>\n<p>The runtime is split into a few core concepts:</p>\n<p>Gateway: Routing, model calls, tool invocation, sessions, presence and scheduling.</p>\n<p>Nodes: Processes that give Clawdbot access to local resources such as file system, browser automation, microphone, camera or platform specific APIs on macOS, Windows, Linux, iOS and Android.</p>\n<p>Channels: Integrations for chat systems like WhatsApp, Telegram, Discord, Slack, Signal, Microsoft Teams, Matrix, Zalo and more. These are configured as channel backends that attach to the Gateway.</p>\n<p>Skills and plugins: Tools that the agent can call, described in a standard SKILL.md format and distributed through ClawdHub.</p>\n<p>This separation lets you run the Gateway on a five dollar virtual server or a spare machine at home, while keeping heavy model compute on remote APIs or local model backends when needed.</p>\n<p>Skills and the SKILL.md standard</p>\n<p>Clawdbot uses an open skills format described in SKILL.md. A skill is defined in Markdown with a small header and an ordered procedure. For example, a deployment skill might specify steps such as checking git status, running tests and deploying only after success.</p>\n<p>---</p>\n<p>name: deploy-production</p>\n<p>description: Deploy the current branch to production. Use only after tests pass.</p>\n<p>disable-model-invocation: true</p>\n<p>---</p>\n<p>1. Check git status ensuring clean working directory.</p>\n<p>2. Run `npm test`</p>\n<p>3. If tests pass, run `npm run deploy`</p>\n<p>The Gateway reads these definitions and exposes them to agents as tools with explicit capabilities and safety constraints. Skills are published to ClawdHub and can be installed or composed into larger workflows.</p>\n<p>This means that operational runbooks can move from ad-hoc wiki pages into machine executable skills, while still being auditable as text.</p>\n<p>Lobster: Typed Workflow Runtime for Agents</p>\n<p>Lobster is the workflow runtime that powers Local Lobster and many advanced Clawdbot automations. It is described as a typed workflow shell that lets Clawdbot run multi step tool sequences as a single deterministic operation with explicit approval gates.</p>\n<p>Instead of having the model call many tools in a loop, Lobster moves orchestration into a small domain specific runtime:</p>\n<p>Pipelines are defined as JSON or YAML, or as a compact shell like pipeline string.</p>\n<p>Steps exchange typed JSON data, not unstructured text.</p>\n<p>The runtime enforces timeouts, output limits and sandbox policies.</p>\n<p>Workflows can pause on side effects and resume later with a resumeToken.</p>\n<p>A simple inbox triage workflow looks like this:</p>\n<p>name: inbox-triage</p>\n<p>steps:</p>\n<ul>\n<li>id: collect</li>\n</ul>\n<p>command: inbox list --json</p>\n<ul>\n<li>id: categorize</li>\n</ul>\n<p>command: inbox categorize --json</p>\n<p>stdin: $collect.stdout</p>\n<ul>\n<li>id: approve</li>\n</ul>\n<p>command: inbox apply --approve</p>\n<p>stdin: $categorize.stdout</p>\n<p>approval: required</p>\n<ul>\n<li>id: execute</li>\n</ul>\n<p>command: inbox apply --execute</p>\n<p>stdin: $categorize.stdout</p>\n<p>condition: $approve.approved</p>\n<p>Clawdbot treats this file as a skill. When you ask it to clean your inbox, it calls one Lobster pipeline instead of improvising many tool calls. The model decides when to run the pipeline and with which parameters, but the pipeline itself stays deterministic and auditable.</p>\n<p>Local Lobster is the reference agent that uses Lobster to drive local workflows and is described in coverage as an open source agent that redefines personal AI by pairing local first workflows with proactive behavior.</p>\n<p>Proactive local first behavior</p>\n<p>A key reason Clawdbot is trending and visible on X and in developer communities is that it behaves like an operator, not just a chat window.</p>\n<p>Because the Gateway can run scheduled jobs and track state across sessions, common patterns include:</p>\n<p>Daily briefings that summarize calendars, tasks and important mail.</p>\n<p>Periodic recaps such as weekly shipped work summaries.</p>\n<p>Monitors that watch for conditions, then message you first on your preferred channel.</p>\n<p>File and repository automations that run locally but are triggered by natural language.</p>\n<p>All of this runs with routing and tool policy on your machine or server. Model calls still go to providers like Anthropic, OpenAI, Google, xAI or local backends, but the assistant brain, memory and integrations are under your control.</p>\n<p>Installation and developer workflow</p>\n<p>The project provides a one line installer that fetches a script from clawd.bot and bootstraps Node, the Gateway and core components. For more control, you can install via npm or clone the TypeScript repository and build with pnpm.</p>\n<p>Typical steps:</p>\n<p>curl -fsSL https://clawd.bot/install.sh | bash</p>\n<p># or</p>\n<p>npm i -g clawdbot</p>\n<p>clawdbot onboard</p>\n<p>After onboarding you connect a channel such as Telegram or WhatsApp, choose a model provider and enable skills. From there you can write your own SKILL.md files, build Lobster workflows and expose them through chat, web chat or the macOS companion application.</p>\n<p>Some Examples</p>\n<p>Just ask @clawdbot to build and deploy a website with a chat message https://t.co/I5bQDCK2Ne pic.twitter.com/EOa1GlPxJe— Peter Yang (@petergyang) January 25, 2026</p>\n<p>Just had Clawdbot set up Ollama with a local model. Now it handles website summaries and simple tasks locally instead of burning API credits.Blown away that an AI just installed another AI to save me money. pic.twitter.com/RRvXQAgBfX— Max  (@talkaboutdesign) January 25, 2026</p>\n<p>Clawdbot is controlling LMStudio remotely from telegram, downloading Qwen, which it will then use to power some of my tasks with Clawdbot.  pic.twitter.com/ll2adg19Za— Matthew Berman (@MatthewBerman) January 25, 2026</p>\n<p>Clawdbot now takes an idea, manages codex and claude, debates them on reviews autonomously, and lets me know when it’s done. Amazing. A whole feature deployed while I’m out on a walk. pic.twitter.com/ws3UDQG2S0— Aaron Ng (@localghost) January 25, 2026</p>\n<p>The post What is Clawdbot? How a Local First Agent Stack Turns Chats into Real Automations appeared first on MarkTechPost.</p>"
    },
    {
      "id": "506a21fac899",
      "title": "How Formula E uses Google Cloud AI to meet net zero targets",
      "content": "Formula E is using Google Cloud AI to meet its net zero targets by driving efficiency across its global logistics and commercial operations. As part of an expanded multi-year agreement, the electric racing series will integrate Gemini models into its ecosystem to support performance analysis, back-office workflows, and event logistics.\n\n\n\nThe collaboration demonstrates how sports organisations are utilising cloud infrastructure to drive tangible business outcomes, rather than just securing surface-level sponsorship. The partnership focuses on optimising business operations, ranging from race management to the fan experience.\n\n\n\nOperational twins and carbon data to achieve net zero targets\n\n\n\nWhile marketing visibility often drives sports partnerships, this agreement builds on a technical foundation first formalised in January 2025. The elevation to &#8220;Principal Partner&#8221; involves Formula E adopting Google Cloud technologies for business-critical functions.\n\n\n\nThe immediate application involves optimising the complex logistics of a global championship. Advanced AI modelling of the back office and the creation of race and event digital twins allow the organisation to simulate and optimise site builds virtually.\n\n\n\nThis application directly affects Scope 3 emissions. The capability to plan infrastructure virtually minimises the need for physical on-site reconnaissance and reduces the transport of heavy equipment.\n\n\n\nFor a championship that is the only sport-certified net zero carbon entity since inception, maintaining this status requires finding efficiencies in the supply chain. The digital twin approach delivers a quantifiable reduction in the operational carbon footprint while maintaining performance.\n\n\n\nBeyond logistical modelling, the Google Cloud AI partnership extends into the workforce productivity layer. Formula E is deploying Google Workspace with Gemini AI to enable greater agility and efficiency across its organisation.\n\n\n\nThe organisation intends to use these tools to accelerate performance and deliver faster operations. This reflects a broader trend where generative AI tools are provisioned to reduce administrative latency in distributed workforces.\n\n\n\nThe viability of these implementations to achieve net zero targets is supported by previous collaborative projects. Formula E recently utilised Google’s AI Studio and Gemini models to execute the ‘Mountain Recharge’ initiative.\n\n\n\nEngineers used the models to map an optimal route for the GENBETA car during a mountain descent. The AI identified and analysed specific braking zones, calculating the necessary regenerative braking required to harvest enough energy to complete a full lap of the Monaco circuit subsequently.\n\n\n\nThis specific use case demonstrates how high-dimensional data – including topography, friction, and energy consumption – can be processed to define physical execution.\n\n\n\nUsing Google Cloud AI to enhance Formula E’s data product\n\n\n\nThe partnership also addresses the commercial requirement to retain and grow a digital audience. Formula E has integrated a ‘Strategy Agent’ into its live broadcasts. This tool processes real-time data to provide viewers with tailored insights and predictions regarding race strategy and driver performance.\n\n\n\nMillions of viewers have utilised these insights, which explain complex race dynamics as they unfold. This mirrors the enterprise challenge of observability (i.e. taking vast streams of real-time technical data and synthesising them into understandable narratives for stakeholders.)\n\n\n\nBeyond helping to achieve net zero targets, the leadership at both organisations frames this expansion as a necessary evolution of their technical stack.\n\n\n\nJeff Dodds, CEO of Formula E, said: &#8220;Our expanded partnership with Google Cloud is a true game-changer for Formula E and for motorsport as a whole. We are already pushing the boundaries of technology in sport, and this Principal Partnership confirms our vision.\n\n\n\n“The integration of Google Cloud&#8217;s AI capabilities will unlock a new dimension of real-time performance optimisation and strategic decision-making, both for the Championship and for our global broadcast audience. This collaboration will redefine how fans experience our races and set a new benchmark for technology integration in sport worldwide.&#8221;\n\n\n\nTara Brady, President of Google Cloud EMEA, added: &#8220;Formula E is a hub of innovation, where milliseconds can define success. This expanded partnership is a testament to the power of Google Cloud&#8217;s AI and data analytics, showing how our technology can deliver a competitive advantage in the most demanding scenarios.&#8221;\n\n\n\nThe progression from the initial partnership in January 2025 to this expanded scope suggests the pilot programs provided sufficient ROI to warrant a broader rollout. As organisations face pressure to balance performance with net zero targets, the use of virtual simulation to optimise physical deployment remains a high-value area for investment.\n\n\n\nSee also: Controlling AI agent sprawl: The CIO’s guide to governance\n\n\n\n\n\n\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.\n\n\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post How Formula E uses Google Cloud AI to meet net zero targets appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/how-formula-e-uses-google-cloud-ai-to-meet-net-zero-targets/",
      "author": "Ryan Daws",
      "published": "2026-01-26T14:38:53",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI and Us",
        "AI Business Strategy",
        "AI in Action",
        "Entertainment & Media",
        "Environment & Sustainability",
        "Inside AI",
        "agentic ai",
        "agents",
        "cloud",
        "digital twins",
        "enterprise",
        "formula e",
        "google cloud",
        "net zero",
        "strategy",
        "sustainability"
      ],
      "summary": "Formula E expanded its Google Cloud partnership to use Gemini models for logistics optimization, performance analysis, and achieving net zero targets. The collaboration includes operational digital twins and carbon tracking systems.",
      "importance_score": 45.0,
      "reasoning": "Standard enterprise AI implementation story with limited frontier AI significance. Demonstrates practical applications but represents routine corporate deployment rather than innovation.",
      "themes": [
        "Enterprise AI",
        "Sustainability",
        "Sports Tech",
        "Google Cloud"
      ],
      "continuation": null,
      "summary_html": "<p>Formula E expanded its Google Cloud partnership to use Gemini models for logistics optimization, performance analysis, and achieving net zero targets. The collaboration includes operational digital twins and carbon tracking systems.</p>",
      "content_html": "<p>Formula E is using Google Cloud AI to meet its net zero targets by driving efficiency across its global logistics and commercial operations. As part of an expanded multi-year agreement, the electric racing series will integrate Gemini models into its ecosystem to support performance analysis, back-office workflows, and event logistics.</p>\n<p>The collaboration demonstrates how sports organisations are utilising cloud infrastructure to drive tangible business outcomes, rather than just securing surface-level sponsorship. The partnership focuses on optimising business operations, ranging from race management to the fan experience.</p>\n<p>Operational twins and carbon data to achieve net zero targets</p>\n<p>While marketing visibility often drives sports partnerships, this agreement builds on a technical foundation first formalised in January 2025. The elevation to “Principal Partner” involves Formula E adopting Google Cloud technologies for business-critical functions.</p>\n<p>The immediate application involves optimising the complex logistics of a global championship. Advanced AI modelling of the back office and the creation of race and event digital twins allow the organisation to simulate and optimise site builds virtually.</p>\n<p>This application directly affects Scope 3 emissions. The capability to plan infrastructure virtually minimises the need for physical on-site reconnaissance and reduces the transport of heavy equipment.</p>\n<p>For a championship that is the only sport-certified net zero carbon entity since inception, maintaining this status requires finding efficiencies in the supply chain. The digital twin approach delivers a quantifiable reduction in the operational carbon footprint while maintaining performance.</p>\n<p>Beyond logistical modelling, the Google Cloud AI partnership extends into the workforce productivity layer. Formula E is deploying Google Workspace with Gemini AI to enable greater agility and efficiency across its organisation.</p>\n<p>The organisation intends to use these tools to accelerate performance and deliver faster operations. This reflects a broader trend where generative AI tools are provisioned to reduce administrative latency in distributed workforces.</p>\n<p>The viability of these implementations to achieve net zero targets is supported by previous collaborative projects. Formula E recently utilised Google’s AI Studio and Gemini models to execute the ‘Mountain Recharge’ initiative.</p>\n<p>Engineers used the models to map an optimal route for the GENBETA car during a mountain descent. The AI identified and analysed specific braking zones, calculating the necessary regenerative braking required to harvest enough energy to complete a full lap of the Monaco circuit subsequently.</p>\n<p>This specific use case demonstrates how high-dimensional data – including topography, friction, and energy consumption – can be processed to define physical execution.</p>\n<p>Using Google Cloud AI to enhance Formula E’s data product</p>\n<p>The partnership also addresses the commercial requirement to retain and grow a digital audience. Formula E has integrated a ‘Strategy Agent’ into its live broadcasts. This tool processes real-time data to provide viewers with tailored insights and predictions regarding race strategy and driver performance.</p>\n<p>Millions of viewers have utilised these insights, which explain complex race dynamics as they unfold. This mirrors the enterprise challenge of observability (i.e. taking vast streams of real-time technical data and synthesising them into understandable narratives for stakeholders.)</p>\n<p>Beyond helping to achieve net zero targets, the leadership at both organisations frames this expansion as a necessary evolution of their technical stack.</p>\n<p>Jeff Dodds, CEO of Formula E, said: “Our expanded partnership with Google Cloud is a true game-changer for Formula E and for motorsport as a whole. We are already pushing the boundaries of technology in sport, and this Principal Partnership confirms our vision.</p>\n<p>“The integration of Google Cloud’s AI capabilities will unlock a new dimension of real-time performance optimisation and strategic decision-making, both for the Championship and for our global broadcast audience. This collaboration will redefine how fans experience our races and set a new benchmark for technology integration in sport worldwide.”</p>\n<p>Tara Brady, President of Google Cloud EMEA, added: “Formula E is a hub of innovation, where milliseconds can define success. This expanded partnership is a testament to the power of Google Cloud’s AI and data analytics, showing how our technology can deliver a competitive advantage in the most demanding scenarios.”</p>\n<p>The progression from the initial partnership in January 2025 to this expanded scope suggests the pilot programs provided sufficient ROI to warrant a broader rollout. As organisations face pressure to balance performance with net zero targets, the use of virtual simulation to optimise physical deployment remains a high-value area for investment.</p>\n<p>See also: Controlling AI agent sprawl: The CIO’s guide to governance</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp; Cloud Expo. Click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post How Formula E uses Google Cloud AI to meet net zero targets appeared first on AI News.</p>"
    }
  ]
}