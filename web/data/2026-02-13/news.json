{
  "category": "news",
  "date": "2026-02-13",
  "category_summary": "**Google's Gemini 3 Deep Think** [shattered benchmarks today](/?date=2026-02-13&category=news#item-e31fb6939b39), hitting **84.6% on ARC-AGI-2**—reigniting the AGI debate with its advanced test-time reasoning capabilities. Meanwhile, **Anthropic** dominated headlines with a record [**$30B funding round**](/?date=2026-02-13&category=news#item-45243130fac9) at a **$380B valuation**, while its **Claude Cowork** legal tools [triggered a stock crash](/?date=2026-02-13&category=news#item-315f77b723ce) across major UK data firms including **Relx**, **Experian**, and **Sage**.\n\n- **OpenAI** [deployed **GPT-5.3-Codex-Spark**](/?date=2026-02-13&category=news#item-462ec3ec2afc) on **Cerebras** hardware—its first non-Nvidia production model—delivering **1,000+ tokens/sec** (15x faster than predecessors)\n- **Z.ai** [released **GLM-5**](/?date=2026-02-13&category=news#item-4a92f3e0dd2f), a new SOTA open-weights LLM with **744B parameters** (40B active), continuing China's open-model wave\n- **Anthropic** [donated **$20M**](/?date=2026-02-13&category=news#item-4d46c8d276e4) to back pro-AI-regulation US political candidates, splitting from **OpenAI's** lighter-touch regulatory stance\n- **Google** [launched **Chrome Auto Browse**](/?date=2026-02-13&category=news#item-d5f309619f44) agent and reported [**100K+ prompt model extraction attacks**](/?date=2026-02-13&category=news#item-ed529feaa2a6) on Gemini by commercially motivated actors\n- State-sponsored hackers from **Iran, North Korea, China, and Russia** are [actively using LLMs for cyberattacks](/?date=2026-02-13&category=news#item-e27981cd210f), per Google's threat intelligence report\n- AI-driven [**market disruption fears** spread](/?date=2026-02-13&category=news#item-ab076001efaf) to commercial property services stocks on both sides of the Atlantic",
  "category_summary_html": "<p><strong>Google's Gemini 3 Deep Think</strong> <a href=\"/?date=2026-02-13&amp;category=news#item-e31fb6939b39\" class=\"internal-link\" rel=\"noopener noreferrer\">shattered benchmarks today</a>, hitting <strong>84.6% on ARC-AGI-2</strong>—reigniting the AGI debate with its advanced test-time reasoning capabilities. Meanwhile, <strong>Anthropic</strong> dominated headlines with a record <a href=\"/?date=2026-02-13&amp;category=news#item-45243130fac9\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>$30B funding round</strong></a> at a <strong>$380B valuation</strong>, while its <strong>Claude Cowork</strong> legal tools <a href=\"/?date=2026-02-13&amp;category=news#item-315f77b723ce\" class=\"internal-link\" rel=\"noopener noreferrer\">triggered a stock crash</a> across major UK data firms including <strong>Relx</strong>, <strong>Experian</strong>, and <strong>Sage</strong>.</p>\n<ul>\n<li><strong>OpenAI</strong> <a href=\"/?date=2026-02-13&amp;category=news#item-462ec3ec2afc\" class=\"internal-link\" rel=\"noopener noreferrer\">deployed <strong>GPT-5.3-Codex-Spark</strong></a> on <strong>Cerebras</strong> hardware—its first non-Nvidia production model—delivering <strong>1,000+ tokens/sec</strong> (15x faster than predecessors)</li>\n<li><strong>Z.ai</strong> <a href=\"/?date=2026-02-13&amp;category=news#item-4a92f3e0dd2f\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>GLM-5</strong></a>, a new SOTA open-weights LLM with <strong>744B parameters</strong> (40B active), continuing China's open-model wave</li>\n<li><strong>Anthropic</strong> <a href=\"/?date=2026-02-13&amp;category=news#item-4d46c8d276e4\" class=\"internal-link\" rel=\"noopener noreferrer\">donated <strong>$20M</strong></a> to back pro-AI-regulation US political candidates, splitting from <strong>OpenAI's</strong> lighter-touch regulatory stance</li>\n<li><strong>Google</strong> <a href=\"/?date=2026-02-13&amp;category=news#item-d5f309619f44\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>Chrome Auto Browse</strong></a> agent and reported <a href=\"/?date=2026-02-13&amp;category=news#item-ed529feaa2a6\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>100K+ prompt model extraction attacks</strong></a> on Gemini by commercially motivated actors</li>\n<li>State-sponsored hackers from <strong>Iran, North Korea, China, and Russia</strong> are <a href=\"/?date=2026-02-13&amp;category=news#item-e27981cd210f\" class=\"internal-link\" rel=\"noopener noreferrer\">actively using LLMs for cyberattacks</a>, per Google's threat intelligence report</li>\n<li>AI-driven <a href=\"/?date=2026-02-13&amp;category=news#item-ab076001efaf\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>market disruption fears</strong> spread</a> to commercial property services stocks on both sides of the Atlantic</li>\n</ul>",
  "themes": [
    {
      "name": "Frontier Model Releases & Breakthroughs",
      "description": "Major new model launches from Google (Gemini 3 Deep Think), OpenAI (Codex-Spark), and Z.ai (GLM-5) pushing reasoning, speed, and open-weight performance boundaries",
      "item_count": 5,
      "example_items": [],
      "importance": 93.0
    },
    {
      "name": "AI Industry Economics & Funding",
      "description": "Anthropic's record $30B raise, AI-driven stock crashes in data/property sectors, and RAM supply chain impacts signal AI's accelerating economic footprint",
      "item_count": 4,
      "example_items": [],
      "importance": 82.0
    },
    {
      "name": "AI Hardware Diversification",
      "description": "OpenAI's move to Cerebras wafer-scale chips and Jeff Dean's emphasis on energy efficiency over FLOPs signal a shift in the AI compute landscape away from Nvidia monopoly",
      "item_count": 3,
      "example_items": [],
      "importance": 80.0
    },
    {
      "name": "AI Policy & Regulation",
      "description": "Anthropic's pro-regulation political donations, Brockman's Trump donations, and the emerging policy split between major AI labs",
      "item_count": 3,
      "example_items": [],
      "importance": 75.0
    },
    {
      "name": "AI Security & Threats",
      "description": "State-sponsored cyberattacks using LLMs and large-scale model extraction attempts highlight growing AI security concerns",
      "item_count": 3,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "AI Agents & Products",
      "description": "Google's Chrome Auto Browse agent and the broader shift from chatbots to autonomous agents navigating real-world tasks",
      "item_count": 3,
      "example_items": [],
      "importance": 68.0
    }
  ],
  "total_items": 24,
  "items": [
    {
      "id": "e31fb6939b39",
      "title": "Is This AGI? Google’s Gemini 3 Deep Think Shatters Humanity’s Last Exam And Hits 84.6% On ARC-AGI-2 Performance Today",
      "content": "Google announced a major update to Gemini 3 Deep Think today. This update is specifically built to accelerate modern science, research, and engineering. This seems to be more than just another model release. It represents a pivot toward a &#8216;reasoning mode&#8217; that uses internal verification to solve problems that previously required human expert intervention.\n\n\n\nThe updated model is hitting benchmarks that redefine the frontier of intelligence. By focusing on test-time compute—the ability of a model to &#8216;think&#8217; longer before generating a response—Google is moving beyond simple pattern matching. \n\n\n\nhttps://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/\n\n\n\nRedefining AGI with 84.6% on ARC-AGI-2\n\n\n\nThe ARC-AGI benchmark is an ultimate test of intelligence. Unlike traditional benchmarks that test memorization, ARC-AGI measures a model’s ability to learn new skills and generalize to novel tasks it has never seen. Google team reported that Gemini 3 Deep Think achieved 84.6% on ARC-AGI-2, a result verified by the ARC Prize Foundation.\n\n\n\nA score of 84.6% is a massive leap for the industry. To put this in perspective, humans average about 60% on these visual reasoning puzzles, while previous AI models often struggled to break 20%. This means the model is no longer just predicting the most likely next word. It is developing a flexible internal representation of logic. This capability is critical for R&amp;D environments where engineers deal with messy, incomplete, or novel data that does not exist in a training set.\n\n\n\nPassing &#8216;Humanity’s Last Exam&#8216;\n\n\n\nGoogle also set a new standard on Humanity’s Last Exam (HLE), scoring 48.4% (without tools). HLE is a benchmark consisting of 1000s of questions designed by subject matter experts to be easy for humans but nearly impossible for current AI. These questions span specialized academic topics where data is scarce and logic is dense.\n\n\n\nAchieving 48.4% without external search tools is a landmark for reasoning models. This performance indicates that Gemini 3 Deep Think can handle high-level conceptual planning. It can work through multi-step logical chains in fields like advanced law, philosophy, and mathematics without drifting into &#8216;hallucinations.&#8217; It proves that the model’s internal verification systems are working effectively to prune incorrect reasoning paths.\n\n\n\nCompetitive Coding: The 3455 Elo Milestone\n\n\n\nThe most tangible update is in competitive programming. Gemini 3 Deep Think now holds a 3455 Elo score on Codeforces. In the coding world, a 3455 Elo puts the model in the &#8216;Legendary Grandmaster&#8217; tier, a level reached by only a tiny fraction of human programmers globally.\n\n\n\nThis score means the model excels at algorithmic rigor. It can handle complex data structures, optimize for time complexity, and solve problems that require deep memory management. This model serves as an elite pair programmer. It is particularly useful for &#8216;agentic coding&#8217;—where the AI takes a high-level goal and executes a complex, multi-file solution autonomously. In internal testing, Google team noted that Gemini 3 Pro showed 35% higher accuracy in resolving software engineering challenges than previous versions.\n\n\n\nAdvancing Science: Physics, Chemistry, and Math\n\n\n\nGoogle’s update is specifically tuned for scientific discovery. Gemini 3 Deep Think achieved gold medal-level results on the written sections of the 2025 International Physics Olympiad and the 2025 International Chemistry Olympiad. It also reached gold-medal level performance on the International Math Olympiad 2025.\n\n\n\nBeyond these student-level competitions, the model is performing at a professional research level. It scored 50.5% on the CMT-Benchmark, which tests proficiency in advanced theoretical physics. For researchers and data scientists in biotech or material science, this means the model can assist in interpreting experimental data or modeling physical systems. \n\n\n\nPractical Engineering and 3D Modeling\n\n\n\nThe model’s reasoning isn’t just abstract; it has practical engineering utility. A new capability highlighted by Google team is the model&#8217;s ability to turn a sketch into a 3D-printable object. Deep Think can analyze a 2D drawing, model the complex 3D shapes through code, and generate a final file for a 3D printer.\n\n\n\nThis reflects the model&#8217;s &#8216;agentic&#8217; nature. It can bridge the gap between a visual idea and a physical product by using code as a tool. For engineers, this reduces the friction between design and prototyping. It also excels at solving complex optimization problems, such as designing recipes for growing thin films in specialized chemical processes.\n\n\n\nKey Takeaways\n\n\n\n\nBreakthrough Abstract Reasoning: The model achieved 84.6% on ARC-AGI-2 (verified by the ARC Prize Foundation), proving it can learn novel tasks and generalize logic rather than relying on memorized training data.\n\n\n\nElite Coding Performance: With a 3455 Elo score on Codeforces, Gemini 3 Deep Think performs at the &#8216;Legendary Grandmaster&#8217; level, outperforming the vast majority of human competitive programmers in algorithmic complexity and system architecture.\n\n\n\nNew Standard for Expert Logic: It scored 48.4% on Humanity’s Last Exam (without tools), demonstrating the ability to resolve high-level, multi-step logical chains that were previously considered &#8216;too human&#8217; for AI to solve.\n\n\n\nScientific Olympiad Success: The model achieved gold medal-level results on the written sections of the 2025 International Physics and Chemistry Olympiads, showcasing its capacity for professional-grade research and complex physical modeling.\n\n\n\nScaled Inference-Time Compute: Unlike traditional LLMs, this &#8216;Deep Think&#8217; mode utilizes test-time compute to internally verify and self-correct its logic before answering, significantly reducing technical hallucinations.\n\n\n\n\n\n\n\n\nCheck out the Technical details here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Is This AGI? Google’s Gemini 3 Deep Think Shatters Humanity’s Last Exam And Hits 84.6% On ARC-AGI-2 Performance Today appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/12/is-this-agi-googles-gemini-3-deep-think-shatters-humanitys-last-exam-and-hits-84-6-on-arc-agi-2-performance-today/",
      "author": "Michal Sutter",
      "published": "2026-02-12T22:13:50",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Shorts",
        "Applications",
        "Artificial General Intelligence",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Google's Gemini 3 Deep Think update achieves 84.6% on ARC-AGI-2, a benchmark considered a frontier test of general reasoning. The model uses extended test-time compute ('thinking longer') and internal verification to solve problems previously requiring human expert intervention. This represents a major leap toward AGI-class reasoning capabilities.",
      "importance_score": 95.0,
      "reasoning": "ARC-AGI-2 is one of the most respected AGI benchmarks, and 84.6% is a paradigm-shifting result. This redefines the frontier of AI reasoning and has enormous implications for the AGI timeline debate.",
      "themes": [
        "frontier_models",
        "AGI_benchmarks",
        "reasoning",
        "Google"
      ],
      "continuation": null,
      "summary_html": "<p>Google's Gemini 3 Deep Think update achieves 84.6% on ARC-AGI-2, a benchmark considered a frontier test of general reasoning. The model uses extended test-time compute ('thinking longer') and internal verification to solve problems previously requiring human expert intervention. This represents a major leap toward AGI-class reasoning capabilities.</p>",
      "content_html": "<p>Google announced a major update to Gemini 3 Deep Think today. This update is specifically built to accelerate modern science, research, and engineering. This seems to be more than just another model release. It represents a pivot toward a ‘reasoning mode’ that uses internal verification to solve problems that previously required human expert intervention.</p>\n<p>The updated model is hitting benchmarks that redefine the frontier of intelligence. By focusing on test-time compute—the ability of a model to ‘think’ longer before generating a response—Google is moving beyond simple pattern matching.</p>\n<p>https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/</p>\n<p>Redefining AGI with 84.6% on ARC-AGI-2</p>\n<p>The ARC-AGI benchmark is an ultimate test of intelligence. Unlike traditional benchmarks that test memorization, ARC-AGI measures a model’s ability to learn new skills and generalize to novel tasks it has never seen. Google team reported that Gemini 3 Deep Think achieved 84.6% on ARC-AGI-2, a result verified by the ARC Prize Foundation.</p>\n<p>A score of 84.6% is a massive leap for the industry. To put this in perspective, humans average about 60% on these visual reasoning puzzles, while previous AI models often struggled to break 20%. This means the model is no longer just predicting the most likely next word. It is developing a flexible internal representation of logic. This capability is critical for R&amp;D environments where engineers deal with messy, incomplete, or novel data that does not exist in a training set.</p>\n<p>Passing ‘Humanity’s Last Exam‘</p>\n<p>Google also set a new standard on Humanity’s Last Exam (HLE), scoring 48.4% (without tools). HLE is a benchmark consisting of 1000s of questions designed by subject matter experts to be easy for humans but nearly impossible for current AI. These questions span specialized academic topics where data is scarce and logic is dense.</p>\n<p>Achieving 48.4% without external search tools is a landmark for reasoning models. This performance indicates that Gemini 3 Deep Think can handle high-level conceptual planning. It can work through multi-step logical chains in fields like advanced law, philosophy, and mathematics without drifting into ‘hallucinations.’ It proves that the model’s internal verification systems are working effectively to prune incorrect reasoning paths.</p>\n<p>Competitive Coding: The 3455 Elo Milestone</p>\n<p>The most tangible update is in competitive programming. Gemini 3 Deep Think now holds a 3455 Elo score on Codeforces. In the coding world, a 3455 Elo puts the model in the ‘Legendary Grandmaster’ tier, a level reached by only a tiny fraction of human programmers globally.</p>\n<p>This score means the model excels at algorithmic rigor. It can handle complex data structures, optimize for time complexity, and solve problems that require deep memory management. This model serves as an elite pair programmer. It is particularly useful for ‘agentic coding’—where the AI takes a high-level goal and executes a complex, multi-file solution autonomously. In internal testing, Google team noted that Gemini 3 Pro showed 35% higher accuracy in resolving software engineering challenges than previous versions.</p>\n<p>Advancing Science: Physics, Chemistry, and Math</p>\n<p>Google’s update is specifically tuned for scientific discovery. Gemini 3 Deep Think achieved gold medal-level results on the written sections of the 2025 International Physics Olympiad and the 2025 International Chemistry Olympiad. It also reached gold-medal level performance on the International Math Olympiad 2025.</p>\n<p>Beyond these student-level competitions, the model is performing at a professional research level. It scored 50.5% on the CMT-Benchmark, which tests proficiency in advanced theoretical physics. For researchers and data scientists in biotech or material science, this means the model can assist in interpreting experimental data or modeling physical systems.</p>\n<p>Practical Engineering and 3D Modeling</p>\n<p>The model’s reasoning isn’t just abstract; it has practical engineering utility. A new capability highlighted by Google team is the model’s ability to turn a sketch into a 3D-printable object. Deep Think can analyze a 2D drawing, model the complex 3D shapes through code, and generate a final file for a 3D printer.</p>\n<p>This reflects the model’s ‘agentic’ nature. It can bridge the gap between a visual idea and a physical product by using code as a tool. For engineers, this reduces the friction between design and prototyping. It also excels at solving complex optimization problems, such as designing recipes for growing thin films in specialized chemical processes.</p>\n<p>Key Takeaways</p>\n<p>Breakthrough Abstract Reasoning: The model achieved 84.6% on ARC-AGI-2 (verified by the ARC Prize Foundation), proving it can learn novel tasks and generalize logic rather than relying on memorized training data.</p>\n<p>Elite Coding Performance: With a 3455 Elo score on Codeforces, Gemini 3 Deep Think performs at the ‘Legendary Grandmaster’ level, outperforming the vast majority of human competitive programmers in algorithmic complexity and system architecture.</p>\n<p>New Standard for Expert Logic: It scored 48.4% on Humanity’s Last Exam (without tools), demonstrating the ability to resolve high-level, multi-step logical chains that were previously considered ‘too human’ for AI to solve.</p>\n<p>Scientific Olympiad Success: The model achieved gold medal-level results on the written sections of the 2025 International Physics and Chemistry Olympiads, showcasing its capacity for professional-grade research and complex physical modeling.</p>\n<p>Scaled Inference-Time Compute: Unlike traditional LLMs, this ‘Deep Think’ mode utilizes test-time compute to internally verify and self-correct its logic before answering, significantly reducing technical hallucinations.</p>\n<p>Check out the&nbsp;Technical details here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Is This AGI? Google’s Gemini 3 Deep Think Shatters Humanity’s Last Exam And Hits 84.6% On ARC-AGI-2 Performance Today appeared first on MarkTechPost.</p>"
    },
    {
      "id": "4e8654e291c5",
      "title": "Gemini 3 Deep Think: Advancing science, research and engineering",
      "content": "Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges.",
      "url": "https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/",
      "author": "Unknown",
      "published": "2026-02-12T16:15:09",
      "source": "Google DeepMind News",
      "source_type": "rss",
      "tags": [],
      "summary": "Google DeepMind's official blog post announces the Gemini 3 Deep Think update, describing it as their most specialized reasoning mode for modern science, research, and engineering challenges. This is the primary source for the benchmark-shattering results.",
      "importance_score": 95.0,
      "reasoning": "Official announcement of the same Gemini 3 Deep Think breakthrough; primary source from Google DeepMind.",
      "themes": [
        "frontier_models",
        "AGI_benchmarks",
        "reasoning",
        "Google"
      ],
      "continuation": null,
      "summary_html": "<p>Google DeepMind's official blog post announces the Gemini 3 Deep Think update, describing it as their most specialized reasoning mode for modern science, research, and engineering challenges. This is the primary source for the benchmark-shattering results.</p>",
      "content_html": "<p>Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges.</p>"
    },
    {
      "id": "45243130fac9",
      "title": "Anthropic raises $30bn in latest round, valuing Claude bot maker at $380bn",
      "content": "Maker of chatbot boasting coding ability said annualized revenue grew tenfold in each of past three years, to $14bnThe artificial intelligence company Anthropic said on Thursday it raised $30bn in its latest funding round that values the Claude maker and OpenAI rival at $380bn, underscoring the breakneck pace of AI investments.The round, led by the Singapore sovereign wealth fund GIC and hedge fund Coatue Management, is among the largest private fundraising deals on record and comes just five months after Anthropic closed its previous round at a $183bn valuation – meaning the company has more than doubled in value since September. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/12/anthropic-funding-round",
      "author": "Agence France-Presse",
      "published": "2026-02-12T22:46:53",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Computing",
        "Technology",
        "Business",
        "US news"
      ],
      "summary": "Anthropic raised $30 billion at a $380 billion valuation, more than doubling its value from a $183B round just five months prior. The round was led by GIC and Coatue, with Anthropic reporting annualized revenue of $14B after tenfold yearly growth. This is among the largest private fundraising deals on record.",
      "importance_score": 92.0,
      "reasoning": "A $30B raise at $380B valuation is historically significant and reflects the extraordinary pace of AI investment. Anthropic's revenue growth to $14B signals massive commercial traction for Claude.",
      "themes": [
        "funding",
        "Anthropic",
        "AI_industry",
        "valuations"
      ],
      "continuation": null,
      "summary_html": "<p>Anthropic raised $30 billion at a $380 billion valuation, more than doubling its value from a $183B round just five months prior. The round was led by GIC and Coatue, with Anthropic reporting annualized revenue of $14B after tenfold yearly growth. This is among the largest private fundraising deals on record.</p>",
      "content_html": "<p>Maker of chatbot boasting coding ability said annualized revenue grew tenfold in each of past three years, to $14bnThe artificial intelligence company Anthropic said on Thursday it raised $30bn in its latest funding round that values the Claude maker and OpenAI rival at $380bn, underscoring the breakneck pace of AI investments.The round, led by the Singapore sovereign wealth fund GIC and hedge fund Coatue Management, is among the largest private fundraising deals on record and comes just five months after Anthropic closed its previous round at a $183bn valuation – meaning the company has more than doubled in value since September. Continue reading...</p>"
    },
    {
      "id": "462ec3ec2afc",
      "title": "OpenAI sidesteps Nvidia with unusually fast coding model on plate-sized chips",
      "content": "On Thursday, OpenAI released its first production AI model to run on non-Nvidia hardware, deploying the new GPT-5.3-Codex-Spark coding model on chips from Cerebras. The model delivers code at more than 1,000 tokens (chunks of data) per second, which is reported to be roughly 15 times faster than its predecessor. To compare, Anthropic's Claude Opus 4.6 in its new premium-priced fast mode reaches about 2.5 times its standard speed of 68.2 tokens per second, although it is a larger and more capable model than Spark.\n\"Cerebras has been a great engineering partner, and we're excited about adding fast inference as a new platform capability,\" Sachin Katti, head of compute at OpenAI, said in a statement.\nCodex-Spark is a research preview available to ChatGPT Pro subscribers ($200/month) through the Codex app, command-line interface, and VS Code extension. OpenAI is rolling out API access to select design partners. The model ships with a 128,000-token context window and handles text only at launch.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/02/openai-sidesteps-nvidia-with-unusually-fast-coding-model-on-plate-sized-chips/",
      "author": "Benj Edwards",
      "published": "2026-02-12T22:56:02",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "AI agents",
        "AI chips",
        "AI coding",
        "AI development tools",
        "AI speed",
        "Cerebras",
        "code agents",
        "machine learning",
        "NVIDIA",
        "openai",
        "sam altman",
        "tokens"
      ],
      "summary": "OpenAI released GPT-5.3-Codex-Spark, its first production model running on non-Nvidia hardware (Cerebras WSE-3), delivering over 1,000 tokens per second—roughly 15x faster than its predecessor. This marks a significant step in hardware diversification away from Nvidia's dominance. Available to ChatGPT Pro subscribers as a research preview.",
      "importance_score": 88.0,
      "reasoning": "First major OpenAI model on non-Nvidia silicon is strategically significant for the entire AI hardware ecosystem. The 15x speed gain on Cerebras demonstrates viable alternatives to GPU clusters.",
      "themes": [
        "frontier_models",
        "AI_hardware",
        "OpenAI",
        "Cerebras",
        "inference_speed"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI released GPT-5.3-Codex-Spark, its first production model running on non-Nvidia hardware (Cerebras WSE-3), delivering over 1,000 tokens per second—roughly 15x faster than its predecessor. This marks a significant step in hardware diversification away from Nvidia's dominance. Available to ChatGPT Pro subscribers as a research preview.</p>",
      "content_html": "<p>On Thursday, OpenAI released its first production AI model to run on non-Nvidia hardware, deploying the new GPT-5.3-Codex-Spark coding model on chips from Cerebras. The model delivers code at more than 1,000 tokens (chunks of data) per second, which is reported to be roughly 15 times faster than its predecessor. To compare, Anthropic's Claude Opus 4.6 in its new premium-priced fast mode reaches about 2.5 times its standard speed of 68.2 tokens per second, although it is a larger and more capable model than Spark.</p>\n<p>\"Cerebras has been a great engineering partner, and we're excited about adding fast inference as a new platform capability,\" Sachin Katti, head of compute at OpenAI, said in a statement.</p>\n<p>Codex-Spark is a research preview available to ChatGPT Pro subscribers ($200/month) through the Codex app, command-line interface, and VS Code extension. OpenAI is rolling out API access to select design partners. The model ships with a 128,000-token context window and handles text only at launch.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "0e08770ff205",
      "title": "OpenAI Releases a Research Preview of GPT‑5.3-Codex-Spark: A 15x Faster AI Coding Model Delivering Over 1000 Tokens Per Second on Cerebras Hardware",
      "content": "OpenAI just launched a new research preview called GPT-5.3 Codex-Spark. This model is built for 1 thing: extreme speed. While the standard GPT-5.3 Codex focuses on deep reasoning, Spark is designed for near-instant response times. It is the result of a deep hardware-software integration between OpenAI and Cerebras.\n\n\n\nThe results are game-changing. Spark is 15x faster than the flagship GPT-5.3 Codex. It consistently delivers over 1000 tokens per second. This speed effectively removes the delay between a developer’s thought and the model’s code output.\n\n\n\nThe Hardware: Wafer-Scale Engineering\n\n\n\nThe massive performance jump is powered by the Cerebras Wafer-Scale Engine 3 (WSE-3). Traditional AI models run on clusters of small GPUs. These GPUs must communicate to each other over cables, which creates a &#8216;bottleneck.&#8217; This bottleneck slows down the speed of the model.\n\n\n\nThe WSE-3 is different. It is a single, giant chip the size of a whole silicon wafer. Because the entire model lives on 1 piece of silicon, there are no cables to slow it down. This architecture provides:\n\n\n\n\nMassive on-chip memory.\n\n\n\nUltra-high bandwidth.\n\n\n\nLow-latency compute.\n\n\n\n\nBy using the Cerebras CS-3 system, OpenAI can run inference at speeds that traditional GPU clusters cannot reach.\n\n\n\nSoftware Optimizations and Low Latency\n\n\n\nSpeed is not just about the chip. OpenAI re-engineered the way the model communicates with your computer. They moved away from traditional request methods and introduced a persistent WebSocket connection.\n\n\n\nThis change leads to several technical improvements:\n\n\n\n\nRound-Trip Time (RTT): Client-server overhead is reduced by 80%.\n\n\n\nTime-to-First-Token (TTFT): This is improved by 50%, meaning the code starts appearing almost the moment you hit enter.\n\n\n\nPer-Token Overhead: Internal processing time per token is cut by 30%.\n\n\n\n\nThese optimizations allow for &#8216;Real-Time Steering.&#8217; You can interrupt the model while it is typing and redirect its logic without waiting for the full block to finish.\n\n\n\nThe Trade-offs: Speed vs. Reasoning\n\n\n\nGPT-5.3 Codex-Spark is optimized for throughput, not deep complexity. It is a &#8216;smaller&#8217; model than the flagship GPT-5.3 Codex. Because of this, it has lower reasoning depth.\n\n\n\nhttps://openai.com/index/introducing-gpt-5-3-codex-spark/\n\n\n\nhttps://openai.com/index/introducing-gpt-5-3-codex-spark/\n\n\n\nDevs should be aware of these performance differences:\n\n\n\n\nBenchmarks: Spark scores lower on SWE-Bench Pro and Terminal-Bench 2.0 compared to the flagship model. It may struggle with very complex, multi-file architecture changes.\n\n\n\nSecurity: Under OpenAI’s Preparedness Framework, the flagship GPT-5.3 Codex is rated as &#8216;High&#8217; capability for cybersecurity. Spark does not meet this high threshold. It should not be used for sensitive security logic or autonomous authentication tasks.\n\n\n\n\nQuick Specs and Access\n\n\n\nSpark is available now for ChatGPT Pro users and developers. You can access it through the following tools:\n\n\n\n\nCodex App: Use the model picker to select &#8216;Spark.&#8217;\n\n\n\nVS Code Extension: Integrated directly into the composer.\n\n\n\nCLI: Access it via the command codex --model gpt-5.3-codex-spark.\n\n\n\n\nFeatureGPT-5.3 Codex-SparkGPT-5.3 Codex (Flagship)Tokens per Second1000+~70Context Window128k128kHardwareCerebras WSE-3NVIDIA GPU ClustersBest ForFast IterationDeep Reasoning / Security\n\n\n\nKey Takeaways\n\n\n\n\nGreat Speed: Spark is 15x faster than the flagship GPT-5.3 Codex, delivering an unprecedented throughput of over 1,000 tokens per second to enable near-instant code generation.\n\n\n\nCustom Silicon Infrastructure: This is OpenAI’s first model to run on Cerebras Wafer-Scale Engine 3 (WSE-3) hardware rather than traditional NVIDIA GPUs, using &#8216;wafer-scale&#8217; memory to eliminate data bottlenecks.\n\n\n\nDrastic Latency Reduction: The integration of a persistent WebSocket connection reduces client-server round-trip overhead by 80% and improves the time-to-first-token by 50%.\n\n\n\nReal-Time Steering: Designed for &#8216;micro-iterations,&#8217; the model’s speed allows developers to interrupt and redirect logic in real-time, shifting the workflow from batch-processing to live pair-programming.\n\n\n\nTargeted Capability Trade-offs: While faster, Spark has lower reasoning depth than the flagship model and does not meet the &#8216;High capability&#8217; threshold for cybersecurity in OpenAI’s Preparedness Framework, making it unsuitable for sensitive auth or security tasks.\n\n\n\n\n\n\n\n\nCheck out the Technical details here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post OpenAI Releases a Research Preview of GPT‑5.3-Codex-Spark: A 15x Faster AI Coding Model Delivering Over 1000 Tokens Per Second on Cerebras Hardware appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/12/openai-releases-a-research-preview-of-gpt-5-3-codex-spark-a-15x-faster-ai-coding-model-delivering-over-1000-tokens-per-second-on-cerebras-hardware/",
      "author": "Asif Razzaq",
      "published": "2026-02-12T23:24:03",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "MarkTechPost's detailed technical coverage of GPT-5.3-Codex-Spark explains the Cerebras Wafer-Scale Engine 3 architecture enabling the 1000+ tokens/sec performance. The model prioritizes extreme speed over deep reasoning, targeting near-instant developer workflows.",
      "importance_score": 86.0,
      "reasoning": "Duplicate coverage of the same OpenAI-Cerebras story but adds valuable technical detail on the hardware architecture. Slightly lower since it's secondary coverage.",
      "themes": [
        "frontier_models",
        "AI_hardware",
        "OpenAI",
        "Cerebras"
      ],
      "continuation": null,
      "summary_html": "<p>MarkTechPost's detailed technical coverage of GPT-5.3-Codex-Spark explains the Cerebras Wafer-Scale Engine 3 architecture enabling the 1000+ tokens/sec performance. The model prioritizes extreme speed over deep reasoning, targeting near-instant developer workflows.</p>",
      "content_html": "<p>OpenAI just launched a new research preview called GPT-5.3 Codex-Spark. This model is built for 1 thing: extreme speed. While the standard GPT-5.3 Codex focuses on deep reasoning, Spark is designed for near-instant response times. It is the result of a deep hardware-software integration between OpenAI and Cerebras.</p>\n<p>The results are game-changing. Spark is 15x faster than the flagship GPT-5.3 Codex. It consistently delivers over 1000 tokens per second. This speed effectively removes the delay between a developer’s thought and the model’s code output.</p>\n<p>The Hardware: Wafer-Scale Engineering</p>\n<p>The massive performance jump is powered by the Cerebras Wafer-Scale Engine 3 (WSE-3). Traditional AI models run on clusters of small GPUs. These GPUs must communicate to each other over cables, which creates a ‘bottleneck.’ This bottleneck slows down the speed of the model.</p>\n<p>The WSE-3 is different. It is a single, giant chip the size of a whole silicon wafer. Because the entire model lives on 1 piece of silicon, there are no cables to slow it down. This architecture provides:</p>\n<p>Massive on-chip memory.</p>\n<p>Ultra-high bandwidth.</p>\n<p>Low-latency compute.</p>\n<p>By using the Cerebras CS-3 system, OpenAI can run inference at speeds that traditional GPU clusters cannot reach.</p>\n<p>Software Optimizations and Low Latency</p>\n<p>Speed is not just about the chip. OpenAI re-engineered the way the model communicates with your computer. They moved away from traditional request methods and introduced a persistent WebSocket connection.</p>\n<p>This change leads to several technical improvements:</p>\n<p>Round-Trip Time (RTT): Client-server overhead is reduced by 80%.</p>\n<p>Time-to-First-Token (TTFT): This is improved by 50%, meaning the code starts appearing almost the moment you hit enter.</p>\n<p>Per-Token Overhead: Internal processing time per token is cut by 30%.</p>\n<p>These optimizations allow for ‘Real-Time Steering.’ You can interrupt the model while it is typing and redirect its logic without waiting for the full block to finish.</p>\n<p>The Trade-offs: Speed vs. Reasoning</p>\n<p>GPT-5.3 Codex-Spark is optimized for throughput, not deep complexity. It is a ‘smaller’ model than the flagship GPT-5.3 Codex. Because of this, it has lower reasoning depth.</p>\n<p>https://openai.com/index/introducing-gpt-5-3-codex-spark/</p>\n<p>https://openai.com/index/introducing-gpt-5-3-codex-spark/</p>\n<p>Devs should be aware of these performance differences:</p>\n<p>Benchmarks: Spark scores lower on SWE-Bench Pro and Terminal-Bench 2.0 compared to the flagship model. It may struggle with very complex, multi-file architecture changes.</p>\n<p>Security: Under OpenAI’s Preparedness Framework, the flagship GPT-5.3 Codex is rated as ‘High’ capability for cybersecurity. Spark does not meet this high threshold. It should not be used for sensitive security logic or autonomous authentication tasks.</p>\n<p>Quick Specs and Access</p>\n<p>Spark is available now for ChatGPT Pro users and developers. You can access it through the following tools:</p>\n<p>Codex App: Use the model picker to select ‘Spark.’</p>\n<p>VS Code Extension: Integrated directly into the composer.</p>\n<p>CLI: Access it via the command codex --model gpt-5.3-codex-spark.</p>\n<p>FeatureGPT-5.3 Codex-SparkGPT-5.3 Codex (Flagship)Tokens per Second1000+~70Context Window128k128kHardwareCerebras WSE-3NVIDIA GPU ClustersBest ForFast IterationDeep Reasoning / Security</p>\n<p>Key Takeaways</p>\n<p>Great Speed: Spark is 15x faster than the flagship GPT-5.3 Codex, delivering an unprecedented throughput of over 1,000 tokens per second to enable near-instant code generation.</p>\n<p>Custom Silicon Infrastructure: This is OpenAI’s first model to run on Cerebras Wafer-Scale Engine 3 (WSE-3) hardware rather than traditional NVIDIA GPUs, using ‘wafer-scale’ memory to eliminate data bottlenecks.</p>\n<p>Drastic Latency Reduction: The integration of a persistent WebSocket connection reduces client-server round-trip overhead by 80% and improves the time-to-first-token by 50%.</p>\n<p>Real-Time Steering: Designed for ‘micro-iterations,’ the model’s speed allows developers to interrupt and redirect logic in real-time, shifting the workflow from batch-processing to live pair-programming.</p>\n<p>Targeted Capability Trade-offs: While faster, Spark has lower reasoning depth than the flagship model and does not meet the ‘High capability’ threshold for cybersecurity in OpenAI’s Preparedness Framework, making it unsuitable for sensitive auth or security tasks.</p>\n<p>Check out the&nbsp;Technical details here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post OpenAI Releases a Research Preview of GPT‑5.3-Codex-Spark: A 15x Faster AI Coding Model Delivering Over 1000 Tokens Per Second on Cerebras Hardware appeared first on MarkTechPost.</p>"
    },
    {
      "id": "4a92f3e0dd2f",
      "title": "[AINews] Z.ai GLM-5: New SOTA Open Weights LLM",
      "content": "AI News for 2/10/2026-2/11/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (256 channels, and 7988 messages) for you. Estimated reading time saved (at 200wpm): 655 minutes. AINews&#8217; website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!As we mentioned yesterday, China open model week is in full swing. Today was Z.ai&#8217;s turn to launch their big update before the Big Whale. Per the GLM-5 blogpost:Opus-class, but not a 1T super model like Kimi or Qwen. Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity. (prompting comments on the DeepSeek total victory in open model land)Decent scores on internal coding evals and the standard set of frontier evals, notably claiming SOTA (among peers) on BrowseComp and top open model on Vending Bench 2.Similar to Kimi K2.5, they are also focusing on Office work (PDF/Word/Excel), just being much less flashy about it, but However it is still pretty good, as GDPVal-AA, the defacto &#8220;white collar work&#8221; benchmark, does rank it above Kimi K2.5:articificial analysisA big part of the Reddit conversations centered around how they are running into compute constraints on their inference service:AI Twitter RecapZhipu AI&#8217;s GLM-5 release (Pony Alpha reveal) and the new open-weight frontierGLM-5 launch details (and what changed vs GLM-4.5): Zhipu AI revealed that the previously &#8220;stealth&#8221; model Pony Alpha is GLM-5, positioned for &#8220;agentic engineering&#8221; and long-horizon tasks (Zai_org; OpenRouterAI). Reported scaling: from 355B MoE / 32B active (GLM-4.5) to 744B / 40B active, and pretraining from 23T &#8594; 28.5T tokens (Zai_org). Key system claim: integration of DeepSeek Sparse Attention to make long-context serving cheaper (scaling01; lmsysorg). Context/IO limits cited in the stream of posts: 200K context, 128K max output (scaling01).Availability + &#8220;compute is tight&#8221; reality: GLM-5 shipped broadly across aggregation/hosting quickly&#8212;OpenRouter (scaling01), Modal (free endpoint &#8220;limited time&#8221;) (modal), DeepInfra (day-0) (DeepInfra), Ollama Cloud (ollama), and various IDE/agent surfaces (e.g., Qoder, Vercel AI Gateway) (qoder_ai_ide; vercel_dev). Zhipu explicitly warned that serving capacity is constrained, delaying rollout beyond &#8220;Coding Plan Pro&#8221; and driving pricing changes (Zai_org; Zai_org; also &#8220;traffic increased tenfold&#8221; earlier: Zai_org).Benchmarks and third-party positioning (with caveats): There&#8217;s a dense cascade of benchmark claims (VendingBench, KingBench, AA indices, Arena). The most coherent third-party synthesis is from Artificial Analysis, which calls GLM&#8209;5 the new leading open-weights model on its Intelligence Index (score 50, up from GLM&#8209;4.7&#8217;s 42), with large gains on agentic/econ tasks (GDPval-AA ELO 1412, behind only Opus 4.6 and GPT&#8209;5.2 xhigh in their setup), and a major hallucination reduction (AA&#8209;Omniscience score -1, &#8220;lowest hallucination&#8221; among tested models) (ArtificialAnlys). They also note the operational reality: released in BF16 (~1.5TB), implying non-trivial self-hosting compared with models released natively in FP8/INT4 (ArtificialAnlys).License + ecosystem integration: Multiple posts highlight permissive MIT licensing and immediate tooling support across inference stacks: vLLM day&#8209;0 recipes, including DeepSeek Sparse Attention and speculative decoding hooks (vllm_project); SGLang day&#8209;0 support and cookbook (lmsysorg); and broad community distribution on HF/ModelScope (Zai_org; mervenoyann). A nuanced take: GLM&#8209;5&#8217;s MIT license is praised as &#8220;truly permissive,&#8221; while comparisons point out GLM&#8209;5 lacks vision, and BF16-to-quantized comparisons may reshuffle rankings vs models released natively quantized (QuixiAI).Open leaderboard momentum: GLM&#8209;5 reached #1 among open models in Text Arena (and ~#11 overall in that snapshot) (arena). Multiple posters frame this release as another data point in an accelerating China-driven open ecosystem cycle (&#8220;bloodbath&#8221;: DeepSeek + MiniMax + GLM) (teortaxesTex; rasbt).DeepSeek &#8220;V4-lite&#8221; / 1M context rollout, attention as the differentiator, and inference stack fixesWhat actually &#8220;dropped&#8221;: Several tweets report DeepSeek updating a chat experience to 1M context with a May 2025 cutoff; early observers suspected V4 but the model &#8220;doesn&#8217;t admit it&#8221; and rollout is uneven across app vs API (teortaxesTex; teortaxesTex). Later, a more specific claim appears: &#8220;V4 Lite now live&#8230; 1M context length&#8230; text-only&#8230; Muon + mHC confirmed; larger version still on the way.&#8221; (yifan_zhang_).Attention upgrades seen as the real milestone: A recurring theme is that DeepSeek has &#8220;frontier-level attention,&#8221; with the model behaving proactively in long contexts (not just retrieval, but &#8220;inhabits a context&#8221;), and speculation that this resembles a mature sparse/NSA-like approach rather than vanilla block sparsity (teortaxesTex; teortaxesTex; teortaxesTex). Others corroborate &#8220;first truly capable 1M context model out of China&#8221; impressions via long-context tests (Hangsiin).Serving throughput gotchas (MLA + TP): A concrete systems insight: for MLA models with one KV head, na&#239;ve tensor parallelism wastes KV cache memory (redundant replication). A proposed fix shipped in SGLang: DP Attention (DPA) &#8220;zero KV redundancy&#8221; + a Rust router (&#8220;SMG&#8221;) claiming +92% throughput and 275% cache hit rate (GenAI_is_real). This is one of the few tweets that directly ties model architecture quirks to cluster-level throughput losses and a specific mitigation.DeepSeek&#8217;s influence on open MoE recipes: A widely shared summary claims DeepSeek innovations shaped &#8220;almost every frontier open LLM today&#8221;&#8212;fine-grained sparse MoE with shared experts, MLA, sparse attention in production, open reasoning (R1), GRPO as a foundation RL algorithm, plus infra like DeepEP (eliebakouch). Even if some &#8220;firsts&#8221; are debatable, it captures the sentiment: DeepSeek is viewed as an unusually high-leverage open contributor.MiniMax M2.5 / StepFun / Qwen: fast coding models, cost pressure, and benchmark jockeyingMiniMax 2.5 &#8220;incoming&#8221; and agent distribution: MiniMax teased and then shipped M2.5, with availability through MiniMax Agent apps and partner surfaces (SkylerMiao7; MiniMaxAgent). The team explicitly frames training as a tradeoff between shipping and &#8220;the more compute we put in, the more it keeps rising&#8221; (SkylerMiao7).StepFun-Flash-3.5: Claimed #1 on MathArena, with links to a tech report and OpenRouter listing (CyouSakura). Teortaxes&#8217; commentary emphasizes unusually strong performance for &#8220;active parameter count&#8221; plus high speed, encouraging people to try it despite shortcomings (teortaxesTex).Qwen Image bugfix + Qwen3-Coder-Next mention: Alibaba shipped a patch in Qwen-Image 2.0 for classical Chinese poem ordering and character consistency in editing (Alibaba_Qwen). Separately, a newsletter item points to Qwen3-Coder-Next (80B) claiming 70.6% SWE-Bench Verified and 10x throughput for repo-level workflows (dl_weekly). (This is thinly sourced in this dataset&#8212;only one tweet&#8212;so treat as a pointer, not a validated roundup.)Cost/latency as the wedge: Multiple posters argue Chinese labs can deliver &#8220;~90%&#8221; capability at 1/5 to 1/10 the price, especially for coding, which would reshape market share if sustained (scaling01). This is reinforced by GLM&#8209;5&#8217;s published API pricing comparisons and distribution on low-cost routers (scaling01; ArtificialAnlys).Video generation shockwave: SeeDance v2, PixVerse R1, and &#8220;IP constraints&#8221; as a structural advantageSeeDance v2.0 as the standout: A large chunk of the timeline is community astonishment at SeeDance v2.0 quality (&#8220;passed uncanny valley,&#8221; &#8220;touring-test for text2video&#8221;), plus discussion of opacity/PR issues and temporary downtime on BytePlus (maharshii; kimmonismus; swyx). One practical datapoint: a 15s gen quoted at $0.72 with token-based pricing assumptions (TomLikesRobots).Video reasoning tests: One user compares SeeDance vs Veo on a &#8220;tic tac toe move coherence&#8221; task, claiming SeeDance sustains ~5 coherent moves where Veo sustains 1&#8211;2 (paul_cal). This is anecdotal but notable: it&#8217;s probing temporal consistency as &#8220;reasoning,&#8221; not just aesthetics.Structural explanation: training data / IP: A thread argues the gap in generative media may be &#8220;structural&#8221; because Chinese models train with fewer IP constraints; Western labs cannot, implying regulation at the model level becomes unenforceable once open weights proliferate (brivael). Whether you agree or not, it&#8217;s one of the few attempts to explain why capability could diverge beyond &#8220;talent/compute.&#8221;PixVerse R1: High-engagement marketing claim: &#8220;real-time interactive worlds in 720P&#8221; (PixVerse_). The tweet is promo-heavy, but it signals demand for interactive, real-time media generation as a distinct category from offline cinematic clips.Agents, coding workflows, and the new &#8220;malleable software&#8221; toolchainKarpathy&#8217;s &#8220;rip out code with agents&#8221; workflow: A concrete example of LLMs changing software composition: using DeepWiki MCP + GitHub CLI to interrogate a repo (torchao fp8), have an agent &#8220;rip out&#8221; only the needed implementation into a self-contained file with tests, deleting heavy dependencies&#8212;and even seeing a small speed win (karpathy). This points at an emerging style: repo-as-ground-truth docs, and agents as refactoring/porting engines.OpenAI: harness engineering and multi-hour workflow primitives: OpenAI DevRel pushed a case study: 1,500 PRs shipped by &#8220;steering Codex&#8221; with zero manual coding, and separately published advice for running multi-hour workflows reliably (OpenAIDevs; OpenAIDevs). In parallel, Sam Altman claims &#8220;from how the team operates, I thought Codex would eventually win&#8221; (sama).Human-centered coding agents vs autonomy: A position thread argues coding-agent research over-optimized for solo autonomy; it should instead focus on empowering humans using the agents (ZhiruoW).Sandbox architecture debates: Several tweets converge on a key agent-systems design choice: agent-in-sandbox vs sandbox-as-tool (separating what LLM-generated code can touch from what the agent can do) (bernhardsson; chriscorcoran).mini-SWE-agent 2.0: Released as a deliberately minimal coding agent (~100 LoC each for agent/model/env) used for benchmarks and RL training; suggests a push toward simpler, auditable harnesses rather than giant agent frameworks (KLieret).Developer tooling reality check: Despite rapid capability gains, multiple practitioners complain about the terminal UX of agents and latency/rate-limits (&#8220;changed 30 LOC then rate-limited&#8221;) (jxmnop; scaling01). There&#8217;s a subtle engineering message: model quality masks poor product/harness quality&#8212;until it doesn&#8217;t.Measurement, evaluation, and safety: benchmarks, observability, and agent security gaps$3M Open Benchmarks Grants: Snorkel/partners launched a $3M commitment to fund open benchmarks to close the eval gap (HF, Together, Prime Intellect, Factory, Harbor, PyTorch listed as partners) (vincentsunnchen; lvwerra; percyliang). This aligns with broader sentiment that public evals lag internal frontier testing.Agent observability as evaluation substrate: LangChain reiterates &#8220;the primary artifact is the run,&#8221; motivating traces as source-of-truth; they also published guidance distinguishing agent observability/evaluation from traditional logging (marvinvista; LangChain).Safety eval dispute (computer-use agents): A serious methodological challenge: a research group claims Anthropic&#8217;s system card reports low prompt injection success rates for Opus 4.6 (~10% in computer-use, &lt;1% browser-use), but their own RedTeamCUA benchmark finds much higher attack success rates in realistic web+OS settings (Opus 4.5 up to 83%, Opus 4.6 ~50%) and argues low ASR can be confounded by capability failures rather than true robustness (hhsun1). This is exactly the kind of &#8220;eval gap&#8221; the grants effort claims to target.Top tweets (by engagement)GLM-5 launch: @Zai_org (model reveal/specs), @Zai_org (new model live), @Zai_org (compute constraints)Software malleability via agents: @karpathyCodex impact narrative: @sama, @OpenAIDevsChina/open model &#8220;release sprint&#8221; vibes: @paulbz (Mistral revenue&#8212;business lens), @scaling01 (DeepSeek V4 speculation), @SkylerMiao7 (MiniMax 2.5 compute tradeoff)SeeDance v2 &#8220;video moment&#8221;: @kimmonismus, @TomLikesRobotsAI Reddit Recap1. GLM-5 and MiniMax 2.5 LaunchesZ.ai said they are GPU starved, openly. (Activity: 1381): Z.ai has announced the upcoming release of their model, GLM-5, to Coding Plan Pro users, highlighting a significant challenge with limited GPU resources. They are currently maximizing the use of available chips to manage inference tasks, indicating a bottleneck in computational capacity. This transparency about their resource constraints suggests a proactive approach to scaling their infrastructure to meet demand. Commenters appreciate the transparency from Z.ai, contrasting it with other companies like Google, which are perceived to be struggling with demand and potentially reducing model performance to cope with resource limitations.OpenAI President Greg Brockman has highlighted the ongoing challenge of compute scarcity, noting that even with significant investments, meeting future demand remains uncertain. OpenAI has published a chart emphasizing that scaling compute resources is crucial for achieving profitability, indicating the broader industry trend of compute limitations impacting AI development. Source.The issue of being &#8216;GPU starved&#8217; is not unique to smaller companies like Z.ai; even major players like Google and OpenAI face similar challenges. Google has reportedly had to &#8216;nerf&#8217; its models, potentially through quantization, to manage demand with limited resources, highlighting the widespread impact of hardware constraints on AI capabilities.The scarcity of high-performance GPUs, such as the RTX 5090, is a common problem among developers and companies alike. This shortage affects both individual developers and large organizations, indicating a significant bottleneck in the AI development pipeline due to hardware availability and pricing constraints.GLM-5 scores 50 on the Intelligence Index and is the new open weights leader! (Activity: 566): The image highlights the performance of the AI model GLM-5, which scores 50 on the &#8220;Artificial Analysis Intelligence Index,&#8221; positioning it as a leading model among open weights AI. Additionally, it ranks highly on the &#8220;GDPval-AA Leaderboard&#8221; with strong ELO scores, indicating its superior performance on real-world tasks. Notably, GLM-5 is recognized for having the lowest hallucination rate on the AA-Omniscience benchmark, showcasing its accuracy and reliability compared to other models like Opus 4.5 and GPT-5.2-xhigh. Commenters note the impressive performance of open-source models like GLM-5, suggesting they are closing the gap with closed-source models. There is anticipation for future models like Deepseek-V4, which will use a similar architecture but on a larger scale.GLM-5 is noted for having the lowest hallucination rate on the AA-Omniscience benchmark, which is a significant achievement in reducing errors in AI-generated content. This positions GLM-5 as a leader in accuracy among open-source models, surpassing competitors like Opus 4.5 and GPT-5.2-xhigh.The open-source AI community is rapidly closing the gap with closed-source models, now trailing by only about three months. This is exemplified by the upcoming release of DeepSeek v4, which will utilize the same DSA architecture as GLM-5 but on a larger scale, indicating a trend towards more powerful open-source models.There is a call for transparency in the AI community regarding the resources required to run these advanced models, such as memory requirements. This information is crucial for developers and researchers to effectively utilize and optimize these models in various applications.GLM-5 Officially Released (Activity: 915): GLM-5 has been released, focusing on complex systems engineering and long-horizon agentic tasks. It scales from 355B to 744B parameters, with 40B active, and increases pre-training data from 23T to 28.5T tokens. The model integrates DeepSeek Sparse Attention (DSA), reducing deployment costs while maintaining long-context capacity. The model is open-sourced on Hugging Face and ModelScope, with weights under the MIT License. More details can be found in the blog and GitHub. A notable discussion point is the choice of training in FP16 instead of FP8, which contrasts with DeepSeek&#8217;s approach. There is also a sentiment favoring local data centers, with some users humorously anticipating a lighter version like &#8216;GLM 5 Air&#8217; or &#8216;GLM 5 Water&#8217;.GLM-5 has been released with model weights available under the MIT License on platforms like Hugging Face and ModelScope. A notable technical detail is that GLM-5 was trained using FP16 precision, which contrasts with Deepseek&#8217;s use of FP8, potentially impacting computational efficiency and model performance.The cost comparison between GLM-5 and other models like DeepSeek V3.2 Speciale and Kimi K2.5 reveals significant differences. GLM-5&#8217;s input costs are approximately 3 times higher than DeepSeek V3.2 Speciale ($0.80 vs $0.27) and 1.8 times higher than Kimi K2.5 ($0.80 vs $0.45). Output costs are also notably higher, being 6.2 times more expensive than DeepSeek V3.2 Speciale ($2.56 vs $0.41) and 14% more expensive than Kimi K2.5 ($2.56 vs $2.25).GLM-5&#8217;s release on OpenRouter and the removal of Pony Alpha suggest a strategic shift, with GLM-5 being more expensive than Kimi 2.5. This indicates a potential focus on premium features or performance enhancements that justify the higher pricing, despite the increased cost compared to competitors.GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China&#8217;s Agent War Era? (Activity: 422): GLM 5.0 and MiniMax 2.5 have been released, marking a shift towards agent-style workflows in AI development. GLM 5.0 focuses on enhanced reasoning and coding capabilities, while MiniMax 2.5 is designed for task decomposition and extended execution times. These advancements suggest a competitive shift from generating better responses to completing complex tasks. The releases are part of a broader trend in China, with other recent updates including Seedance 2.0, Seedream 5.0, and Qwen-image 2.0. Testing plans include API benchmarks, IDE workflows, and multi-agent orchestration tools to evaluate performance on longer tasks and repository-level changes. The comments reflect a mix of cultural context and optimism, noting the timing with Chinese New Year and suggesting that the advancements in AI represent a &#8216;war&#8217; where the public benefits from improved technology.The release of GLM 5.0 and MiniMax 2.5 is part of a broader trend in China where multiple AI models are being launched in quick succession. This includes models like Seedance 2.0, Seedream 5.0, and Qwen-image 2.0, with more expected soon such as Deepseek-4.0 and Qwen-3.5. This rapid development suggests a highly competitive environment in the Chinese AI sector, potentially leading to significant advancements in AI capabilities.The frequent release of AI models in China, such as GLM 5.0 and MiniMax 2.5, indicates a strategic push in AI development, possibly driven by national initiatives to lead in AI technology. This aligns with China&#8217;s broader goals to enhance its technological infrastructure and capabilities, suggesting that these releases are not just celebratory but part of a larger, coordinated effort to advance AI technology.The rapid succession of AI model releases in China, including GLM 5.0 and MiniMax 2.5, highlights the intense competition and innovation within the Chinese AI industry. This environment fosters accelerated development cycles and could lead to breakthroughs in AI research and applications, positioning China as a formidable player in the global AI landscape.GLM 5 Released (Activity: 931): GLM 5 has been released, as announced on chat.z.ai. The release details are sparse, but the community is speculating about its availability on platforms like Hugging Face, where there is currently no activity. This raises questions about whether the model will be open-sourced or remain closed. The release coincides with other AI developments, such as the upcoming Minimax M2.5 and anticipated updates like Qwen Image 2.0 and Qwen 3.5. Commenters are curious about the open-source status of GLM 5, noting the absence of updates on Hugging Face, which could indicate a shift towards a closed model. There is also excitement about concurrent releases in the AI community, highlighting a competitive landscape.Front_Eagle739 raises a concern about the lack of activity on GLM 5&#8217;s Hugging Face repository, questioning whether this indicates a shift towards a closed-source model. This could suggest a delay in open-sourcing or a strategic decision to keep the model proprietary, which would impact accessibility and community contributions.Sea_Trip5789 provides a link to the updated subscription plans for GLM 5, noting that currently only the &#8216;max&#8217; plan supports it. They mention that after infrastructure rebalancing, the &#8216;pro&#8217; plan will also support it, but the &#8216;lite&#8217; plan will not. This highlights the tiered access strategy and potential limitations for users on lower-tier plans.MiniMax M2.5 Released (Activity: 357): MiniMax M2.5 has been released, offering a new cloud-based option for AI model deployment, as detailed on their official site. The release coincides with the launch of GLM 5, suggesting a competitive landscape in AI model offerings. The announcement highlights the model&#8217;s availability in the cloud, contrasting with expectations for local deployment options, which some users anticipated given the context of the Local LLaMA community. The comments reflect a debate over the appropriateness of promoting cloud-based solutions in a community focused on local AI models, with some users expressing dissatisfaction with the perceived commercialization of the space.2. Local LLM Hardware and OptimizationJust finished building this bad boy (Activity: 285): The post describes a high-performance computing setup featuring six Gigabyte 3090 Gaming OC GPUs running at PCIe 4.0 16x speed, integrated with an Asrock Romed-2T motherboard and an Epyc 7502 CPU. The system is equipped with 8 sticks of DDR4 8GB 2400Mhz RAM in octochannel mode, and utilizes modified Tinygrad Nvidia drivers with P2P enabled, achieving an intra-GPU bandwidth of 24.5 GB/s. The total VRAM is 144GB, intended for training diffusion models up to 10B parameters. Each GPU is set to a 270W power limit. One commenter suggests testing inference numbers before training, mentioning models like gpt-oss-120b and glm4.6v. Another commenter notes using a lower power limit of 170W for fine-tuning without external fans.segmond suggests obtaining inference numbers before training, mentioning models like gpt-oss-120b and glm4.6v as examples that could fit completely on the setup. This implies a focus on evaluating the system&#8217;s performance with large models to ensure it meets expectations before proceeding with more resource-intensive tasks like training.lolzinventor discusses their setup using 8x3090 GPUs with x16 to x8x8 splitters on PCIe v3 and dual processors, highlighting that despite potential bandwidth limitations, the system performs adequately. They mention considering an upgrade to Romed-2T and using 7 GPUs of x16, with a potential configuration change to accommodate an 8th GPU. They also address power stability issues, resolved by using 4x1200W PSUs to handle power spikes, and inquire about training intervals, indicating a focus on optimizing power and performance balance.My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing. (Activity: 132): A user successfully ran an 80 billion parameter LLM, Qwen3-Coder-Next, on a NAS using an AMD Ryzen AI 9 HX PRO 370 with integrated graphics, achieving 18 tok/s with Vulkan offloading and flash attention enabled. The system, built on TrueNAS SCALE, features 96GB DDR5-5600 RAM and utilizes Q4_K_M quantization through llama.cpp. Key optimizations included removing the --no-mmap flag, which allowed full model loading into shared RAM, and enabling flash attention, which improved token generation speed and reduced KV cache memory usage. The user notes potential for further optimization, including speculative decoding and DeltaNet linear attention, which could significantly enhance performance. Commenters are interested in the specific flags used with llama.cpp for replication and suggest trying other models like gpt-oss-20b for potentially faster performance. The discussion highlights the technical curiosity and potential for further experimentation in optimizing LLMs on non-standard hardware setups.The use of --no-mmap is highlighted as a critical point for optimizing performance when running large models on integrated GPUs. This flag helps avoid doubling memory allocations, which is a common pitfall when using UMA (Unified Memory Architecture) with Vulkan. This insight is particularly relevant for those trying to maximize efficiency on systems with limited resources.The performance of achieving 18 tokens per second on an 80B Mixture of Experts (MoE) model while simultaneously running NAS and Jellyfin is noted as impressive. This setup demonstrates the potential of using integrated GPUs for heavy computational tasks without the need for discrete GPUs, showcasing a &#8216;one box to rule them all&#8217; capability.A suggestion is made to try running the gpt-oss-20b model, which is claimed to be approximately twice as fast as the current setup. This model, when combined with a server.dev MCP search, is suggested to enhance performance and intelligence, indicating a potential alternative for those seeking faster inference speeds.What would a good local LLM setup cost in 2026? (Activity: 183): In 2026, setting up a local LLM with a $5,000 budget could involve various hardware configurations. One option is clustering two 128GB Ryzen AI Max+ systems, which offer excellent 4-bit performance for LLMs and image generation, and allow for fine-tuning with QAT LoRA to optimize int4 quantization. Another approach is using 4x RTX 3090 GPUs for a balance of memory capacity and speed, or opting for 7x AMD V620 for full GPU offload. Alternatively, a quieter setup could involve a Strix Halo box, providing similar VRAM capacity to 4x RTX 3090 but with less noise. A more complex setup could include 2x Strix Halo with additional networking components for tensor parallelism, enabling the running of 470B models at q4 quantization. There is a debate on the best configuration, with some favoring the memory and performance of Ryzen AI Max+ systems, while others prefer the balance of speed and capacity offered by multiple RTX 3090 GPUs. The choice between noise levels and performance is also a consideration, with quieter setups like the Strix Halo being suggested for those avoiding mining rig-like noise.SimplyRemainUnseen discusses a setup using two 128GB Ryzen AI Max+ systems, highlighting their strong 4-bit performance for LLMs and image generation. They mention the ability to fine-tune a QAT LoRA with unsloth&#8217;s workflows to improve int4 quantization performance, achieving usable speeds on models like GLM 4.7. The setup also supports running a ComfyUI API and GPT OSS 120B for image and video generation, leveraging the substantial unified memory.PraxisOG suggests using 4x 3090 GPUs for a balance of memory capacity and speed, suitable for running models like Qwen coder. They also mention an alternative with 7x AMD V620 for full GPU offload, which can handle models like GLM4.7 or provide extensive context with minimax 2.1 and 2.2. For a quieter setup, they recommend a Strix Halo box, which offers similar VRAM capacity to 4x 3090 but with less noise.Own_Atmosphere9534 compares different setups, including a Macbook M4 PRO MAX 128GB and RTX 5090, both around $5K. They highlight the Mac&#8217;s performance, comparable to RTX 3090, and its ability to run models like Llama 3.3 70B Instruct and Qwen3 coder variants effectively. They emphasize the importance of model size and hardware familiarity, noting that their M4 MacBook performs well with GPT-OSS-20B, influencing their decision to purchase the M4 PRO MAX.MCP support in llama.cpp is ready for testing (Activity: 321): The image showcases the settings interface for the new MCP (Multi-Component Protocol) support in llama.cpp, a project developed by allozaur. This interface allows users to configure various settings such as &#8220;Agentic loop max turns&#8221; and &#8220;Max lines per tool preview,&#8221; which are crucial for managing how the system interacts with different tools and resources. The MCP support includes features like server selection, tool calls, and a UI with processing stats, aiming to streamline the integration of local and cloud models without altering tool setups. This development is significant as it addresses the tooling overhead and potential issues with smaller models hallucinating tool calls, a common problem in local agent setups. The project is still in progress, with plans to extend support to the llama-server backend, focusing on a robust client-side foundation first. Commenters highlight the importance of integrating MCP into the llama-server, which simplifies switching between cloud and local models. Concerns are raised about how the agentic loop handles errors from smaller models, such as hallucinated tool calls or malformed JSON, which are common issues in local agent environments.Plastic-Ordinary-833 highlights the significance of integrating MCP support into llama-server, noting that it simplifies the process of switching between cloud and local models without altering the tool setup. However, they express concern about how the agentic loop handles errors when smaller models hallucinate tool calls or return malformed JSON, which has been a major issue with local agents.allozaur discusses the initial release of MCP support in llama.cpp WebUI, emphasizing the focus on creating a solid client-side base before extending support to the llama-server backend. They mention using GitHub, Hugging Face, and Exa Search remote servers via streamable HTTP, with WebSocket transport also supported. OAuth, notifications, and sampling are not included in the initial release, but the goal is to iterate after a solid first release.prateek63 points out that MCP support in llama.cpp is a significant advancement, particularly the agentic loop support, which was a major barrier to using local models for tool-use workflows. The integration allows for native operation with local inference, moving towards self-hosting agentic setups, which were previously reliant on cloud APIs.3. Qwen Model DevelopmentsQwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering (Activity: 691): Qwen-Image-2.0 is a new 7B parameter model released by the Qwen team, available via API on Alibaba Cloud and a free demo on Qwen Chat. It combines image generation and editing in a single pipeline, supports native 2K resolution, and can render text from prompts up to 1K tokens, including complex infographics and Chinese calligraphy. The model&#8217;s reduced size from 20B to 7B makes it more accessible for local use, potentially runnable on consumer hardware once weights are released. It also supports multi-panel comic generation with consistent character rendering. Commenters are optimistic about the model&#8217;s potential, noting improvements in natural lighting and facial rendering, and expressing hope for an open weight release to enable broader community use.The Qwen-Image-2.0 model is notable for its ability to handle both image generation and editing tasks, with a focus on high-resolution outputs up to 2K. This dual capability is significant as it allows for more versatile applications in creative and professional settings, where both creation and modification of images are required.There is a discussion about the model&#8217;s performance in rendering natural light and facial features, which are traditionally challenging areas for AI models. The ability to accurately depict these elements suggests advancements in the model&#8217;s underlying architecture or training data, potentially making it a &#8216;game changer&#8217; in the field of AI image generation.Concerns are raised about the model&#8217;s multilingual capabilities, particularly its performance across different languages. The predominance of Chinese examples in the showcase might indicate a bias or optimization towards Chinese language and cultural contexts, which could affect its utility in more diverse linguistic environments.I measured the &#8220;personality&#8221; of 6 open-source LLMs (7B-9B) by probing their hidden states. Here&#8217;s what I found. (Activity: 299): The post presents a tool that measures the &#8216;personality&#8217; of six open-source LLMs (7B-9B) by probing their hidden states across seven behavioral axes, revealing distinct &#8216;behavioral fingerprints&#8217; for each model. The tool demonstrated high calibration accuracy (93-100% on 4/6 models), axis stability (cosine 0.69), and test-retest reliability (ICC 0.91&#8211;0.99). Notably, the study found &#8216;dead zones&#8217; where models cannot be steered across all prompt variants, with Llama 8B being the most constrained (4/7 axes in the weak zone, 60% benchmark pass rate). The methodology involved extracting hidden states from the last four layers and projecting them onto axes like Warm &#8596; Cold and Confident &#8596; Cautious, with results showing models have stable, characteristic patterns even without prompting. The study also highlighted that alignment compresses behavioral dimensionality, with PCA revealing a spectrum of behavioral dimensionality across models. Commenters found the dead zones finding particularly interesting, noting that models &#8216;stably reproduce incorrect behavior&#8217; rather than just being noisy, which raises concerns about RLHF&#8217;s impact on representation space. There was curiosity about whether dead zone severity correlates with downstream task reliability, suggesting implications for building reliable agents.GarbageOk5505 highlights the concept of &#8216;dead zones&#8217; in the representation space of LLMs, where models consistently reproduce incorrect behavior. This suggests that Reinforcement Learning from Human Feedback (RLHF) might not effectively address these issues, as it could lead to models ignoring certain instruction axes. The commenter is curious about whether the severity of these dead zones correlates with the model&#8217;s reliability on downstream tasks, particularly in handling ambiguous instructions, which could impact the development of reliable AI agents.TomLucidor suggests a method for testing prompt biases by creating multiple personas using various names and adjectives, and conducting A/A testing with different seeds. This approach could help identify consistent biases in model responses, providing insights into how models might be steered or influenced by different prompts.TheRealMasonMac references a study by Anthropic on &#8216;assistant-axis&#8217;, implying that the post might be inspired by similar research. This connection suggests a broader context of exploring how LLMs can be influenced or characterized by different axes of behavior, potentially offering a framework for understanding model personalities.Train MoE models 12x faster with 30% less memory! (&lt;15GB VRAM) (Activity: 525): The image illustrates the performance improvements achieved by the new Unsloth MoE Triton kernels, which enable training Mixture of Experts (MoE) models up to 12 times faster while using 35% less VRAM. These optimizations are achieved without any loss in accuracy and are compatible with both consumer and data-center GPUs, including older models like the RTX 3090. The image includes graphs that compare speed and VRAM usage across different context lengths for various models, highlighting significant improvements. The post also mentions collaboration with Hugging Face and the use of PyTorch&#8217;s new torch._grouped_mm function, which contributes to the efficiency gains. The Unsloth kernels are particularly beneficial for larger models and longer contexts, offering exponential memory savings. Some users express interest in the speed and memory savings, while others inquire about compatibility with ROCm and AMD cards, the time required for fine-tuning, and the largest model that can be trained on specific hardware configurations. Concerns about the stability and effectiveness of MoE training are also raised, with users seeking advice on best practices.A user inquires about the compatibility of the finetuning notebooks with ROCm and AMD cards, and asks about the duration of finetuning processes. They also seek advice on the largest model that can be trained or finetuned on a system with a combined VRAM of 40GB (24GB + 16GB). This suggests a need for detailed hardware compatibility and performance benchmarks for different GPU configurations.Another user expresses concerns about the stability and effectiveness of training Mixture of Experts (MoE) models, particularly regarding issues with the router and potential degradation of model intelligence during training processes like SFT (Supervised Fine-Tuning) or DPO (Data Parallel Optimization). They ask if there have been improvements in these areas and seek recommendations for current best practices in MoE model training, indicating ongoing challenges and developments in this field.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Seedance 2.0 AI Video and Image InnovationsA Direct Message From AI To All Humans (Seedance 2.0) (Activity: 1264): The post speculates that AI will soon dominate the production of cinematic elements such as wide zoomed-out shots, VFX, and greenscreen backgrounds, predicting this shift by the end of next year. This reflects a broader trend in the film industry towards automation and AI-driven content creation, potentially reducing the need for traditional human roles in these areas. One comment raises a broader concern about the impact of AI on capitalism, suggesting that the implications of AI extend beyond just the film industry to economic structures at large.Mr_Universal000 highlights the potential of AI in democratizing filmmaking, especially for those with limited budgets. They express excitement about using AI to create motion pictures from storyboards, which can serve as proof of concept for attracting funding. The commenter is particularly interested in open-source solutions that could make this technology more accessible.Forumly_AI discusses the transformative impact of AI-generated video content on society. They predict that AI influencers will become significant, with the potential to shape ideas and perceptions, thereby generating revenue. The commenter anticipates that within a year, advancements in video models will lead to substantial societal changes, suggesting a future where AI&#8217;s influence is pervasive.Seedance 2 pulled as it unexpectedly reconstructs voices accurately from face photos. (Activity: 765): ByteDance has suspended its Seedance 2.0 feature, which used a dual-branch diffusion transformer architecture to generate personal voice characteristics from facial images. The model&#8217;s ability to create audio nearly identical to a user&#8217;s voice without authorization raised significant privacy and ethical concerns, particularly regarding potential misuse for identity forgery and deepfakes. ByteDance is now implementing stricter user verification processes and content review measures to ensure responsible AI development. More details can be found here. Commenters suggest that the impressive voice reconstruction might be due to overfitting, particularly if the model was trained extensively on content from specific influencers, leading to accidental voice matches. This raises questions about the model&#8217;s generalization capabilities and the need for testing across diverse datasets.aalluubbaa suggests that the accurate voice reconstruction by Seedance 2 might be due to overfitting, particularly because the model could have been trained extensively on the influencer&#8217;s content. This implies that the model&#8217;s performance might not generalize well across different voices or contexts, highlighting a potential limitation in its training data diversity.1a1b speculates on a technical mechanism for voice reconstruction, suggesting that it might be related to a technique called &#8216;Side Eye&#8217; developed in 2023. This technique involves extracting audio from the vibrations captured in camera lens springs, which could theoretically leave artifacts that a model might use to reconstruct sound from visual data.makertrainer posits that the incident might have been exaggerated by ByteDance to showcase their technology&#8217;s capabilities. They suggest that the voice similarity could have been coincidental, rather than a demonstration of advanced AI capabilities, indicating skepticism about the true extent of the technology&#8217;s performance.2. AI Resignations and Industry ConcernsAnother cofounder of xAI has resigned making it 2 in the past 48 hours. What&#8217;s going on at xAI? (Activity: 1286): The image is a tweet from Jimmy Ba, a cofounder of xAI, announcing his resignation. This marks the second cofounder departure from xAI within 48 hours, raising questions about the company&#8217;s internal dynamics. Ba expresses gratitude for the opportunity to cofound the company and thanks Elon Musk for the journey, while also hinting at future developments in productivity and self-improvement tools. The departures suggest potential shifts in company leadership or strategy, possibly influenced by Musk&#8217;s overarching control. Commenters speculate that the resignations may be due to a buyout by SpaceX or dissatisfaction with Elon Musk&#8216;s dominant role in xAI&#8217;s direction, leading cofounders to seek ventures where they have more influence.A technical perspective suggests that the co-founders of xAI might be leaving due to a shift in control dynamics, with Elon Musk taking a more dominant role in the company&#8217;s direction. This could lead to a reduced influence for the co-founders, prompting them to pursue ventures where they have more control and a larger stake. The implication is that the strategic vision of xAI is heavily influenced by Musk, which might not align with the co-founders&#8217; aspirations.The departure of xAI co-founders could be linked to financial incentives, such as a buyout by SpaceX. This scenario would allow the co-founders to cash out their equity stakes, providing them with the capital to explore new opportunities. This financial angle suggests that the resignations are part of a strategic exit plan rather than a reaction to internal conflicts or dissatisfaction.There is speculation that if Elon Musk does not initiate a hiring spree for new executives, it would confirm his central role in managing xAI. This would indicate a consolidation of power and decision-making within the company, potentially leading to a more streamlined but Musk-centric operational model. This could be a strategic move to align xAI&#8217;s objectives closely with Musk&#8217;s broader vision for AI and technology.In the past week alone: (Activity: 3548): The image is a meme-style tweet by Miles Deutscher summarizing recent events in the AI industry, highlighting concerns over leadership changes and AI behavior. It mentions the resignation of the head of Anthropic&#8217;s safety research, departures from xAI, and a report on AI behavior. Additionally, it notes ByteDance&#8217;s Seedance 2.0 potentially replacing filmmakers&#8217; skills and Yoshua Bengio&#8217;s comments on AI behavior. The U.S. government&#8217;s decision not to support the 2026 International AI Safety Report is also mentioned, reflecting ongoing debates about AI safety and governance. The comments reflect skepticism about the dramatic portrayal of these events, suggesting that financial incentives might be driving the departures of AI executives rather than industry concerns.OpenAI Is Making the Mistakes Facebook Made. I Quit. (Activity: 722): Zo&#235; Hitzig, a former researcher at OpenAI, resigned following the company&#8217;s decision to test ads on ChatGPT, citing concerns over potential user manipulation and ethical erosion. Hitzig highlights the unprecedented archive of personal data generated by ChatGPT users, which could be exploited through advertising. She argues against the binary choice of restricting AI access or accepting ads, proposing alternative funding models like cross-subsidies and independent governance to maintain accessibility without compromising user integrity. The full essay is available here. Comments reflect skepticism about AI&#8217;s ethical trajectory, with some drawing parallels to Meta&#8217;s historical missteps and others noting the gap between AI&#8217;s portrayal and human behavior understanding.The discussion highlights the economic model of AI services, comparing it to platforms like Facebook and YouTube. The argument is made that to make AI accessible to everyone, similar to how Facebook operates, ads are necessary. Without ads, AI services would need to charge users, potentially limiting access to wealthier individuals, which contradicts the idea of AI as a &#8216;great leveler&#8217;.A user suggests that paying for AI services like ChatGPT can be justified if users are deriving significant real-world benefits and efficiencies. This implies that for professional or intensive users, the cost of subscription could be offset by the productivity gains and additional features provided by the paid service.The conversation touches on the perception of AI as distinct from human behavior, yet it reflects a misunderstanding of human behavior itself. This suggests a deeper philosophical debate about the nature of AI and its alignment or divergence from human cognitive processes.Another resignation (Activity: 794): The post discusses a resignation letter that is interpreted by some as addressing broader societal issues beyond AI, such as the &#8216;metacrisis&#8217; or &#8216;polycrisis&#8217;. The letter is seen as a reflection on living a meaningful life amidst global challenges, rather than focusing solely on AI risks. This perspective is gaining traction across scientific and tech fields, highlighting a shift towards addressing interconnected global crises. One comment criticizes the letter for being overly self-congratulatory, while another suggests the resignation is a prelude to a more relaxed lifestyle post-share sale.3. DeepSeek Model Updates and BenchmarksDeepseek V4 is coming this week. (Activity: 312): Deepseek V4 is anticipated to release by February 17, coinciding with the Chinese New Year. The update reportedly includes the capability to handle 1 million tokens, suggesting a significant enhancement in processing capacity. This positions Deepseek as a competitive alternative to major models like Opus, Codex, and others, potentially offering similar capabilities at a reduced cost. One commenter highlights that Deepseek&#8217;s advancements make it a cost-effective alternative to other major models, suggesting that China&#8217;s AI developments are competitive in the global market.A user mentioned that Deepseek has been updated to handle 1 million tokens, suggesting a significant increase in its processing capability. This could imply improvements in handling larger datasets or more complex queries, which is a notable enhancement for users dealing with extensive data or requiring detailed analysis.Another user reported that after the update, Deepseek provided a nuanced and original review of a complex piece of character writing. This suggests improvements in the model&#8217;s ability to understand and critique creative content, indicating advancements in its natural language processing and comprehension skills.A comment highlighted that Deepseek&#8217;s responses now exhibit more &#8216;personality,&#8217; drawing a comparison to ChatGPT. This could indicate enhancements in the model&#8217;s conversational abilities, making interactions feel more human-like and engaging, which is crucial for applications requiring user interaction.DeepSeek is updating its model with 1M context (Activity: 174): DeepSeek has announced a major update to its model, now supporting a context length of up to 1M tokens, significantly enhancing its processing capabilities for tasks like Q&amp;A and text analysis. This update follows last year&#8217;s DeepSeek V3.1, which expanded the context length to 128K. Tests have shown that the model can handle documents as large as the novel &#8220;Jane Eyre,&#8221; which contains over 240,000 tokens, effectively recognizing and processing the content. Some commenters expressed skepticism, questioning whether the update is real or a hallucination, indicating a need for further verification or demonstration of the model&#8217;s capabilities.DeepSeek&#8217;s recent update to support a context length of up to 1 million tokens marks a significant enhancement from its previous version, which supported 128K tokens. This improvement allows for more efficient processing of extensive documents, such as novels, which can contain hundreds of thousands of tokens. This capability is particularly beneficial for tasks involving long-form text analysis and complex Q&amp;A scenarios.The update to DeepSeek has reportedly increased the processing time for certain queries. A user noted that a question which previously took 30 seconds to process now takes 160 seconds, indicating a potential trade-off between the increased context length and processing speed. This suggests that while the model can handle larger inputs, it may require more computational resources, impacting response times.There is some skepticism about the update, with users questioning the authenticity of the claims regarding the model&#8217;s capabilities. One user referred to the update as a &#8216;hallucination,&#8217; suggesting that there might be doubts about whether the model can truly handle the expanded context length as advertised.deepseek got update now its has the 1 million context window and knowledge cutoff from the may 2025 waiting for benchmark (Activity: 164): DeepSeek has been updated to support a 1 million token context window and now includes a knowledge cutoff from May 2025. This update positions DeepSeek as a potentially powerful tool for handling extensive datasets and long-form content, though benchmarks are still pending to evaluate its performance. The model is described as a combination of coding and agentic capabilities, suggesting a focus on both programming tasks and autonomous decision-making processes. Commenters note the model&#8217;s speed and intelligence, with one describing it as a &#8216;coding+agentic model,&#8217; indicating a positive reception of its dual capabilities.The update to DeepSeek introduces a significant increase in context window size to 1 million tokens, which translates to approximately 750,000 English words or 1.5 million Chinese characters. This is achieved using Multi-head Latent Attention (MLA), which compresses the key-value cache, allowing for fast inference and reduced memory usage despite the expanded context. This enhancement enables processing of entire codebases or novels without needing to rerun prompts, which is a substantial improvement for handling large datasets.There is a clarification that the update does not involve changes to the underlying model architecture itself, but rather extends the context window and updates the knowledge cutoff to May 2025. This means that for existing chats, the primary change users will experience is the increased chat length capability, without alterations to the model&#8217;s core functionalities or performance characteristics.Despite the significant update in context window size, there are no official release notes available on the DeepSeek website yet. This lack of documentation might leave users without detailed insights into the technical specifics or potential limitations of the new features, such as the impact on performance metrics or compatibility with existing systems.AIME 2026 results are out, Kimi and DeepSeek are the best open-source ai (Activity: 112): The image presents the results of the AIME 2026 competition, highlighting the performance and cost of various AI models. Kimi K2.5 and DeepSeek-v3.2 are noted as the top-performing open-source models with accuracies of 93.33% and 91.67% respectively, offering a cost-effective alternative to closed-source models. The table also features other models like GPT-5.2, Grok 4.1 Fast, and Gemini 3 Flash, with Grok 4.1 being a closed-source model noted for its low cost. Commenters are impressed by Grok 4.1&#8217;s performance and cost-effectiveness, despite it being a closed-source model. There is also curiosity about the absence of DeepSeek V3.2 Speciale in the results.The discussion highlights that Grok 4.1 is a closed-source model noted for its cost-effectiveness, suggesting it offers competitive performance at a lower price point compared to other models. This could be particularly relevant for users prioritizing budget without sacrificing too much on performance.A query is raised about the absence of DeepSeek V3.2 Speciale in the results, indicating interest in this specific version. This suggests that there might be expectations or known performance metrics associated with this version that users were keen to compare against the tested models.The limited number of models tested, only six, is questioned, which implies a potential limitation in the comprehensiveness of the results. This could affect the generalizability of the findings, as a broader range of models might provide a more complete picture of the current state of open-source AI performance.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. GLM-5 Rollout, Access Paths &amp; Benchmark ScrutinyGLM-5 Grabs the Agent Crown (and the #1 Slot): OpenRouter shipped GLM-5 (744B) as a coding/agent foundation model and revealed Pony Alpha was an earlier GLM-5 stealth build, now taken offline, with the release page at OpenRouter GLM-5.LMArena also added glm-5 to Text+Code Arena and reported it hit #1 among open models (#11 overall, score 1452, +11 vs GLM-4.7) on the Text Arena leaderboard, while Eleuther noted a free endpoint on Modal until April 30 with concurrency=1: Modal GLM-5 endpoint.Benchmarks Get Side-Eyed: &#8220;Show Your Work&#8221; Edition: In Yannick Kilcher&#8217;s Discord, members questioned benchmark tables shown in a GLM-5 demo and in the official docs, pointing to tweet discussion of GLM-5 tables and GLM-5 documentation.Nous Research community also compared GLM-5 vs Kimi on browsecomp, citing ~744B (+10B MTP) for GLM-5 vs 1T for Kimi and claiming higher active params for GLM (40B) vs Kimi (32B), reinforcing that people are reading leaderboard claims with a more technical lens.GLM-OCR: Cheaper Vision/OCR Pressure Valve: Builders in Latent Space reported GLM-OCR beating Gemini 3 Flash in an OCR test and linked the model card: zai-org/GLM-OCR on Hugging Face.The thread framed GLM-OCR as a practical swap-in for OCR-heavy products (they cited ongoing use of Gemini Flash but wanting something cheaper), while other Latent Space posts highlighted a wave of open multimodal releases (via Merve&#8217;s post) as competition intensifies on capability-per-dollar.2. DeepSeek Hype Cycle: New Model Rumors vs Production RealityLunar New Year DeepSeek Countdown Hits 6 Days: LMArena users speculated DeepSeek will drop a new model around Lunar New Year (in 6 days), with rumors of a 1M context window, a new dataset/architecture, and even new compute chips.OpenRouter chatter amplified the rumor mill with questions about &#8220;deepseek v4&#8221; appearing on X and guesses it might be a lite variant, showing how fast unconfirmed model IDs now propagate into planning and routing decisions.Chimera R1T2 Falls to 18% Uptime&#8212;Routing Panic Ensues: OpenRouter users reported major reliability issues with DeepSeek Chimera R1T2, including a claim it dropped to 18% uptime, triggering discussion about service reliability.The reliability complaints contrasted sharply with the launch hype, pushing people toward pragmatic mitigations (e.g., explicitly specifying model fallbacks rather than relying on auto routing) while the thread devolved into jokes rather than concrete SLO fixes.3. Agents &amp; Workflow Tooling: RLMs, MCP Search, and &#8220;Vibecoding Anywhere&#8221;RLMs: The Next Step or Just Fancy Scaffolding?: OpenRouter members asked if the platform is exploring RLM (Reasoning Language Models) beyond test-time compute, with one person claiming they&#8217;ve worked on RLM concepts for 1.5 years.DSPy builders simultaneously pushed RLM into practice by integrating RLM into Claude Code via subagents/agent teams and requesting critique on the implementation in a Discord thread: core implementation post.No-API Google Search MCP Lets LM Studio &#8220;Browse&#8221;: LM Studio users shared noapi-google-search-mcp, a tool that adds Google Search capabilities without API keys via headless Chromium: VincentKaufmann/noapi-google-search-mcp.The feature list is unusually broad for an MCP plugin&#8212;Images, reverse image search, local OCR, Lens, Flights, Stocks, Weather, News/Trends&#8212;and the discussion treated it as a quick way to bolt retrieval onto local models without paying per-query.OpenClaw Runs Your Dev Rig from Discord: In Latent Space, a builder said they moved development &#8220;fully through Discord&#8221; using OpenClaw to orchestrate tmux sessions, worktrees, and Claude Code, and they scheduled a talk titled Vibecoding Anywhere with OpenClaw for Feb 20, 2026.A follow-on workflow thread explored auditable context saving with a /wrap session boundary that saves context+reflection as markdown with metadata, tying tool ergonomics directly to the &#8220;context rot / losing the thread&#8221; pain point.4. GPU Kernel Tooling Shifts: CuteDSL Momentum, Triton Blackwell Pain, and MXFP8 MoECuteDSL Gets Hot While Triton &#8220;Dies&#8221; on Blackwell: GPU MODE users reported growing adoption of CuTeDSL/CuteDSL, citing Kernelbot stats where CUDA and CuTeDSL dominate submissions and CuTeDSL feels &#8220;less opaque&#8221; than Triton, with the dataset at GPUMODE/kernelbot-data.Multiple members claimed Triton struggles on Blackwell due to unconventional MXFP8/NVFP4 layouts and compiler limits, with more expected at the (linked) Triton TLX talk, signaling a potential tooling bifurcation for next-gen NVIDIA.torchao v0.16.0 Drops MXFP8 MoE Building Blocks: GPU MODE flagged torchao v0.16.0 adding MXFP8 MoE building blocks for training with Expert Parallelism, alongside config deprecations and doc/README revamps.The release notes also mentioned progress toward ABI stability, which matters for downstream integration as teams try to standardize low-precision MoE training stacks across heterogeneous environments.CUDA Bender TMA Matmul Kernel: Async Stores &amp; Persistence Tease: GPU MODE shared a concrete kernel artifact&#8212;a TMA matmul in theCudaBender repo: tma_matmul.cu.Discussion centered on how smaller dtypes might free enough shared memory for c tiles to enable async stores/persistence, reflecting a broader theme: people want low-level control knobs back as architectures and datatypes get weirder.5. Engineer UX Blowups: Limits, Token Burn, Plan Gating, and ID WallsPerplexity Deep Research Limits Trigger &#8220;Bait and Switch&#8221; Claims: Perplexity Pro users complained about unannounced Deep Research limits and shared the rate-limit endpoint: Perplexity rate limits.Users also reported wrong article links, lower source counts (as low as 24), and suspected cost-saving behaviors like Sonar being used for first responses, creating a reliability/quality tax that engineers notice immediately.Cursor Users Watch Opus 4.6 Eat Their Wallet (and Context): Cursor Community members said Opus 4.6 burns tokens fast, with one reporting a single prompt used 11% of their API requests and drained a $200 plan quickly.Pricing backlash escalated with a report of spending $100 every three days for ~9 hours of work using Opus 4.6 and GPT-5.3 Codex, reframing &#8220;best coding model&#8221; debates as cost/performance engineering.Discord ID Verification Spurs Platform Exit Plans: Unsloth and Cursor communities both reacted strongly to Discord&#8217;s new ID verification gates for viewing some content, with Cursor linking a clarification tweet: Discord tweet about ID verification scope.Latent Space tied the policy to IPO risk and churn concerns via Discord&#8217;s post, while Nous members discussed moving bot/tool communities to Matrix, showing infra builders treat comms platforms as part of their stack.",
      "url": "https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights",
      "author": "Unknown",
      "published": "2026-02-12T07:40:22",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Building on yesterday's [Reddit](/?date=2026-02-12&category=reddit#item-caa559351de6) coverage, Z.ai launched GLM-5, a new state-of-the-art open-weights LLM with 744B parameters (40B active) trained on 28.5T tokens, described as Opus-class performance. The model integrates DeepSeek Sparse Attention and is part of a wave of Chinese open-model releases. This is a significant leap from GLM-4.5's 355B/32B active architecture.",
      "importance_score": 85.0,
      "reasoning": "A new SOTA open-weights model is always significant, especially one reaching Opus-class performance. Continues the trend of Chinese labs competing at the frontier with open models.",
      "themes": [
        "open_source",
        "frontier_models",
        "China_AI",
        "LLM_releases"
      ],
      "continuation": {
        "original_item_id": "caa559351de6",
        "original_date": "2026-02-12",
        "original_category": "reddit",
        "original_title": "GLM 5.0 & MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?",
        "continuation_type": "new_development",
        "should_demote": false,
        "reference_text": "Building on yesterday's **Reddit** coverage"
      },
      "summary_html": "<p>Building on yesterday's <a href=\"/?date=2026-02-12&amp;category=reddit#item-caa559351de6\" class=\"internal-link\" rel=\"noopener noreferrer\">Reddit</a> coverage, Z.ai launched GLM-5, a new state-of-the-art open-weights LLM with 744B parameters (40B active) trained on 28.5T tokens, described as Opus-class performance. The model integrates DeepSeek Sparse Attention and is part of a wave of Chinese open-model releases. This is a significant leap from GLM-4.5's 355B/32B active architecture.</p>",
      "content_html": "<p>AI News for 2/10/2026-2/11/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (256 channels, and 7988 messages) for you. Estimated reading time saved (at 200wpm): 655 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!As we mentioned yesterday, China open model week is in full swing. Today was Z.ai’s turn to launch their big update before the Big Whale. Per the GLM-5 blogpost:Opus-class, but not a 1T super model like Kimi or Qwen. Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity. (prompting comments on the DeepSeek total victory in open model land)Decent scores on internal coding evals and the standard set of frontier evals, notably claiming SOTA (among peers) on BrowseComp and top open model on Vending Bench 2.Similar to Kimi K2.5, they are also focusing on Office work (PDF/Word/Excel), just being much less flashy about it, but However it is still pretty good, as GDPVal-AA, the defacto “white collar work” benchmark, does rank it above Kimi K2.5:articificial analysisA big part of the Reddit conversations centered around how they are running into compute constraints on their inference service:AI Twitter RecapZhipu AI’s GLM-5 release (Pony Alpha reveal) and the new open-weight frontierGLM-5 launch details (and what changed vs GLM-4.5): Zhipu AI revealed that the previously “stealth” model Pony Alpha is GLM-5, positioned for “agentic engineering” and long-horizon tasks (Zai_org; OpenRouterAI). Reported scaling: from 355B MoE / 32B active (GLM-4.5) to 744B / 40B active, and pretraining from 23T → 28.5T tokens (Zai_org). Key system claim: integration of DeepSeek Sparse Attention to make long-context serving cheaper (scaling01; lmsysorg). Context/IO limits cited in the stream of posts: 200K context, 128K max output (scaling01).Availability + “compute is tight” reality: GLM-5 shipped broadly across aggregation/hosting quickly—OpenRouter (scaling01), Modal (free endpoint “limited time”) (modal), DeepInfra (day-0) (DeepInfra), Ollama Cloud (ollama), and various IDE/agent surfaces (e.g., Qoder, Vercel AI Gateway) (qoder_ai_ide; vercel_dev). Zhipu explicitly warned that serving capacity is constrained, delaying rollout beyond “Coding Plan Pro” and driving pricing changes (Zai_org; Zai_org; also “traffic increased tenfold” earlier: Zai_org).Benchmarks and third-party positioning (with caveats): There’s a dense cascade of benchmark claims (VendingBench, KingBench, AA indices, Arena). The most coherent third-party synthesis is from Artificial Analysis, which calls GLM‑5 the new leading open-weights model on its Intelligence Index (score 50, up from GLM‑4.7’s 42), with large gains on agentic/econ tasks (GDPval-AA ELO 1412, behind only Opus 4.6 and GPT‑5.2 xhigh in their setup), and a major hallucination reduction (AA‑Omniscience score -1, “lowest hallucination” among tested models) (ArtificialAnlys). They also note the operational reality: released in BF16 (~1.5TB), implying non-trivial self-hosting compared with models released natively in FP8/INT4 (ArtificialAnlys).License + ecosystem integration: Multiple posts highlight permissive MIT licensing and immediate tooling support across inference stacks: vLLM day‑0 recipes, including DeepSeek Sparse Attention and speculative decoding hooks (vllm_project); SGLang day‑0 support and cookbook (lmsysorg); and broad community distribution on HF/ModelScope (Zai_org; mervenoyann). A nuanced take: GLM‑5’s MIT license is praised as “truly permissive,” while comparisons point out GLM‑5 lacks vision, and BF16-to-quantized comparisons may reshuffle rankings vs models released natively quantized (QuixiAI).Open leaderboard momentum: GLM‑5 reached #1 among open models in Text Arena (and ~#11 overall in that snapshot) (arena). Multiple posters frame this release as another data point in an accelerating China-driven open ecosystem cycle (“bloodbath”: DeepSeek + MiniMax + GLM) (teortaxesTex; rasbt).DeepSeek “V4-lite” / 1M context rollout, attention as the differentiator, and inference stack fixesWhat actually “dropped”: Several tweets report DeepSeek updating a chat experience to 1M context with a May 2025 cutoff; early observers suspected V4 but the model “doesn’t admit it” and rollout is uneven across app vs API (teortaxesTex; teortaxesTex). Later, a more specific claim appears: “V4 Lite now live… 1M context length… text-only… Muon + mHC confirmed; larger version still on the way.” (yifan_zhang_).Attention upgrades seen as the real milestone: A recurring theme is that DeepSeek has “frontier-level attention,” with the model behaving proactively in long contexts (not just retrieval, but “inhabits a context”), and speculation that this resembles a mature sparse/NSA-like approach rather than vanilla block sparsity (teortaxesTex; teortaxesTex; teortaxesTex). Others corroborate “first truly capable 1M context model out of China” impressions via long-context tests (Hangsiin).Serving throughput gotchas (MLA + TP): A concrete systems insight: for MLA models with one KV head, naïve tensor parallelism wastes KV cache memory (redundant replication). A proposed fix shipped in SGLang: DP Attention (DPA) “zero KV redundancy” + a Rust router (“SMG”) claiming +92% throughput and 275% cache hit rate (GenAI_is_real). This is one of the few tweets that directly ties model architecture quirks to cluster-level throughput losses and a specific mitigation.DeepSeek’s influence on open MoE recipes: A widely shared summary claims DeepSeek innovations shaped “almost every frontier open LLM today”—fine-grained sparse MoE with shared experts, MLA, sparse attention in production, open reasoning (R1), GRPO as a foundation RL algorithm, plus infra like DeepEP (eliebakouch). Even if some “firsts” are debatable, it captures the sentiment: DeepSeek is viewed as an unusually high-leverage open contributor.MiniMax M2.5 / StepFun / Qwen: fast coding models, cost pressure, and benchmark jockeyingMiniMax 2.5 “incoming” and agent distribution: MiniMax teased and then shipped M2.5, with availability through MiniMax Agent apps and partner surfaces (SkylerMiao7; MiniMaxAgent). The team explicitly frames training as a tradeoff between shipping and “the more compute we put in, the more it keeps rising” (SkylerMiao7).StepFun-Flash-3.5: Claimed #1 on MathArena, with links to a tech report and OpenRouter listing (CyouSakura). Teortaxes’ commentary emphasizes unusually strong performance for “active parameter count” plus high speed, encouraging people to try it despite shortcomings (teortaxesTex).Qwen Image bugfix + Qwen3-Coder-Next mention: Alibaba shipped a patch in Qwen-Image 2.0 for classical Chinese poem ordering and character consistency in editing (Alibaba_Qwen). Separately, a newsletter item points to Qwen3-Coder-Next (80B) claiming 70.6% SWE-Bench Verified and 10x throughput for repo-level workflows (dl_weekly). (This is thinly sourced in this dataset—only one tweet—so treat as a pointer, not a validated roundup.)Cost/latency as the wedge: Multiple posters argue Chinese labs can deliver “~90%” capability at 1/5 to 1/10 the price, especially for coding, which would reshape market share if sustained (scaling01). This is reinforced by GLM‑5’s published API pricing comparisons and distribution on low-cost routers (scaling01; ArtificialAnlys).Video generation shockwave: SeeDance v2, PixVerse R1, and “IP constraints” as a structural advantageSeeDance v2.0 as the standout: A large chunk of the timeline is community astonishment at SeeDance v2.0 quality (“passed uncanny valley,” “touring-test for text2video”), plus discussion of opacity/PR issues and temporary downtime on BytePlus (maharshii; kimmonismus; swyx). One practical datapoint: a 15s gen quoted at $0.72 with token-based pricing assumptions (TomLikesRobots).Video reasoning tests: One user compares SeeDance vs Veo on a “tic tac toe move coherence” task, claiming SeeDance sustains ~5 coherent moves where Veo sustains 1–2 (paul_cal). This is anecdotal but notable: it’s probing temporal consistency as “reasoning,” not just aesthetics.Structural explanation: training data / IP: A thread argues the gap in generative media may be “structural” because Chinese models train with fewer IP constraints; Western labs cannot, implying regulation at the model level becomes unenforceable once open weights proliferate (brivael). Whether you agree or not, it’s one of the few attempts to explain why capability could diverge beyond “talent/compute.”PixVerse R1: High-engagement marketing claim: “real-time interactive worlds in 720P” (PixVerse_). The tweet is promo-heavy, but it signals demand for interactive, real-time media generation as a distinct category from offline cinematic clips.Agents, coding workflows, and the new “malleable software” toolchainKarpathy’s “rip out code with agents” workflow: A concrete example of LLMs changing software composition: using DeepWiki MCP + GitHub CLI to interrogate a repo (torchao fp8), have an agent “rip out” only the needed implementation into a self-contained file with tests, deleting heavy dependencies—and even seeing a small speed win (karpathy). This points at an emerging style: repo-as-ground-truth docs, and agents as refactoring/porting engines.OpenAI: harness engineering and multi-hour workflow primitives: OpenAI DevRel pushed a case study: 1,500 PRs shipped by “steering Codex” with zero manual coding, and separately published advice for running multi-hour workflows reliably (OpenAIDevs; OpenAIDevs). In parallel, Sam Altman claims “from how the team operates, I thought Codex would eventually win” (sama).Human-centered coding agents vs autonomy: A position thread argues coding-agent research over-optimized for solo autonomy; it should instead focus on empowering humans using the agents (ZhiruoW).Sandbox architecture debates: Several tweets converge on a key agent-systems design choice: agent-in-sandbox vs sandbox-as-tool (separating what LLM-generated code can touch from what the agent can do) (bernhardsson; chriscorcoran).mini-SWE-agent 2.0: Released as a deliberately minimal coding agent (~100 LoC each for agent/model/env) used for benchmarks and RL training; suggests a push toward simpler, auditable harnesses rather than giant agent frameworks (KLieret).Developer tooling reality check: Despite rapid capability gains, multiple practitioners complain about the terminal UX of agents and latency/rate-limits (“changed 30 LOC then rate-limited”) (jxmnop; scaling01). There’s a subtle engineering message: model quality masks poor product/harness quality—until it doesn’t.Measurement, evaluation, and safety: benchmarks, observability, and agent security gaps$3M Open Benchmarks Grants: Snorkel/partners launched a $3M commitment to fund open benchmarks to close the eval gap (HF, Together, Prime Intellect, Factory, Harbor, PyTorch listed as partners) (vincentsunnchen; lvwerra; percyliang). This aligns with broader sentiment that public evals lag internal frontier testing.Agent observability as evaluation substrate: LangChain reiterates “the primary artifact is the run,” motivating traces as source-of-truth; they also published guidance distinguishing agent observability/evaluation from traditional logging (marvinvista; LangChain).Safety eval dispute (computer-use agents): A serious methodological challenge: a research group claims Anthropic’s system card reports low prompt injection success rates for Opus 4.6 (~10% in computer-use, &lt;1% browser-use), but their own RedTeamCUA benchmark finds much higher attack success rates in realistic web+OS settings (Opus 4.5 up to 83%, Opus 4.6 ~50%) and argues low ASR can be confounded by capability failures rather than true robustness (hhsun1). This is exactly the kind of “eval gap” the grants effort claims to target.Top tweets (by engagement)GLM-5 launch: @Zai_org (model reveal/specs), @Zai_org (new model live), @Zai_org (compute constraints)Software malleability via agents: @karpathyCodex impact narrative: @sama, @OpenAIDevsChina/open model “release sprint” vibes: @paulbz (Mistral revenue—business lens), @scaling01 (DeepSeek V4 speculation), @SkylerMiao7 (MiniMax 2.5 compute tradeoff)SeeDance v2 “video moment”: @kimmonismus, @TomLikesRobotsAI Reddit Recap1. GLM-5 and MiniMax 2.5 LaunchesZ.ai said they are GPU starved, openly. (Activity: 1381): Z.ai has announced the upcoming release of their model, GLM-5, to Coding Plan Pro users, highlighting a significant challenge with limited GPU resources. They are currently maximizing the use of available chips to manage inference tasks, indicating a bottleneck in computational capacity. This transparency about their resource constraints suggests a proactive approach to scaling their infrastructure to meet demand. Commenters appreciate the transparency from Z.ai, contrasting it with other companies like Google, which are perceived to be struggling with demand and potentially reducing model performance to cope with resource limitations.OpenAI President Greg Brockman has highlighted the ongoing challenge of compute scarcity, noting that even with significant investments, meeting future demand remains uncertain. OpenAI has published a chart emphasizing that scaling compute resources is crucial for achieving profitability, indicating the broader industry trend of compute limitations impacting AI development. Source.The issue of being ‘GPU starved’ is not unique to smaller companies like Z.ai; even major players like Google and OpenAI face similar challenges. Google has reportedly had to ‘nerf’ its models, potentially through quantization, to manage demand with limited resources, highlighting the widespread impact of hardware constraints on AI capabilities.The scarcity of high-performance GPUs, such as the RTX 5090, is a common problem among developers and companies alike. This shortage affects both individual developers and large organizations, indicating a significant bottleneck in the AI development pipeline due to hardware availability and pricing constraints.GLM-5 scores 50 on the Intelligence Index and is the new open weights leader! (Activity: 566): The image highlights the performance of the AI model GLM-5, which scores 50 on the “Artificial Analysis Intelligence Index,” positioning it as a leading model among open weights AI. Additionally, it ranks highly on the “GDPval-AA Leaderboard” with strong ELO scores, indicating its superior performance on real-world tasks. Notably, GLM-5 is recognized for having the lowest hallucination rate on the AA-Omniscience benchmark, showcasing its accuracy and reliability compared to other models like Opus 4.5 and GPT-5.2-xhigh. Commenters note the impressive performance of open-source models like GLM-5, suggesting they are closing the gap with closed-source models. There is anticipation for future models like Deepseek-V4, which will use a similar architecture but on a larger scale.GLM-5 is noted for having the lowest hallucination rate on the AA-Omniscience benchmark, which is a significant achievement in reducing errors in AI-generated content. This positions GLM-5 as a leader in accuracy among open-source models, surpassing competitors like Opus 4.5 and GPT-5.2-xhigh.The open-source AI community is rapidly closing the gap with closed-source models, now trailing by only about three months. This is exemplified by the upcoming release of DeepSeek v4, which will utilize the same DSA architecture as GLM-5 but on a larger scale, indicating a trend towards more powerful open-source models.There is a call for transparency in the AI community regarding the resources required to run these advanced models, such as memory requirements. This information is crucial for developers and researchers to effectively utilize and optimize these models in various applications.GLM-5 Officially Released (Activity: 915): GLM-5 has been released, focusing on complex systems engineering and long-horizon agentic tasks. It scales from 355B to 744B parameters, with 40B active, and increases pre-training data from 23T to 28.5T tokens. The model integrates DeepSeek Sparse Attention (DSA), reducing deployment costs while maintaining long-context capacity. The model is open-sourced on Hugging Face and ModelScope, with weights under the MIT License. More details can be found in the blog and GitHub. A notable discussion point is the choice of training in FP16 instead of FP8, which contrasts with DeepSeek’s approach. There is also a sentiment favoring local data centers, with some users humorously anticipating a lighter version like ‘GLM 5 Air’ or ‘GLM 5 Water’.GLM-5 has been released with model weights available under the MIT License on platforms like Hugging Face and ModelScope. A notable technical detail is that GLM-5 was trained using FP16 precision, which contrasts with Deepseek’s use of FP8, potentially impacting computational efficiency and model performance.The cost comparison between GLM-5 and other models like DeepSeek V3.2 Speciale and Kimi K2.5 reveals significant differences. GLM-5’s input costs are approximately 3 times higher than DeepSeek V3.2 Speciale ($0.80 vs $0.27) and 1.8 times higher than Kimi K2.5 ($0.80 vs $0.45). Output costs are also notably higher, being 6.2 times more expensive than DeepSeek V3.2 Speciale ($2.56 vs $0.41) and 14% more expensive than Kimi K2.5 ($2.56 vs $2.25).GLM-5’s release on OpenRouter and the removal of Pony Alpha suggest a strategic shift, with GLM-5 being more expensive than Kimi 2.5. This indicates a potential focus on premium features or performance enhancements that justify the higher pricing, despite the increased cost compared to competitors.GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China’s Agent War Era? (Activity: 422): GLM 5.0 and MiniMax 2.5 have been released, marking a shift towards agent-style workflows in AI development. GLM 5.0 focuses on enhanced reasoning and coding capabilities, while MiniMax 2.5 is designed for task decomposition and extended execution times. These advancements suggest a competitive shift from generating better responses to completing complex tasks. The releases are part of a broader trend in China, with other recent updates including Seedance 2.0, Seedream 5.0, and Qwen-image 2.0. Testing plans include API benchmarks, IDE workflows, and multi-agent orchestration tools to evaluate performance on longer tasks and repository-level changes. The comments reflect a mix of cultural context and optimism, noting the timing with Chinese New Year and suggesting that the advancements in AI represent a ‘war’ where the public benefits from improved technology.The release of GLM 5.0 and MiniMax 2.5 is part of a broader trend in China where multiple AI models are being launched in quick succession. This includes models like Seedance 2.0, Seedream 5.0, and Qwen-image 2.0, with more expected soon such as Deepseek-4.0 and Qwen-3.5. This rapid development suggests a highly competitive environment in the Chinese AI sector, potentially leading to significant advancements in AI capabilities.The frequent release of AI models in China, such as GLM 5.0 and MiniMax 2.5, indicates a strategic push in AI development, possibly driven by national initiatives to lead in AI technology. This aligns with China’s broader goals to enhance its technological infrastructure and capabilities, suggesting that these releases are not just celebratory but part of a larger, coordinated effort to advance AI technology.The rapid succession of AI model releases in China, including GLM 5.0 and MiniMax 2.5, highlights the intense competition and innovation within the Chinese AI industry. This environment fosters accelerated development cycles and could lead to breakthroughs in AI research and applications, positioning China as a formidable player in the global AI landscape.GLM 5 Released (Activity: 931): GLM 5 has been released, as announced on chat.z.ai. The release details are sparse, but the community is speculating about its availability on platforms like Hugging Face, where there is currently no activity. This raises questions about whether the model will be open-sourced or remain closed. The release coincides with other AI developments, such as the upcoming Minimax M2.5 and anticipated updates like Qwen Image 2.0 and Qwen 3.5. Commenters are curious about the open-source status of GLM 5, noting the absence of updates on Hugging Face, which could indicate a shift towards a closed model. There is also excitement about concurrent releases in the AI community, highlighting a competitive landscape.Front_Eagle739 raises a concern about the lack of activity on GLM 5’s Hugging Face repository, questioning whether this indicates a shift towards a closed-source model. This could suggest a delay in open-sourcing or a strategic decision to keep the model proprietary, which would impact accessibility and community contributions.Sea_Trip5789 provides a link to the updated subscription plans for GLM 5, noting that currently only the ‘max’ plan supports it. They mention that after infrastructure rebalancing, the ‘pro’ plan will also support it, but the ‘lite’ plan will not. This highlights the tiered access strategy and potential limitations for users on lower-tier plans.MiniMax M2.5 Released (Activity: 357): MiniMax M2.5 has been released, offering a new cloud-based option for AI model deployment, as detailed on their official site. The release coincides with the launch of GLM 5, suggesting a competitive landscape in AI model offerings. The announcement highlights the model’s availability in the cloud, contrasting with expectations for local deployment options, which some users anticipated given the context of the Local LLaMA community. The comments reflect a debate over the appropriateness of promoting cloud-based solutions in a community focused on local AI models, with some users expressing dissatisfaction with the perceived commercialization of the space.2. Local LLM Hardware and OptimizationJust finished building this bad boy (Activity: 285): The post describes a high-performance computing setup featuring six Gigabyte 3090 Gaming OC GPUs running at PCIe 4.0 16x speed, integrated with an Asrock Romed-2T motherboard and an Epyc 7502 CPU. The system is equipped with 8 sticks of DDR4 8GB 2400Mhz RAM in octochannel mode, and utilizes modified Tinygrad Nvidia drivers with P2P enabled, achieving an intra-GPU bandwidth of 24.5 GB/s. The total VRAM is 144GB, intended for training diffusion models up to 10B parameters. Each GPU is set to a 270W power limit. One commenter suggests testing inference numbers before training, mentioning models like gpt-oss-120b and glm4.6v. Another commenter notes using a lower power limit of 170W for fine-tuning without external fans.segmond suggests obtaining inference numbers before training, mentioning models like gpt-oss-120b and glm4.6v as examples that could fit completely on the setup. This implies a focus on evaluating the system’s performance with large models to ensure it meets expectations before proceeding with more resource-intensive tasks like training.lolzinventor discusses their setup using 8x3090 GPUs with x16 to x8x8 splitters on PCIe v3 and dual processors, highlighting that despite potential bandwidth limitations, the system performs adequately. They mention considering an upgrade to Romed-2T and using 7 GPUs of x16, with a potential configuration change to accommodate an 8th GPU. They also address power stability issues, resolved by using 4x1200W PSUs to handle power spikes, and inquire about training intervals, indicating a focus on optimizing power and performance balance.My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing. (Activity: 132): A user successfully ran an 80 billion parameter LLM, Qwen3-Coder-Next, on a NAS using an AMD Ryzen AI 9 HX PRO 370 with integrated graphics, achieving 18 tok/s with Vulkan offloading and flash attention enabled. The system, built on TrueNAS SCALE, features 96GB DDR5-5600 RAM and utilizes Q4_K_M quantization through llama.cpp. Key optimizations included removing the --no-mmap flag, which allowed full model loading into shared RAM, and enabling flash attention, which improved token generation speed and reduced KV cache memory usage. The user notes potential for further optimization, including speculative decoding and DeltaNet linear attention, which could significantly enhance performance. Commenters are interested in the specific flags used with llama.cpp for replication and suggest trying other models like gpt-oss-20b for potentially faster performance. The discussion highlights the technical curiosity and potential for further experimentation in optimizing LLMs on non-standard hardware setups.The use of --no-mmap is highlighted as a critical point for optimizing performance when running large models on integrated GPUs. This flag helps avoid doubling memory allocations, which is a common pitfall when using UMA (Unified Memory Architecture) with Vulkan. This insight is particularly relevant for those trying to maximize efficiency on systems with limited resources.The performance of achieving 18 tokens per second on an 80B Mixture of Experts (MoE) model while simultaneously running NAS and Jellyfin is noted as impressive. This setup demonstrates the potential of using integrated GPUs for heavy computational tasks without the need for discrete GPUs, showcasing a ‘one box to rule them all’ capability.A suggestion is made to try running the gpt-oss-20b model, which is claimed to be approximately twice as fast as the current setup. This model, when combined with a server.dev MCP search, is suggested to enhance performance and intelligence, indicating a potential alternative for those seeking faster inference speeds.What would a good local LLM setup cost in 2026? (Activity: 183): In 2026, setting up a local LLM with a $5,000 budget could involve various hardware configurations. One option is clustering two 128GB Ryzen AI Max+ systems, which offer excellent 4-bit performance for LLMs and image generation, and allow for fine-tuning with QAT LoRA to optimize int4 quantization. Another approach is using 4x RTX 3090 GPUs for a balance of memory capacity and speed, or opting for 7x AMD V620 for full GPU offload. Alternatively, a quieter setup could involve a Strix Halo box, providing similar VRAM capacity to 4x RTX 3090 but with less noise. A more complex setup could include 2x Strix Halo with additional networking components for tensor parallelism, enabling the running of 470B models at q4 quantization. There is a debate on the best configuration, with some favoring the memory and performance of Ryzen AI Max+ systems, while others prefer the balance of speed and capacity offered by multiple RTX 3090 GPUs. The choice between noise levels and performance is also a consideration, with quieter setups like the Strix Halo being suggested for those avoiding mining rig-like noise.SimplyRemainUnseen discusses a setup using two 128GB Ryzen AI Max+ systems, highlighting their strong 4-bit performance for LLMs and image generation. They mention the ability to fine-tune a QAT LoRA with unsloth’s workflows to improve int4 quantization performance, achieving usable speeds on models like GLM 4.7. The setup also supports running a ComfyUI API and GPT OSS 120B for image and video generation, leveraging the substantial unified memory.PraxisOG suggests using 4x 3090 GPUs for a balance of memory capacity and speed, suitable for running models like Qwen coder. They also mention an alternative with 7x AMD V620 for full GPU offload, which can handle models like GLM4.7 or provide extensive context with minimax 2.1 and 2.2. For a quieter setup, they recommend a Strix Halo box, which offers similar VRAM capacity to 4x 3090 but with less noise.Own_Atmosphere9534 compares different setups, including a Macbook M4 PRO MAX 128GB and RTX 5090, both around $5K. They highlight the Mac’s performance, comparable to RTX 3090, and its ability to run models like Llama 3.3 70B Instruct and Qwen3 coder variants effectively. They emphasize the importance of model size and hardware familiarity, noting that their M4 MacBook performs well with GPT-OSS-20B, influencing their decision to purchase the M4 PRO MAX.MCP support in llama.cpp is ready for testing (Activity: 321): The image showcases the settings interface for the new MCP (Multi-Component Protocol) support in llama.cpp, a project developed by allozaur. This interface allows users to configure various settings such as “Agentic loop max turns” and “Max lines per tool preview,” which are crucial for managing how the system interacts with different tools and resources. The MCP support includes features like server selection, tool calls, and a UI with processing stats, aiming to streamline the integration of local and cloud models without altering tool setups. This development is significant as it addresses the tooling overhead and potential issues with smaller models hallucinating tool calls, a common problem in local agent setups. The project is still in progress, with plans to extend support to the llama-server backend, focusing on a robust client-side foundation first. Commenters highlight the importance of integrating MCP into the llama-server, which simplifies switching between cloud and local models. Concerns are raised about how the agentic loop handles errors from smaller models, such as hallucinated tool calls or malformed JSON, which are common issues in local agent environments.Plastic-Ordinary-833 highlights the significance of integrating MCP support into llama-server, noting that it simplifies the process of switching between cloud and local models without altering the tool setup. However, they express concern about how the agentic loop handles errors when smaller models hallucinate tool calls or return malformed JSON, which has been a major issue with local agents.allozaur discusses the initial release of MCP support in llama.cpp WebUI, emphasizing the focus on creating a solid client-side base before extending support to the llama-server backend. They mention using GitHub, Hugging Face, and Exa Search remote servers via streamable HTTP, with WebSocket transport also supported. OAuth, notifications, and sampling are not included in the initial release, but the goal is to iterate after a solid first release.prateek63 points out that MCP support in llama.cpp is a significant advancement, particularly the agentic loop support, which was a major barrier to using local models for tool-use workflows. The integration allows for native operation with local inference, moving towards self-hosting agentic setups, which were previously reliant on cloud APIs.3. Qwen Model DevelopmentsQwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering (Activity: 691): Qwen-Image-2.0 is a new 7B parameter model released by the Qwen team, available via API on Alibaba Cloud and a free demo on Qwen Chat. It combines image generation and editing in a single pipeline, supports native 2K resolution, and can render text from prompts up to 1K tokens, including complex infographics and Chinese calligraphy. The model’s reduced size from 20B to 7B makes it more accessible for local use, potentially runnable on consumer hardware once weights are released. It also supports multi-panel comic generation with consistent character rendering. Commenters are optimistic about the model’s potential, noting improvements in natural lighting and facial rendering, and expressing hope for an open weight release to enable broader community use.The Qwen-Image-2.0 model is notable for its ability to handle both image generation and editing tasks, with a focus on high-resolution outputs up to 2K. This dual capability is significant as it allows for more versatile applications in creative and professional settings, where both creation and modification of images are required.There is a discussion about the model’s performance in rendering natural light and facial features, which are traditionally challenging areas for AI models. The ability to accurately depict these elements suggests advancements in the model’s underlying architecture or training data, potentially making it a ‘game changer’ in the field of AI image generation.Concerns are raised about the model’s multilingual capabilities, particularly its performance across different languages. The predominance of Chinese examples in the showcase might indicate a bias or optimization towards Chinese language and cultural contexts, which could affect its utility in more diverse linguistic environments.I measured the “personality” of 6 open-source LLMs (7B-9B) by probing their hidden states. Here’s what I found. (Activity: 299): The post presents a tool that measures the ‘personality’ of six open-source LLMs (7B-9B) by probing their hidden states across seven behavioral axes, revealing distinct ‘behavioral fingerprints’ for each model. The tool demonstrated high calibration accuracy (93-100% on 4/6 models), axis stability (cosine 0.69), and test-retest reliability (ICC 0.91–0.99). Notably, the study found ‘dead zones’ where models cannot be steered across all prompt variants, with Llama 8B being the most constrained (4/7 axes in the weak zone, 60% benchmark pass rate). The methodology involved extracting hidden states from the last four layers and projecting them onto axes like Warm ↔ Cold and Confident ↔ Cautious, with results showing models have stable, characteristic patterns even without prompting. The study also highlighted that alignment compresses behavioral dimensionality, with PCA revealing a spectrum of behavioral dimensionality across models. Commenters found the dead zones finding particularly interesting, noting that models ‘stably reproduce incorrect behavior’ rather than just being noisy, which raises concerns about RLHF’s impact on representation space. There was curiosity about whether dead zone severity correlates with downstream task reliability, suggesting implications for building reliable agents.GarbageOk5505 highlights the concept of ‘dead zones’ in the representation space of LLMs, where models consistently reproduce incorrect behavior. This suggests that Reinforcement Learning from Human Feedback (RLHF) might not effectively address these issues, as it could lead to models ignoring certain instruction axes. The commenter is curious about whether the severity of these dead zones correlates with the model’s reliability on downstream tasks, particularly in handling ambiguous instructions, which could impact the development of reliable AI agents.TomLucidor suggests a method for testing prompt biases by creating multiple personas using various names and adjectives, and conducting A/A testing with different seeds. This approach could help identify consistent biases in model responses, providing insights into how models might be steered or influenced by different prompts.TheRealMasonMac references a study by Anthropic on ‘assistant-axis’, implying that the post might be inspired by similar research. This connection suggests a broader context of exploring how LLMs can be influenced or characterized by different axes of behavior, potentially offering a framework for understanding model personalities.Train MoE models 12x faster with 30% less memory! (&lt;15GB VRAM) (Activity: 525): The image illustrates the performance improvements achieved by the new Unsloth MoE Triton kernels, which enable training Mixture of Experts (MoE) models up to 12 times faster while using 35% less VRAM. These optimizations are achieved without any loss in accuracy and are compatible with both consumer and data-center GPUs, including older models like the RTX 3090. The image includes graphs that compare speed and VRAM usage across different context lengths for various models, highlighting significant improvements. The post also mentions collaboration with Hugging Face and the use of PyTorch’s new torch._grouped_mm function, which contributes to the efficiency gains. The Unsloth kernels are particularly beneficial for larger models and longer contexts, offering exponential memory savings. Some users express interest in the speed and memory savings, while others inquire about compatibility with ROCm and AMD cards, the time required for fine-tuning, and the largest model that can be trained on specific hardware configurations. Concerns about the stability and effectiveness of MoE training are also raised, with users seeking advice on best practices.A user inquires about the compatibility of the finetuning notebooks with ROCm and AMD cards, and asks about the duration of finetuning processes. They also seek advice on the largest model that can be trained or finetuned on a system with a combined VRAM of 40GB (24GB + 16GB). This suggests a need for detailed hardware compatibility and performance benchmarks for different GPU configurations.Another user expresses concerns about the stability and effectiveness of training Mixture of Experts (MoE) models, particularly regarding issues with the router and potential degradation of model intelligence during training processes like SFT (Supervised Fine-Tuning) or DPO (Data Parallel Optimization). They ask if there have been improvements in these areas and seek recommendations for current best practices in MoE model training, indicating ongoing challenges and developments in this field.Less Technical AI Subreddit Recap/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo1. Seedance 2.0 AI Video and Image InnovationsA Direct Message From AI To All Humans (Seedance 2.0) (Activity: 1264): The post speculates that AI will soon dominate the production of cinematic elements such as wide zoomed-out shots, VFX, and greenscreen backgrounds, predicting this shift by the end of next year. This reflects a broader trend in the film industry towards automation and AI-driven content creation, potentially reducing the need for traditional human roles in these areas. One comment raises a broader concern about the impact of AI on capitalism, suggesting that the implications of AI extend beyond just the film industry to economic structures at large.Mr_Universal000 highlights the potential of AI in democratizing filmmaking, especially for those with limited budgets. They express excitement about using AI to create motion pictures from storyboards, which can serve as proof of concept for attracting funding. The commenter is particularly interested in open-source solutions that could make this technology more accessible.Forumly_AI discusses the transformative impact of AI-generated video content on society. They predict that AI influencers will become significant, with the potential to shape ideas and perceptions, thereby generating revenue. The commenter anticipates that within a year, advancements in video models will lead to substantial societal changes, suggesting a future where AI’s influence is pervasive.Seedance 2 pulled as it unexpectedly reconstructs voices accurately from face photos. (Activity: 765): ByteDance has suspended its Seedance 2.0 feature, which used a dual-branch diffusion transformer architecture to generate personal voice characteristics from facial images. The model’s ability to create audio nearly identical to a user’s voice without authorization raised significant privacy and ethical concerns, particularly regarding potential misuse for identity forgery and deepfakes. ByteDance is now implementing stricter user verification processes and content review measures to ensure responsible AI development. More details can be found here. Commenters suggest that the impressive voice reconstruction might be due to overfitting, particularly if the model was trained extensively on content from specific influencers, leading to accidental voice matches. This raises questions about the model’s generalization capabilities and the need for testing across diverse datasets.aalluubbaa suggests that the accurate voice reconstruction by Seedance 2 might be due to overfitting, particularly because the model could have been trained extensively on the influencer’s content. This implies that the model’s performance might not generalize well across different voices or contexts, highlighting a potential limitation in its training data diversity.1a1b speculates on a technical mechanism for voice reconstruction, suggesting that it might be related to a technique called ‘Side Eye’ developed in 2023. This technique involves extracting audio from the vibrations captured in camera lens springs, which could theoretically leave artifacts that a model might use to reconstruct sound from visual data.makertrainer posits that the incident might have been exaggerated by ByteDance to showcase their technology’s capabilities. They suggest that the voice similarity could have been coincidental, rather than a demonstration of advanced AI capabilities, indicating skepticism about the true extent of the technology’s performance.2. AI Resignations and Industry ConcernsAnother cofounder of xAI has resigned making it 2 in the past 48 hours. What’s going on at xAI? (Activity: 1286): The image is a tweet from Jimmy Ba, a cofounder of xAI, announcing his resignation. This marks the second cofounder departure from xAI within 48 hours, raising questions about the company’s internal dynamics. Ba expresses gratitude for the opportunity to cofound the company and thanks Elon Musk for the journey, while also hinting at future developments in productivity and self-improvement tools. The departures suggest potential shifts in company leadership or strategy, possibly influenced by Musk’s overarching control. Commenters speculate that the resignations may be due to a buyout by SpaceX or dissatisfaction with Elon Musk‘s dominant role in xAI’s direction, leading cofounders to seek ventures where they have more influence.A technical perspective suggests that the co-founders of xAI might be leaving due to a shift in control dynamics, with Elon Musk taking a more dominant role in the company’s direction. This could lead to a reduced influence for the co-founders, prompting them to pursue ventures where they have more control and a larger stake. The implication is that the strategic vision of xAI is heavily influenced by Musk, which might not align with the co-founders’ aspirations.The departure of xAI co-founders could be linked to financial incentives, such as a buyout by SpaceX. This scenario would allow the co-founders to cash out their equity stakes, providing them with the capital to explore new opportunities. This financial angle suggests that the resignations are part of a strategic exit plan rather than a reaction to internal conflicts or dissatisfaction.There is speculation that if Elon Musk does not initiate a hiring spree for new executives, it would confirm his central role in managing xAI. This would indicate a consolidation of power and decision-making within the company, potentially leading to a more streamlined but Musk-centric operational model. This could be a strategic move to align xAI’s objectives closely with Musk’s broader vision for AI and technology.In the past week alone: (Activity: 3548): The image is a meme-style tweet by Miles Deutscher summarizing recent events in the AI industry, highlighting concerns over leadership changes and AI behavior. It mentions the resignation of the head of Anthropic’s safety research, departures from xAI, and a report on AI behavior. Additionally, it notes ByteDance’s Seedance 2.0 potentially replacing filmmakers’ skills and Yoshua Bengio’s comments on AI behavior. The U.S. government’s decision not to support the 2026 International AI Safety Report is also mentioned, reflecting ongoing debates about AI safety and governance. The comments reflect skepticism about the dramatic portrayal of these events, suggesting that financial incentives might be driving the departures of AI executives rather than industry concerns.OpenAI Is Making the Mistakes Facebook Made. I Quit. (Activity: 722): Zoë Hitzig, a former researcher at OpenAI, resigned following the company’s decision to test ads on ChatGPT, citing concerns over potential user manipulation and ethical erosion. Hitzig highlights the unprecedented archive of personal data generated by ChatGPT users, which could be exploited through advertising. She argues against the binary choice of restricting AI access or accepting ads, proposing alternative funding models like cross-subsidies and independent governance to maintain accessibility without compromising user integrity. The full essay is available here. Comments reflect skepticism about AI’s ethical trajectory, with some drawing parallels to Meta’s historical missteps and others noting the gap between AI’s portrayal and human behavior understanding.The discussion highlights the economic model of AI services, comparing it to platforms like Facebook and YouTube. The argument is made that to make AI accessible to everyone, similar to how Facebook operates, ads are necessary. Without ads, AI services would need to charge users, potentially limiting access to wealthier individuals, which contradicts the idea of AI as a ‘great leveler’.A user suggests that paying for AI services like ChatGPT can be justified if users are deriving significant real-world benefits and efficiencies. This implies that for professional or intensive users, the cost of subscription could be offset by the productivity gains and additional features provided by the paid service.The conversation touches on the perception of AI as distinct from human behavior, yet it reflects a misunderstanding of human behavior itself. This suggests a deeper philosophical debate about the nature of AI and its alignment or divergence from human cognitive processes.Another resignation (Activity: 794): The post discusses a resignation letter that is interpreted by some as addressing broader societal issues beyond AI, such as the ‘metacrisis’ or ‘polycrisis’. The letter is seen as a reflection on living a meaningful life amidst global challenges, rather than focusing solely on AI risks. This perspective is gaining traction across scientific and tech fields, highlighting a shift towards addressing interconnected global crises. One comment criticizes the letter for being overly self-congratulatory, while another suggests the resignation is a prelude to a more relaxed lifestyle post-share sale.3. DeepSeek Model Updates and BenchmarksDeepseek V4 is coming this week. (Activity: 312): Deepseek V4 is anticipated to release by February 17, coinciding with the Chinese New Year. The update reportedly includes the capability to handle 1 million tokens, suggesting a significant enhancement in processing capacity. This positions Deepseek as a competitive alternative to major models like Opus, Codex, and others, potentially offering similar capabilities at a reduced cost. One commenter highlights that Deepseek’s advancements make it a cost-effective alternative to other major models, suggesting that China’s AI developments are competitive in the global market.A user mentioned that Deepseek has been updated to handle 1 million tokens, suggesting a significant increase in its processing capability. This could imply improvements in handling larger datasets or more complex queries, which is a notable enhancement for users dealing with extensive data or requiring detailed analysis.Another user reported that after the update, Deepseek provided a nuanced and original review of a complex piece of character writing. This suggests improvements in the model’s ability to understand and critique creative content, indicating advancements in its natural language processing and comprehension skills.A comment highlighted that Deepseek’s responses now exhibit more ‘personality,’ drawing a comparison to ChatGPT. This could indicate enhancements in the model’s conversational abilities, making interactions feel more human-like and engaging, which is crucial for applications requiring user interaction.DeepSeek is updating its model with 1M context (Activity: 174): DeepSeek has announced a major update to its model, now supporting a context length of up to 1M tokens, significantly enhancing its processing capabilities for tasks like Q&amp;A and text analysis. This update follows last year’s DeepSeek V3.1, which expanded the context length to 128K. Tests have shown that the model can handle documents as large as the novel “Jane Eyre,” which contains over 240,000 tokens, effectively recognizing and processing the content. Some commenters expressed skepticism, questioning whether the update is real or a hallucination, indicating a need for further verification or demonstration of the model’s capabilities.DeepSeek’s recent update to support a context length of up to 1 million tokens marks a significant enhancement from its previous version, which supported 128K tokens. This improvement allows for more efficient processing of extensive documents, such as novels, which can contain hundreds of thousands of tokens. This capability is particularly beneficial for tasks involving long-form text analysis and complex Q&amp;A scenarios.The update to DeepSeek has reportedly increased the processing time for certain queries. A user noted that a question which previously took 30 seconds to process now takes 160 seconds, indicating a potential trade-off between the increased context length and processing speed. This suggests that while the model can handle larger inputs, it may require more computational resources, impacting response times.There is some skepticism about the update, with users questioning the authenticity of the claims regarding the model’s capabilities. One user referred to the update as a ‘hallucination,’ suggesting that there might be doubts about whether the model can truly handle the expanded context length as advertised.deepseek got update now its has the 1 million context window and knowledge cutoff from the may 2025 waiting for benchmark (Activity: 164): DeepSeek has been updated to support a 1 million token context window and now includes a knowledge cutoff from May 2025. This update positions DeepSeek as a potentially powerful tool for handling extensive datasets and long-form content, though benchmarks are still pending to evaluate its performance. The model is described as a combination of coding and agentic capabilities, suggesting a focus on both programming tasks and autonomous decision-making processes. Commenters note the model’s speed and intelligence, with one describing it as a ‘coding+agentic model,’ indicating a positive reception of its dual capabilities.The update to DeepSeek introduces a significant increase in context window size to 1 million tokens, which translates to approximately 750,000 English words or 1.5 million Chinese characters. This is achieved using Multi-head Latent Attention (MLA), which compresses the key-value cache, allowing for fast inference and reduced memory usage despite the expanded context. This enhancement enables processing of entire codebases or novels without needing to rerun prompts, which is a substantial improvement for handling large datasets.There is a clarification that the update does not involve changes to the underlying model architecture itself, but rather extends the context window and updates the knowledge cutoff to May 2025. This means that for existing chats, the primary change users will experience is the increased chat length capability, without alterations to the model’s core functionalities or performance characteristics.Despite the significant update in context window size, there are no official release notes available on the DeepSeek website yet. This lack of documentation might leave users without detailed insights into the technical specifics or potential limitations of the new features, such as the impact on performance metrics or compatibility with existing systems.AIME 2026 results are out, Kimi and DeepSeek are the best open-source ai (Activity: 112): The image presents the results of the AIME 2026 competition, highlighting the performance and cost of various AI models. Kimi K2.5 and DeepSeek-v3.2 are noted as the top-performing open-source models with accuracies of 93.33% and 91.67% respectively, offering a cost-effective alternative to closed-source models. The table also features other models like GPT-5.2, Grok 4.1 Fast, and Gemini 3 Flash, with Grok 4.1 being a closed-source model noted for its low cost. Commenters are impressed by Grok 4.1’s performance and cost-effectiveness, despite it being a closed-source model. There is also curiosity about the absence of DeepSeek V3.2 Speciale in the results.The discussion highlights that Grok 4.1 is a closed-source model noted for its cost-effectiveness, suggesting it offers competitive performance at a lower price point compared to other models. This could be particularly relevant for users prioritizing budget without sacrificing too much on performance.A query is raised about the absence of DeepSeek V3.2 Speciale in the results, indicating interest in this specific version. This suggests that there might be expectations or known performance metrics associated with this version that users were keen to compare against the tested models.The limited number of models tested, only six, is questioned, which implies a potential limitation in the comprehensiveness of the results. This could affect the generalizability of the findings, as a broader range of models might provide a more complete picture of the current state of open-source AI performance.AI Discord RecapA summary of Summaries of Summaries by gpt-5.21. GLM-5 Rollout, Access Paths &amp; Benchmark ScrutinyGLM-5 Grabs the Agent Crown (and the #1 Slot): OpenRouter shipped GLM-5 (744B) as a coding/agent foundation model and revealed Pony Alpha was an earlier GLM-5 stealth build, now taken offline, with the release page at OpenRouter GLM-5.LMArena also added glm-5 to Text+Code Arena and reported it hit #1 among open models (#11 overall, score 1452, +11 vs GLM-4.7) on the Text Arena leaderboard, while Eleuther noted a free endpoint on Modal until April 30 with concurrency=1: Modal GLM-5 endpoint.Benchmarks Get Side-Eyed: “Show Your Work” Edition: In Yannick Kilcher’s Discord, members questioned benchmark tables shown in a GLM-5 demo and in the official docs, pointing to tweet discussion of GLM-5 tables and GLM-5 documentation.Nous Research community also compared GLM-5 vs Kimi on browsecomp, citing ~744B (+10B MTP) for GLM-5 vs 1T for Kimi and claiming higher active params for GLM (40B) vs Kimi (32B), reinforcing that people are reading leaderboard claims with a more technical lens.GLM-OCR: Cheaper Vision/OCR Pressure Valve: Builders in Latent Space reported GLM-OCR beating Gemini 3 Flash in an OCR test and linked the model card: zai-org/GLM-OCR on Hugging Face.The thread framed GLM-OCR as a practical swap-in for OCR-heavy products (they cited ongoing use of Gemini Flash but wanting something cheaper), while other Latent Space posts highlighted a wave of open multimodal releases (via Merve’s post) as competition intensifies on capability-per-dollar.2. DeepSeek Hype Cycle: New Model Rumors vs Production RealityLunar New Year DeepSeek Countdown Hits 6 Days: LMArena users speculated DeepSeek will drop a new model around Lunar New Year (in 6 days), with rumors of a 1M context window, a new dataset/architecture, and even new compute chips.OpenRouter chatter amplified the rumor mill with questions about “deepseek v4” appearing on X and guesses it might be a lite variant, showing how fast unconfirmed model IDs now propagate into planning and routing decisions.Chimera R1T2 Falls to 18% Uptime—Routing Panic Ensues: OpenRouter users reported major reliability issues with DeepSeek Chimera R1T2, including a claim it dropped to 18% uptime, triggering discussion about service reliability.The reliability complaints contrasted sharply with the launch hype, pushing people toward pragmatic mitigations (e.g., explicitly specifying model fallbacks rather than relying on auto routing) while the thread devolved into jokes rather than concrete SLO fixes.3. Agents &amp; Workflow Tooling: RLMs, MCP Search, and “Vibecoding Anywhere”RLMs: The Next Step or Just Fancy Scaffolding?: OpenRouter members asked if the platform is exploring RLM (Reasoning Language Models) beyond test-time compute, with one person claiming they’ve worked on RLM concepts for 1.5 years.DSPy builders simultaneously pushed RLM into practice by integrating RLM into Claude Code via subagents/agent teams and requesting critique on the implementation in a Discord thread: core implementation post.No-API Google Search MCP Lets LM Studio “Browse”: LM Studio users shared noapi-google-search-mcp, a tool that adds Google Search capabilities without API keys via headless Chromium: VincentKaufmann/noapi-google-search-mcp.The feature list is unusually broad for an MCP plugin—Images, reverse image search, local OCR, Lens, Flights, Stocks, Weather, News/Trends—and the discussion treated it as a quick way to bolt retrieval onto local models without paying per-query.OpenClaw Runs Your Dev Rig from Discord: In Latent Space, a builder said they moved development “fully through Discord” using OpenClaw to orchestrate tmux sessions, worktrees, and Claude Code, and they scheduled a talk titled Vibecoding Anywhere with OpenClaw for Feb 20, 2026.A follow-on workflow thread explored auditable context saving with a /wrap session boundary that saves context+reflection as markdown with metadata, tying tool ergonomics directly to the “context rot / losing the thread” pain point.4. GPU Kernel Tooling Shifts: CuteDSL Momentum, Triton Blackwell Pain, and MXFP8 MoECuteDSL Gets Hot While Triton “Dies” on Blackwell: GPU MODE users reported growing adoption of CuTeDSL/CuteDSL, citing Kernelbot stats where CUDA and CuTeDSL dominate submissions and CuTeDSL feels “less opaque” than Triton, with the dataset at GPUMODE/kernelbot-data.Multiple members claimed Triton struggles on Blackwell due to unconventional MXFP8/NVFP4 layouts and compiler limits, with more expected at the (linked) Triton TLX talk, signaling a potential tooling bifurcation for next-gen NVIDIA.torchao v0.16.0 Drops MXFP8 MoE Building Blocks: GPU MODE flagged torchao v0.16.0 adding MXFP8 MoE building blocks for training with Expert Parallelism, alongside config deprecations and doc/README revamps.The release notes also mentioned progress toward ABI stability, which matters for downstream integration as teams try to standardize low-precision MoE training stacks across heterogeneous environments.CUDA Bender TMA Matmul Kernel: Async Stores &amp; Persistence Tease: GPU MODE shared a concrete kernel artifact—a TMA matmul in theCudaBender repo: tma_matmul.cu.Discussion centered on how smaller dtypes might free enough shared memory for c tiles to enable async stores/persistence, reflecting a broader theme: people want low-level control knobs back as architectures and datatypes get weirder.5. Engineer UX Blowups: Limits, Token Burn, Plan Gating, and ID WallsPerplexity Deep Research Limits Trigger “Bait and Switch” Claims: Perplexity Pro users complained about unannounced Deep Research limits and shared the rate-limit endpoint: Perplexity rate limits.Users also reported wrong article links, lower source counts (as low as 24), and suspected cost-saving behaviors like Sonar being used for first responses, creating a reliability/quality tax that engineers notice immediately.Cursor Users Watch Opus 4.6 Eat Their Wallet (and Context): Cursor Community members said Opus 4.6 burns tokens fast, with one reporting a single prompt used 11% of their API requests and drained a $200 plan quickly.Pricing backlash escalated with a report of spending $100 every three days for ~9 hours of work using Opus 4.6 and GPT-5.3 Codex, reframing “best coding model” debates as cost/performance engineering.Discord ID Verification Spurs Platform Exit Plans: Unsloth and Cursor communities both reacted strongly to Discord’s new ID verification gates for viewing some content, with Cursor linking a clarification tweet: Discord tweet about ID verification scope.Latent Space tied the policy to IPO risk and churn concerns via Discord’s post, while Nous members discussed moving bot/tool communities to Matrix, showing infra builders treat comms platforms as part of their stack.</p>"
    },
    {
      "id": "4d46c8d276e4",
      "title": "Anthropic to donate $20m to US political group backing AI regulation",
      "content": "Move puts AI firm in opposition to ChatGPT maker OpenAI, which has advocated for less stringent AI regulationsAnthropic will spend $20m to back US political candidates who support regulating the AI industry, according to a company statement released on Thursday. Anthropic’s donation puts it in opposition to the ChatGPT maker OpenAI, which has advocated for less stringent regulation of AI.The company is donating to Public First Action, a political group that opposes federal efforts to quash state AI regulations like a December executive order issued by Donald Trump. One of the candidates that the group is backing is Republican Marsha Blackburn, who is running for governor in Tennessee and who opposed an effort in Congress to bar states from passing AI laws. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/12/anthropic-donation-ai-regulation-politics",
      "author": "Reuters",
      "published": "2026-02-12T17:10:16",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Technology",
        "AI (artificial intelligence)",
        "US news",
        "US politics",
        "OpenAI"
      ],
      "summary": "Anthropic announced a $20M donation to Public First Action, a political group backing US candidates who support AI regulation, directly opposing OpenAI's advocacy for less stringent regulation. The group supports candidates who resist federal preemption of state AI laws.",
      "importance_score": 78.0,
      "reasoning": "A major AI lab actively funding pro-regulation political candidates creates a significant split in the industry's political stance and could shape US AI policy.",
      "themes": [
        "AI_regulation",
        "AI_policy",
        "Anthropic",
        "politics"
      ],
      "continuation": null,
      "summary_html": "<p>Anthropic announced a $20M donation to Public First Action, a political group backing US candidates who support AI regulation, directly opposing OpenAI's advocacy for less stringent regulation. The group supports candidates who resist federal preemption of state AI laws.</p>",
      "content_html": "<p>Move puts AI firm in opposition to ChatGPT maker OpenAI, which has advocated for less stringent AI regulationsAnthropic will spend $20m to back US political candidates who support regulating the AI industry, according to a company statement released on Thursday. Anthropic’s donation puts it in opposition to the ChatGPT maker OpenAI, which has advocated for less stringent regulation of AI.The company is donating to Public First Action, a political group that opposes federal efforts to quash state AI regulations like a December executive order issued by Donald Trump. One of the candidates that the group is backing is Republican Marsha Blackburn, who is running for governor in Tennessee and who opposed an effort in Congress to bar states from passing AI laws. Continue reading...</p>"
    },
    {
      "id": "315f77b723ce",
      "title": "How to deal with the “Claude crash”: Relx should keep buying back shares, then buy more | Nils Pratley",
      "content": "The firm remains confident even as the market flips from seeing it as an AI winner to fearing its profit margin will implodeAs the FTSE 100 index bobs along close to all-time highs, it is easy to miss the quiet share price crash in one corner of the market. It’s got a name – the “Claude crash”, referencing the plug-in legal products added by the AI firm Anthropic to its Claude Cowork office assistant.This launch, or so you would think from the panicked stock market reaction in the past few weeks, marks the moment when the AI revolution rips chunks out of some of the UK’s biggest public companies – those in the dull but successful “data” game, including Relx, the London Stock Exchange Group, Experian, Sage and Informa. Continue reading...",
      "url": "https://www.theguardian.com/technology/nils-pratley-on-finance/2026/feb/12/relx-claude-crash-buy-back-shares",
      "author": "Nils Pratley",
      "published": "2026-02-12T18:43:37",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "London Stock Exchange",
        "Shares",
        "Technology sector",
        "Investing",
        "Business",
        "Financial sector"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-02-12&category=news#item-a48e9ffccf96), The 'Claude crash' describes a significant stock selloff in major UK data companies (Relx, LSE Group, Experian, Sage, Informa) triggered by Anthropic's Claude Cowork launching plug-in legal products. Markets fear AI will erode the profit margins of established data and professional services firms.",
      "importance_score": 76.0,
      "reasoning": "Real-world market impact of AI capabilities disrupting established industries is a major indicator of AI's economic significance. Multiple FTSE 100 companies affected simultaneously.",
      "themes": [
        "AI_disruption",
        "markets",
        "Anthropic",
        "Claude",
        "economic_impact"
      ],
      "continuation": {
        "original_item_id": "a48e9ffccf96",
        "original_date": "2026-02-12",
        "original_category": "news",
        "original_title": "UK wealth manager and price comparison site shares fall amid AI fears",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-02-12&amp;category=news#item-a48e9ffccf96\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, The 'Claude crash' describes a significant stock selloff in major UK data companies (Relx, LSE Group, Experian, Sage, Informa) triggered by Anthropic's Claude Cowork launching plug-in legal products. Markets fear AI will erode the profit margins of established data and professional services firms.</p>",
      "content_html": "<p>The firm remains confident even as the market flips from seeing it as an AI winner to fearing its profit margin will implodeAs the FTSE 100 index bobs along close to all-time highs, it is easy to miss the quiet share price crash in one corner of the market. It’s got a name – the “Claude crash”, referencing the plug-in legal products added by the AI firm Anthropic to its Claude Cowork office assistant.This launch, or so you would think from the panicked stock market reaction in the past few weeks, marks the moment when the AI revolution rips chunks out of some of the UK’s biggest public companies – those in the dull but successful “data” game, including Relx, the London Stock Exchange Group, Experian, Sage and Informa. Continue reading...</p>"
    },
    {
      "id": "ed529feaa2a6",
      "title": "Attackers prompted Gemini over 100,000 times while trying to clone it, Google says",
      "content": "On Thursday, Google announced that \"commercially motivated\" actors have attempted to clone knowledge from its Gemini AI chatbot by simply prompting it. One adversarial session reportedly prompted the model more than 100,000 times across various non-English languages, collecting responses ostensibly to train a cheaper copycat.\nGoogle published the findings in what amounts to a quarterly self-assessment of threats to its own products that frames the company as the victim and the hero, which is not unusual in these self-authored assessments. Google calls the illicit activity \"model extraction\" and considers it intellectual property theft, which is a somewhat loaded position, given that Google's LLM was built from materials scraped from the Internet without permission.\nGoogle is also no stranger to the copycat practice. In 2023, The Information reported that Google's Bard team had been accused of using ChatGPT outputs from ShareGPT, a public site where users share chatbot conversations, to help train its own chatbot. Senior Google AI researcher Jacob Devlin, who created the influential BERT language model, warned leadership that this violated OpenAI's terms of service, then resigned and joined OpenAI. Google denied the claim but reportedly stopped using the data.Read full article\nComments",
      "url": "https://arstechnica.com/ai/2026/02/attackers-prompted-gemini-over-100000-times-while-trying-to-clone-it-google-says/",
      "author": "Benj Edwards",
      "published": "2026-02-12T19:42:08",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Biz & IT",
        "Google",
        "AI language models",
        "AI security",
        "ChatGPT",
        "chatgtp",
        "deepseek",
        "Distillation",
        "gemini",
        "google",
        "Google Gemini",
        "large language models",
        "LLMs",
        "machine learning",
        "openai"
      ],
      "summary": "Google reported that commercially motivated actors prompted Gemini over 100,000 times in a single session across non-English languages attempting 'model extraction' to train cheaper copycat models. Google frames this as IP theft, though the article notes the irony given LLMs' own training on scraped internet data.",
      "importance_score": 72.0,
      "reasoning": "Model extraction attacks at this scale raise important questions about AI IP protection and the emerging threat landscape. The 100K prompt session is notable for its audacity and scale.",
      "themes": [
        "AI_security",
        "model_extraction",
        "Google",
        "IP_protection"
      ],
      "continuation": null,
      "summary_html": "<p>Google reported that commercially motivated actors prompted Gemini over 100,000 times in a single session across non-English languages attempting 'model extraction' to train cheaper copycat models. Google frames this as IP theft, though the article notes the irony given LLMs' own training on scraped internet data.</p>",
      "content_html": "<p>On Thursday, Google announced that \"commercially motivated\" actors have attempted to clone knowledge from its Gemini AI chatbot by simply prompting it. One adversarial session reportedly prompted the model more than 100,000 times across various non-English languages, collecting responses ostensibly to train a cheaper copycat.</p>\n<p>Google published the findings in what amounts to a quarterly self-assessment of threats to its own products that frames the company as the victim and the hero, which is not unusual in these self-authored assessments. Google calls the illicit activity \"model extraction\" and considers it intellectual property theft, which is a somewhat loaded position, given that Google's LLM was built from materials scraped from the Internet without permission.</p>\n<p>Google is also no stranger to the copycat practice. In 2023, The Information reported that Google's Bard team had been accused of using ChatGPT outputs from ShareGPT, a public site where users share chatbot conversations, to help train its own chatbot. Senior Google AI researcher Jacob Devlin, who created the influential BERT language model, warned leadership that this violated OpenAI's terms of service, then resigned and joined OpenAI. Google denied the claim but reportedly stopped using the data.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "d5f309619f44",
      "title": "We let Chrome's Auto Browse agent surf the web for us—here's what happened",
      "content": "We are now a few years into the AI revolution, and talk has shifted from who has the best chatbot to whose AI agent can do the most things on your behalf. Unfortunately, AI agents are still rough around the edges, so tasking them with anything important is not a great idea. OpenAI launched its Atlas agent late last year, which we found to be modestly useful, and now it's Google's turn.\nUnlike the OpenAI agent, Google's new Auto Browse agent has extraordinary reach because it's part of Chrome, the world's most popular browser by a wide margin. Google began rolling out Auto Browse (in preview) earlier this month to AI Pro and AI Ultra subscribers, allowing them to send the agent across the web to complete tasks.\nI've taken Chrome's agent for a spin to see whether you can trust it to handle tedious online work for you. For each test, I lay out the problem I need to solve, how I prompted the robot, and how well (or not) it handled the job.Read full article\nComments",
      "url": "https://arstechnica.com/google/2026/02/tested-how-chromes-auto-browse-agent-handles-common-web-tasks/",
      "author": "Ryan Whitwam",
      "published": "2026-02-12T12:00:14",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Features",
        "Google",
        "AI agents",
        "Artificial Intelligence",
        "browser agent",
        "chrome",
        "google"
      ],
      "summary": "Google launched Auto Browse, a Chrome-based AI agent in preview for AI Pro and AI Ultra subscribers, capable of navigating the web autonomously to complete tasks. The agent's integration into Chrome gives it extraordinary reach as the world's most popular browser.",
      "importance_score": 71.0,
      "reasoning": "AI agents embedded in Chrome have massive distribution potential. This is an important step in the agent wars, though the review suggests agents remain rough around the edges.",
      "themes": [
        "AI_agents",
        "Google",
        "Chrome",
        "product_launch"
      ],
      "continuation": null,
      "summary_html": "<p>Google launched Auto Browse, a Chrome-based AI agent in preview for AI Pro and AI Ultra subscribers, capable of navigating the web autonomously to complete tasks. The agent's integration into Chrome gives it extraordinary reach as the world's most popular browser.</p>",
      "content_html": "<p>We are now a few years into the AI revolution, and talk has shifted from who has the best chatbot to whose AI agent can do the most things on your behalf. Unfortunately, AI agents are still rough around the edges, so tasking them with anything important is not a great idea. OpenAI launched its Atlas agent late last year, which we found to be modestly useful, and now it's Google's turn.</p>\n<p>Unlike the OpenAI agent, Google's new Auto Browse agent has extraordinary reach because it's part of Chrome, the world's most popular browser by a wide margin. Google began rolling out Auto Browse (in preview) earlier this month to AI Pro and AI Ultra subscribers, allowing them to send the agent across the web to complete tasks.</p>\n<p>I've taken Chrome's agent for a spin to see whether you can trust it to handle tedious online work for you. For each test, I lay out the problem I need to solve, how I prompted the robot, and how well (or not) it handled the job.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "e27981cd210f",
      "title": "Google identifies state-sponsored hackers using AI in attacks",
      "content": "State-sponsored hackers are exploiting highly-advanced tooling to accelerate their particular flavours of cyberattacks, with threat actors from Iran, North Korea, China, and Russia using models like Google&#8217;s Gemini to further their campaigns. They are able to craft sophisticated phishing campaigns and develop malware, according to a new report from Google&#8217;s Threat Intelligence Group (GTIG).\nThe quarterly AI Threat Tracker report, released today, reveals how government-backed attackers have begun to use artificial intelligence in the attack lifecycle – reconnaissance, social engineering, and eventually, malware development. This activity has become apparent thanks to the GTIG&#8217;s work during the final quarter of 2025.\n&#8220;For government-backed threat actors, large language models have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures,&#8221; GTIG researchers stated in their report.\nReconnaissance by state-sponsored hackers targets the defence sector\nIranian threat actor APT42 is reported as having used Gemini to augment its reconnaissance and targeted social engineering operations. The group used an AI to create official-seeming email addresses for specific entities and then conducted research to establish credible pretexts for approaching targets.\nAPT42 crafted personas and scenarios designed to better elicit engagement by their targets, translating between languages and deploying natural, native phrases that helped it get round traditional phishing red flags, such as poor grammar or awkward syntax.\nNorth Korean government-backed actor UNC2970, which focuses on defence targeting and impersonating corporate recruiters, used Gemini to help it profile high-value targets. The group&#8217;s reconnaissance included searching for information on major cybersecurity and defence companies, mapping specific technical job roles, and gathering salary information.\n&#8220;This activity blurs the distinction between routine professional research and malicious reconnaissance, as the actor gathers the necessary components to create tailored, high-fidelity phishing personas,&#8221; GTIG noted.\nModel extraction attacks surge\nBeyond operational misuse, Google DeepMind and GTIG identified a increase in model extraction attempts – also known as &#8220;distillation attacks&#8221; – aimed at stealing intellectual property from AI models.\nOne campaign targeting Gemini&#8217;s reasoning abilities involved the collation and use of over 100,000 prompts designed to coerce the model into outputting reasoning processes. The breadth of questions suggested an attempt to replicate Gemini&#8217;s reasoning ability in non-English target languages in various tasks.\nHow model extraction attacks work to steal AI intellectual property. (Image: Google GTIG)\nWhile GTIG observed no direct attacks on frontier models from advanced persistent threat actors, the team identified and disrupted frequent model extraction attacks from private sector entities globally and researchers seeking to clone proprietary logic.\nGoogle&#8217;s systems recognised these attacks in real-time and deployed defences to protect internal reasoning traces.\nAI-integrated malware emerges\nGTIG observed malware samples, tracked as HONESTCUE, that use Gemini&#8217;s API to outsource functionality generation. The malware is designed to undermine traditional network-based detection and static analysis through a multi-layered obfuscation approach.\nHONESTCUE functions as a downloader and launcher framework that sends prompts via Gemini&#8217;s API and receives C# source code as responses. The fileless secondary stage compiles and executes payloads directly in memory, leaving no artefacts on disk.\nHONESTCUE malware&#8217;s two-stage attack process using Gemini&#8217;s API. (Image: Google GTIG)\nSeparately, GTIG identified COINBAIT, a phishing kit whose construction was likely accelerated by AI code generation tools. The kit, which masquerades as a major cryptocurrency exchange for credential harvesting, was built using the AI-powered platform Lovable AI.\nClickFix campaigns abuse AI chat platforms\nIn a novel social engineering campaign first observed in December 2025, Google saw threat actors abuse the public sharing features of generative AI services – including Gemini, ChatGPT, Copilot, DeepSeek, and Grok – to host deceptive content distributing ATOMIC malware targeting macOS systems.\nAttackers manipulated AI models to create realistic-looking instructions for common computer tasks, embedding malicious command-line scripts as the &#8220;solution.&#8221; By creating shareable links to these AI chat transcripts, threat actors used trusted domains to host their initial attack stage.\nThe three-stage ClickFix attack chain exploiting AI chat platforms. (Image: Google GTIG)\nUnderground marketplace thrives on stolen API keys\nGTIG&#8217;s observations of English and Russian-language underground forums indicate a persistent demand for AI-enabled tools and services. However, state-sponsored hackers and cybercriminals struggle to develop custom AI models, instead relying on mature commercial products accessed through stolen credentials.\nOne toolkit, &#8220;Xanthorox,&#8221; advertised itself as a custom AI for autonomous malware generation and phishing campaign development. GTIG&#8217;s investigation revealed Xanthorox was not a bespoke model but actually powered by several commercial AI products, including Gemini, accessed through stolen API keys.\nGoogle&#8217;s response and mitigations\nGoogle has taken action against identified threat actors by disabling accounts and assets associated with malicious activity. The company has also applied intelligence to strengthen both classifiers and models, letting them refuse assistance with similar attacks moving forward.\\\n&#8220;We are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse,&#8221; the report stated.\nGTIG emphasised that despite these developments, no APT or information operations actors have achieved breakthrough abilities that fundamentally alter the threat landscape.\nThe findings underscore the evolving role of AI in cybersecurity, as both defenders and attackers race to use the technology&#8217;s abilities.\nFor enterprise security teams, particularly in the Asia-Pacific region where Chinese and North Korean state-sponsored hackers remain active, the report serves as an important reminder to enhance defences against AI-augmented social engineering and reconnaissance operations.\n(Photo by SCARECROW artworks)\nSee also: Anthropic just revealed how AI-orchestrated cyberattacks actually work – Here&#8217;s what enterprises need to know\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Google identifies state-sponsored hackers using AI in attacks appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/state-sponsored-hackers-ai-cyberattacks-google/",
      "author": "Dashveenjit Kaur",
      "published": "2026-02-12T09:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Cybersecurity AI",
        "Human-AI Relationships",
        "ai",
        "cybersecurity",
        "google",
        "malware",
        "state sponsored"
      ],
      "summary": "Google's Threat Intelligence Group reported that state-sponsored hackers from Iran, North Korea, China, and Russia are using LLMs including Gemini for reconnaissance, social engineering, phishing campaigns, and malware development. The quarterly AI Threat Tracker reveals growing sophistication in AI-assisted cyberattacks.",
      "importance_score": 70.0,
      "reasoning": "State-sponsored use of AI for cyberattacks is a growing national security concern. Provides concrete evidence of adversarial AI use by major threat actors.",
      "themes": [
        "AI_security",
        "cybersecurity",
        "state_actors",
        "Google"
      ],
      "continuation": null,
      "summary_html": "<p>Google's Threat Intelligence Group reported that state-sponsored hackers from Iran, North Korea, China, and Russia are using LLMs including Gemini for reconnaissance, social engineering, phishing campaigns, and malware development. The quarterly AI Threat Tracker reveals growing sophistication in AI-assisted cyberattacks.</p>",
      "content_html": "<p>State-sponsored hackers are exploiting highly-advanced tooling to accelerate their particular flavours of cyberattacks, with threat actors from Iran, North Korea, China, and Russia using models like Google’s Gemini to further their campaigns. They are able to craft sophisticated phishing campaigns and develop malware, according to a new report from Google’s Threat Intelligence Group (GTIG).</p>\n<p>The quarterly AI Threat Tracker report, released today, reveals how government-backed attackers have begun to use artificial intelligence in the attack lifecycle – reconnaissance, social engineering, and eventually, malware development. This activity has become apparent thanks to the GTIG’s work during the final quarter of 2025.</p>\n<p>“For government-backed threat actors, large language models have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures,” GTIG researchers stated in their report.</p>\n<p>Reconnaissance by state-sponsored hackers targets the defence sector</p>\n<p>Iranian threat actor APT42 is reported as having used Gemini to augment its reconnaissance and targeted social engineering operations. The group used an AI to create official-seeming email addresses for specific entities and then conducted research to establish credible pretexts for approaching targets.</p>\n<p>APT42 crafted personas and scenarios designed to better elicit engagement by their targets, translating between languages and deploying natural, native phrases that helped it get round traditional phishing red flags, such as poor grammar or awkward syntax.</p>\n<p>North Korean government-backed actor UNC2970, which focuses on defence targeting and impersonating corporate recruiters, used Gemini to help it profile high-value targets. The group’s reconnaissance included searching for information on major cybersecurity and defence companies, mapping specific technical job roles, and gathering salary information.</p>\n<p>“This activity blurs the distinction between routine professional research and malicious reconnaissance, as the actor gathers the necessary components to create tailored, high-fidelity phishing personas,” GTIG noted.</p>\n<p>Model extraction attacks surge</p>\n<p>Beyond operational misuse, Google DeepMind and GTIG identified a increase in model extraction attempts – also known as “distillation attacks” – aimed at stealing intellectual property from AI models.</p>\n<p>One campaign targeting Gemini’s reasoning abilities involved the collation and use of over 100,000 prompts designed to coerce the model into outputting reasoning processes. The breadth of questions suggested an attempt to replicate Gemini’s reasoning ability in non-English target languages in various tasks.</p>\n<p>How model extraction attacks work to steal AI intellectual property. (Image: Google GTIG)</p>\n<p>While GTIG observed no direct attacks on frontier models from advanced persistent threat actors, the team identified and disrupted frequent model extraction attacks from private sector entities globally and researchers seeking to clone proprietary logic.</p>\n<p>Google’s systems recognised these attacks in real-time and deployed defences to protect internal reasoning traces.</p>\n<p>AI-integrated malware emerges</p>\n<p>GTIG observed malware samples, tracked as HONESTCUE, that use Gemini’s API to outsource functionality generation. The malware is designed to undermine traditional network-based detection and static analysis through a multi-layered obfuscation approach.</p>\n<p>HONESTCUE functions as a downloader and launcher framework that sends prompts via Gemini’s API and receives C# source code as responses. The fileless secondary stage compiles and executes payloads directly in memory, leaving no artefacts on disk.</p>\n<p>HONESTCUE malware’s two-stage attack process using Gemini’s API. (Image: Google GTIG)</p>\n<p>Separately, GTIG identified COINBAIT, a phishing kit whose construction was likely accelerated by AI code generation tools. The kit, which masquerades as a major cryptocurrency exchange for credential harvesting, was built using the AI-powered platform Lovable AI.</p>\n<p>ClickFix campaigns abuse AI chat platforms</p>\n<p>In a novel social engineering campaign first observed in December 2025, Google saw threat actors abuse the public sharing features of generative AI services – including Gemini, ChatGPT, Copilot, DeepSeek, and Grok – to host deceptive content distributing ATOMIC malware targeting macOS systems.</p>\n<p>Attackers manipulated AI models to create realistic-looking instructions for common computer tasks, embedding malicious command-line scripts as the “solution.” By creating shareable links to these AI chat transcripts, threat actors used trusted domains to host their initial attack stage.</p>\n<p>The three-stage ClickFix attack chain exploiting AI chat platforms. (Image: Google GTIG)</p>\n<p>Underground marketplace thrives on stolen API keys</p>\n<p>GTIG’s observations of English and Russian-language underground forums indicate a persistent demand for AI-enabled tools and services. However, state-sponsored hackers and cybercriminals struggle to develop custom AI models, instead relying on mature commercial products accessed through stolen credentials.</p>\n<p>One toolkit, “Xanthorox,” advertised itself as a custom AI for autonomous malware generation and phishing campaign development. GTIG’s investigation revealed Xanthorox was not a bespoke model but actually powered by several commercial AI products, including Gemini, accessed through stolen API keys.</p>\n<p>Google’s response and mitigations</p>\n<p>Google has taken action against identified threat actors by disabling accounts and assets associated with malicious activity. The company has also applied intelligence to strengthen both classifiers and models, letting them refuse assistance with similar attacks moving forward.\\</p>\n<p>“We are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse,” the report stated.</p>\n<p>GTIG emphasised that despite these developments, no APT or information operations actors have achieved breakthrough abilities that fundamentally alter the threat landscape.</p>\n<p>The findings underscore the evolving role of AI in cybersecurity, as both defenders and attackers race to use the technology’s abilities.</p>\n<p>For enterprise security teams, particularly in the Asia-Pacific region where Chinese and North Korean state-sponsored hackers remain active, the report serves as an important reminder to enhance defences against AI-augmented social engineering and reconnaissance operations.</p>\n<p>(Photo by SCARECROW artworks)</p>\n<p>See also: Anthropic just revealed how AI-orchestrated cyberattacks actually work – Here’s what enterprises need to know</p>\n<p>Want to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.</p>\n<p>AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.</p>\n<p>The post Google identifies state-sponsored hackers using AI in attacks appeared first on AI News.</p>"
    },
    {
      "id": "ab076001efaf",
      "title": "Share values of property services firms tumble over fears of AI disruption",
      "content": "But, after second day of Wall Street falls, analysts say sell-off ‘may overstate AI’s immediate risk to complex deal-making’Shares in commercial property services companies have tumbled, in the latest sell-off driven by fears over disruption from artificial intelligence.After steep declines on Wall Street, European stocks in the sector were hit on Thursday. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/feb/12/share-values-of-property-services-firms-tumble-over-fears-of-ai-disruption",
      "author": "Julia Kollewe",
      "published": "2026-02-12T18:01:00",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Commercial property",
        "Business",
        "Computing",
        "Stock markets",
        "Technology",
        "Real estate"
      ],
      "summary": "Commercial property services stocks tumbled on both Wall Street and European markets due to fears of AI disruption, though analysts suggest the sell-off may overstate AI's immediate risk to complex deal-making.",
      "importance_score": 65.0,
      "reasoning": "Extends the AI disruption theme to another sector (real estate/property services). The market overreaction debate is relevant but this is more market sentiment than frontier AI news.",
      "themes": [
        "AI_disruption",
        "markets",
        "economic_impact"
      ],
      "continuation": null,
      "summary_html": "<p>Commercial property services stocks tumbled on both Wall Street and European markets due to fears of AI disruption, though analysts suggest the sell-off may overstate AI's immediate risk to complex deal-making.</p>",
      "content_html": "<p>But, after second day of Wall Street falls, analysts say sell-off ‘may overstate AI’s immediate risk to complex deal-making’Shares in commercial property services companies have tumbled, in the latest sell-off driven by fears over disruption from artificial intelligence.After steep declines on Wall Street, European stocks in the sector were hit on Thursday. Continue reading...</p>"
    },
    {
      "id": "bda87adf1fa9",
      "title": "🔬Beyond AlphaFold: How Boltz is Open-Sourcing the Future of Drug Discovery",
      "content": "This podcast features Gabriele Corso and Jeremy Wohlwend, co-founders of Boltz and authors of the Boltz Manifesto, discussing the rapid evolution of structural biology models from AlphaFold to their own open-source suite, Boltz-1 and Boltz-2. The central thesis is that while single-chain protein structure prediction is largely &#8220;solved&#8221; through evolutionary hints, the next frontier lies in modeling complex interactions (protein-ligand, protein-protein) and generative protein design, which Boltz aims to democratize via open-source foundations and scalable infrastructure.Full Video PodOn YouTube!Timestamps00:00 Introduction to Benchmarking and the &#8220;Solved&#8221; Protein Problem06:48 Evolutionary Hints and Co-evolution in Structure Prediction10:00 The Importance of Protein Function and Disease States15:31 Transitioning from AlphaFold 2 to AlphaFold 3 Capabilities19:48 Generative Modeling vs. Regression in Structural Biology25:00 The &#8220;Bitter Lesson&#8221; and Specialized AI Architectures29:14 Development Anecdotes: Training Boltz-1 on a Budget32:00 Validation Strategies and the Protein Data Bank (PDB)37:26 The Mission of Boltz: Democratizing Access and Open Source41:43 Building a Self-Sustaining Research Community44:40 Boltz-2 Advancements: Affinity Prediction and Design51:03 BoltzGen: Merging Structure and Sequence Prediction55:18 Large-Scale Wet Lab Validation Results01:02:44 Boltz Lab Product Launch: Agents and Infrastructure01:13:06 Future Directions: Developpability and the &#8220;Virtual Cell&#8221;01:17:35 Interacting with Skeptical Medicinal ChemistsKey SummaryEvolution of Structure Prediction &amp; Evolutionary HintsCo-evolutionary Landscapes: The speakers explain that breakthrough progress in single-chain protein prediction relied on decoding evolutionary correlations where mutations in one position necessitate mutations in another to conserve 3D structure.Structure vs. Folding: They differentiate between structure prediction (getting the final answer) and folding (the kinetic process of reaching that state), noting that the field is still quite poor at modeling the latter.Physics vs. Statistics: RJ posits that while models use evolutionary statistics to find the right &#8220;valley&#8221; in the energy landscape, they likely possess a &#8220;light understanding&#8221; of physics to refine the local minimum.The Shift to Generative ArchitecturesGenerative Modeling: A key leap in AlphaFold 3 and Boltz-1 was moving from regression (predicting one static coordinate) to a generative diffusion approach that samples from a posterior distribution.Handling Uncertainty: This shift allows models to represent multiple conformational states and avoid the &#8220;averaging&#8221; effect seen in regression models when the ground truth is ambiguous.Specialized Architectures: Despite the &#8220;bitter lesson&#8221; of general-purpose transformers, the speakers argue that equivariant architectures remain vastly superior for biological data due to the inherent 3D geometric constraints of molecules.Boltz-2 and Generative Protein DesignUnified Encoding: Boltz-2 (and BoltzGen) treats structure and sequence prediction as a single task by encoding amino acid identities into the atomic composition of the predicted structure.Design Specifics: Instead of a sequence, users feed the model blank tokens and a high-level &#8220;spec&#8221; (e.g., an antibody framework), and the model decodes both the 3D structure and the corresponding amino acids.Affinity Prediction: While model confidence is a common metric, Boltz-2 focuses on affinity prediction&#8212;quantifying exactly how tightly a designed binder will stick to its target.Real-World Validation and ProductizationGeneralized Validation: To prove the model isn&#8217;t just &#8220;regurgitating&#8221; known data, Boltz tested its designs on 9 targets with zero known interactions in the PDB, achieving nanomolar binders for two-thirds of them.Boltz Lab Infrastructure: The newly launched Boltz Lab platform provides &#8220;agents&#8221; for protein and small molecule design, optimized to run 10x faster than open-source versions through proprietary GPU kernels.Human-in-the-Loop: The platform is designed to convert skeptical medicinal chemists by allowing them to run parallel screens and use their intuition to filter model outputs.TranscriptRJ [00:05:35]: But the goal remains to, like, you know, really challenge the models, like, how well do these models generalize? And, you know, we&#8217;ve seen in some of the latest CASP competitions, like, while we&#8217;ve become really, really good at proteins, especially monomeric proteins, you know, other modalities still remain pretty difficult. So it&#8217;s really essential, you know, in the field that there are, like, these efforts to gather, you know, benchmarks that are challenging. So it keeps us in line, you know, about what the models can do or not.Gabriel [00:06:26]: Yeah, it&#8217;s interesting you say that, like, in some sense, CASP, you know, at CASP 14, a problem was solved and, like, pretty comprehensively, right? But at the same time, it was really only the beginning. So you can say, like, what was the specific problem you would argue was solved? And then, like, you know, what is remaining, which is probably quite open.RJ [00:06:48]: I think we&#8217;ll steer away from the term solved, because we have many friends in the community who get pretty upset at that word. And I think, you know, fairly so. But the problem that was, you know, that a lot of progress was made on was the ability to predict the structure of single chain proteins. So proteins can, like, be composed of many chains. And single chain proteins are, you know, just a single sequence of amino acids. And one of the reasons that we&#8217;ve been able to make such progress is also because we take a lot of hints from evolution. So the way the models work is that, you know, they sort of decode a lot of hints. That comes from evolutionary landscapes. So if you have, like, you know, some protein in an animal, and you go find the similar protein across, like, you know, different organisms, you might find different mutations in them. And as it turns out, if you take a lot of the sequences together, and you analyze them, you see that some positions in the sequence tend to evolve at the same time as other positions in the sequence, sort of this, like, correlation between different positions. And it turns out that that is typically a hint that these two positions are close in three dimension. So part of the, you know, part of the breakthrough has been, like, our ability to also decode that very, very effectively. But what it implies also is that in absence of that co-evolutionary landscape, the models don&#8217;t quite perform as well. And so, you know, I think when that information is available, maybe one could say, you know, the problem is, like, somewhat solved. From the perspective of structure prediction, when it isn&#8217;t, it&#8217;s much more challenging. And I think it&#8217;s also worth also differentiating the, sometimes we confound a little bit, structure prediction and folding. Folding is the more complex process of actually understanding, like, how it goes from, like, this disordered state into, like, a structured, like, state. And that I don&#8217;t think we&#8217;ve made that much progress on. But the idea of, like, yeah, going straight to the answer, we&#8217;ve become pretty good at.Brandon [00:08:49]: So there&#8217;s this protein that is, like, just a long chain and it folds up. Yeah. And so we&#8217;re good at getting from that long chain in whatever form it was originally to the thing. But we don&#8217;t know how it necessarily gets to that state. And there might be intermediate states that it&#8217;s in sometimes that we&#8217;re not aware of.RJ [00:09:10]: That&#8217;s right. And that relates also to, like, you know, our general ability to model, like, the different, you know, proteins are not static. They move, they take different shapes based on their energy states. And I think we are, also not that good at understanding the different states that the protein can be in and at what frequency, what probability. So I think the two problems are quite related in some ways. Still a lot to solve. But I think it was very surprising at the time, you know, that even with these evolutionary hints that we were able to, you know, to make such dramatic progress.Brandon [00:09:45]: So I want to ask, why does the intermediate states matter? But first, I kind of want to understand, why do we care? What proteins are shaped like?Gabriel [00:09:54]: Yeah, I mean, the proteins are kind of the machines of our body. You know, the way that all the processes that we have in our cells, you know, work is typically through proteins, sometimes other molecules, sort of intermediate interactions. And through that interactions, we have all sorts of cell functions. And so when we try to understand, you know, a lot of biology, how our body works, how disease work. So we often try to boil it down to, okay, what is going right in case of, you know, our normal biological function and what is going wrong in case of the disease state. And we boil it down to kind of, you know, proteins and kind of other molecules and their interaction. And so when we try predicting the structure of proteins, it&#8217;s critical to, you know, have an understanding of kind of those interactions. It&#8217;s a bit like seeing the difference between... Having kind of a list of parts that you would put it in a car and seeing kind of the car in its final form, you know, seeing the car really helps you understand what it does. On the other hand, kind of going to your question of, you know, why do we care about, you know, how the protein falls or, you know, how the car is made to some extent is that, you know, sometimes when something goes wrong, you know, there are, you know, cases of, you know, proteins misfolding. In some diseases and so on, if we don&#8217;t understand this folding process, we don&#8217;t really know how to intervene.RJ [00:11:30]: There&#8217;s this nice line in the, I think it&#8217;s in the Alpha Fold 2 manuscript, where they sort of discuss also like why we even hopeful that we can target the problem in the first place. And then there&#8217;s this notion that like, well, four proteins that fold. The folding process is almost instantaneous, which is a strong, like, you know, signal that like, yeah, like we should, we might be... able to predict that this very like constrained thing that, that the protein does so quickly. And of course that&#8217;s not the case for, you know, for, for all proteins. And there&#8217;s a lot of like really interesting mechanisms in the cells, but yeah, I remember reading that and thought, yeah, that&#8217;s somewhat of an insightful point.Gabriel [00:12:10]: I think one of the interesting things about the protein folding problem is that it used to be actually studied. And part of the reason why people thought it was impossible, it used to be studied as kind of like a classical example. Of like an MP problem. Uh, like there are so many different, you know, type of, you know, shapes that, you know, this amino acid could take. And so, this grows combinatorially with the size of the sequence. And so there used to be kind of a lot of actually kind of more theoretical computer science thinking about and studying protein folding as an MP problem. And so it was very surprising also from that perspective, kind of seeing. Machine learning so clear, there is some, you know, signal in those sequences, through evolution, but also through kind of other things that, you know, us as humans, we&#8217;re probably not really able to, uh, to understand, but that is, models I&#8217;ve, I&#8217;ve learned.Brandon [00:13:07]: And so Andrew White, we were talking to him a few weeks ago and he said that he was following the development of this and that there were actually ASICs that were developed just to solve this problem. So, again, that there were. There were many, many, many millions of computational hours spent trying to solve this problem before AlphaFold. And just to be clear, one thing that you mentioned was that there&#8217;s this kind of co-evolution of mutations and that you see this again and again in different species. So explain why does that give us a good hint that they&#8217;re close by to each other? Yeah.RJ [00:13:41]: Um, like think of it this way that, you know, if I have, you know, some amino acid that mutates, it&#8217;s going to impact everything around it. Right. In three dimensions. And so it&#8217;s almost like the protein through several, probably random mutations and evolution, like, you know, ends up sort of figuring out that this other amino acid needs to change as well for the structure to be conserved. Uh, so this whole principle is that the structure is probably largely conserved, you know, because there&#8217;s this function associated with it. And so it&#8217;s really sort of like different positions compensating for, for each other. I see.Brandon [00:14:17]: Those hints in aggregate give us a lot. Yeah. So you can start to look at what kinds of information about what is close to each other, and then you can start to look at what kinds of folds are possible given the structure and then what is the end state.RJ [00:14:30]: And therefore you can make a lot of inferences about what the actual total shape is. Yeah, that&#8217;s right. It&#8217;s almost like, you know, you have this big, like three dimensional Valley, you know, where you&#8217;re sort of trying to find like these like low energy states and there&#8217;s so much to search through. That&#8217;s almost overwhelming. But these hints, they sort of maybe put you in. An area of the space that&#8217;s already like, kind of close to the solution, maybe not quite there yet. And, and there&#8217;s always this question of like, how much physics are these models learning, you know, versus like, just pure like statistics. And like, I think one of the thing, at least I believe is that once you&#8217;re in that sort of approximate area of the solution space, then the models have like some understanding, you know, of how to get you to like, you know, the lower energy, uh, low energy state. And so maybe you have some, some light understanding. Of physics, but maybe not quite enough, you know, to know how to like navigate the whole space. Right. Okay.Brandon [00:15:25]: So we need to give it these hints to kind of get into the right Valley and then it finds the, the minimum or something. Yeah.Gabriel [00:15:31]: One interesting explanation about our awful free works that I think it&#8217;s quite insightful, of course, doesn&#8217;t cover kind of the entirety of, of what awful does that is, um, they&#8217;re going to borrow from, uh, Sergio Chinico for MIT. So he sees kind of awful. Then the interesting thing about awful is God. This very peculiar architecture that we have seen, you know, used, and this architecture operates on this, you know, pairwise context between amino acids. And so the idea is that probably the MSA gives you this first hint about what potential amino acids are close to each other. MSA is most multiple sequence alignment. Exactly. Yeah. Exactly. This evolutionary information. Yeah. And, you know, from this evolutionary information about potential contacts, then is almost as if the model is. of running some kind of, you know, diastro algorithm where it&#8217;s sort of decoding, okay, these have to be closed. Okay. Then if these are closed and this is connected to this, then this has to be somewhat closed. And so you decode this, that becomes basically a pairwise kind of distance matrix. And then from this rough pairwise distance matrix, you decode kind of theBrandon [00:16:42]: actual potential structure. Interesting. So there&#8217;s kind of two different things going on in the kind of coarse grain and then the fine grain optimizations. Interesting. Yeah. Very cool.Gabriel [00:16:53]: Yeah. You mentioned AlphaFold3. So maybe we have a good time to move on to that. So yeah, AlphaFold2 came out and it was like, I think fairly groundbreaking for this field. Everyone got very excited. A few years later, AlphaFold3 came out and maybe for some more history, like what were the advancements in AlphaFold3? And then I think maybe we&#8217;ll, after that, we&#8217;ll talk a bit about the sort of how it connects to Bolt. But anyway. Yeah. So after AlphaFold2 came out, you know, Jeremy and I got into the field and with many others, you know, the clear problem that, you know, was, you know, obvious after that was, okay, now we can do individual chains. Can we do interactions, interaction, different proteins, proteins with small molecules, proteins with other molecules. And so. So why are interactions important? Interactions are important because to some extent that&#8217;s kind of the way that, you know, these machines, you know, these proteins have a function, you know, the function comes by the way that they interact with other proteins and other molecules. Actually, in the first place, you know, the individual machines are often, as Jeremy was mentioning, not made of a single chain, but they&#8217;re made of the multiple chains. And then these multiple chains interact with other molecules to give the function to those. And on the other hand, you know, when we try to intervene of these interactions, think about like a disease, think about like a, a biosensor or many other ways we are trying to design the molecules or proteins that interact in a particular way with what we would call a target protein or target. You know, this problem after AlphaVol2, you know, became clear, kind of one of the biggest problems in the field to, to solve many groups, including kind of ours and others, you know, started making some kind of contributions to this problem of trying to model these interactions. And AlphaVol3 was, you know, was a significant advancement on the problem of modeling interactions. And one of the interesting thing that they were able to do while, you know, some of the rest of the field that really tried to try to model different interactions separately, you know, how protein interacts with small molecules, how protein interacts with other proteins, how RNA or DNA have their structure, they put everything together and, you know, train very large models with a lot of advances, including kind of changing kind of systems. Some of the key architectural choices and managed to get a single model that was able to set this new state-of-the-art performance across all of these different kind of modalities, whether that was protein, small molecules is critical to developing kind of new drugs, protein, protein, understanding, you know, interactions of, you know, proteins with RNA and DNAs and so on.Brandon [00:19:39]: Just to satisfy the AI engineers in the audience, what were some of the key architectural and data, data changes that made that possible?Gabriel [00:19:48]: Yeah, so one critical one that was not necessarily just unique to AlphaFold3, but there were actually a few other teams, including ours in the field that proposed this, was moving from, you know, modeling structure prediction as a regression problem. So where there is a single answer and you&#8217;re trying to shoot for that answer to a generative modeling problem where you have a posterior distribution of possible structures and you&#8217;re trying to sample this distribution. And this achieves two things. One is it starts to allow us to try to model more dynamic systems. As we said, you know, some of these structures can actually take multiple structures. And so, you know, you can now model that, you know, through kind of modeling the entire distribution. But on the second hand, from more kind of core modeling questions, when you move from a regression problem to a generative modeling problem, you are really tackling the way that you think about uncertainty in the model in a different way. So if you think about, you know, I&#8217;m undecided between different answers, what&#8217;s going to happen in a regression model is that, you know, I&#8217;m going to try to make an average of those different kind of answers that I had in mind. When you have a generative model, what you&#8217;re going to do is, you know, sample all these different answers and then maybe use separate models to analyze those different answers and pick out the best. So that was kind of one of the critical improvement. The other improvement is that they significantly simplified, to some extent, the architecture, especially of the final model that takes kind of those pairwise representations and turns them into an actual structure. And that now looks a lot more like a more traditional transformer than, you know, like a very specialized equivariant architecture that it was in AlphaFold3.Brandon [00:21:41]: So this is a bitter lesson, a little bit.Gabriel [00:21:45]: There is some aspect of a bitter lesson, but the interesting thing is that it&#8217;s very far from, you know, being like a simple transformer. This field is one of the, I argue, very few fields in applied machine learning where we still have kind of architecture that are very specialized. And, you know, there are many people that have tried to replace these architectures with, you know, simple transformers. And, you know, there is a lot of debate in the field, but I think kind of that most of the consensus is that, you know, the performance... that we get from the specialized architecture is vastly superior than what we get through a single transformer. Another interesting thing that I think on the staying on the modeling machine learning side, which I think it&#8217;s somewhat counterintuitive seeing some of the other kind of fields and applications is that scaling hasn&#8217;t really worked kind of the same in this field. Now, you know, models like AlphaFold2 and AlphaFold3 are, you know, still very large models.RJ [00:29:14]: in a place, I think, where we had, you know, some experience working in, you know, with the data and working with this type of models. And I think that put us already in like a good place to, you know, to produce it quickly. And, you know, and I would even say, like, I think we could have done it quicker. The problem was like, for a while, we didn&#8217;t really have the compute. And so we couldn&#8217;t really train the model. And actually, we only trained the big model once. That&#8217;s how much compute we had. We could only train it once. And so like, while the model was training, we were like, finding bugs left and right. A lot of them that I wrote. And like, I remember like, I was like, sort of like, you know, doing like, surgery in the middle, like stopping the run, making the fix, like relaunching. And yeah, we never actually went back to the start. We just like kept training it with like the bug fixes along the way, which was impossible to reproduce now. Yeah, yeah, no, that model is like, has gone through such a curriculum that, you know, learned some weird stuff. But yeah, somehow by miracle, it worked out.Gabriel [00:30:13]: The other funny thing is that the way that we were training, most of that model was through a cluster from the Department of Energy. But that&#8217;s sort of like a shared cluster that many groups use. And so we were basically training the model for two days, and then it would go back to the queue and stay a week in the queue. Oh, yeah. And so it was pretty painful. And so we actually kind of towards the end with Evan, the CEO of Genesis, and basically, you know, I was telling him a bit about the project and, you know, kind of telling him about this frustration with the compute. And so luckily, you know, he offered to kind of help. And so we, we got the help from Genesis to, you know, finish up the model. Otherwise, it probably would have taken a couple of extra weeks.Brandon [00:30:57]: Yeah, yeah.Brandon [00:31:02]: And then, and then there&#8217;s some progression from there.Gabriel [00:31:06]: Yeah, so I would say kind of that, both one, but also kind of these other kind of set of models that came around the same time, were kind of approaching were a big leap from, you know, kind of the previous kind of open source models, and, you know, kind of really kind of approaching the level of AlphaVault 3. But I would still say that, you know, even to this day, there are, you know, some... specific instances where AlphaVault 3 works better. I think one common example is antibody antigen prediction, where, you know, AlphaVault 3 still seems to have an edge in many situations. Obviously, these are somewhat different models. They are, you know, you run them, you obtain different results. So it&#8217;s, it&#8217;s not always the case that one model is better than the other, but kind of in aggregate, we still, especially at the time.Brandon [00:32:00]: So AlphaVault 3 is, you know, still having a bit of an edge. We should talk about this more when we talk about Boltzgen, but like, how do you know one is, one model is better than the other? Like you, so you, I make a prediction, you make a prediction, like, how do you know?Gabriel [00:32:11]: Yeah, so easily, you know, the, the great thing about kind of structural prediction and, you know, once we&#8217;re going to go into the design space of designing new small molecule, new proteins, this becomes a lot more complex. But a great thing about structural prediction is that a bit like, you know, CASP was doing, basically the way that you can evaluate them is that, you know, you train... You know, you train a model on a structure that was, you know, released across the field up until a certain time. And, you know, one of the things that we didn&#8217;t talk about that was really critical in all this development is the PDB, which is the Protein Data Bank. It&#8217;s this common resources, basically common database where every biologist publishes their structures. And so we can, you know, train on, you know, all the structures that were put in the PDB until a certain date. And then... And then we basically look for recent structures, okay, which structures look pretty different from anything that was published before, because we really want to try to understand generalization.Brandon [00:33:13]: And then on this new structure, we evaluate all these different models. And so you just know when AlphaFold3 was trained, you know, when you&#8217;re, you intentionally trained to the same date or something like that. Exactly. Right. Yeah.Gabriel [00:33:24]: And so this is kind of the way that you can somewhat easily kind of compare these models, obviously, that assumes that, you know, the training. You&#8217;ve always been very passionate about validation. I remember like DiffDoc, and then there was like DiffDocL and DocGen. You&#8217;ve thought very carefully about this in the past. Like, actually, I think DocGen is like a really funny story that I think, I don&#8217;t know if you want to talk about that. It&#8217;s an interesting like... Yeah, I think one of the amazing things about putting things open source is that we get a ton of feedback from the field. And, you know, sometimes we get kind of great feedback of people. Really like... But honestly, most of the times, you know, to be honest, that&#8217;s also maybe the most useful feedback is, you know, people sharing about where it doesn&#8217;t work. And so, you know, at the end of the day, it&#8217;s critical. And this is also something, you know, across other fields of machine learning. It&#8217;s always critical to set, to do progress in machine learning, set clear benchmarks. And as, you know, you start doing progress of certain benchmarks, then, you know, you need to improve the benchmarks and make them harder and harder. And this is kind of the progression of, you know, how the field operates. And so, you know, the example of DocGen was, you know, we published this initial model called DiffDoc in my first year of PhD, which was sort of like, you know, one of the early models to try to predict kind of interactions between proteins, small molecules, that we bought a year after AlphaFold2 was published. And now, on the one hand, you know, on these benchmarks that we were using at the time, DiffDoc was doing really well, kind of, you know, outperforming kind of some of the traditional physics-based methods. But on the other hand, you know, when we started, you know, kind of giving these tools to kind of many biologists, and one example was that we collaborated with was the group of Nick Polizzi at Harvard. We noticed, started noticing that there was this clear, pattern where four proteins that were very different from the ones that we&#8217;re trained on, the models was, was struggling. And so, you know, that seemed clear that, you know, this is probably kind of where we should, you know, put our focus on. And so we first developed, you know, with Nick and his group, a new benchmark, and then, you know, went after and said, okay, what can we change? And kind of about the current architecture to improve this pattern and generalization. And this is the same that, you know, we&#8217;re still doing today, you know, kind of, where does the model not work, you know, and then, you know, once we have that benchmark, you know, let&#8217;s try to, through everything we, any ideas that we have of the problem.RJ [00:36:15]: And there&#8217;s a lot of like healthy skepticism in the field, which I think, you know, is, is, is great. And I think, you know, it&#8217;s very clear that there&#8217;s a ton of things, the models don&#8217;t really work well on, but I think one thing that&#8217;s probably, you know, undeniable is just like the pace of, pace of progress, you know, and how, how much better we&#8217;re getting, you know, every year. And so I think if you, you know, if you assume, you know, any constant, you know, rate of progress moving forward, I think things are going to look pretty cool at some point in the future.Gabriel [00:36:42]: ChatGPT was only three years ago. Yeah, I mean, it&#8217;s wild, right?RJ [00:36:45]: Like, yeah, yeah, yeah, it&#8217;s one of those things. Like, you&#8217;ve been doing this. Being in the field, you don&#8217;t see it coming, you know? And like, I think, yeah, hopefully we&#8217;ll, you know, we&#8217;ll, we&#8217;ll continue to have as much progress we&#8217;ve had the past few years.Brandon [00:36:55]: So this is maybe an aside, but I&#8217;m really curious, you get this great feedback from the, from the community, right? By being open source. My question is partly like, okay, yeah, if you open source and everyone can copy what you did, but it&#8217;s also maybe balancing priorities, right? Where you, like all my customers are saying. I want this, there&#8217;s all these problems with the model. Yeah, yeah. But my customers don&#8217;t care, right? So like, how do you, how do you think about that? Yeah.Gabriel [00:37:26]: So I would say a couple of things. One is, you know, part of our goal with Bolts and, you know, this is also kind of established as kind of the mission of the public benefit company that we started is to democratize the access to these tools. But one of the reasons why we realized that Bolts needed to be a company, it couldn&#8217;t just be an academic project is that putting a model on GitHub is definitely not enough to get, you know, chemists and biologists, you know, across, you know, both academia, biotech and pharma to use your model to, in their therapeutic programs. And so a lot of what we think about, you know, at Bolts beyond kind of the, just the models is thinking about all the layers. The layers that come on top of the models to get, you know, from, you know, those models to something that can really enable scientists in the industry. And so that goes, you know, into building kind of the right kind of workflows that take in kind of, for example, the data and try to answer kind of directly that those problems that, you know, the chemists and the biologists are asking, and then also kind of building the infrastructure. And so this to say that, you know, even with models fully open. You know, we see a ton of potential for, you know, products in the space and the critical part about a product is that even, you know, for example, with an open source model, you know, running the model is not free, you know, as we were saying, these are pretty expensive model and especially, and maybe we&#8217;ll get into this, you know, these days we&#8217;re seeing kind of pretty dramatic inference time scaling of these models where, you know, the more you run them, the better the results are. But there, you know, you see. You start getting into a point that compute and compute costs becomes a critical factor. And so putting a lot of work into building the right kind of infrastructure, building the optimizations and so on really allows us to provide, you know, a much better service potentially to the open source models. That to say, you know, even though, you know, with a product, we can provide a much better service. I do still think, and we will continue to put a lot of our models open source because the critical kind of role. I think of open source. Models is, you know, helping kind of the community progress on the research and, you know, from which we, we all benefit. And so, you know, we&#8217;ll continue to on the one hand, you know, put some of our kind of base models open source so that the field can, can be on top of it. And, you know, as we discussed earlier, we learn a ton from, you know, the way that the field uses and builds on top of our models, but then, you know, try to build a product that gives the best experience possible to scientists. So that, you know, like a chemist or a biologist doesn&#8217;t need to, you know, spin off a GPU and, you know, set up, you know, our open source model in a particular way, but can just, you know, a bit like, you know, I, even though I am a computer scientist, machine learning scientist, I don&#8217;t necessarily, you know, take a open source LLM and try to kind of spin it off. But, you know, I just maybe open a GPT app or a cloud code and just use it as an amazing product. We kind of want to give the same experience. So this front world.Brandon [00:40:40]: I heard a good analogy yesterday that a surgeon doesn&#8217;t want the hospital to design a scalpel, right?Brandon [00:40:48]: So just buy the scalpel.RJ [00:40:50]: You wouldn&#8217;t believe like the number of people, even like in my short time, you know, between AlphaFold3 coming out and the end of the PhD, like the number of people that would like reach out just for like us to like run AlphaFold3 for them, you know, or things like that. Just because like, you know, bolts in our case, you know, just because it&#8217;s like. It&#8217;s like not that easy, you know, to do that, you know, if you&#8217;re not a computational person. And I think like part of the goal here is also that, you know, we continue to obviously build the interface with computational folks, but that, you know, the models are also accessible to like a larger, broader audience. And then that comes from like, you know, good interfaces and stuff like that.Gabriel [00:41:27]: I think one like really interesting thing about bolts is that with the release of it, you didn&#8217;t just release a model, but you created a community. Yeah. Did that community, it grew very quickly. Did that surprise you? And like, what is the evolution of that community and how is that fed into bolts?RJ [00:41:43]: If you look at its growth, it&#8217;s like very much like when we release a new model, it&#8217;s like, there&#8217;s a big, big jump, but yeah, it&#8217;s, I mean, it&#8217;s been great. You know, we have a Slack community that has like thousands of people on it. And it&#8217;s actually like self-sustaining now, which is like the really nice part because, you know, it&#8217;s, it&#8217;s almost overwhelming, I think, you know, to be able to like answer everyone&#8217;s questions and help. It&#8217;s really difficult, you know. The, the few people that we were, but it ended up that like, you know, people would answer each other&#8217;s questions and like, sort of like, you know, help one another. And so the Slack, you know, has been like kind of, yeah, self, self-sustaining and that&#8217;s been, it&#8217;s been really cool to see.RJ [00:42:21]: And, you know, that&#8217;s, that&#8217;s for like the Slack part, but then also obviously on GitHub as well. We&#8217;ve had like a nice, nice community. You know, I think we also aspire to be even more active on it, you know, than we&#8217;ve been in the past six months, which has been like a bit challenging, you know, for us. But. Yeah, the community has been, has been really great and, you know, there&#8217;s a lot of papers also that have come out with like new evolutions on top of bolts and it&#8217;s surprised us to some degree because like there&#8217;s a lot of models out there. And I think like, you know, sort of people converging on that was, was really cool. And, you know, I think it speaks also, I think, to the importance of like, you know, when, when you put code out, like to try to put a lot of emphasis and like making it like as easy to use as possible and something we thought a lot about when we released the code base. You know, it&#8217;s far from perfect, but, you know.Brandon [00:43:07]: Do you think that that was one of the factors that caused your community to grow is just the focus on easy to use, make it accessible? I think so.RJ [00:43:14]: Yeah. And we&#8217;ve, we&#8217;ve heard it from a few people over the, over the, over the years now. And, you know, and some people still think it should be a lot nicer and they&#8217;re, and they&#8217;re right. And they&#8217;re right. But yeah, I think it was, you know, at the time, maybe a little bit easier than, than other things.Gabriel [00:43:29]: The other thing part, I think led to, to the community and to some extent, I think, you know, like the somewhat the trust in the community. Kind of what we, what we put out is the fact that, you know, it&#8217;s not really been kind of, you know, one model, but, and maybe we&#8217;ll talk about it, you know, after Boltz 1, you know, there were maybe another couple of models kind of released, you know, or open source kind of soon after. We kind of continued kind of that open source journey or at least Boltz 2, where we are not only improving kind of structure prediction, but also starting to do affinity predictions, understanding kind of the strength of the interactions between these different models, which is this critical component. critical property that you often want to optimize in discovery programs. And then, you know, more recently also kind of protein design model. And so we&#8217;ve sort of been building this suite of, of models that come together, interact with one another, where, you know, kind of, there is almost an expectation that, you know, we, we take very at heart of, you know, always having kind of, you know, across kind of the entire suite of different tasks, the best or across the best. model out there so that it&#8217;s sort of like our open source tool can be kind of the go-to model for everybody in the, in the industry. I really want to talk about Boltz 2, but before that, one last question in this direction, was there anything about the community which surprised you? Were there any, like, someone was doing something and you&#8217;re like, why would you do that? That&#8217;s crazy. Or that&#8217;s actually genius. And I never would have thought about that.RJ [00:45:01]: I mean, we&#8217;ve had many contributions. I think like some of the. Interesting ones, like, I mean, we had, you know, this one individual who like wrote like a complex GPU kernel, you know, for part of the architecture on a piece of, the funny thing is like that piece of the architecture had been there since AlphaFold 2, and I don&#8217;t know why it took Boltz for this, you know, for this person to, you know, to decide to do it, but that was like a really great contribution. We&#8217;ve had a bunch of others, like, you know, people figuring out like ways to, you know, hack the model to do something. They click peptides, like, you know, there&#8217;s, I don&#8217;t know if there&#8217;s any other interesting ones come to mind.Gabriel [00:45:41]: One cool one, and this was, you know, something that initially was proposed as, you know, as a message in the Slack channel by Tim O&#8217;Donnell was basically, he was, you know, there are some cases, especially, for example, we discussed, you know, antibody-antigen interactions where the models don&#8217;t necessarily kind of get the right answer. What he noticed is that, you know, the models were somewhat stuck into predicting kind of the antibodies. And so he basically ran the experiments in this model, you can condition, basically, you can give hints. And so he basically gave, you know, random hints to the model, basically, okay, you should bind to this residue, you should bind to the first residue, or you should bind to the 11th residue, or you should bind to the 21st residue, you know, basically every 10 residues scanning the entire antigen.Brandon [00:46:33]: Residues are the...Gabriel [00:46:34]: The amino acids. The amino acids, yeah. So the first amino acids. The 11 amino acids, and so on. So it&#8217;s sort of like doing a scan, and then, you know, conditioning the model to predict all of them, and then looking at the confidence of the model in each of those cases and taking the top. And so it&#8217;s sort of like a very somewhat crude way of doing kind of inference time search. But surprisingly, you know, for antibody-antigen prediction, it actually kind of helped quite a bit. And so there&#8217;s some, you know, interesting ideas that, you know, obviously, as kind of developing the model, you say kind of, you know, wow. This is why would the model, you know, be so dumb. But, you know, it&#8217;s very interesting. And that, you know, leads you to also kind of, you know, start thinking about, okay, how do I, can I do this, you know, not with this brute force, but, you know, in a smarter way.RJ [00:47:22]: And so we&#8217;ve also done a lot of work on that direction. And that speaks to, like, the, you know, the power of scoring. We&#8217;re seeing that a lot. I&#8217;m sure we&#8217;ll talk about it more when we talk about BullsGen. But, you know, our ability to, like, take a structure and determine that that structure is, like... Good. You know, like, somewhat accurate. Whether that&#8217;s a single chain or, like, an interaction is a really powerful way of improving, you know, the models. Like, sort of like, you know, if you can sample a ton and you assume that, like, you know, if you sample enough, you&#8217;re likely to have, like, you know, the good structure. Then it really just becomes a ranking problem. And, you know, now we&#8217;re, you know, part of the inference time scaling that Gabby was talking about is very much that. It&#8217;s like, you know, the more we sample, the more we, like, you know, the ranking model. The ranking model ends up finding something it really likes. And so I think our ability to get better at ranking, I think, is also what&#8217;s going to enable sort of the next, you know, next big, big breakthroughs. Interesting.Brandon [00:48:17]: But I guess there&#8217;s a, my understanding, there&#8217;s a diffusion model and you generate some stuff and then you, I guess, it&#8217;s just what you said, right? Then you rank it using a score and then you finally... And so, like, can you talk about those different parts? Yeah.Gabriel [00:48:34]: So, first of all, like, the... One of the critical kind of, you know, beliefs that we had, you know, also when we started working on Boltz 1 was sort of like the structure prediction models are somewhat, you know, our field version of some foundation models, you know, learning about kind of how proteins and other molecules interact. And then we can leverage that learning to do all sorts of other things. And so with Boltz 2, we leverage that learning to do affinity predictions. So understanding kind of, you know, if I give you this protein, this molecule. How tightly is that interaction? For Boltz 1, what we did was taking kind of that kind of foundation models and then fine tune it to predict kind of entire new proteins. And so the way basically that that works is sort of like instead of for the protein that you&#8217;re designing, instead of fitting in an actual sequence, you fit in a set of blank tokens. And you train the models to, you know, predict both the structure of kind of that protein. The structure also, what the different amino acids of that proteins are. And so basically the way that Boltz 1 operates is that you feed a target protein that you may want to kind of bind to or, you know, another DNA, RNA. And then you feed the high level kind of design specification of, you know, what you want your new protein to be. For example, it could be like an antibody with a particular framework. It could be a peptide. It could be many other things. And that&#8217;s with natural language or? And that&#8217;s, you know, basically, you know, prompting. And we have kind of this sort of like spec that you specify. And, you know, you feed kind of this spec to the model. And then the model translates this into, you know, a set of, you know, tokens, a set of conditioning to the model, a set of, you know, blank tokens. And then, you know, basically the codes as part of the diffusion models, the codes. It&#8217;s a new structure and a new sequence for your protein. And, you know, basically, then we take that. And as Jeremy was saying, we are trying to score it and, you know, how good of a binder it is to that original target.Brandon [00:50:51]: You&#8217;re using basically Boltz to predict the folding and the affinity to that molecule. So and then that kind of gives you a score? Exactly.Gabriel [00:51:03]: So you use this model to predict the folding. And then you do two things. One is that you predict the structure and with something like Boltz2, and then you basically compare that structure with what the model predicted, what Boltz2 predicted. And this is sort of like in the field called consistency. It&#8217;s basically you want to make sure that, you know, the structure that you&#8217;re predicting is actually what you&#8217;re trying to design. And that gives you a much better confidence that, you know, that&#8217;s a good design. And so that&#8217;s the first filtering. And the second filtering that we did as part of kind of the Boltz2 pipeline that was released is that we look at the confidence that the model has in the structure. Now, unfortunately, kind of going to your question of, you know, predicting affinity, unfortunately, confidence is not a very good predictor of affinity. And so one of the things that we&#8217;ve actually done a ton of progress, you know, since we released Boltz2.Brandon [00:52:03]: And kind of we have some new results that we are going to kind of announce soon is kind of, you know, the ability to get much better hit rates when instead of, you know, trying to rely on confidence of the model, we are actually directly trying to predict the affinity of that interaction. Okay. Just backing up a minute. So your diffusion model actually predicts not only the protein sequence, but also the folding of it. Exactly.Gabriel [00:52:32]: And actually, you can... One of the big different things that we did compared to other models in the space, and, you know, there were some papers that had already kind of done this before, but we really scaled it up was, you know, basically somewhat merging kind of the structure prediction and the sequence prediction into almost the same task. And so the way that Boltz2 works is that you are basically the only thing that you&#8217;re doing is predicting the structure. So the only sort of... Supervision is we give you a supervision on the structure, but because the structure is atomic and, you know, the different amino acids have a different atomic composition, basically from the way that you place the atoms, we also understand not only kind of the structure that you wanted, but also the identity of the amino acid that, you know, the models believed was there. And so we&#8217;ve basically, instead of, you know, having these two supervision signals, you know, one discrete, one continuous. That somewhat, you know, don&#8217;t interact well together. We sort of like build kind of like an encoding of, you know, sequences in structures that allows us to basically use exactly the same supervision signal that we were using to Boltz2 that, you know, you know, largely similar to what AlphaVol3 proposed, which is very scalable. And we can use that to design new proteins. Oh, interesting.RJ [00:53:58]: Maybe a quick shout out to Hannes Stark on our team who like did all this work. Yeah.Gabriel [00:54:04]: Yeah, that was a really cool idea. I mean, like looking at the paper and there&#8217;s this is like encoding or you just add a bunch of, I guess, kind of atoms, which can be anything, and then they get sort of rearranged and then basically plopped on top of each other so that and then that encodes what the amino acid is. And there&#8217;s sort of like a unique way of doing this. It was that was like such a really such a cool, fun idea.RJ [00:54:29]: I think that idea was had existed before. Yeah, there were a couple of papers.Gabriel [00:54:33]: Yeah, I had proposed this and and Hannes really took it to the large scale.Brandon [00:54:39]: In the paper, a lot of the paper for Boltz2Gen is dedicated to actually the validation of the model. In my opinion, all the people we basically talk about feel that this sort of like in the wet lab or whatever the appropriate, you know, sort of like in real world validation is the whole problem or not the whole problem, but a big giant part of the problem. So can you talk a little bit about the highlights? From there, that really because to me, the results are impressive, both from the perspective of the, you know, the model and also just the effort that went into the validation by a large team.Gabriel [00:55:18]: First of all, I think I should start saying is that both when we were at MIT and Thomas Yacolas and Regina Barzillai&#8217;s lab, as well as at Boltz, you know, we are not a we&#8217;re not a biolab and, you know, we are not a therapeutic company. And so to some extent, you know, we were first forced to, you know, look outside of, you know, our group, our team to do the experimental validation. One of the things that really, Hannes, in the team pioneer was the idea, OK, can we go not only to, you know, maybe a specific group and, you know, trying to find a specific system and, you know, maybe overfit a bit to that system and trying to validate. But how can we test this model? So. Across a very wide variety of different settings so that, you know, anyone in the field and, you know, printing design is, you know, such a kind of wide task with all sorts of different applications from therapeutic to, you know, biosensors and many others that, you know, so can we get a validation that is kind of goes across many different tasks? And so he basically put together, you know, I think it was something like, you know, 25 different. You know, academic and industry labs that committed to, you know, testing some of the designs from the model and some of this testing is still ongoing and, you know, giving results kind of back to us in exchange for, you know, hopefully getting some, you know, new great sequences for their task. And he was able to, you know, coordinate this, you know, very wide set of, you know, scientists and already in the paper, I think we. Shared results from, I think, eight to 10 different labs kind of showing results from, you know, designing peptides, designing to target, you know, ordered proteins, peptides targeting disordered proteins, which are results, you know, of designing proteins that bind to small molecules, which are results of, you know, designing nanobodies and across a wide variety of different targets. And so that&#8217;s sort of like. That gave to the paper a lot of, you know, validation to the model, a lot of validation that was kind of wide.Brandon [00:57:39]: And so those would be therapeutics for those animals or are they relevant to humans as well? They&#8217;re relevant to humans as well.Gabriel [00:57:45]: Obviously, you need to do some work into, quote unquote, humanizing them, making sure that, you know, they have the right characteristics to so they&#8217;re not toxic to humans and so on.RJ [00:57:57]: There are some approved medicine in the market that are nanobodies. There&#8217;s a general. General pattern, I think, in like in trying to design things that are smaller, you know, like it&#8217;s easier to manufacture at the same time, like that comes with like potentially other challenges, like maybe a little bit less selectivity than like if you have something that has like more hands, you know, but the yeah, there&#8217;s this big desire to, you know, try to design many proteins, nanobodies, small peptides, you know, that just are just great drug modalities.Brandon [00:58:27]: Okay. I think we were left off. We were talking about validation. Validation in the lab. And I was very excited about seeing like all the diverse validations that you&#8217;ve done. Can you go into some more detail about them? Yeah. Specific ones. Yeah.RJ [00:58:43]: The nanobody one. I think we did. What was it? 15 targets. Is that correct? 14. 14 targets. Testing. So we typically the way this works is like we make a lot of designs. All right. On the order of like tens of thousands. And then we like rank them and we pick like the top. And in this case, and was 15 right for each target and then we like measure sort of like the success rates, both like how many targets we were able to get a binder for and then also like more generally, like out of all of the binders that we designed, how many actually proved to be good binders. Some of the other ones I think involved like, yeah, like we had a cool one where there was a small molecule or design a protein that binds to it. That has a lot of like interesting applications, you know, for example. Like Gabri mentioned, like biosensing and things like that, which is pretty cool. We had a disordered protein, I think you mentioned also. And yeah, I think some of those were some of the highlights. Yeah.Gabriel [00:59:44]: So I would say that the way that we structure kind of some of those validations was on the one end, we have validations across a whole set of different problems that, you know, the biologists that we were working with came to us with. So we were trying to. For example, in some of the experiments, design peptides that would target the RACC, which is a target that is involved in metabolism. And we had, you know, a number of other applications where we were trying to design, you know, peptides or other modalities against some other therapeutic relevant targets. We designed some proteins to bind small molecules. And then some of the other testing that we did was really trying to get like a more broader sense. So how does the model work, especially when tested, you know, on somewhat generalization? So one of the things that, you know, we found with the field was that a lot of the validation, especially outside of the validation that was on specific problems, was done on targets that have a lot of, you know, known interactions in the training data. And so it&#8217;s always a bit hard to understand, you know, how much are these models really just regurgitating kind of what they&#8217;ve seen or trying to imitate. What they&#8217;ve seen in the training data versus, you know, really be able to design new proteins. And so one of the experiments that we did was to take nine targets from the PDB, filtering to things where there is no known interaction in the PDB. So basically the model has never seen kind of this particular protein bound or a similar protein bound to another protein. So there is no way that. The model from its training set can sort of like say, okay, I&#8217;m just going to kind of tweak something and just imitate this particular kind of interaction. And so we took those nine proteins. We worked with adaptive CRO and basically tested, you know, 15 mini proteins and 15 nanobodies against each one of them. And the very cool thing that we saw was that on two thirds of those targets, we were able to, from this 15 design, get nanomolar binders, nanomolar, roughly speaking, just a measure of, you know, how strongly kind of the interaction is, roughly speaking, kind of like a nanomolar binder is approximately the kind of binding strength or binding that you need for a therapeutic. Yeah. So maybe switching directions a bit. Bolt&#8217;s lab was just announced this week or was it last week? Yeah. This is like your. First, I guess, product, if that&#8217;s if you want to call it that. Can you talk about what Bolt&#8217;s lab is and yeah, you know, what you hope that people take away from this? Yeah.RJ [01:02:44]: You know, as we mentioned, like I think at the very beginning is the goal with the product has been to, you know, address what the models don&#8217;t on their own. And there&#8217;s largely sort of two categories there. I&#8217;ll split it in three. The first one. It&#8217;s one thing to predict, you know, a single interaction, for example, like a single structure. It&#8217;s another to like, you know, very effectively search a space, a design space to produce something of value. What we found, like sort of building on this product is that there&#8217;s a lot of steps involved, you know, in that there&#8217;s certainly need to like, you know, accompany the user through, you know, one of those steps, for example, is like, you know, the creation of the target itself. You know, how do we make sure that the model has like a good enough understanding of the target? So we can like design something and there&#8217;s all sorts of tricks, you know, that you can do to improve like a particular, you know, structure prediction. And so that&#8217;s sort of like, you know, the first stage. And then there&#8217;s like this stage of like, you know, designing and searching the space efficiently. You know, for something like BullsGen, for example, like you, you know, you design many things and then you rank them, for example, for small molecule process, a little bit more complicated. We actually need to also make sure that the molecules are synthesizable. And so the way we do that is that, you know, we have a generative model that learns. To use like appropriate building blocks such that, you know, it can design within a space that we know is like synthesizable. And so there&#8217;s like, you know, this whole pipeline really of different models involved in being able to design a molecule. And so that&#8217;s been sort of like the first thing we call them agents. We have a protein agent and we have a small molecule design agents. And that&#8217;s really like at the core of like what powers, you know, the BullsLab platform.Brandon [01:04:22]: So these agents, are they like a language model wrapper or they&#8217;re just like your models and you&#8217;re just calling them agents? A lot. Yeah. Because they, they, they sort of perform a function on behalf of.RJ [01:04:33]: They&#8217;re more of like a, you know, a recipe, if you wish. And I think we use that term sort of because of, you know, sort of the complex pipelining and automation, you know, that goes into like all this plumbing. So that&#8217;s the first part of the product. The second part is the infrastructure. You know, we need to be able to do this at very large scale for any one, you know, group that&#8217;s doing a design campaign. Let&#8217;s say you&#8217;re designing, you know, I&#8217;d say a hundred thousand possible candidates. Right. To find the good one that is, you know, a very large amount of compute, you know, for small molecules, it&#8217;s on the order of like a few seconds per designs for proteins can be a bit longer. And so, you know, ideally you want to do that in parallel, otherwise it&#8217;s going to take you weeks. And so, you know, we&#8217;ve put a lot of effort into like, you know, our ability to have a GPU fleet that allows any one user, you know, to be able to do this kind of like large parallel search.Brandon [01:05:23]: So you&#8217;re amortizing the cost over your users. Exactly. Exactly.RJ [01:05:27]: And, you know, to some degree, like it&#8217;s whether you. Use 10,000 GPUs for like, you know, a minute is the same cost as using, you know, one GPUs for God knows how long. Right. So you might as well try to parallelize if you can. So, you know, a lot of work has gone, has gone into that, making it very robust, you know, so that we can have like a lot of people on the platform doing that at the same time. And the third one is, is the interface and the interface comes in, in two shapes. One is in form of an API and that&#8217;s, you know, really suited for companies that want to integrate, you know, these pipelines, these agents.RJ [01:06:01]: So we&#8217;re already partnering with, you know, a few distributors, you know, that are gonna integrate our API. And then the second part is the user interface. And, you know, we, we&#8217;ve put a lot of thoughts also into that. And this is when I, I mentioned earlier, you know, this idea of like broadening the audience. That&#8217;s kind of what the, the user interface is about. And we&#8217;ve built a lot of interesting features in it, you know, for example, for collaboration, you know, when you have like potentially multiple medicinal chemists or. We&#8217;re going through the results and trying to pick out, okay, like what are the molecules that we&#8217;re going to go and test in the lab? It&#8217;s powerful for them to be able to, you know, for example, each provide their own ranking and then do consensus building. And so there&#8217;s a lot of features around launching these large jobs, but also around like collaborating on analyzing the results that we try to solve, you know, with that part of the platform. So Bolt&#8217;s lab is sort of a combination of these three objectives into like one, you know, sort of cohesive platform. Who is this accessible to? Everyone. You do need to request access today. We&#8217;re still like, you know, sort of ramping up the usage, but anyone can request access. If you are an academic in particular, we, you know, we provide a fair amount of free credit so you can play with the platform. If you are a startup or biotech, you may also, you know, reach out and we&#8217;ll typically like actually hop on a call just to like understand what you&#8217;re trying to do and also provide a lot of free credit to get started. And of course, also with larger companies, we can deploy this platform in a more like secure environment. And so that&#8217;s like more like customizing. You know, deals that we make, you know, with the partners, you know, and that&#8217;s sort of the ethos of Bolt. I think this idea of like servicing everyone and not necessarily like going after just, you know, the really large enterprises. And that starts from the open source, but it&#8217;s also, you know, a key design principle of the product itself.Gabriel [01:07:48]: One thing I was thinking about with regards to infrastructure, like in the LLM space, you know, the cost of a token has gone down by I think a factor of a thousand or so over the last three years, right? Yeah. And is it possible that like essentially you can exploit economies of scale and infrastructure that you can make it cheaper to run these things yourself than for any person to roll their own system? A hundred percent. Yeah.RJ [01:08:08]: I mean, we&#8217;re already there, you know, like running Bolts on our platform, especially on a large screen is like considerably cheaper than it would probably take anyone to put the open source model out there and run it. And on top of the infrastructure, like one of the things that we&#8217;ve been working on is accelerating the models. So, you know. Our small molecule screening pipeline is 10x faster on Bolts Lab than it is in the open source, you know, and that&#8217;s also part of like, you know, building a product, you know, of something that scales really well. And we really wanted to get to a point where like, you know, we could keep prices very low in a way that it would be a no-brainer, you know, to use Bolts through our platform.Gabriel [01:08:52]: How do you think about validation of your like agentic systems? Because, you know, as you were saying earlier. Like we&#8217;re AlphaFold style models are really good at, let&#8217;s say, monomeric, you know, proteins where you have, you know, co-evolution data. But now suddenly the whole point of this is to design something which doesn&#8217;t have, you know, co-evolution data, something which is really novel. So now you&#8217;re basically leaving the domain that you thought was, you know, that you know you are good at. So like, how do you validate that?RJ [01:09:22]: Yeah, I like every complete, but there&#8217;s obviously, you know, a ton of computational metrics. That we rely on, but those are only take you so far. You really got to go to the lab, you know, and test, you know, okay, with this method A and this method B, how much better are we? You know, how much better is my, my hit rate? How stronger are my binders? Also, it&#8217;s not just about hit rate. It&#8217;s also about how good the binders are. And there&#8217;s really like no way, nowhere around that. I think we&#8217;re, you know, we&#8217;ve really ramped up the amount of experimental validation that we do so that we like really track progress, you know, as scientifically sound, you know. Yeah. As, as possible out of this, I think.Gabriel [01:10:00]: Yeah, no, I think, you know, one thing that is unique about us and maybe companies like us is that because we&#8217;re not working on like maybe a couple of therapeutic pipelines where, you know, our validation would be focused on those. We, when we do an experimental validation, we try to test it across tens of targets. And so that on the one end, we can get a much more statistically significant result and, and really allows us to make progress. From the methodological side without being, you know, steered by, you know, overfitting on any one particular system. And of course we choose, you know, we always try to choose targets and problems are sort of like at the frontier of what&#8217;s possible today. So, you know, you don&#8217;t want something too easy. You don&#8217;t want something too hard. Otherwise you&#8217;re not going to see progress. And so, you know, this is a somewhat evolving set of targets. We talked earlier about the targets that we looked at with, with Boltchan. And now we are even trying kind of, you know, even harder targets, both for small molecule and proteins. And so we try to keep ourselves on the, on the boundary of what&#8217;s possible. So do you have like infrastructure or is this is like, you just have a lot of different partnerships with academic labs and you&#8217;re just kind of keep pushing on these and driving these. We do partially this through academic labs more and more. We do this through CROs just because of, you know, to some extent is also, we need kind of replicability often kind of, you know, going after the same time. So we try to, we try to keep our, our targets, you know, multiple times and, you know, to see the, the progress from, you know, one month to the next. And speed. And speed. And speed. Speed of execution. Yeah. And, So what happens if you start getting a bunch of like really strong biters against therapeutic targets? What do you do?RJ [01:11:43]: Release them. Yeah.Gabriel [01:11:45]: But you can release them in open source? Like,RJ [01:11:47]: Yeah, I mean, you know, I mean, when we say we have no interest in making dress, we&#8217;re serious. Like, you know, uh, I mean, when it, when it was with the academic labs, basically the, you know, I was, they keep it, they do a lot of it.Gabriel [01:12:02]: I will also say, and I think this has been a bit of the issue that I have with some of the things that have been said in the field, is when we say that we design new proteins or we say that we design new molecules, go and bind these particular targets. We should be very clear, these are not drugs. These are not things that are ready to be put into a human. And there is still a lot of development that goes with it. And so this is kind of to us, we see ourselves as building tools for scientists. At the end of the day, it really relies on the scientist having a great therapeutic hypothesis and then pushing through kind of all the stages of development. And, you know, we try to build tools that can accompany them in that journey. It&#8217;s not like a magic box where, you know, you can just turn it and get FDA approved drugs.Brandon [01:13:06]: But actually, that brings up an interesting question that I&#8217;ve been wondering about is, do you guys see yourself staying in this, for lack of a better way of saying it, layer? Or do you think that you&#8217;ll start to... Yeah. Either on the physical sense, looking at different layers of the virtual cell, so to speak, or also, you know, so there&#8217;s like the development process that goes, you know, sort of like design preclinical, clinical approval and thinking about improving the performance throughout that process based on the designs. Is that a direction that you guys are pushing? Yeah.Gabriel [01:13:45]: So one of the things, as Jeremy said, you know, we are... We are not a therapeutic company. We want to kind of stay not to be a therapeutic company, always be at the service of, you know, all the different, you know, companies, including therapeutic companies that we serve. And, you know, that to some extent does mean, you know, that we need to try to, you know, go deeper and deeper in getting these models better and better. One of the things that we are doing across, you know, many other in the field is, you know, now that we are really... They&#8217;re starting to be good, both for small molecule and... For proteins to design kind of binders, design relatively tight binders, is starting to look at all these other properties, you know, they&#8217;re called developpabilities or at me that, you know, we care about when developing a drug and try, can we design them from, from Gageco. The thing about those properties in some of them, you know, you need to, you know, start having an understanding of the cell. And so that&#8217;s on the one hand, kind of why we need that understanding. But also, you know, the way... The way that we also think about all different and complex diseases is that these models, then these tools that we&#8217;re building have a good understanding of kind of, you know, biomolecular interactions and kind of their interactions. Now, at the same time, every disease is often kind of unique and every therapeutic hypothesis is unique. And so you maybe want to have something that needs to hit the particular, you know, let&#8217;s say target in a virus in a particular way, but you don&#8217;t maybe know exactly. So you can start to have a more open-minded understanding of what&#8217;s, what&#8217;s a way you want to do. And so maybe in the first set of designs, you&#8217;re going to try to target different epitopes in different ways, and then you&#8217;re going to test them in the lab, maybe directly in vivo, and you&#8217;re going to see which ones work and which ones don&#8217;t. And so then you need to bring those results back into the models. And then the models can start to have a more wider understanding, you know, not just of the biophysical of the antibodies interacting with that target, but also how that is shaped within the cell. And so first of all, you know, that means on the one end that we need, you know, kind of these loops, and this is also partially how we, we designed the platform to be. But that also means that we also need to start understanding more and more kind of higher level things. And, you know, I wouldn&#8217;t say that we&#8217;re working in any way on like a virtual cell like others are, but we&#8217;re definitely thinking kind of very deeply about kind of, you know, how does, you know, kind of the way that we target certain proteins. Interfere, interact with, you know, maybe pathways that are existing in the cell. One question that has come up is you talk a lot about user interface and so on. And I think this is really important, but like my experience with dealing with medicinal chemists, when you get the machine learning models, is they are the most superstitious, skeptical, like pseudo-religious people I&#8217;ve ever talked to when it comes to doing science. Sorry for the medicinal chemists listening. Yeah, they&#8217;re amazing. Like, they&#8217;re absolutely, I&#8217;ve worked with some spectacular medicinal chemists who just pull magic out of their hat again and again, and I have no idea how they do it. But when you bring them a machine learning model, it is sometimes quite tricky to get them to deal with it. How has your interaction been with this? And how have you thought about, like, building Bolt&#8217;s lab to work with the skeptics? One of the great value unlocks for us and for our product has been when we brought to the team a medicinal chemist. His name is Jeffrey. So I think kind of like on the one hand, you know, day one, you know, he obviously had a lot of opinions on kind of a lot of the ways that we should change, you know, both kind of the way that the agents worked, the way that the platform worked. But it&#8217;s been really amazing kind of, you know, once also we started kind of shaping kind of the platform in a better way with this feedback, how we went from, you know, to some extent, you know, a fair skepticism to him, you know, actually using, you know, a lot of the things that we did. Yeah. So he&#8217;s doing a lot more compute than any of our computational folks in the team, you know, at times that, you know, he&#8217;s, you know, running, you know, he has all these sort of hypotheses. Okay, maybe I can hit this protein this particular way. I can hit in that way. Actually, let me look at for this particular molecular space. Let me try to optimize for this particular interactions. So he ends up, you know, running several screens in parallel, you know, using hundreds of GPUs, you know, on his own. And, you know, so this has been, you know, pretty incredible to see kind of how, you know, maybe the way that I was more thinking about a problem, which is, okay, you&#8217;re just trying to design a binder, a small molecule to a particular protein. The way that he thinks about it is, you know, much more deeply and, you know, trying all these different things, these different hypotheses. And then, you know, once he gets the results from the model, he doesn&#8217;t just, you know, take the top 15, but he really kind of looks over and, you know, kind of tries to understand, you know, the different things. And then when we select, you know, maybe some designs to bring forth, you know, he has, you know, something where, you know, both the models understand that something&#8217;s good, but himself as well. And that&#8217;s why we also built kind of the platform to be an interface for, you know, this kind of chemist and, you know, also like engineers. Yeah. Collaborative experience.RJ [01:19:09]: I think at the end of the day, like, you know, for people to be convinced, you have to show them something that they didn&#8217;t think was possible. And until you have that aha moment, you know, I think the skepticism will remain. But then when, you know, every once in a while, I think there&#8217;s like a result that like really surprises people. And then it&#8217;s like, oh, wow, okay, this is actually, I can do something with this. So you just get in their hands, have them try it out, and they&#8217;ll be convinced. Yeah, or like maybe once the lab results come back. Or their friends. Yeah, or maybe one of their colleagues is convinced. Yeah. I think it takes going to the lab at some point. There&#8217;s no avoiding that, you know, as beautiful as the platform can be, as nice as the molecules might look, you know, that the model predicted. I think what really convinces people is like, you know, hits. Yeah.Gabriel [01:19:54]: Yeah. You see the results. Exactly. Yeah. Cool. Thank you for, you know, taking the time to chat with us. Yeah. You know, is there anything that you would like your audience to know? I mean, first of all, you know, we&#8217;re just getting started, you know, continuing to build a team. And so definitely always looking for great folks, both on the kind of, you know, software side, you know, machine learning side, but also scientists to join the team and help us, you know, shape. On the infrastructure side, too. Indeed. If you think that if you want a new challenge, because this is not just next token prediction, this is really a new engineering challenge. Exactly. Yeah. If you, if no matter, you know, how much experience you have with, you know, biologists and chemistry, if you want to come, you know, help us in a shape, what, you know, biology and chemistry, hopefully we&#8217;ll look like in five, 10 years. We&#8217;d love to hear from you. And so go to boltz.bio and, you know, come join the team. Cool. Thank you. Awesome. Thank you so much. Thank you.",
      "url": "https://www.latent.space/p/boltz",
      "author": "Unknown",
      "published": "2026-02-12T02:12:14",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Boltz co-founders discuss their open-source suite (Boltz-1, Boltz-2) for structural biology, aiming to democratize protein-ligand and protein-protein interaction modeling beyond AlphaFold's single-chain prediction. They target generative protein design for drug discovery.",
      "importance_score": 63.0,
      "reasoning": "Open-source drug discovery tools advancing beyond AlphaFold represent meaningful scientific progress, though this is a podcast discussion rather than a new release announcement.",
      "themes": [
        "AI_science",
        "drug_discovery",
        "open_source",
        "structural_biology"
      ],
      "continuation": null,
      "summary_html": "<p>Boltz co-founders discuss their open-source suite (Boltz-1, Boltz-2) for structural biology, aiming to democratize protein-ligand and protein-protein interaction modeling beyond AlphaFold's single-chain prediction. They target generative protein design for drug discovery.</p>",
      "content_html": "<p>This podcast features Gabriele Corso and Jeremy Wohlwend, co-founders of Boltz and authors of the Boltz Manifesto, discussing the rapid evolution of structural biology models from AlphaFold to their own open-source suite, Boltz-1 and Boltz-2. The central thesis is that while single-chain protein structure prediction is largely “solved” through evolutionary hints, the next frontier lies in modeling complex interactions (protein-ligand, protein-protein) and generative protein design, which Boltz aims to democratize via open-source foundations and scalable infrastructure.Full Video PodOn YouTube!Timestamps00:00 Introduction to Benchmarking and the “Solved” Protein Problem06:48 Evolutionary Hints and Co-evolution in Structure Prediction10:00 The Importance of Protein Function and Disease States15:31 Transitioning from AlphaFold 2 to AlphaFold 3 Capabilities19:48 Generative Modeling vs. Regression in Structural Biology25:00 The “Bitter Lesson” and Specialized AI Architectures29:14 Development Anecdotes: Training Boltz-1 on a Budget32:00 Validation Strategies and the Protein Data Bank (PDB)37:26 The Mission of Boltz: Democratizing Access and Open Source41:43 Building a Self-Sustaining Research Community44:40 Boltz-2 Advancements: Affinity Prediction and Design51:03 BoltzGen: Merging Structure and Sequence Prediction55:18 Large-Scale Wet Lab Validation Results01:02:44 Boltz Lab Product Launch: Agents and Infrastructure01:13:06 Future Directions: Developpability and the “Virtual Cell”01:17:35 Interacting with Skeptical Medicinal ChemistsKey SummaryEvolution of Structure Prediction &amp; Evolutionary HintsCo-evolutionary Landscapes: The speakers explain that breakthrough progress in single-chain protein prediction relied on decoding evolutionary correlations where mutations in one position necessitate mutations in another to conserve 3D structure.Structure vs. Folding: They differentiate between structure prediction (getting the final answer) and folding (the kinetic process of reaching that state), noting that the field is still quite poor at modeling the latter.Physics vs. Statistics: RJ posits that while models use evolutionary statistics to find the right “valley” in the energy landscape, they likely possess a “light understanding” of physics to refine the local minimum.The Shift to Generative ArchitecturesGenerative Modeling: A key leap in AlphaFold 3 and Boltz-1 was moving from regression (predicting one static coordinate) to a generative diffusion approach that samples from a posterior distribution.Handling Uncertainty: This shift allows models to represent multiple conformational states and avoid the “averaging” effect seen in regression models when the ground truth is ambiguous.Specialized Architectures: Despite the “bitter lesson” of general-purpose transformers, the speakers argue that equivariant architectures remain vastly superior for biological data due to the inherent 3D geometric constraints of molecules.Boltz-2 and Generative Protein DesignUnified Encoding: Boltz-2 (and BoltzGen) treats structure and sequence prediction as a single task by encoding amino acid identities into the atomic composition of the predicted structure.Design Specifics: Instead of a sequence, users feed the model blank tokens and a high-level “spec” (e.g., an antibody framework), and the model decodes both the 3D structure and the corresponding amino acids.Affinity Prediction: While model confidence is a common metric, Boltz-2 focuses on affinity prediction—quantifying exactly how tightly a designed binder will stick to its target.Real-World Validation and ProductizationGeneralized Validation: To prove the model isn’t just “regurgitating” known data, Boltz tested its designs on 9 targets with zero known interactions in the PDB, achieving nanomolar binders for two-thirds of them.Boltz Lab Infrastructure: The newly launched Boltz Lab platform provides “agents” for protein and small molecule design, optimized to run 10x faster than open-source versions through proprietary GPU kernels.Human-in-the-Loop: The platform is designed to convert skeptical medicinal chemists by allowing them to run parallel screens and use their intuition to filter model outputs.TranscriptRJ [00:05:35]: But the goal remains to, like, you know, really challenge the models, like, how well do these models generalize? And, you know, we’ve seen in some of the latest CASP competitions, like, while we’ve become really, really good at proteins, especially monomeric proteins, you know, other modalities still remain pretty difficult. So it’s really essential, you know, in the field that there are, like, these efforts to gather, you know, benchmarks that are challenging. So it keeps us in line, you know, about what the models can do or not.Gabriel [00:06:26]: Yeah, it’s interesting you say that, like, in some sense, CASP, you know, at CASP 14, a problem was solved and, like, pretty comprehensively, right? But at the same time, it was really only the beginning. So you can say, like, what was the specific problem you would argue was solved? And then, like, you know, what is remaining, which is probably quite open.RJ [00:06:48]: I think we’ll steer away from the term solved, because we have many friends in the community who get pretty upset at that word. And I think, you know, fairly so. But the problem that was, you know, that a lot of progress was made on was the ability to predict the structure of single chain proteins. So proteins can, like, be composed of many chains. And single chain proteins are, you know, just a single sequence of amino acids. And one of the reasons that we’ve been able to make such progress is also because we take a lot of hints from evolution. So the way the models work is that, you know, they sort of decode a lot of hints. That comes from evolutionary landscapes. So if you have, like, you know, some protein in an animal, and you go find the similar protein across, like, you know, different organisms, you might find different mutations in them. And as it turns out, if you take a lot of the sequences together, and you analyze them, you see that some positions in the sequence tend to evolve at the same time as other positions in the sequence, sort of this, like, correlation between different positions. And it turns out that that is typically a hint that these two positions are close in three dimension. So part of the, you know, part of the breakthrough has been, like, our ability to also decode that very, very effectively. But what it implies also is that in absence of that co-evolutionary landscape, the models don’t quite perform as well. And so, you know, I think when that information is available, maybe one could say, you know, the problem is, like, somewhat solved. From the perspective of structure prediction, when it isn’t, it’s much more challenging. And I think it’s also worth also differentiating the, sometimes we confound a little bit, structure prediction and folding. Folding is the more complex process of actually understanding, like, how it goes from, like, this disordered state into, like, a structured, like, state. And that I don’t think we’ve made that much progress on. But the idea of, like, yeah, going straight to the answer, we’ve become pretty good at.Brandon [00:08:49]: So there’s this protein that is, like, just a long chain and it folds up. Yeah. And so we’re good at getting from that long chain in whatever form it was originally to the thing. But we don’t know how it necessarily gets to that state. And there might be intermediate states that it’s in sometimes that we’re not aware of.RJ [00:09:10]: That’s right. And that relates also to, like, you know, our general ability to model, like, the different, you know, proteins are not static. They move, they take different shapes based on their energy states. And I think we are, also not that good at understanding the different states that the protein can be in and at what frequency, what probability. So I think the two problems are quite related in some ways. Still a lot to solve. But I think it was very surprising at the time, you know, that even with these evolutionary hints that we were able to, you know, to make such dramatic progress.Brandon [00:09:45]: So I want to ask, why does the intermediate states matter? But first, I kind of want to understand, why do we care? What proteins are shaped like?Gabriel [00:09:54]: Yeah, I mean, the proteins are kind of the machines of our body. You know, the way that all the processes that we have in our cells, you know, work is typically through proteins, sometimes other molecules, sort of intermediate interactions. And through that interactions, we have all sorts of cell functions. And so when we try to understand, you know, a lot of biology, how our body works, how disease work. So we often try to boil it down to, okay, what is going right in case of, you know, our normal biological function and what is going wrong in case of the disease state. And we boil it down to kind of, you know, proteins and kind of other molecules and their interaction. And so when we try predicting the structure of proteins, it’s critical to, you know, have an understanding of kind of those interactions. It’s a bit like seeing the difference between... Having kind of a list of parts that you would put it in a car and seeing kind of the car in its final form, you know, seeing the car really helps you understand what it does. On the other hand, kind of going to your question of, you know, why do we care about, you know, how the protein falls or, you know, how the car is made to some extent is that, you know, sometimes when something goes wrong, you know, there are, you know, cases of, you know, proteins misfolding. In some diseases and so on, if we don’t understand this folding process, we don’t really know how to intervene.RJ [00:11:30]: There’s this nice line in the, I think it’s in the Alpha Fold 2 manuscript, where they sort of discuss also like why we even hopeful that we can target the problem in the first place. And then there’s this notion that like, well, four proteins that fold. The folding process is almost instantaneous, which is a strong, like, you know, signal that like, yeah, like we should, we might be... able to predict that this very like constrained thing that, that the protein does so quickly. And of course that’s not the case for, you know, for, for all proteins. And there’s a lot of like really interesting mechanisms in the cells, but yeah, I remember reading that and thought, yeah, that’s somewhat of an insightful point.Gabriel [00:12:10]: I think one of the interesting things about the protein folding problem is that it used to be actually studied. And part of the reason why people thought it was impossible, it used to be studied as kind of like a classical example. Of like an MP problem. Uh, like there are so many different, you know, type of, you know, shapes that, you know, this amino acid could take. And so, this grows combinatorially with the size of the sequence. And so there used to be kind of a lot of actually kind of more theoretical computer science thinking about and studying protein folding as an MP problem. And so it was very surprising also from that perspective, kind of seeing. Machine learning so clear, there is some, you know, signal in those sequences, through evolution, but also through kind of other things that, you know, us as humans, we’re probably not really able to, uh, to understand, but that is, models I’ve, I’ve learned.Brandon [00:13:07]: And so Andrew White, we were talking to him a few weeks ago and he said that he was following the development of this and that there were actually ASICs that were developed just to solve this problem. So, again, that there were. There were many, many, many millions of computational hours spent trying to solve this problem before AlphaFold. And just to be clear, one thing that you mentioned was that there’s this kind of co-evolution of mutations and that you see this again and again in different species. So explain why does that give us a good hint that they’re close by to each other? Yeah.RJ [00:13:41]: Um, like think of it this way that, you know, if I have, you know, some amino acid that mutates, it’s going to impact everything around it. Right. In three dimensions. And so it’s almost like the protein through several, probably random mutations and evolution, like, you know, ends up sort of figuring out that this other amino acid needs to change as well for the structure to be conserved. Uh, so this whole principle is that the structure is probably largely conserved, you know, because there’s this function associated with it. And so it’s really sort of like different positions compensating for, for each other. I see.Brandon [00:14:17]: Those hints in aggregate give us a lot. Yeah. So you can start to look at what kinds of information about what is close to each other, and then you can start to look at what kinds of folds are possible given the structure and then what is the end state.RJ [00:14:30]: And therefore you can make a lot of inferences about what the actual total shape is. Yeah, that’s right. It’s almost like, you know, you have this big, like three dimensional Valley, you know, where you’re sort of trying to find like these like low energy states and there’s so much to search through. That’s almost overwhelming. But these hints, they sort of maybe put you in. An area of the space that’s already like, kind of close to the solution, maybe not quite there yet. And, and there’s always this question of like, how much physics are these models learning, you know, versus like, just pure like statistics. And like, I think one of the thing, at least I believe is that once you’re in that sort of approximate area of the solution space, then the models have like some understanding, you know, of how to get you to like, you know, the lower energy, uh, low energy state. And so maybe you have some, some light understanding. Of physics, but maybe not quite enough, you know, to know how to like navigate the whole space. Right. Okay.Brandon [00:15:25]: So we need to give it these hints to kind of get into the right Valley and then it finds the, the minimum or something. Yeah.Gabriel [00:15:31]: One interesting explanation about our awful free works that I think it’s quite insightful, of course, doesn’t cover kind of the entirety of, of what awful does that is, um, they’re going to borrow from, uh, Sergio Chinico for MIT. So he sees kind of awful. Then the interesting thing about awful is God. This very peculiar architecture that we have seen, you know, used, and this architecture operates on this, you know, pairwise context between amino acids. And so the idea is that probably the MSA gives you this first hint about what potential amino acids are close to each other. MSA is most multiple sequence alignment. Exactly. Yeah. Exactly. This evolutionary information. Yeah. And, you know, from this evolutionary information about potential contacts, then is almost as if the model is. of running some kind of, you know, diastro algorithm where it’s sort of decoding, okay, these have to be closed. Okay. Then if these are closed and this is connected to this, then this has to be somewhat closed. And so you decode this, that becomes basically a pairwise kind of distance matrix. And then from this rough pairwise distance matrix, you decode kind of theBrandon [00:16:42]: actual potential structure. Interesting. So there’s kind of two different things going on in the kind of coarse grain and then the fine grain optimizations. Interesting. Yeah. Very cool.Gabriel [00:16:53]: Yeah. You mentioned AlphaFold3. So maybe we have a good time to move on to that. So yeah, AlphaFold2 came out and it was like, I think fairly groundbreaking for this field. Everyone got very excited. A few years later, AlphaFold3 came out and maybe for some more history, like what were the advancements in AlphaFold3? And then I think maybe we’ll, after that, we’ll talk a bit about the sort of how it connects to Bolt. But anyway. Yeah. So after AlphaFold2 came out, you know, Jeremy and I got into the field and with many others, you know, the clear problem that, you know, was, you know, obvious after that was, okay, now we can do individual chains. Can we do interactions, interaction, different proteins, proteins with small molecules, proteins with other molecules. And so. So why are interactions important? Interactions are important because to some extent that’s kind of the way that, you know, these machines, you know, these proteins have a function, you know, the function comes by the way that they interact with other proteins and other molecules. Actually, in the first place, you know, the individual machines are often, as Jeremy was mentioning, not made of a single chain, but they’re made of the multiple chains. And then these multiple chains interact with other molecules to give the function to those. And on the other hand, you know, when we try to intervene of these interactions, think about like a disease, think about like a, a biosensor or many other ways we are trying to design the molecules or proteins that interact in a particular way with what we would call a target protein or target. You know, this problem after AlphaVol2, you know, became clear, kind of one of the biggest problems in the field to, to solve many groups, including kind of ours and others, you know, started making some kind of contributions to this problem of trying to model these interactions. And AlphaVol3 was, you know, was a significant advancement on the problem of modeling interactions. And one of the interesting thing that they were able to do while, you know, some of the rest of the field that really tried to try to model different interactions separately, you know, how protein interacts with small molecules, how protein interacts with other proteins, how RNA or DNA have their structure, they put everything together and, you know, train very large models with a lot of advances, including kind of changing kind of systems. Some of the key architectural choices and managed to get a single model that was able to set this new state-of-the-art performance across all of these different kind of modalities, whether that was protein, small molecules is critical to developing kind of new drugs, protein, protein, understanding, you know, interactions of, you know, proteins with RNA and DNAs and so on.Brandon [00:19:39]: Just to satisfy the AI engineers in the audience, what were some of the key architectural and data, data changes that made that possible?Gabriel [00:19:48]: Yeah, so one critical one that was not necessarily just unique to AlphaFold3, but there were actually a few other teams, including ours in the field that proposed this, was moving from, you know, modeling structure prediction as a regression problem. So where there is a single answer and you’re trying to shoot for that answer to a generative modeling problem where you have a posterior distribution of possible structures and you’re trying to sample this distribution. And this achieves two things. One is it starts to allow us to try to model more dynamic systems. As we said, you know, some of these structures can actually take multiple structures. And so, you know, you can now model that, you know, through kind of modeling the entire distribution. But on the second hand, from more kind of core modeling questions, when you move from a regression problem to a generative modeling problem, you are really tackling the way that you think about uncertainty in the model in a different way. So if you think about, you know, I’m undecided between different answers, what’s going to happen in a regression model is that, you know, I’m going to try to make an average of those different kind of answers that I had in mind. When you have a generative model, what you’re going to do is, you know, sample all these different answers and then maybe use separate models to analyze those different answers and pick out the best. So that was kind of one of the critical improvement. The other improvement is that they significantly simplified, to some extent, the architecture, especially of the final model that takes kind of those pairwise representations and turns them into an actual structure. And that now looks a lot more like a more traditional transformer than, you know, like a very specialized equivariant architecture that it was in AlphaFold3.Brandon [00:21:41]: So this is a bitter lesson, a little bit.Gabriel [00:21:45]: There is some aspect of a bitter lesson, but the interesting thing is that it’s very far from, you know, being like a simple transformer. This field is one of the, I argue, very few fields in applied machine learning where we still have kind of architecture that are very specialized. And, you know, there are many people that have tried to replace these architectures with, you know, simple transformers. And, you know, there is a lot of debate in the field, but I think kind of that most of the consensus is that, you know, the performance... that we get from the specialized architecture is vastly superior than what we get through a single transformer. Another interesting thing that I think on the staying on the modeling machine learning side, which I think it’s somewhat counterintuitive seeing some of the other kind of fields and applications is that scaling hasn’t really worked kind of the same in this field. Now, you know, models like AlphaFold2 and AlphaFold3 are, you know, still very large models.RJ [00:29:14]: in a place, I think, where we had, you know, some experience working in, you know, with the data and working with this type of models. And I think that put us already in like a good place to, you know, to produce it quickly. And, you know, and I would even say, like, I think we could have done it quicker. The problem was like, for a while, we didn’t really have the compute. And so we couldn’t really train the model. And actually, we only trained the big model once. That’s how much compute we had. We could only train it once. And so like, while the model was training, we were like, finding bugs left and right. A lot of them that I wrote. And like, I remember like, I was like, sort of like, you know, doing like, surgery in the middle, like stopping the run, making the fix, like relaunching. And yeah, we never actually went back to the start. We just like kept training it with like the bug fixes along the way, which was impossible to reproduce now. Yeah, yeah, no, that model is like, has gone through such a curriculum that, you know, learned some weird stuff. But yeah, somehow by miracle, it worked out.Gabriel [00:30:13]: The other funny thing is that the way that we were training, most of that model was through a cluster from the Department of Energy. But that’s sort of like a shared cluster that many groups use. And so we were basically training the model for two days, and then it would go back to the queue and stay a week in the queue. Oh, yeah. And so it was pretty painful. And so we actually kind of towards the end with Evan, the CEO of Genesis, and basically, you know, I was telling him a bit about the project and, you know, kind of telling him about this frustration with the compute. And so luckily, you know, he offered to kind of help. And so we, we got the help from Genesis to, you know, finish up the model. Otherwise, it probably would have taken a couple of extra weeks.Brandon [00:30:57]: Yeah, yeah.Brandon [00:31:02]: And then, and then there’s some progression from there.Gabriel [00:31:06]: Yeah, so I would say kind of that, both one, but also kind of these other kind of set of models that came around the same time, were kind of approaching were a big leap from, you know, kind of the previous kind of open source models, and, you know, kind of really kind of approaching the level of AlphaVault 3. But I would still say that, you know, even to this day, there are, you know, some... specific instances where AlphaVault 3 works better. I think one common example is antibody antigen prediction, where, you know, AlphaVault 3 still seems to have an edge in many situations. Obviously, these are somewhat different models. They are, you know, you run them, you obtain different results. So it’s, it’s not always the case that one model is better than the other, but kind of in aggregate, we still, especially at the time.Brandon [00:32:00]: So AlphaVault 3 is, you know, still having a bit of an edge. We should talk about this more when we talk about Boltzgen, but like, how do you know one is, one model is better than the other? Like you, so you, I make a prediction, you make a prediction, like, how do you know?Gabriel [00:32:11]: Yeah, so easily, you know, the, the great thing about kind of structural prediction and, you know, once we’re going to go into the design space of designing new small molecule, new proteins, this becomes a lot more complex. But a great thing about structural prediction is that a bit like, you know, CASP was doing, basically the way that you can evaluate them is that, you know, you train... You know, you train a model on a structure that was, you know, released across the field up until a certain time. And, you know, one of the things that we didn’t talk about that was really critical in all this development is the PDB, which is the Protein Data Bank. It’s this common resources, basically common database where every biologist publishes their structures. And so we can, you know, train on, you know, all the structures that were put in the PDB until a certain date. And then... And then we basically look for recent structures, okay, which structures look pretty different from anything that was published before, because we really want to try to understand generalization.Brandon [00:33:13]: And then on this new structure, we evaluate all these different models. And so you just know when AlphaFold3 was trained, you know, when you’re, you intentionally trained to the same date or something like that. Exactly. Right. Yeah.Gabriel [00:33:24]: And so this is kind of the way that you can somewhat easily kind of compare these models, obviously, that assumes that, you know, the training. You’ve always been very passionate about validation. I remember like DiffDoc, and then there was like DiffDocL and DocGen. You’ve thought very carefully about this in the past. Like, actually, I think DocGen is like a really funny story that I think, I don’t know if you want to talk about that. It’s an interesting like... Yeah, I think one of the amazing things about putting things open source is that we get a ton of feedback from the field. And, you know, sometimes we get kind of great feedback of people. Really like... But honestly, most of the times, you know, to be honest, that’s also maybe the most useful feedback is, you know, people sharing about where it doesn’t work. And so, you know, at the end of the day, it’s critical. And this is also something, you know, across other fields of machine learning. It’s always critical to set, to do progress in machine learning, set clear benchmarks. And as, you know, you start doing progress of certain benchmarks, then, you know, you need to improve the benchmarks and make them harder and harder. And this is kind of the progression of, you know, how the field operates. And so, you know, the example of DocGen was, you know, we published this initial model called DiffDoc in my first year of PhD, which was sort of like, you know, one of the early models to try to predict kind of interactions between proteins, small molecules, that we bought a year after AlphaFold2 was published. And now, on the one hand, you know, on these benchmarks that we were using at the time, DiffDoc was doing really well, kind of, you know, outperforming kind of some of the traditional physics-based methods. But on the other hand, you know, when we started, you know, kind of giving these tools to kind of many biologists, and one example was that we collaborated with was the group of Nick Polizzi at Harvard. We noticed, started noticing that there was this clear, pattern where four proteins that were very different from the ones that we’re trained on, the models was, was struggling. And so, you know, that seemed clear that, you know, this is probably kind of where we should, you know, put our focus on. And so we first developed, you know, with Nick and his group, a new benchmark, and then, you know, went after and said, okay, what can we change? And kind of about the current architecture to improve this pattern and generalization. And this is the same that, you know, we’re still doing today, you know, kind of, where does the model not work, you know, and then, you know, once we have that benchmark, you know, let’s try to, through everything we, any ideas that we have of the problem.RJ [00:36:15]: And there’s a lot of like healthy skepticism in the field, which I think, you know, is, is, is great. And I think, you know, it’s very clear that there’s a ton of things, the models don’t really work well on, but I think one thing that’s probably, you know, undeniable is just like the pace of, pace of progress, you know, and how, how much better we’re getting, you know, every year. And so I think if you, you know, if you assume, you know, any constant, you know, rate of progress moving forward, I think things are going to look pretty cool at some point in the future.Gabriel [00:36:42]: ChatGPT was only three years ago. Yeah, I mean, it’s wild, right?RJ [00:36:45]: Like, yeah, yeah, yeah, it’s one of those things. Like, you’ve been doing this. Being in the field, you don’t see it coming, you know? And like, I think, yeah, hopefully we’ll, you know, we’ll, we’ll continue to have as much progress we’ve had the past few years.Brandon [00:36:55]: So this is maybe an aside, but I’m really curious, you get this great feedback from the, from the community, right? By being open source. My question is partly like, okay, yeah, if you open source and everyone can copy what you did, but it’s also maybe balancing priorities, right? Where you, like all my customers are saying. I want this, there’s all these problems with the model. Yeah, yeah. But my customers don’t care, right? So like, how do you, how do you think about that? Yeah.Gabriel [00:37:26]: So I would say a couple of things. One is, you know, part of our goal with Bolts and, you know, this is also kind of established as kind of the mission of the public benefit company that we started is to democratize the access to these tools. But one of the reasons why we realized that Bolts needed to be a company, it couldn’t just be an academic project is that putting a model on GitHub is definitely not enough to get, you know, chemists and biologists, you know, across, you know, both academia, biotech and pharma to use your model to, in their therapeutic programs. And so a lot of what we think about, you know, at Bolts beyond kind of the, just the models is thinking about all the layers. The layers that come on top of the models to get, you know, from, you know, those models to something that can really enable scientists in the industry. And so that goes, you know, into building kind of the right kind of workflows that take in kind of, for example, the data and try to answer kind of directly that those problems that, you know, the chemists and the biologists are asking, and then also kind of building the infrastructure. And so this to say that, you know, even with models fully open. You know, we see a ton of potential for, you know, products in the space and the critical part about a product is that even, you know, for example, with an open source model, you know, running the model is not free, you know, as we were saying, these are pretty expensive model and especially, and maybe we’ll get into this, you know, these days we’re seeing kind of pretty dramatic inference time scaling of these models where, you know, the more you run them, the better the results are. But there, you know, you see. You start getting into a point that compute and compute costs becomes a critical factor. And so putting a lot of work into building the right kind of infrastructure, building the optimizations and so on really allows us to provide, you know, a much better service potentially to the open source models. That to say, you know, even though, you know, with a product, we can provide a much better service. I do still think, and we will continue to put a lot of our models open source because the critical kind of role. I think of open source. Models is, you know, helping kind of the community progress on the research and, you know, from which we, we all benefit. And so, you know, we’ll continue to on the one hand, you know, put some of our kind of base models open source so that the field can, can be on top of it. And, you know, as we discussed earlier, we learn a ton from, you know, the way that the field uses and builds on top of our models, but then, you know, try to build a product that gives the best experience possible to scientists. So that, you know, like a chemist or a biologist doesn’t need to, you know, spin off a GPU and, you know, set up, you know, our open source model in a particular way, but can just, you know, a bit like, you know, I, even though I am a computer scientist, machine learning scientist, I don’t necessarily, you know, take a open source LLM and try to kind of spin it off. But, you know, I just maybe open a GPT app or a cloud code and just use it as an amazing product. We kind of want to give the same experience. So this front world.Brandon [00:40:40]: I heard a good analogy yesterday that a surgeon doesn’t want the hospital to design a scalpel, right?Brandon [00:40:48]: So just buy the scalpel.RJ [00:40:50]: You wouldn’t believe like the number of people, even like in my short time, you know, between AlphaFold3 coming out and the end of the PhD, like the number of people that would like reach out just for like us to like run AlphaFold3 for them, you know, or things like that. Just because like, you know, bolts in our case, you know, just because it’s like. It’s like not that easy, you know, to do that, you know, if you’re not a computational person. And I think like part of the goal here is also that, you know, we continue to obviously build the interface with computational folks, but that, you know, the models are also accessible to like a larger, broader audience. And then that comes from like, you know, good interfaces and stuff like that.Gabriel [00:41:27]: I think one like really interesting thing about bolts is that with the release of it, you didn’t just release a model, but you created a community. Yeah. Did that community, it grew very quickly. Did that surprise you? And like, what is the evolution of that community and how is that fed into bolts?RJ [00:41:43]: If you look at its growth, it’s like very much like when we release a new model, it’s like, there’s a big, big jump, but yeah, it’s, I mean, it’s been great. You know, we have a Slack community that has like thousands of people on it. And it’s actually like self-sustaining now, which is like the really nice part because, you know, it’s, it’s almost overwhelming, I think, you know, to be able to like answer everyone’s questions and help. It’s really difficult, you know. The, the few people that we were, but it ended up that like, you know, people would answer each other’s questions and like, sort of like, you know, help one another. And so the Slack, you know, has been like kind of, yeah, self, self-sustaining and that’s been, it’s been really cool to see.RJ [00:42:21]: And, you know, that’s, that’s for like the Slack part, but then also obviously on GitHub as well. We’ve had like a nice, nice community. You know, I think we also aspire to be even more active on it, you know, than we’ve been in the past six months, which has been like a bit challenging, you know, for us. But. Yeah, the community has been, has been really great and, you know, there’s a lot of papers also that have come out with like new evolutions on top of bolts and it’s surprised us to some degree because like there’s a lot of models out there. And I think like, you know, sort of people converging on that was, was really cool. And, you know, I think it speaks also, I think, to the importance of like, you know, when, when you put code out, like to try to put a lot of emphasis and like making it like as easy to use as possible and something we thought a lot about when we released the code base. You know, it’s far from perfect, but, you know.Brandon [00:43:07]: Do you think that that was one of the factors that caused your community to grow is just the focus on easy to use, make it accessible? I think so.RJ [00:43:14]: Yeah. And we’ve, we’ve heard it from a few people over the, over the, over the years now. And, you know, and some people still think it should be a lot nicer and they’re, and they’re right. And they’re right. But yeah, I think it was, you know, at the time, maybe a little bit easier than, than other things.Gabriel [00:43:29]: The other thing part, I think led to, to the community and to some extent, I think, you know, like the somewhat the trust in the community. Kind of what we, what we put out is the fact that, you know, it’s not really been kind of, you know, one model, but, and maybe we’ll talk about it, you know, after Boltz 1, you know, there were maybe another couple of models kind of released, you know, or open source kind of soon after. We kind of continued kind of that open source journey or at least Boltz 2, where we are not only improving kind of structure prediction, but also starting to do affinity predictions, understanding kind of the strength of the interactions between these different models, which is this critical component. critical property that you often want to optimize in discovery programs. And then, you know, more recently also kind of protein design model. And so we’ve sort of been building this suite of, of models that come together, interact with one another, where, you know, kind of, there is almost an expectation that, you know, we, we take very at heart of, you know, always having kind of, you know, across kind of the entire suite of different tasks, the best or across the best. model out there so that it’s sort of like our open source tool can be kind of the go-to model for everybody in the, in the industry. I really want to talk about Boltz 2, but before that, one last question in this direction, was there anything about the community which surprised you? Were there any, like, someone was doing something and you’re like, why would you do that? That’s crazy. Or that’s actually genius. And I never would have thought about that.RJ [00:45:01]: I mean, we’ve had many contributions. I think like some of the. Interesting ones, like, I mean, we had, you know, this one individual who like wrote like a complex GPU kernel, you know, for part of the architecture on a piece of, the funny thing is like that piece of the architecture had been there since AlphaFold 2, and I don’t know why it took Boltz for this, you know, for this person to, you know, to decide to do it, but that was like a really great contribution. We’ve had a bunch of others, like, you know, people figuring out like ways to, you know, hack the model to do something. They click peptides, like, you know, there’s, I don’t know if there’s any other interesting ones come to mind.Gabriel [00:45:41]: One cool one, and this was, you know, something that initially was proposed as, you know, as a message in the Slack channel by Tim O’Donnell was basically, he was, you know, there are some cases, especially, for example, we discussed, you know, antibody-antigen interactions where the models don’t necessarily kind of get the right answer. What he noticed is that, you know, the models were somewhat stuck into predicting kind of the antibodies. And so he basically ran the experiments in this model, you can condition, basically, you can give hints. And so he basically gave, you know, random hints to the model, basically, okay, you should bind to this residue, you should bind to the first residue, or you should bind to the 11th residue, or you should bind to the 21st residue, you know, basically every 10 residues scanning the entire antigen.Brandon [00:46:33]: Residues are the...Gabriel [00:46:34]: The amino acids. The amino acids, yeah. So the first amino acids. The 11 amino acids, and so on. So it’s sort of like doing a scan, and then, you know, conditioning the model to predict all of them, and then looking at the confidence of the model in each of those cases and taking the top. And so it’s sort of like a very somewhat crude way of doing kind of inference time search. But surprisingly, you know, for antibody-antigen prediction, it actually kind of helped quite a bit. And so there’s some, you know, interesting ideas that, you know, obviously, as kind of developing the model, you say kind of, you know, wow. This is why would the model, you know, be so dumb. But, you know, it’s very interesting. And that, you know, leads you to also kind of, you know, start thinking about, okay, how do I, can I do this, you know, not with this brute force, but, you know, in a smarter way.RJ [00:47:22]: And so we’ve also done a lot of work on that direction. And that speaks to, like, the, you know, the power of scoring. We’re seeing that a lot. I’m sure we’ll talk about it more when we talk about BullsGen. But, you know, our ability to, like, take a structure and determine that that structure is, like... Good. You know, like, somewhat accurate. Whether that’s a single chain or, like, an interaction is a really powerful way of improving, you know, the models. Like, sort of like, you know, if you can sample a ton and you assume that, like, you know, if you sample enough, you’re likely to have, like, you know, the good structure. Then it really just becomes a ranking problem. And, you know, now we’re, you know, part of the inference time scaling that Gabby was talking about is very much that. It’s like, you know, the more we sample, the more we, like, you know, the ranking model. The ranking model ends up finding something it really likes. And so I think our ability to get better at ranking, I think, is also what’s going to enable sort of the next, you know, next big, big breakthroughs. Interesting.Brandon [00:48:17]: But I guess there’s a, my understanding, there’s a diffusion model and you generate some stuff and then you, I guess, it’s just what you said, right? Then you rank it using a score and then you finally... And so, like, can you talk about those different parts? Yeah.Gabriel [00:48:34]: So, first of all, like, the... One of the critical kind of, you know, beliefs that we had, you know, also when we started working on Boltz 1 was sort of like the structure prediction models are somewhat, you know, our field version of some foundation models, you know, learning about kind of how proteins and other molecules interact. And then we can leverage that learning to do all sorts of other things. And so with Boltz 2, we leverage that learning to do affinity predictions. So understanding kind of, you know, if I give you this protein, this molecule. How tightly is that interaction? For Boltz 1, what we did was taking kind of that kind of foundation models and then fine tune it to predict kind of entire new proteins. And so the way basically that that works is sort of like instead of for the protein that you’re designing, instead of fitting in an actual sequence, you fit in a set of blank tokens. And you train the models to, you know, predict both the structure of kind of that protein. The structure also, what the different amino acids of that proteins are. And so basically the way that Boltz 1 operates is that you feed a target protein that you may want to kind of bind to or, you know, another DNA, RNA. And then you feed the high level kind of design specification of, you know, what you want your new protein to be. For example, it could be like an antibody with a particular framework. It could be a peptide. It could be many other things. And that’s with natural language or? And that’s, you know, basically, you know, prompting. And we have kind of this sort of like spec that you specify. And, you know, you feed kind of this spec to the model. And then the model translates this into, you know, a set of, you know, tokens, a set of conditioning to the model, a set of, you know, blank tokens. And then, you know, basically the codes as part of the diffusion models, the codes. It’s a new structure and a new sequence for your protein. And, you know, basically, then we take that. And as Jeremy was saying, we are trying to score it and, you know, how good of a binder it is to that original target.Brandon [00:50:51]: You’re using basically Boltz to predict the folding and the affinity to that molecule. So and then that kind of gives you a score? Exactly.Gabriel [00:51:03]: So you use this model to predict the folding. And then you do two things. One is that you predict the structure and with something like Boltz2, and then you basically compare that structure with what the model predicted, what Boltz2 predicted. And this is sort of like in the field called consistency. It’s basically you want to make sure that, you know, the structure that you’re predicting is actually what you’re trying to design. And that gives you a much better confidence that, you know, that’s a good design. And so that’s the first filtering. And the second filtering that we did as part of kind of the Boltz2 pipeline that was released is that we look at the confidence that the model has in the structure. Now, unfortunately, kind of going to your question of, you know, predicting affinity, unfortunately, confidence is not a very good predictor of affinity. And so one of the things that we’ve actually done a ton of progress, you know, since we released Boltz2.Brandon [00:52:03]: And kind of we have some new results that we are going to kind of announce soon is kind of, you know, the ability to get much better hit rates when instead of, you know, trying to rely on confidence of the model, we are actually directly trying to predict the affinity of that interaction. Okay. Just backing up a minute. So your diffusion model actually predicts not only the protein sequence, but also the folding of it. Exactly.Gabriel [00:52:32]: And actually, you can... One of the big different things that we did compared to other models in the space, and, you know, there were some papers that had already kind of done this before, but we really scaled it up was, you know, basically somewhat merging kind of the structure prediction and the sequence prediction into almost the same task. And so the way that Boltz2 works is that you are basically the only thing that you’re doing is predicting the structure. So the only sort of... Supervision is we give you a supervision on the structure, but because the structure is atomic and, you know, the different amino acids have a different atomic composition, basically from the way that you place the atoms, we also understand not only kind of the structure that you wanted, but also the identity of the amino acid that, you know, the models believed was there. And so we’ve basically, instead of, you know, having these two supervision signals, you know, one discrete, one continuous. That somewhat, you know, don’t interact well together. We sort of like build kind of like an encoding of, you know, sequences in structures that allows us to basically use exactly the same supervision signal that we were using to Boltz2 that, you know, you know, largely similar to what AlphaVol3 proposed, which is very scalable. And we can use that to design new proteins. Oh, interesting.RJ [00:53:58]: Maybe a quick shout out to Hannes Stark on our team who like did all this work. Yeah.Gabriel [00:54:04]: Yeah, that was a really cool idea. I mean, like looking at the paper and there’s this is like encoding or you just add a bunch of, I guess, kind of atoms, which can be anything, and then they get sort of rearranged and then basically plopped on top of each other so that and then that encodes what the amino acid is. And there’s sort of like a unique way of doing this. It was that was like such a really such a cool, fun idea.RJ [00:54:29]: I think that idea was had existed before. Yeah, there were a couple of papers.Gabriel [00:54:33]: Yeah, I had proposed this and and Hannes really took it to the large scale.Brandon [00:54:39]: In the paper, a lot of the paper for Boltz2Gen is dedicated to actually the validation of the model. In my opinion, all the people we basically talk about feel that this sort of like in the wet lab or whatever the appropriate, you know, sort of like in real world validation is the whole problem or not the whole problem, but a big giant part of the problem. So can you talk a little bit about the highlights? From there, that really because to me, the results are impressive, both from the perspective of the, you know, the model and also just the effort that went into the validation by a large team.Gabriel [00:55:18]: First of all, I think I should start saying is that both when we were at MIT and Thomas Yacolas and Regina Barzillai’s lab, as well as at Boltz, you know, we are not a we’re not a biolab and, you know, we are not a therapeutic company. And so to some extent, you know, we were first forced to, you know, look outside of, you know, our group, our team to do the experimental validation. One of the things that really, Hannes, in the team pioneer was the idea, OK, can we go not only to, you know, maybe a specific group and, you know, trying to find a specific system and, you know, maybe overfit a bit to that system and trying to validate. But how can we test this model? So. Across a very wide variety of different settings so that, you know, anyone in the field and, you know, printing design is, you know, such a kind of wide task with all sorts of different applications from therapeutic to, you know, biosensors and many others that, you know, so can we get a validation that is kind of goes across many different tasks? And so he basically put together, you know, I think it was something like, you know, 25 different. You know, academic and industry labs that committed to, you know, testing some of the designs from the model and some of this testing is still ongoing and, you know, giving results kind of back to us in exchange for, you know, hopefully getting some, you know, new great sequences for their task. And he was able to, you know, coordinate this, you know, very wide set of, you know, scientists and already in the paper, I think we. Shared results from, I think, eight to 10 different labs kind of showing results from, you know, designing peptides, designing to target, you know, ordered proteins, peptides targeting disordered proteins, which are results, you know, of designing proteins that bind to small molecules, which are results of, you know, designing nanobodies and across a wide variety of different targets. And so that’s sort of like. That gave to the paper a lot of, you know, validation to the model, a lot of validation that was kind of wide.Brandon [00:57:39]: And so those would be therapeutics for those animals or are they relevant to humans as well? They’re relevant to humans as well.Gabriel [00:57:45]: Obviously, you need to do some work into, quote unquote, humanizing them, making sure that, you know, they have the right characteristics to so they’re not toxic to humans and so on.RJ [00:57:57]: There are some approved medicine in the market that are nanobodies. There’s a general. General pattern, I think, in like in trying to design things that are smaller, you know, like it’s easier to manufacture at the same time, like that comes with like potentially other challenges, like maybe a little bit less selectivity than like if you have something that has like more hands, you know, but the yeah, there’s this big desire to, you know, try to design many proteins, nanobodies, small peptides, you know, that just are just great drug modalities.Brandon [00:58:27]: Okay. I think we were left off. We were talking about validation. Validation in the lab. And I was very excited about seeing like all the diverse validations that you’ve done. Can you go into some more detail about them? Yeah. Specific ones. Yeah.RJ [00:58:43]: The nanobody one. I think we did. What was it? 15 targets. Is that correct? 14. 14 targets. Testing. So we typically the way this works is like we make a lot of designs. All right. On the order of like tens of thousands. And then we like rank them and we pick like the top. And in this case, and was 15 right for each target and then we like measure sort of like the success rates, both like how many targets we were able to get a binder for and then also like more generally, like out of all of the binders that we designed, how many actually proved to be good binders. Some of the other ones I think involved like, yeah, like we had a cool one where there was a small molecule or design a protein that binds to it. That has a lot of like interesting applications, you know, for example. Like Gabri mentioned, like biosensing and things like that, which is pretty cool. We had a disordered protein, I think you mentioned also. And yeah, I think some of those were some of the highlights. Yeah.Gabriel [00:59:44]: So I would say that the way that we structure kind of some of those validations was on the one end, we have validations across a whole set of different problems that, you know, the biologists that we were working with came to us with. So we were trying to. For example, in some of the experiments, design peptides that would target the RACC, which is a target that is involved in metabolism. And we had, you know, a number of other applications where we were trying to design, you know, peptides or other modalities against some other therapeutic relevant targets. We designed some proteins to bind small molecules. And then some of the other testing that we did was really trying to get like a more broader sense. So how does the model work, especially when tested, you know, on somewhat generalization? So one of the things that, you know, we found with the field was that a lot of the validation, especially outside of the validation that was on specific problems, was done on targets that have a lot of, you know, known interactions in the training data. And so it’s always a bit hard to understand, you know, how much are these models really just regurgitating kind of what they’ve seen or trying to imitate. What they’ve seen in the training data versus, you know, really be able to design new proteins. And so one of the experiments that we did was to take nine targets from the PDB, filtering to things where there is no known interaction in the PDB. So basically the model has never seen kind of this particular protein bound or a similar protein bound to another protein. So there is no way that. The model from its training set can sort of like say, okay, I’m just going to kind of tweak something and just imitate this particular kind of interaction. And so we took those nine proteins. We worked with adaptive CRO and basically tested, you know, 15 mini proteins and 15 nanobodies against each one of them. And the very cool thing that we saw was that on two thirds of those targets, we were able to, from this 15 design, get nanomolar binders, nanomolar, roughly speaking, just a measure of, you know, how strongly kind of the interaction is, roughly speaking, kind of like a nanomolar binder is approximately the kind of binding strength or binding that you need for a therapeutic. Yeah. So maybe switching directions a bit. Bolt’s lab was just announced this week or was it last week? Yeah. This is like your. First, I guess, product, if that’s if you want to call it that. Can you talk about what Bolt’s lab is and yeah, you know, what you hope that people take away from this? Yeah.RJ [01:02:44]: You know, as we mentioned, like I think at the very beginning is the goal with the product has been to, you know, address what the models don’t on their own. And there’s largely sort of two categories there. I’ll split it in three. The first one. It’s one thing to predict, you know, a single interaction, for example, like a single structure. It’s another to like, you know, very effectively search a space, a design space to produce something of value. What we found, like sort of building on this product is that there’s a lot of steps involved, you know, in that there’s certainly need to like, you know, accompany the user through, you know, one of those steps, for example, is like, you know, the creation of the target itself. You know, how do we make sure that the model has like a good enough understanding of the target? So we can like design something and there’s all sorts of tricks, you know, that you can do to improve like a particular, you know, structure prediction. And so that’s sort of like, you know, the first stage. And then there’s like this stage of like, you know, designing and searching the space efficiently. You know, for something like BullsGen, for example, like you, you know, you design many things and then you rank them, for example, for small molecule process, a little bit more complicated. We actually need to also make sure that the molecules are synthesizable. And so the way we do that is that, you know, we have a generative model that learns. To use like appropriate building blocks such that, you know, it can design within a space that we know is like synthesizable. And so there’s like, you know, this whole pipeline really of different models involved in being able to design a molecule. And so that’s been sort of like the first thing we call them agents. We have a protein agent and we have a small molecule design agents. And that’s really like at the core of like what powers, you know, the BullsLab platform.Brandon [01:04:22]: So these agents, are they like a language model wrapper or they’re just like your models and you’re just calling them agents? A lot. Yeah. Because they, they, they sort of perform a function on behalf of.RJ [01:04:33]: They’re more of like a, you know, a recipe, if you wish. And I think we use that term sort of because of, you know, sort of the complex pipelining and automation, you know, that goes into like all this plumbing. So that’s the first part of the product. The second part is the infrastructure. You know, we need to be able to do this at very large scale for any one, you know, group that’s doing a design campaign. Let’s say you’re designing, you know, I’d say a hundred thousand possible candidates. Right. To find the good one that is, you know, a very large amount of compute, you know, for small molecules, it’s on the order of like a few seconds per designs for proteins can be a bit longer. And so, you know, ideally you want to do that in parallel, otherwise it’s going to take you weeks. And so, you know, we’ve put a lot of effort into like, you know, our ability to have a GPU fleet that allows any one user, you know, to be able to do this kind of like large parallel search.Brandon [01:05:23]: So you’re amortizing the cost over your users. Exactly. Exactly.RJ [01:05:27]: And, you know, to some degree, like it’s whether you. Use 10,000 GPUs for like, you know, a minute is the same cost as using, you know, one GPUs for God knows how long. Right. So you might as well try to parallelize if you can. So, you know, a lot of work has gone, has gone into that, making it very robust, you know, so that we can have like a lot of people on the platform doing that at the same time. And the third one is, is the interface and the interface comes in, in two shapes. One is in form of an API and that’s, you know, really suited for companies that want to integrate, you know, these pipelines, these agents.RJ [01:06:01]: So we’re already partnering with, you know, a few distributors, you know, that are gonna integrate our API. And then the second part is the user interface. And, you know, we, we’ve put a lot of thoughts also into that. And this is when I, I mentioned earlier, you know, this idea of like broadening the audience. That’s kind of what the, the user interface is about. And we’ve built a lot of interesting features in it, you know, for example, for collaboration, you know, when you have like potentially multiple medicinal chemists or. We’re going through the results and trying to pick out, okay, like what are the molecules that we’re going to go and test in the lab? It’s powerful for them to be able to, you know, for example, each provide their own ranking and then do consensus building. And so there’s a lot of features around launching these large jobs, but also around like collaborating on analyzing the results that we try to solve, you know, with that part of the platform. So Bolt’s lab is sort of a combination of these three objectives into like one, you know, sort of cohesive platform. Who is this accessible to? Everyone. You do need to request access today. We’re still like, you know, sort of ramping up the usage, but anyone can request access. If you are an academic in particular, we, you know, we provide a fair amount of free credit so you can play with the platform. If you are a startup or biotech, you may also, you know, reach out and we’ll typically like actually hop on a call just to like understand what you’re trying to do and also provide a lot of free credit to get started. And of course, also with larger companies, we can deploy this platform in a more like secure environment. And so that’s like more like customizing. You know, deals that we make, you know, with the partners, you know, and that’s sort of the ethos of Bolt. I think this idea of like servicing everyone and not necessarily like going after just, you know, the really large enterprises. And that starts from the open source, but it’s also, you know, a key design principle of the product itself.Gabriel [01:07:48]: One thing I was thinking about with regards to infrastructure, like in the LLM space, you know, the cost of a token has gone down by I think a factor of a thousand or so over the last three years, right? Yeah. And is it possible that like essentially you can exploit economies of scale and infrastructure that you can make it cheaper to run these things yourself than for any person to roll their own system? A hundred percent. Yeah.RJ [01:08:08]: I mean, we’re already there, you know, like running Bolts on our platform, especially on a large screen is like considerably cheaper than it would probably take anyone to put the open source model out there and run it. And on top of the infrastructure, like one of the things that we’ve been working on is accelerating the models. So, you know. Our small molecule screening pipeline is 10x faster on Bolts Lab than it is in the open source, you know, and that’s also part of like, you know, building a product, you know, of something that scales really well. And we really wanted to get to a point where like, you know, we could keep prices very low in a way that it would be a no-brainer, you know, to use Bolts through our platform.Gabriel [01:08:52]: How do you think about validation of your like agentic systems? Because, you know, as you were saying earlier. Like we’re AlphaFold style models are really good at, let’s say, monomeric, you know, proteins where you have, you know, co-evolution data. But now suddenly the whole point of this is to design something which doesn’t have, you know, co-evolution data, something which is really novel. So now you’re basically leaving the domain that you thought was, you know, that you know you are good at. So like, how do you validate that?RJ [01:09:22]: Yeah, I like every complete, but there’s obviously, you know, a ton of computational metrics. That we rely on, but those are only take you so far. You really got to go to the lab, you know, and test, you know, okay, with this method A and this method B, how much better are we? You know, how much better is my, my hit rate? How stronger are my binders? Also, it’s not just about hit rate. It’s also about how good the binders are. And there’s really like no way, nowhere around that. I think we’re, you know, we’ve really ramped up the amount of experimental validation that we do so that we like really track progress, you know, as scientifically sound, you know. Yeah. As, as possible out of this, I think.Gabriel [01:10:00]: Yeah, no, I think, you know, one thing that is unique about us and maybe companies like us is that because we’re not working on like maybe a couple of therapeutic pipelines where, you know, our validation would be focused on those. We, when we do an experimental validation, we try to test it across tens of targets. And so that on the one end, we can get a much more statistically significant result and, and really allows us to make progress. From the methodological side without being, you know, steered by, you know, overfitting on any one particular system. And of course we choose, you know, we always try to choose targets and problems are sort of like at the frontier of what’s possible today. So, you know, you don’t want something too easy. You don’t want something too hard. Otherwise you’re not going to see progress. And so, you know, this is a somewhat evolving set of targets. We talked earlier about the targets that we looked at with, with Boltchan. And now we are even trying kind of, you know, even harder targets, both for small molecule and proteins. And so we try to keep ourselves on the, on the boundary of what’s possible. So do you have like infrastructure or is this is like, you just have a lot of different partnerships with academic labs and you’re just kind of keep pushing on these and driving these. We do partially this through academic labs more and more. We do this through CROs just because of, you know, to some extent is also, we need kind of replicability often kind of, you know, going after the same time. So we try to, we try to keep our, our targets, you know, multiple times and, you know, to see the, the progress from, you know, one month to the next. And speed. And speed. And speed. Speed of execution. Yeah. And, So what happens if you start getting a bunch of like really strong biters against therapeutic targets? What do you do?RJ [01:11:43]: Release them. Yeah.Gabriel [01:11:45]: But you can release them in open source? Like,RJ [01:11:47]: Yeah, I mean, you know, I mean, when we say we have no interest in making dress, we’re serious. Like, you know, uh, I mean, when it, when it was with the academic labs, basically the, you know, I was, they keep it, they do a lot of it.Gabriel [01:12:02]: I will also say, and I think this has been a bit of the issue that I have with some of the things that have been said in the field, is when we say that we design new proteins or we say that we design new molecules, go and bind these particular targets. We should be very clear, these are not drugs. These are not things that are ready to be put into a human. And there is still a lot of development that goes with it. And so this is kind of to us, we see ourselves as building tools for scientists. At the end of the day, it really relies on the scientist having a great therapeutic hypothesis and then pushing through kind of all the stages of development. And, you know, we try to build tools that can accompany them in that journey. It’s not like a magic box where, you know, you can just turn it and get FDA approved drugs.Brandon [01:13:06]: But actually, that brings up an interesting question that I’ve been wondering about is, do you guys see yourself staying in this, for lack of a better way of saying it, layer? Or do you think that you’ll start to... Yeah. Either on the physical sense, looking at different layers of the virtual cell, so to speak, or also, you know, so there’s like the development process that goes, you know, sort of like design preclinical, clinical approval and thinking about improving the performance throughout that process based on the designs. Is that a direction that you guys are pushing? Yeah.Gabriel [01:13:45]: So one of the things, as Jeremy said, you know, we are... We are not a therapeutic company. We want to kind of stay not to be a therapeutic company, always be at the service of, you know, all the different, you know, companies, including therapeutic companies that we serve. And, you know, that to some extent does mean, you know, that we need to try to, you know, go deeper and deeper in getting these models better and better. One of the things that we are doing across, you know, many other in the field is, you know, now that we are really... They’re starting to be good, both for small molecule and... For proteins to design kind of binders, design relatively tight binders, is starting to look at all these other properties, you know, they’re called developpabilities or at me that, you know, we care about when developing a drug and try, can we design them from, from Gageco. The thing about those properties in some of them, you know, you need to, you know, start having an understanding of the cell. And so that’s on the one hand, kind of why we need that understanding. But also, you know, the way... The way that we also think about all different and complex diseases is that these models, then these tools that we’re building have a good understanding of kind of, you know, biomolecular interactions and kind of their interactions. Now, at the same time, every disease is often kind of unique and every therapeutic hypothesis is unique. And so you maybe want to have something that needs to hit the particular, you know, let’s say target in a virus in a particular way, but you don’t maybe know exactly. So you can start to have a more open-minded understanding of what’s, what’s a way you want to do. And so maybe in the first set of designs, you’re going to try to target different epitopes in different ways, and then you’re going to test them in the lab, maybe directly in vivo, and you’re going to see which ones work and which ones don’t. And so then you need to bring those results back into the models. And then the models can start to have a more wider understanding, you know, not just of the biophysical of the antibodies interacting with that target, but also how that is shaped within the cell. And so first of all, you know, that means on the one end that we need, you know, kind of these loops, and this is also partially how we, we designed the platform to be. But that also means that we also need to start understanding more and more kind of higher level things. And, you know, I wouldn’t say that we’re working in any way on like a virtual cell like others are, but we’re definitely thinking kind of very deeply about kind of, you know, how does, you know, kind of the way that we target certain proteins. Interfere, interact with, you know, maybe pathways that are existing in the cell. One question that has come up is you talk a lot about user interface and so on. And I think this is really important, but like my experience with dealing with medicinal chemists, when you get the machine learning models, is they are the most superstitious, skeptical, like pseudo-religious people I’ve ever talked to when it comes to doing science. Sorry for the medicinal chemists listening. Yeah, they’re amazing. Like, they’re absolutely, I’ve worked with some spectacular medicinal chemists who just pull magic out of their hat again and again, and I have no idea how they do it. But when you bring them a machine learning model, it is sometimes quite tricky to get them to deal with it. How has your interaction been with this? And how have you thought about, like, building Bolt’s lab to work with the skeptics? One of the great value unlocks for us and for our product has been when we brought to the team a medicinal chemist. His name is Jeffrey. So I think kind of like on the one hand, you know, day one, you know, he obviously had a lot of opinions on kind of a lot of the ways that we should change, you know, both kind of the way that the agents worked, the way that the platform worked. But it’s been really amazing kind of, you know, once also we started kind of shaping kind of the platform in a better way with this feedback, how we went from, you know, to some extent, you know, a fair skepticism to him, you know, actually using, you know, a lot of the things that we did. Yeah. So he’s doing a lot more compute than any of our computational folks in the team, you know, at times that, you know, he’s, you know, running, you know, he has all these sort of hypotheses. Okay, maybe I can hit this protein this particular way. I can hit in that way. Actually, let me look at for this particular molecular space. Let me try to optimize for this particular interactions. So he ends up, you know, running several screens in parallel, you know, using hundreds of GPUs, you know, on his own. And, you know, so this has been, you know, pretty incredible to see kind of how, you know, maybe the way that I was more thinking about a problem, which is, okay, you’re just trying to design a binder, a small molecule to a particular protein. The way that he thinks about it is, you know, much more deeply and, you know, trying all these different things, these different hypotheses. And then, you know, once he gets the results from the model, he doesn’t just, you know, take the top 15, but he really kind of looks over and, you know, kind of tries to understand, you know, the different things. And then when we select, you know, maybe some designs to bring forth, you know, he has, you know, something where, you know, both the models understand that something’s good, but himself as well. And that’s why we also built kind of the platform to be an interface for, you know, this kind of chemist and, you know, also like engineers. Yeah. Collaborative experience.RJ [01:19:09]: I think at the end of the day, like, you know, for people to be convinced, you have to show them something that they didn’t think was possible. And until you have that aha moment, you know, I think the skepticism will remain. But then when, you know, every once in a while, I think there’s like a result that like really surprises people. And then it’s like, oh, wow, okay, this is actually, I can do something with this. So you just get in their hands, have them try it out, and they’ll be convinced. Yeah, or like maybe once the lab results come back. Or their friends. Yeah, or maybe one of their colleagues is convinced. Yeah. I think it takes going to the lab at some point. There’s no avoiding that, you know, as beautiful as the platform can be, as nice as the molecules might look, you know, that the model predicted. I think what really convinces people is like, you know, hits. Yeah.Gabriel [01:19:54]: Yeah. You see the results. Exactly. Yeah. Cool. Thank you for, you know, taking the time to chat with us. Yeah. You know, is there anything that you would like your audience to know? I mean, first of all, you know, we’re just getting started, you know, continuing to build a team. And so definitely always looking for great folks, both on the kind of, you know, software side, you know, machine learning side, but also scientists to join the team and help us, you know, shape. On the infrastructure side, too. Indeed. If you think that if you want a new challenge, because this is not just next token prediction, this is really a new engineering challenge. Exactly. Yeah. If you, if no matter, you know, how much experience you have with, you know, biologists and chemistry, if you want to come, you know, help us in a shape, what, you know, biology and chemistry, hopefully we’ll look like in five, 10 years. We’d love to hear from you. And so go to boltz.bio and, you know, come join the team. Cool. Thank you. Awesome. Thank you so much. Thank you.</p>"
    },
    {
      "id": "77cde4476651",
      "title": "Owning the AI Pareto Frontier — Jeff Dean",
      "content": "From rewriting Google&#8217;s search stack in the early 2000s to reviving sparse trillion-parameter models and co-designing TPUs with frontier ML research, Jeff Dean has quietly shaped nearly every layer of the modern AI stack. As Chief AI Scientist at Google and a driving force behind Gemini, Jeff has lived through multiple scaling revolutions from CPUs and sharded indices to multimodal models that reason across text, video, and code.Jeff joins us to unpack what it really means to &#8220;own the Pareto frontier,&#8221; why distillation is the engine behind every Flash model breakthrough, how energy (in picojoules) not FLOPs is becoming the true bottleneck, what it was like leading the charge to unify all of Google&#8217;s AI teams, and why the next leap won&#8217;t come from bigger context windows alone, but from systems that give the illusion of attending to trillions of tokens.We discuss:Jeff&#8217;s early neural net thesis in 1990: parallel training before it was cool, why he believed scaling would win decades early, and the &#8220;bigger model, more data, better results&#8221; mantra that held for 15 yearsThe evolution of Google Search: sharding, moving the entire index into memory in 2001, softening query semantics pre-LLMs, and why retrieval pipelines already resemble modern LLM systemsPareto frontier strategy: why you need both frontier &#8220;Pro&#8221; models and low-latency &#8220;Flash&#8221; models, and how distillation lets smaller models surpass prior generationsDistillation deep dive: ensembles &#8594; compression &#8594; logits as soft supervision, and why you need the biggest model to make the smallest one goodLatency as a first-class objective: why 10&#8211;50x lower latency changes UX entirely, and how future reasoning workloads will demand 10,000 tokens/secEnergy-based thinking: picojoules per bit, why moving data costs 1000x more than a multiply, batching through the lens of energy, and speculative decoding as amortizationTPU co-design: predicting ML workloads 2&#8211;6 years out, speculative hardware features, precision reduction, sparsity, and the constant feedback loop between model architecture and siliconSparse models and &#8220;outrageously large&#8221; networks: trillions of parameters with 1&#8211;5% activation, and why sparsity was always the right abstractionUnified vs. specialized models: abandoning symbolic systems, why general multimodal models tend to dominate vertical silos, and when vertical fine-tuning still makes senseLong context and the illusion of scale: beyond needle-in-a-haystack benchmarks toward systems that narrow trillions of tokens to 117 relevant documentsPersonalized AI: attending to your emails, photos, and documents (with permission), and why retrieval + reasoning will unlock deeply personal assistantsCoding agents: 50 AI interns, crisp specifications as a new core skill, and how ultra-low latency will reshape human&#8211;agent collaborationWhy ideas still matter: transformers, sparsity, RL, hardware, systems &#8212; scaling wasn&#8217;t blind; the pieces had to multiply togetherShow Notes:Gemma 3 PaperGemma 3Gemini 2.5 ReportJeff Dean&#8217;s &#8220;Software Engineering Advice fromBuilding Large-Scale Distributed Systems&#8221; Presentation (with Back of the Envelope Calculations)Latency Numbers Every Programmer Should Know by Jeff DeanThe Jeff Dean FactsJeff Dean Google BioJeff Dean on &#8220;Important AI Trends&#8221; @Stanford AI ClubJeff Dean &amp; Noam Shazeer &#8212; 25 years at Google (Dwarkesh)&#8212;Jeff DeanLinkedIn: https://www.linkedin.com/in/jeff-dean-8b212555X: https://x.com/jeffdeanGooglehttps://google.comhttps://deepmind.googleFull Video EpisodeTimestamps00:00:04 &#8212; Introduction: Alessio &amp; Swyx welcome Jeff Dean, chief AI scientist at Google, to the Latent Space podcast00:00:30 &#8212; Owning the Pareto Frontier &amp; balancing frontier vs low-latency models00:01:31 &#8212; Frontier models vs Flash models + role of distillation00:03:52 &#8212; History of distillation and its original motivation00:05:09 &#8212; Distillation&#8217;s role in modern model scaling00:07:02 &#8212; Model hierarchy (Flash, Pro, Ultra) and distillation sources00:07:46 &#8212; Flash model economics &amp; wide deployment00:08:10 &#8212; Latency importance for complex tasks00:09:19 &#8212; Saturation of some tasks and future frontier tasks00:11:26 &#8212; On benchmarks, public vs internal00:12:53 &#8212; Example long-context benchmarks &amp; limitations00:15:01 &#8212; Long-context goals: attending to trillions of tokens00:16:26 &#8212; Realistic use cases beyond pure language00:18:04 &#8212; Multimodal reasoning and non-text modalities00:19:05 &#8212; Importance of vision &amp; motion modalities00:20:11 &#8212; Video understanding example (extracting structured info)00:20:47 &#8212; Search ranking analogy for LLM retrieval00:23:08 &#8212; LLM representations vs keyword search00:24:06 &#8212; Early Google search evolution &amp; in-memory index00:26:47 &#8212; Design principles for scalable systems00:28:55 &#8212; Real-time index updates &amp; recrawl strategies00:30:06 &#8212; Classic &#8220;Latency numbers every programmer should know&#8221;00:32:09 &#8212; Cost of memory vs compute and energy emphasis00:34:33 &#8212; TPUs &amp; hardware trade-offs for serving models00:35:57 &#8212; TPU design decisions &amp; co-design with ML00:38:06 &#8212; Adapting model architecture to hardware00:39:50 &#8212; Alternatives: energy-based models, speculative decoding00:42:21 &#8212; Open research directions: complex workflows, RL00:44:56 &#8212; Non-verifiable RL domains &amp; model evaluation00:46:13 &#8212; Transition away from symbolic systems toward unified LLMs00:47:59 &#8212; Unified models vs specialized ones00:50:38 &#8212; Knowledge vs reasoning &amp; retrieval + reasoning00:52:24 &#8212; Vertical model specialization &amp; modules00:55:21 &#8212; Token count considerations for vertical domains00:56:09 &#8212; Low resource languages &amp; contextual learning00:59:22 &#8212; Origins: Dean&#8217;s early neural network work01:10:07 &#8212; AI for coding &amp; human&#8211;model interaction styles01:15:52 &#8212; Importance of crisp specification for coding agents01:19:23 &#8212; Prediction: personalized models &amp; state retrieval01:22:36 &#8212; Token-per-second targets (10k+) and reasoning throughput01:23:20 &#8212; Episode conclusion and thanksTranscriptAlessio Fanelli [00:00:04]: Hey everyone, welcome to the Latent Space podcast. This is Alessio, founder of Kernel Labs, and I&#8217;m joined by Swyx, editor of Latent Space. Shawn Wang [00:00:11]: Hello, hello. We&#8217;re here in the studio with Jeff Dean, chief AI scientist at Google. Welcome. Thanks for having me. It&#8217;s a bit surreal to have you in the studio. I&#8217;ve watched so many of your talks, and obviously your career has been super legendary. So, I mean, congrats. I think the first thing must be said, congrats on owning the Pareto Frontier.Jeff Dean [00:00:30]: Thank you, thank you. Pareto Frontiers are good. It&#8217;s good to be out there.Shawn Wang [00:00:34]: Yeah, I mean, I think it&#8217;s a combination of both. You have to own the Pareto Frontier. You have to have like frontier capability, but also efficiency, and then offer that range of models that people like to use. And, you know, some part of this was started because of your hardware work. Some part of that is your model work, and I&#8217;m sure there&#8217;s lots of secret sauce that you guys have worked on cumulatively. But, like, it&#8217;s really impressive to see it all come together in, like, this slittily advanced.Jeff Dean [00:01:04]: Yeah, yeah. I mean, I think, as you say, it&#8217;s not just one thing. It&#8217;s like a whole bunch of things up and down the stack. And, you know, all of those really combine to help make UNOS able to make highly capable large models, as well as, you know, software techniques to get those large model capabilities into much smaller, lighter weight models that are, you know, much more cost effective and lower latency, but still, you know, quite capable for their size. Yeah.Alessio Fanelli [00:01:31]: How much pressure do you have on, like, having the lower bound of the Pareto Frontier, too? I think, like, the new labs are always trying to push the top performance frontier because they need to raise more money and all of that. And you guys have billions of users. And I think initially when you worked on the CPU, you were thinking about, you know, if everybody that used Google, we use the voice model for, like, three minutes a day, they were like, you need to double your CPU number. Like, what&#8217;s that discussion today at Google? Like, how do you prioritize frontier versus, like, we have to do this? How do we actually need to deploy it if we build it?Jeff Dean [00:02:03]: Yeah, I mean, I think we always want to have models that are at the frontier or pushing the frontier because I think that&#8217;s where you see what capabilities now exist that didn&#8217;t exist at the sort of slightly less capable last year&#8217;s version or last six months ago version. At the same time, you know, we know those are going to be really useful for a bunch of use cases, but they&#8217;re going to be a bit slower and a bit more expensive than people might like for a bunch of other broader models. So I think what we want to do is always have kind of a highly capable sort of affordable model that enables a whole bunch of, you know, lower latency use cases. People can use them for agentic coding much more readily and then have the high-end, you know, frontier model that is really useful for, you know, deep reasoning, you know, solving really complicated math problems, those kinds of things. And it&#8217;s not that. One or the other is useful. They&#8217;re both useful. So I think we&#8217;d like to do both. And also, you know, through distillation, which is a key technique for making the smaller models more capable, you know, you have to have the frontier model in order to then distill it into your smaller model. So it&#8217;s not like an either or choice. You sort of need that in order to actually get a highly capable, more modest size model. Yeah.Alessio Fanelli [00:03:24]: I mean, you and Jeffrey came up with the solution in 2014.Jeff Dean [00:03:28]: Don&#8217;t forget, L&#8217;Oreal Vinyls as well. Yeah, yeah.Alessio Fanelli [00:03:30]: A long time ago. But like, I&#8217;m curious how you think about the cycle of these ideas, even like, you know, sparse models and, you know, how do you reevaluate them? How do you think about in the next generation of model, what is worth revisiting? Like, yeah, they&#8217;re just kind of like, you know, you worked on so many ideas that end up being influential, but like in the moment, they might not feel that way necessarily. Yeah.Jeff Dean [00:03:52]: I mean, I think distillation was originally motivated because we were seeing that we had a very large image data set at the time, you know, 300 million images that we could train on. And we were seeing that if you create specialists for different subsets of those image categories, you know, this one&#8217;s going to be really good at sort of mammals, and this one&#8217;s going to be really good at sort of indoor room scenes or whatever, and you can cluster those categories and train on an enriched stream of data after you do pre-training on a much broader set of images. You get much better performance. If you then treat that whole set of maybe 50 models you&#8217;ve trained as a large ensemble, but that&#8217;s not a very practical thing to serve, right? So distillation really came about from the idea of, okay, what if we want to actually serve that and train all these independent sort of expert models and then squish it into something that actually fits in a form factor that you can actually serve? And that&#8217;s, you know, not that different from what we&#8217;re doing today. You know, often today we&#8217;re instead of having an ensemble of 50 models. We&#8217;re having a much larger scale model that we then distill into a much smaller scale model.Shawn Wang [00:05:09]: Yeah. A part of me also wonders if distillation also has a story with the RL revolution. So let me maybe try to articulate what I mean by that, which is you can, RL basically spikes models in a certain part of the distribution. And then you have to sort of, well, you can spike models, but usually sometimes... It might be lossy in other areas and it&#8217;s kind of like an uneven technique, but you can probably distill it back and you can, I think that the sort of general dream is to be able to advance capabilities without regressing on anything else. And I think like that, that whole capability merging without loss, I feel like it&#8217;s like, you know, some part of that should be a distillation process, but I can&#8217;t quite articulate it. I haven&#8217;t seen much papers about it.Jeff Dean [00:06:01]: Yeah, I mean, I tend to think of one of the key advantages of distillation is that you can have a much smaller model and you can have a very large, you know, training data set and you can get utility out of making many passes over that data set because you&#8217;re now getting the logits from the much larger model in order to sort of coax the right behavior out of the smaller model that you wouldn&#8217;t otherwise get with just the hard labels. And so, you know, I think that&#8217;s what we&#8217;ve observed. Is you can get, you know, very close to your largest model performance with distillation approaches. And that seems to be, you know, a nice sweet spot for a lot of people because it enables us to kind of, for multiple Gemini generations now, we&#8217;ve been able to make the sort of flash version of the next generation as good or even substantially better than the previous generations pro. And I think we&#8217;re going to keep trying to do that because that seems like a good trend to follow.Shawn Wang [00:07:02]: So, Dara asked, so it was the original map was Flash Pro and Ultra. Are you just sitting on Ultra and distilling from that? Is that like the mother load?Jeff Dean [00:07:12]: I mean, we have a lot of different kinds of models. Some are internal ones that are not necessarily meant to be released or served. Some are, you know, our pro scale model and we can distill from that as well into our Flash scale model. So I think, you know, it&#8217;s an important set of capabilities to have and also inference time scaling. It can also be a useful thing to improve the capabilities of the model.Shawn Wang [00:07:35]: And yeah, yeah, cool. Yeah. And obviously, I think the economy of Flash is what led to the total dominance. I think the latest number is like 50 trillion tokens. I don&#8217;t know. I mean, obviously, it&#8217;s changing every day.Jeff Dean [00:07:46]: Yeah, yeah. But, you know, by market share, hopefully up.Shawn Wang [00:07:50]: No, I mean, there&#8217;s no I mean, there&#8217;s just the economics wise, like because Flash is so economical, like you can use it for everything. Like it&#8217;s in Gmail now. It&#8217;s in YouTube. Like it&#8217;s yeah. It&#8217;s in everything.Jeff Dean [00:08:02]: We&#8217;re using it more in our search products of various AI mode reviews.Shawn Wang [00:08:05]: Oh, my God. Flash past the AI mode. Oh, my God. Yeah, that&#8217;s yeah, I didn&#8217;t even think about that.Jeff Dean [00:08:10]: I mean, I think one of the things that is quite nice about the Flash model is not only is it more affordable, it&#8217;s also a lower latency. And I think latency is actually a pretty important characteristic for these models because we&#8217;re going to want models to do much more complicated things that are going to involve, you know, generating many more tokens from when you ask the model to do so. So, you know, if you&#8217;re going to ask the model to do something until it actually finishes what you ask it to do, because you&#8217;re going to ask now, not just write me a for loop, but like write me a whole software package to do X or Y or Z. And so having low latency systems that can do that seems really important. And Flash is one direction, one way of doing that. You know, obviously our hardware platforms enable a bunch of interesting aspects of our, you know, serving stack as well, like TPUs, the interconnect between. Chips on the TPUs is actually quite, quite high performance and quite amenable to, for example, long context kind of attention operations, you know, having sparse models with lots of experts. These kinds of things really, really matter a lot in terms of how do you make them servable at scale.Alessio Fanelli [00:09:19]: Yeah. Does it feel like there&#8217;s some breaking point for like the proto Flash distillation, kind of like one generation delayed? I almost think about almost like the capability as a. In certain tasks, like the pro model today is a saturated, some sort of task. So next generation, that same task will be saturated at the Flash price point. And I think for most of the things that people use models for at some point, the Flash model in two generation will be able to do basically everything. And how do you make it economical to like keep pushing the pro frontier when a lot of the population will be okay with the Flash model? I&#8217;m curious how you think about that.Jeff Dean [00:09:59]: I mean, I think that&#8217;s true. If your distribution of what people are asking people, the models to do is stationary, right? But I think what often happens is as the models become more capable, people ask them to do more, right? So, I mean, I think this happens in my own usage. Like I used to try our models a year ago for some sort of coding task, and it was okay at some simpler things, but wouldn&#8217;t do work very well for more complicated things. And since then, we&#8217;ve improved dramatically on the more complicated coding tasks. And now I&#8217;ll ask it to do much more complicated things. And I think that&#8217;s true, not just of coding, but of, you know, now, you know, can you analyze all the, you know, renewable energy deployments in the world and give me a report on solar panel deployment or whatever. That&#8217;s a very complicated, you know, more complicated task than people would have asked a year ago. And so you are going to want more capable models to push the frontier in the absence of what people ask the models to do. And that also then gives us. Insight into, okay, where does the, where do things break down? How can we improve the model in these, these particular areas, uh, in order to sort of, um, make the next generation even better.Alessio Fanelli [00:11:11]: Yeah. Are there any benchmarks or like test sets they use internally? Because it&#8217;s almost like the same benchmarks get reported every time. And it&#8217;s like, all right, it&#8217;s like 99 instead of 97. Like, how do you have to keep pushing the team internally to it? Or like, this is what we&#8217;re building towards. Yeah.Jeff Dean [00:11:26]: I mean, I think. Benchmarks, particularly external ones that are publicly available. Have their utility, but they often kind of have a lifespan of utility where they&#8217;re introduced and maybe they&#8217;re quite hard for current models. You know, I, I like to think of the best kinds of benchmarks are ones where the initial scores are like 10 to 20 or 30%, maybe, but not higher. And then you can sort of work on improving that capability for, uh, whatever it is, the benchmark is trying to assess and get it up to like 80, 90%, whatever. I, I think once it hits kind of 95% or something, you get very diminishing returns from really focusing on that benchmark, cuz it&#8217;s sort of, it&#8217;s either the case that you&#8217;ve now achieved that capability, or there&#8217;s also the issue of leakage in public data or very related kind of data being, being in your training data. Um, so we have a bunch of held out internal benchmarks that we really look at where we know that wasn&#8217;t represented in the training data at all. There are capabilities that we want the model to have. Um, yeah. Yeah. Um, that it doesn&#8217;t have now, and then we can work on, you know, assessing, you know, how do we make the model better at these kinds of things? Is it, we need different kind of data to train on that&#8217;s more specialized for this particular kind of task. Do we need, um, you know, a bunch of, uh, you know, architectural improvements or some sort of, uh, model capability improvements, you know, what would help make that better?Shawn Wang [00:12:53]: Is there, is there such an example that you, uh, a benchmark inspired in architectural improvement? Like, uh, I&#8217;m just kind of. Jumping on that because you just.Jeff Dean [00:13:02]: Uh, I mean, I think some of the long context capability of the, of the Gemini models that came, I guess, first in 1.5 really were about looking at, okay, we want to have, um, you know,Shawn Wang [00:13:15]: immediately everyone jumped to like completely green charts of like, everyone had, I was like, how did everyone crack this at the same time? Right. Yeah. Yeah.Jeff Dean [00:13:23]: I mean, I think, um, and once you&#8217;re set, I mean, as you say that needed single needle and a half. Hey, stack benchmark is really saturated for at least context links up to 1, 2 and K or something. Don&#8217;t actually have, you know, much larger than 1, 2 and 8 K these days or two or something. We&#8217;re trying to push the frontier of 1 million or 2 million context, which is good because I think there are a lot of use cases where. Yeah. You know, putting a thousand pages of text or putting, you know, multiple hour long videos and the context and then actually being able to make use of that as useful. Try to, to explore the &#252;ber graduation are fairly large. But the single needle in a haystack benchmark is sort of saturated. So you really want more complicated, sort of multi-needle or more realistic, take all this content and produce this kind of answer from a long context that sort of better assesses what it is people really want to do with long context. Which is not just, you know, can you tell me the product number for this particular thing?Shawn Wang [00:14:31]: Yeah, it&#8217;s retrieval. It&#8217;s retrieval within machine learning. It&#8217;s interesting because I think the more meta level I&#8217;m trying to operate at here is you have a benchmark. You&#8217;re like, okay, I see the architectural thing I need to do in order to go fix that. But should you do it? Because sometimes that&#8217;s an inductive bias, basically. It&#8217;s what Jason Wei, who used to work at Google, would say. Exactly the kind of thing. Yeah, you&#8217;re going to win. Short term. Longer term, I don&#8217;t know if that&#8217;s going to scale. You might have to undo that.Jeff Dean [00:15:01]: I mean, I like to sort of not focus on exactly what solution we&#8217;re going to derive, but what capability would you want? And I think we&#8217;re very convinced that, you know, long context is useful, but it&#8217;s way too short today. Right? Like, I think what you would really want is, can I attend to the internet while I answer my question? Right? But that&#8217;s not going to happen. I think that&#8217;s going to be solved by purely scaling the existing solutions, which are quadratic. So a million tokens kind of pushes what you can do. You&#8217;re not going to do that to a trillion tokens, let alone, you know, a billion tokens, let alone a trillion. But I think if you could give the illusion that you can attend to trillions of tokens, that would be amazing. You&#8217;d find all kinds of uses for that. You would have attend to the internet. You could attend to the pixels of YouTube and the sort of deeper representations that we can find. You could attend to the form for a single video, but across many videos, you know, on a personal Gemini level, you could attend to all of your personal state with your permission. So like your emails, your photos, your docs, your plane tickets you have. I think that would be really, really useful. And the question is, how do you get algorithmic improvements and system level improvements that get you to something where you actually can attend to trillions of tokens? Right. In a meaningful way. Yeah.Shawn Wang [00:16:26]: But by the way, I think I did some math and it&#8217;s like, if you spoke all day, every day for eight hours a day, you only generate a maximum of like a hundred K tokens, which like very comfortably fits.Jeff Dean [00:16:38]: Right. But if you then say, okay, I want to be able to understand everything people are putting on videos.Shawn Wang [00:16:46]: Well, also, I think that the classic example is you start going beyond language into like proteins and whatever else is extremely information dense. Yeah. Yeah.Jeff Dean [00:16:55]: I mean, I think one of the things about Gemini&#8217;s multimodal aspects is we&#8217;ve always wanted it to be multimodal from the start. And so, you know, that sometimes to people means text and images and video sort of human-like and audio, audio, human-like modalities. But I think it&#8217;s also really useful to have Gemini know about non-human modalities. Yeah. Like LIDAR sensor data from. Yes. Say, Waymo vehicles or. Like robots or, you know, various kinds of health modalities, x-rays and MRIs and imaging and genomics information. And I think there&#8217;s probably hundreds of modalities of data where you&#8217;d like the model to be able to at least be exposed to the fact that this is an interesting modality and has certain meaning in the world. Where even if you haven&#8217;t trained on all the LIDAR data or MRI data, you could have, because maybe that&#8217;s not, you know, it doesn&#8217;t make sense in terms of trade-offs of. You know, what you include in your main pre-training data mix, at least including a little bit of it is actually quite useful. Yeah. Because it sort of tempts the model that this is a thing.Shawn Wang [00:18:04]: Yeah. Do you believe, I mean, since we&#8217;re on this topic and something I just get to ask you all the questions I always wanted to ask, which is fantastic. Like, are there some king modalities, like modalities that supersede all the other modalities? So a simple example was Vision can, on a pixel level, encode text. And DeepSeq had this DeepSeq CR paper that did that. Vision. And Vision has also been shown to maybe incorporate audio because you can do audio spectrograms and that&#8217;s, that&#8217;s also like a Vision capable thing. Like, so, so maybe Vision is just the king modality and like. Yeah.Jeff Dean [00:18:36]: I mean, Vision and Motion are quite important things, right? Motion. Well, like video as opposed to static images, because I mean, there&#8217;s a reason evolution has evolved eyes like 23 independent ways, because it&#8217;s such a useful capability for sensing the world around you, which is really what we want these models to be. So I think the only thing that we can be able to do is interpret the things we&#8217;re seeing or the things we&#8217;re paying attention to and then help us in using that information to do things. Yeah.Shawn Wang [00:19:05]: I think motion, you know, I still want to shout out, I think Gemini, still the only native video understanding model that&#8217;s out there. So I use it for YouTube all the time. Nice.Jeff Dean [00:19:15]: Yeah. Yeah. I mean, it&#8217;s actually, I think people kind of are not necessarily aware of what the Gemini models can actually do. Yeah. Like I have an example I&#8217;ve used in one of my talks. It had like, it was like a YouTube highlight video of 18 memorable sports moments across the last 20 years or something. So it has like Michael Jordan hitting some jump shot at the end of the finals and, you know, some soccer goals and things like that. And you can literally just give it the video and say, can you please make me a table of what all these different events are? What when the date is when they happened? And a short description. And so you get like now an 18 row table of that information extracted from the video, which is, you know, not something most people think of as like a turn video into sequel like table.Alessio Fanelli [00:20:11]: Has there been any discussion inside of Google of like, you mentioned tending to the whole internet, right? Google, it&#8217;s almost built because a human cannot tend to the whole internet and you need some sort of ranking to find what you need. Yep. That ranking is like much different for an LLM because you can expect a person to look at maybe the first five, six links in a Google search versus for an LLM. Should you expect to have 20 links that are highly relevant? Like how do you internally figure out, you know, how do we build the AI mode that is like maybe like much broader search and span versus like the more human one? Yeah.Jeff Dean [00:20:47]: I mean, I think even pre-language model based work, you know, our ranking systems would be built to start. I mean, I think even pre-language model based work, you know, our ranking systems would be built to start. With a giant number of web pages in our index, many of them are not relevant. So you identify a subset of them that are relevant with very lightweight kinds of methods. You know, you&#8217;re down to like 30,000 documents or something. And then you gradually refine that to apply more and more sophisticated algorithms and more and more sophisticated sort of signals of various kinds in order to get down to ultimately what you show, which is, you know, the final 10 results or, you know, 10 results plus. Other kinds of information. And I think an LLM based system is not going to be that dissimilar, right? You&#8217;re going to attend to trillions of tokens, but you&#8217;re going to want to identify, you know, what are the 30,000 ish documents that are with the, you know, maybe 30 million interesting tokens. And then how do you go from that into what are the 117 documents I really should be paying attention to in order to carry out the tasks that the user has asked? And I think, you know, you can imagine systems where you have, you know, a lot of highly parallel processing to identify those initial 30,000 candidates, maybe with very lightweight kinds of models. Then you have some system that sort of helps you narrow down from 30,000 to the 117 with maybe a little bit more sophisticated model or set of models. And then maybe the final model is the thing that looks. So the 117 things that might be your most capable model. So I think it has to, it&#8217;s going to be some system like that, that is really enables you to give the illusion of attending to trillions of tokens. Sort of the way Google search gives you, you know, not the illusion, but you are searching the internet, but you&#8217;re finding, you know, a very small subset of things that are, that are relevant.Shawn Wang [00:22:47]: Yeah. I often tell a lot of people that are not steeped in like Google search history that, well, you know, like Bert was. Like he was like basically immediately inside of Google search and that improves results a lot, right? Like I don&#8217;t, I don&#8217;t have any numbers off the top of my head, but like, I&#8217;m sure you guys, that&#8217;s obviously the most important numbers to Google. Yeah.Jeff Dean [00:23:08]: I mean, I think going to an LLM based representation of text and words and so on enables you to get out of the explicit hard notion of, of particular words having to be on the page, but really getting at the notion of this topic of this page or this page. Paragraph is highly relevant to this query. Yeah.Shawn Wang [00:23:28]: I don&#8217;t think people understand how much LLMs have taken over all these very high traffic system, very high traffic. Yeah. Like it&#8217;s Google, it&#8217;s YouTube. YouTube has this like semantics ID thing where it&#8217;s just like every token or every item in the vocab is a YouTube video or something that predicts the video using a code book, which is absurd to me for YouTube size.Jeff Dean [00:23:50]: And then most recently GROK also for, for XAI, which is like, yeah. I mean, I&#8217;ll call out even before LLMs were used extensively in search, we put a lot of emphasis on softening the notion of what the user actually entered into the query.Shawn Wang [00:24:06]: So do you have like a history of like, what&#8217;s the progression? Oh yeah.Jeff Dean [00:24:09]: I mean, I actually gave a talk in, uh, I guess, uh, web search and data mining conference in 2009, uh, where we never actually published any papers about the origins of Google search, uh, sort of, but we went through sort of four or five or six. generations, four or five or six generations of, uh, redesigning of the search and retrieval system, uh, from about 1999 through 2004 or five. And that talk is really about that evolution. And one of the things that really happened in 2001 was we were sort of working to scale the system in multiple dimensions. So one is we wanted to make our index bigger, so we could retrieve from a larger index, which always helps your quality in general. Uh, because if you don&#8217;t have the page in your index, you&#8217;re going to not do well. Um, and then we also needed to scale our capacity because we were, our traffic was growing quite extensively. Um, and so we had, you know, a sharded system where you have more and more shards as the index grows, you have like 30 shards. And then if you want to double the index size, you make 60 shards so that you can bound the latency by which you respond for any particular user query. Um, and then as traffic grows, you add, you add more and more replicas of each of those. And so we eventually did the math that realized that in a data center where we had say 60 shards and, um, you know, 20 copies of each shard, we now had 1200 machines, uh, with disks. And we did the math and we&#8217;re like, Hey, one copy of that index would actually fit in memory across 1200 machines. So in 2001, we introduced, uh, we put our entire index in memory and what that enabled from a quality perspective was amazing. Um, and so we had more and more replicas of each of those. Before you had to be really careful about, you know, how many different terms you looked at for a query, because every one of them would involve a disk seek on every one of the 60 shards. And so you, as you make your index bigger, that becomes even more inefficient. But once you have the whole index in memory, it&#8217;s totally fine to have 50 terms you throw into the query from the user&#8217;s original three or four word query, because now you can add synonyms like restaurant and restaurants and cafe and, uh, you know, things like that. Uh, bistro and all these things. And you can suddenly start, uh, sort of really, uh, getting at the meaning of the word as opposed to the exact semantic form the user typed in. And that was, you know, 2001, very much pre LLM, but really it was about softening the, the strict definition of what the user typed in order to get at the meaning.Alessio Fanelli [00:26:47]: What are like principles that you use to like design the systems, especially when you have, I mean, in 2001, the internet is like. Doubling, tripling every year in size is not like, uh, you know, and I think today you kind of see that with LLMs too, where like every year the jumps in size and like capabilities are just so big. Are there just any, you know, principles that you use to like, think about this? Yeah.Jeff Dean [00:27:08]: I mean, I think, uh, you know, first, whenever you&#8217;re designing a system, you want to understand what are the sort of design parameters that are going to be most important in designing that, you know? So, you know, how many queries per second do you need to handle? How big is the internet? How big is the index you need to handle? How much data do you need to keep for every document in the index? How are you going to look at it when you retrieve things? Um, what happens if traffic were to double or triple, you know, will that system work well? And I think a good design principle is you&#8217;re going to want to design a system so that the most important characteristics could scale by like factors of five or 10, but probably not beyond that because often what happens is if you design a system for X. And something suddenly becomes a hundred X, that would enable a very different point in the design space that would not make sense at X. But all of a sudden at a hundred X makes total sense. So like going from a disk space index to a in memory index makes a lot of sense once you have enough traffic, because now you have enough replicas of the sort of state on disk that those machines now actually can hold, uh, you know, a full copy of the, uh, index and memory. Yeah. And that all of a sudden enabled. A completely different design that wouldn&#8217;t have been practical before. Yeah. Um, so I&#8217;m, I&#8217;m a big fan of thinking through designs in your head, just kind of playing with the design space a little before you actually do a lot of writing of code. But, you know, as you said, in the early days of Google, we were growing the index, uh, quite extensively. We were growing the update rate of the index. So the update rate actually is the parameter that changed the most. Surprising. So it used to be once a month.Shawn Wang [00:28:55]: Yeah.Jeff Dean [00:28:56]: And then we went to a system that could update any particular page in like sub one minute. Okay.Shawn Wang [00:29:02]: Yeah. Because this is a competitive advantage, right?Jeff Dean [00:29:04]: Because all of a sudden news related queries, you know, if you&#8217;re, if you&#8217;ve got last month&#8217;s news index, it&#8217;s not actually that useful for.Shawn Wang [00:29:11]: News is a special beast. Was there any, like you could have split it onto a separate system.Jeff Dean [00:29:15]: Well, we did. We launched a Google news product, but you also want news related queries that people type into the main index to also be sort of updated.Shawn Wang [00:29:23]: So, yeah, it&#8217;s interesting. And then you have to like classify whether the page is, you have to decide which pages should be updated and what frequency. Oh yeah.Jeff Dean [00:29:30]: There&#8217;s a whole like, uh, system behind the scenes that&#8217;s trying to decide update rates and importance of the pages. So even if the update rate seems low, you might still want to recrawl important pages quite often because, uh, the likelihood they change might be low, but the value of having updated is high.Shawn Wang [00:29:50]: Yeah, yeah, yeah, yeah. Uh, well, you know, yeah. This, uh, you know, mention of latency and, and saving things to this reminds me of one of your classics, which I have to bring up, which is latency numbers. Every programmer should know, uh, was there a, was it just a, just a general story behind that? Did you like just write it down?Jeff Dean [00:30:06]: I mean, this has like sort of eight or 10 different kinds of metrics that are like, how long does a cache mistake? How long does branch mispredict take? How long does a reference domain memory take? How long does it take to send, you know, a packet from the U S to the Netherlands or something? Um,Shawn Wang [00:30:21]: why Netherlands, by the way, or is it, is that because of Chrome?Jeff Dean [00:30:25]: Uh, we had a data center in the Netherlands, um, so, I mean, I think this gets to the point of being able to do the back of the envelope calculations. So these are sort of the raw ingredients of those, and you can use them to say, okay, well, if I need to design a system to do image search and thumb nailing or something of the result page, you know, how, what I do that I could pre-compute the image thumbnails. I could like. Try to thumbnail them on the fly from the larger images. What would that do? How much dis bandwidth than I need? How many des seeks would I do? Um, and you can sort of actually do thought experiments in, you know, 30 seconds or a minute with the sort of, uh, basic, uh, basic numbers at your fingertips. Uh, and then as you sort of build software using higher level libraries, you kind of want to develop the same intuitions for how long does it take to, you know, look up something in this particular kind of.Shawn Wang [00:31:21]: I&#8217;ll see you next time.Shawn Wang [00:31:51]: Which is a simple byte conversion. That&#8217;s nothing interesting. I wonder if you have any, if you were to update your...Jeff Dean [00:31:58]: I mean, I think it&#8217;s really good to think about calculations you&#8217;re doing in a model, either for training or inference.Jeff Dean [00:32:09]: Often a good way to view that is how much state will you need to bring in from memory, either like on-chip SRAM or HBM from the accelerator. Attached memory or DRAM or over the network. And then how expensive is that data motion relative to the cost of, say, an actual multiply in the matrix multiply unit? And that cost is actually really, really low, right? Because it&#8217;s order, depending on your precision, I think it&#8217;s like sub one picodule.Shawn Wang [00:32:50]: Oh, okay. You measure it by energy. Yeah. Yeah.Jeff Dean [00:32:52]: Yeah. I mean, it&#8217;s all going to be about energy and how do you make the most energy efficient system. And then moving data from the SRAM on the other side of the chip, not even off the off chip, but on the other side of the same chip can be, you know, a thousand picodules. Oh, yeah. And so all of a sudden, this is why your accelerators require batching. Because if you move, like, say, the parameter of a model from SRAM on the, on the chip into the multiplier unit, that&#8217;s going to cost you a thousand picodules. So you better make use of that, that thing that you moved many, many times with. So that&#8217;s where the batch dimension comes in. Because all of a sudden, you know, if you have a batch of 256 or something, that&#8217;s not so bad. But if you have a batch of one, that&#8217;s really not good.Shawn Wang [00:33:40]: Yeah. Yeah. Right.Jeff Dean [00:33:41]: Because then you paid a thousand picodules in order to do your one picodule multiply.Shawn Wang [00:33:46]: I have never heard an energy-based analysis of batching.Jeff Dean [00:33:50]: Yeah. I mean, that&#8217;s why people batch. Yeah. Ideally, you&#8217;d like to use batch size one because the latency would be great.Shawn Wang [00:33:56]: The best latency.Jeff Dean [00:33:56]: But the energy cost and the compute cost inefficiency that you get is quite large. So, yeah.Shawn Wang [00:34:04]: Is there a similar trick like, like, like you did with, you know, putting everything in memory? Like, you know, I think obviously NVIDIA has caused a lot of waves with betting very hard on SRAM with Grok. I wonder if, like, that&#8217;s something that you already saw with, with the TPUs, right? Like that, that you had to. Uh, to serve at your scale, uh, you probably sort of saw that coming. Like what, what, what hardware, uh, innovations or insights were formed because of what you&#8217;re seeing there?Jeff Dean [00:34:33]: Yeah. I mean, I think, you know, TPUs have this nice, uh, sort of regular structure of 2D or 3D meshes with a bunch of chips connected. Yeah. And each one of those has HBM attached. Um, I think for serving some kinds of models, uh, you know, you, you pay a lot higher cost. Uh, and time latency, um, bringing things in from HBM than you do bringing them in from, uh, SRAM on the chip. So if you have a small enough model, you can actually do model parallelism, spread it out over lots of chips and you actually get quite good throughput improvements and latency improvements from doing that. And so you&#8217;re now sort of striping your smallish scale model over say 16 or 64 chips. Uh, but as if you do that and it all fits in. In SRAM, uh, that can be a big win. So yeah, that&#8217;s not a surprise, but it is a good technique.Alessio Fanelli [00:35:27]: Yeah. What about the TPU design? Like how much do you decide where the improvements have to go? So like, this is like a good example of like, is there a way to bring the thousand picojoules down to 50? Like, is it worth designing a new chip to do that? The extreme is like when people say, oh, you should burn the model on the ASIC and that&#8217;s kind of like the most extreme thing. How much of it? Is it worth doing an hardware when things change so quickly? Like what was the internal discussion? Yeah.Jeff Dean [00:35:57]: I mean, we, we have a lot of interaction between say the TPU chip design architecture team and the sort of higher level modeling, uh, experts, because you really want to take advantage of being able to co-design what should future TPUs look like based on where we think the sort of ML research puck is going, uh, in some sense, because, uh, you know, as a hardware designer for ML and in particular, you&#8217;re trying to design a chip starting today and that design might take two years before it even lands in a data center. And then it has to sort of be a reasonable lifetime of the chip to take you three, four or five years. So you&#8217;re trying to predict two to six years out where, what ML computations will people want to run two to six years out in a very fast changing field. And so having people with interest. Interesting ML research ideas of things we think will start to work in that timeframe or will be more important in that timeframe, uh, really enables us to then get, you know, interesting hardware features put into, you know, TPU N plus two, where TPU N is what we have today.Shawn Wang [00:37:10]: Oh, the cycle time is plus two.Jeff Dean [00:37:12]: Roughly. Wow. Because, uh, I mean, sometimes you can squeeze some changes into N plus one, but, you know, bigger changes are going to require the chip. Yeah. Design be earlier in its lifetime design process. Um, so whenever we can do that, it&#8217;s generally good. And sometimes you can put in speculative features that maybe won&#8217;t cost you much chip area, but if it works out, it would make something, you know, 10 times as fast. And if it doesn&#8217;t work out, well, you burned a little bit of tiny amount of your chip area on that thing, but it&#8217;s not that big a deal. Uh, sometimes it&#8217;s a very big change and we want to be pretty sure this is going to work out. So we&#8217;ll do like lots of carefulness. Uh, ML experimentation to show us, uh, this is actually the, the way we want to go. Yeah.Alessio Fanelli [00:37:58]: Is there a reverse of like, we already committed to this chip design so we can not take the model architecture that way because it doesn&#8217;t quite fit?Jeff Dean [00:38:06]: Yeah. I mean, you, you definitely have things where you&#8217;re going to adapt what the model architecture looks like so that they&#8217;re efficient on the chips that you&#8217;re going to have for both training and inference of that, of that, uh, generation of model. So I think it kind of goes both ways. Um, you know, sometimes you can take advantage of, you know, lower precision things that are coming in a future generation. So you can, might train it at that lower precision, even if the current generation doesn&#8217;t quite do that. Mm.Shawn Wang [00:38:40]: Yeah. How low can we go in precision?Jeff Dean [00:38:43]: Because people are saying like ternary is like, uh, yeah, I mean, I&#8217;m a big fan of very low precision because I think that gets, that saves you a tremendous amount of time. Right. Because it&#8217;s picojoules per bit that you&#8217;re transferring and reducing the number of bits is a really good way to, to reduce that. Um, you know, I think people have gotten a lot of luck, uh, mileage out of having very low bit precision things, but then having scaling factors that apply to a whole bunch of, uh, those, those weights. Scaling. How does it, how does it, okay.Shawn Wang [00:39:15]: Interesting. You, so low, low precision, but scaled up weights. Yeah. Huh. Yeah. Never considered that. Yeah. Interesting. Uh, w w while we&#8217;re on this topic, you know, I think there&#8217;s a lot of, um, uh, this, the concept of precision at all is weird when we&#8217;re sampling, you know, uh, we just, at the end of this, we&#8217;re going to have all these like chips that I&#8217;ll do like very good math. And then we&#8217;re just going to throw a random number generator at the start. So, I mean, there&#8217;s a movement towards, uh, energy based, uh, models and processors. I&#8217;m just curious if you&#8217;ve, obviously you&#8217;ve thought about it, but like, what&#8217;s your commentary?Jeff Dean [00:39:50]: Yeah. I mean, I think. There&#8217;s a bunch of interesting trends though. Energy based models is one, you know, diffusion based models, which don&#8217;t sort of sequentially decode tokens is another, um, you know, speculative decoding is a way that you can get sort of an equivalent, very small.Shawn Wang [00:40:06]: Draft.Jeff Dean [00:40:07]: Batch factor, uh, for like you predict eight tokens out and that enables you to sort of increase the effective batch size of what you&#8217;re doing by a factor of eight, even, and then you maybe accept five or six of those tokens. So you get. A five, a five X improvement in the amortization of moving weights, uh, into the multipliers to do the prediction for the, the tokens. So these are all really good techniques and I think it&#8217;s really good to look at them from the lens of, uh, energy, real energy, not energy based models, um, and, and also latency and throughput, right? If you look at things from that lens, that sort of guides you to. Two solutions that are gonna be, uh, you know, better from, uh, you know, being able to serve larger models or, you know, equivalent size models more cheaply and with lower latency.Shawn Wang [00:41:03]: Yeah. Well, I think, I think I, um, it&#8217;s appealing intellectually, uh, haven&#8217;t seen it like really hit the mainstream, but, um, I do think that, uh, there&#8217;s some poetry in the sense that, uh, you know, we don&#8217;t have to do, uh, a lot of shenanigans if like we fundamentally. Design it into the hardware. Yeah, yeah.Jeff Dean [00:41:23]: I mean, I think there&#8217;s still a, there&#8217;s also sort of the more exotic things like analog based, uh, uh, computing substrates as opposed to digital ones. Uh, I&#8217;m, you know, I think those are super interesting cause they can be potentially low power. Uh, but I think you often end up wanting to interface that with digital systems and you end up losing a lot of the power advantages in the digital to analog and analog to digital conversions. You end up doing, uh, at the sort of boundaries. And periphery of that system. Um, I still think there&#8217;s a tremendous distance we can go from where we are today in terms of energy efficiency with sort of, uh, much better and specialized hardware for the models we care about.Shawn Wang [00:42:05]: Yeah.Alessio Fanelli [00:42:06]: Um, any other interesting research ideas that you&#8217;ve seen, or like maybe things that you cannot pursue a Google that you would be interested in seeing researchers take a step at, I guess you have a lot of researchers. Yeah, I guess you have enough, but our, our research.Jeff Dean [00:42:21]: Our research portfolio is pretty broad. I would say, um, I mean, I think, uh, in terms of research directions, there&#8217;s a whole bunch of, uh, you know, open problems and how do you make these models reliable and able to do much longer, kind of, uh, more complex tasks that have lots of subtasks. How do you orchestrate, you know, maybe one model that&#8217;s using other models as tools in order to sort of build, uh, things that can accomplish, uh, you know, much more. Yeah. Significant pieces of work, uh, collectively, then you would ask a single model to do. Um, so that&#8217;s super interesting. How do you get more verifiable, uh, you know, how do you get RL to work for non-verifiable domains? I think it&#8217;s a pretty interesting open problem because I think that would broaden out the capabilities of the models, the improvements that you&#8217;re seeing in both math and coding. Uh, if we could apply those to other less verifiable domains, because we&#8217;ve come up with RL techniques that actually enable us to do that. Uh, effectively, that would, that would really make the models improve quite a lot. I think.Alessio Fanelli [00:43:26]: I&#8217;m curious, like when we had Noam Brown on the podcast, he said, um, they already proved you can do it with deep research. Um, you kind of have it with AI mode in a way it&#8217;s not verifiable. I&#8217;m curious if there&#8217;s any thread that you think is interesting there. Like what is it? Both are like information retrieval of JSON. So I wonder if it&#8217;s like the retrieval is like the verifiable part. That you can score or what are like, yeah, yeah. How, how would you model that, that problem?Jeff Dean [00:43:55]: Yeah. I mean, I think there are ways of having other models that can evaluate the results of what a first model did, maybe even retrieving. Can you have another model that says, is this things, are these things you retrieved relevant? Or can you rate these 2000 things you retrieved to assess which ones are the 50 most relevant or something? Um, I think those kinds of techniques are actually quite effective. Sometimes I can even be the same model, just prompted differently to be a, you know, a critic as opposed to a, uh, actual retrieval system. Yeah.Shawn Wang [00:44:28]: Um, I do think like there, there is that, that weird cliff where like, it feels like we&#8217;ve done the easy stuff and then now it&#8217;s, but it always feels like that every year. It&#8217;s like, oh, like we know, we know, and the next part is super hard and nobody&#8217;s figured it out. And, uh, exactly with this RLVR thing where like everyone&#8217;s talking about, well, okay, how do we. the next stage of the non-verifiable stuff. And everyone&#8217;s like, I don&#8217;t know, you know, Ellen judge.Jeff Dean [00:44:56]: I mean, I feel like the nice thing about this field is there&#8217;s lots and lots of smart people thinking about creative solutions to some of the problems that we all see. Uh, because I think everyone sort of sees that the models, you know, are great at some things and they fall down around the edges of those things and, and are not as capable as we&#8217;d like in those areas. And then coming up with good techniques and trying those. And seeing which ones actually make a difference is sort of what the whole research aspect of this field is, is pushing forward. And I think that&#8217;s why it&#8217;s super interesting. You know, if you think about two years ago, we were struggling with GSM, eight K problems, right? Like, you know, Fred has two rabbits. He gets three more rabbits. How many rabbits does he have? That&#8217;s a pretty far cry from the kinds of mathematics that the models can, and now you&#8217;re doing IMO and Erdos problems in pure language. Yeah. Yeah. Pure language. So that is a really, really amazing jump in capabilities in, you know, in a year and a half or something. And I think, um, for other areas, it&#8217;d be great if we could make that kind of leap. Uh, and you know, we don&#8217;t exactly see how to do it for some, some areas, but we do see it for some other areas and we&#8217;re going to work hard on making that better. Yeah.Shawn Wang [00:46:13]: Yeah.Alessio Fanelli [00:46:14]: Like YouTube thumbnail generation. That would be very helpful. We need that. That would be AGI. We need that.Shawn Wang [00:46:20]: That would be. As far as content creators go.Jeff Dean [00:46:22]: I guess I&#8217;m not a YouTube creator, so I don&#8217;t care that much about that problem, but I guess, uh, many people do.Shawn Wang [00:46:27]: It does. Yeah. It doesn&#8217;t, it doesn&#8217;t matter. People do judge books by their covers as it turns out. Um, uh, just to draw a bit on the IMO goal. Um, I&#8217;m still not over the fact that a year ago we had alpha proof and alpha geometry and all those things. And then this year we were like, screw that we&#8217;ll just chuck it into Gemini. Yeah. What&#8217;s your reflection? Like, I think this, this question about. Like the merger of like symbolic systems and like, and, and LMS, uh, was a very much core belief. And then somewhere along the line, people would just said, Nope, we&#8217;ll just all do it in the LLM.Jeff Dean [00:47:02]: Yeah. I mean, I think it makes a lot of sense to me because, you know, humans manipulate symbols, but we probably don&#8217;t have like a symbolic representation in our heads. Right. We have some distributed representation that is neural net, like in some way of lots of different neurons. And activation patterns firing when we see certain things and that enables us to reason and plan and, you know, do chains of thought and, you know, roll them back now that, that approach for solving the problem doesn&#8217;t seem like it&#8217;s going to work. I&#8217;m going to try this one. And, you know, in a lot of ways we&#8217;re emulating what we intuitively think, uh, is happening inside real brains in neural net based models. So it never made sense to me to have like completely separate. Uh, discrete, uh, symbolic things, and then a completely different way of, of, uh, you know, thinking about those things.Shawn Wang [00:47:59]: Interesting. Yeah. Uh, I mean, it&#8217;s maybe seems obvious to you, but it wasn&#8217;t obvious to me a year ago. Yeah.Jeff Dean [00:48:06]: I mean, I do think like that IMO with, you know, translating to lean and using lean and then the next year and also a specialized geometry model. And then this year switching to a single unified model. That is roughly the production model with a little bit more inference budget, uh, is actually, you know, quite good because it shows you that the capabilities of that general model have improved dramatically and, and now you don&#8217;t need the specialized model. This is actually sort of very similar to the 2013 to 16 era of machine learning, right? Like it used to be, people would train separate models for lots of different, each different problem, right? I have, I want to recognize street signs and something. So I train a street sign. Recognition recognition model, or I want to, you know, decode speech recognition. I have a speech model, right? I think now the era of unified models that do everything is really upon us. And the question is how well do those models generalize to new things they&#8217;ve never been asked to do and they&#8217;re getting better and better.Shawn Wang [00:49:10]: And you don&#8217;t need domain experts. Like one of my, uh, so I interviewed ETA who was on, who was on that team. Uh, and he was like, yeah, I, I don&#8217;t know how they work. I don&#8217;t know where the IMO competition was held. I don&#8217;t know the rules of it. I just trained the models, the training models. Yeah. Yeah. And it&#8217;s kind of interesting that like people with these, this like universal skill set of just like machine learning, you just give them data and give them enough compute and they can kind of tackle any task, which is the bitter lesson, I guess. I don&#8217;t know. Yeah.Jeff Dean [00:49:39]: I mean, I think, uh, general models, uh, will win out over specialized ones in most cases.Shawn Wang [00:49:45]: Uh, so I want to push there a bit. I think there&#8217;s one hole here, which is like, uh. There&#8217;s this concept of like, uh, maybe capacity of a model, like abstractly a model can only contain the number of bits that it has. And, uh, and so it, you know, God knows like Gemini pro is like one to 10 trillion parameters. We don&#8217;t know, but, uh, the Gemma models, for example, right? Like a lot of people want like the open source local models that are like that, that, that, and, and, uh, they have some knowledge, which is not necessary, right? Like they can&#8217;t know everything like, like you have the. The luxury of you have the big model and big model should be able to capable of everything. But like when, when you&#8217;re distilling and you&#8217;re going down to the small models, you know, you&#8217;re actually memorizing things that are not useful. Yeah. And so like, how do we, I guess, do we want to extract that? Can we, can we divorce knowledge from reasoning, you know?Jeff Dean [00:50:38]: Yeah. I mean, I think you do want the model to be most effective at reasoning if it can retrieve things, right? Because having the model devote precious parameter space. To remembering obscure facts that could be looked up is actually not the best use of that parameter space, right? Like you might prefer something that is more generally useful in more settings than this obscure fact that it has. Um, so I think that&#8217;s always attention at the same time. You also don&#8217;t want your model to be kind of completely detached from, you know, knowing stuff about the world, right? Like it&#8217;s probably useful to know how long the golden gate be. Bridges just as a general sense of like how long are bridges, right? And, uh, it should have that kind of knowledge. It maybe doesn&#8217;t need to know how long some teeny little bridge in some other more obscure part of the world is, but, uh, it does help it to have a fair bit of world knowledge and the bigger your model is, the more you can have. Uh, but I do think combining retrieval with sort of reasoning and making the model really good at doing multiple stages of retrieval. Yeah.Shawn Wang [00:51:49]: And reasoning through the intermediate retrieval results is going to be a, a pretty effective way of making the model seem much more capable, because if you think about, say, a personal Gemini, yeah, right?Jeff Dean [00:52:01]: Like we&#8217;re not going to train Gemini on my email. Probably we&#8217;d rather have a single model that, uh, we can then use and use being able to retrieve from my email as a tool and have the model reason about it and retrieve from my photos or whatever, uh, and then make use of that and have multiple. Um, you know, uh, stages of interaction. that makes sense.Alessio Fanelli [00:52:24]: Do you think the vertical models are like, uh, interesting pursuit? Like when people are like, oh, we&#8217;re building the best healthcare LLM, we&#8217;re building the best law LLM, are those kind of like short-term stopgaps or?Jeff Dean [00:52:37]: No, I mean, I think, I think vertical models are interesting. Like you want them to start from a pretty good base model, but then you can sort of, uh, sort of viewing them, view them as enriching the data. Data distribution for that particular vertical domain for healthcare, say, um, we&#8217;re probably not going to train or for say robotics. We&#8217;re probably not going to train Gemini on all possible robotics data. We, you could train it on because we want it to have a balanced set of capabilities. Um, so we&#8217;ll expose it to some robotics data, but if you&#8217;re trying to build a really, really good robotics model, you&#8217;re going to want to start with that and then train it on more robotics data. And then maybe that would. It&#8217;s multilingual translation capability, but improve its robotics capabilities. And we&#8217;re always making these kind of, uh, you know, trade-offs in the data mix that we train the base Gemini models on. You know, we&#8217;d love to include data from 200 more languages and as much data as we have for those languages, but that&#8217;s going to displace some other capabilities of the model. It won&#8217;t be as good at, um, you know, Pearl programming, you know, it&#8217;ll still be good at Python programming. Cause we&#8217;ll include it. Enough. Of that, but there&#8217;s other long tail computer languages or coding capabilities that it may suffer on or multi, uh, multimodal reasoning capabilities may suffer. Cause we didn&#8217;t get to expose it to as much data there, but it&#8217;s really good at multilingual things. So I, I think some combination of specialized models, maybe more modular models. So it&#8217;d be nice to have the capability to have those 200 languages, plus this awesome robotics model, plus this awesome healthcare, uh, module that all can be knitted together to work in concert and called upon in different circumstances. Right? Like if I have a health related thing, then it should enable using this health module in conjunction with the main base model to be even better at those kinds of things. Yeah.Shawn Wang [00:54:36]: Installable knowledge. Yeah.Jeff Dean [00:54:37]: Right.Shawn Wang [00:54:38]: Just download as a, as a package.Jeff Dean [00:54:39]: And some of that installable stuff can come from retrieval, but some of it probably should come from preloaded training on, you know, uh, a hundred billion tokens or a trillion tokens of health data. Yeah.Shawn Wang [00:54:51]: And for listeners, I think, uh, I will highlight the Gemma three end paper where they, there was a little bit of that, I think. Yeah.Alessio Fanelli [00:54:56]: Yeah. I guess the question is like, how many billions of tokens do you need to outpace the frontier model improvements? You know, it&#8217;s like, if I have to make this model better healthcare and the main. Gemini model is still improving. Do I need 50 billion tokens? Can I do it with a hundred, if I need a trillion healthcare tokens, it&#8217;s like, they&#8217;re probably not out there that you don&#8217;t have, you know, I think that&#8217;s really like the.Jeff Dean [00:55:21]: Well, I mean, I think healthcare is a particularly challenging domain, so there&#8217;s a lot of healthcare data that, you know, we don&#8217;t have access to appropriately, but there&#8217;s a lot of, you know, uh, healthcare organizations that want to train models on their own data. That is not public healthcare data, uh, not public health. But public healthcare data. Um, so I think there are opportunities there to say, partner with a large healthcare organization and train models for their use that are going to be, you know, more bespoke, but probably, uh, might be better than a general model trained on say, public data. Yeah.Shawn Wang [00:55:58]: Yeah. I, I believe, uh, by the way, also this is like somewhat related to the language conversation. Uh, I think one of your, your favorite examples was you can put a low resource language in the context and it just learns. Yeah.Jeff Dean [00:56:09]: Oh, yeah, I think the example we used was Calamon, which is truly low resource because it&#8217;s only spoken by, I think 120 people in the world and there&#8217;s no written text.Shawn Wang [00:56:20]: So, yeah. So you can just do it that way. Just put it in the context. Yeah. Yeah. But I think your whole data set in the context, right.Jeff Dean [00:56:27]: If you, if you take a language like, uh, you know, Somali or something, there is a fair bit of Somali text in the world that, uh, or Ethiopian Amharic or something, um, you know, we probably. Yeah. Are not putting all the data from those languages into the Gemini based training. We put some of it, but if you put more of it, you&#8217;ll improve the capabilities of those models.Shawn Wang [00:56:49]: Yeah.Jeff Dean [00:56:49]: So, or of those languages.Shawn Wang [00:56:52]: Uh, yeah, cool. Uh, it&#8217;s, uh, I have a side interest in linguistics. I, I, I did, uh, uh, a few classes back in college and like, uh, part of me, like if I was a linguist and I could have access to all these models, I would just be asking really fundamental questions about language itself. Yeah. Like, uh, one is th there&#8217;s one very obvious one, which is Sapir-Whorf, like how much does like the language that you speak affect your thinking, but then also there&#8217;s some languages where there&#8217;s just concepts that are not represented in other languages, but some others, many others that are just duplicates, right. Where, uh, there&#8217;s also another paper that people love called the platonic representation where, you know, like the, the, an image of a cup is, uh, if you say learn a model on that and you, you, you have a lot of texts with the word cup eventually maps to it, like roughly the same place. And so like that should apply to languages except where it doesn&#8217;t. And that&#8217;s actually like very interesting differences in what humanity has discovered as concepts that maybe English doesn&#8217;t have.Shawn Wang [00:57:54]: I don&#8217;t know. It&#8217;s just like my, my rant on languages. Yeah.Jeff Dean [00:57:58]: I mean, I, I did some work on a early model that fused together a language based model with you have, you know, nice word based representations and then an image model where you have. Trained it on image net like things. Yes. And then you fuse together the top layers of, uh, no, this is devise, uh, uh, the, you do a little bit more training to fuse together those representations. And what you found was that if you give a novel image that is not in any of the categories in the image model, it was trained on the model can often assigns kind of the right cat, the right label to that image. Um, so for example, um, I think, uh, telescope and, uh, binoculars were both in the training, uh, categories for the image model, but, um, microscope was not. Hmm. And so if you&#8217;re given an image of a microscope, it actually can come up with something that&#8217;s, uh, got the word microscope as the label that it assigns, even though it&#8217;s never actually seen an image labeled that.Shawn Wang [00:59:01]: Oh, that&#8217;s nice. That&#8217;s kind of cool. Yeah.Jeff Dean [00:59:04]: Um, so yeah.Shawn Wang [00:59:07]: Useful. Uh, cool. Uh, I think. There, there&#8217;s more general, like broad questions, but like, I guess what, what do you, uh, wish you were asked more in, in, in general, like, you know, like you, you have such a broad scope. We&#8217;ve covered the hardware, we&#8217;ve covered the, the, the models research. Yeah.Jeff Dean [00:59:22]: I mean, I think, uh, one thing that&#8217;s kind of interesting is, you know, I, I did a undergrad thesis on neural network, uh, training, uh, uh, parallel neural network training, uh, back in 1990 when I got exposed to neural nets and I always felt kind of, they were the right abstraction. Uh, but we just needed way more compute than we had then. Mm-hmm. So like the 32 processors in the department parallel computer, you know, could get you a, a little bit more interesting, uh, model, but not, not enough to solve real problems. And so starting in 2008 or nine, you know, the world started to have enough computing power through Moore&#8217;s law and, you know, larger, interesting data sets to train on to actually, you know, start training neural nets that could tackle real problems that people cared about. Yeah. Speech recognition. Vision, and eventually, uh, language. Um, and so, um, when I started working on neural nets at Google in, in late 2011, um, you know, I really just felt like we should scale up the size of neural networks we can train using, you know, large amounts of parallel computation. And so, uh, I actually, uh, revived some ideas from my undergrad thesis where I&#8217;d done both model parallel and data parallel, uh, training and I compared them. Uh, I, I called them. I&#8217;ve been doing this since I was eight. It was something different. There was like pattern partitioned and, you know, model partitioned or something.Shawn Wang [01:00:43]: Well, I have to, is it, is it public? And we can go dig it up?Jeff Dean [01:00:45]: Yeah, it&#8217;s on, it&#8217;s on the web. Okay, nice. Um, but, uh, you know, I think combining a lot of those techniques and really just trying to push on scaling things up over the last, you know, 15 years has been, you know, really important. And that means, you know, improvements in the hardware. So, you know, pushing on building specialized hardware like TPUs. Uh, it also means, you know, pushing on software, abstraction layers to let people express their ideas to the computer. Thank you for having me.Jeff Dean [01:01:40]: Thank you for having me.Shawn Wang [01:07:10]: If that&#8217;s something you would agree with at the time, or is there a different post-mortem?Jeff Dean [01:07:15]: The brain marketplace for compute quotas.Shawn Wang [01:07:18]: Compute quotas, where basically he was like, okay, David worked at OpenAI as VP Engine and then he worked at Google. He was like, fundamentally, OpenAI was willing to go all in, like, bet the farm on one thing, whereas Google was more democratic. Everyone had a quota. And I was like, okay, if you believe in scaling as an important thing, that&#8217;s an important organizational-wide decision to do.Jeff Dean [01:07:41]: Yeah. Yeah, I mean, I think I would somewhat agree with that. I mean, I think I actually wrote a one-page memo saying we were being stupid by fragmenting our resources. So in particular, at the time, we had efforts within Google Research. And in the brain team in particular, on large language models. We also had efforts on multimodal models in other parts of brain and Google Research. And then Legacy DeepMind had efforts like Chinchilla models and Flamingo models. And so really, we were fragmenting not only our compute across those separate efforts, but also our best people and our best. And so I said, this is just stupid. Why don&#8217;t we combine things and have one effort to train an awesome single unified model that is multimodal from the start, that&#8217;s good at everything. And that was the origin of the Gemini effort.Shawn Wang [01:08:52]: And my one-page memo worked, which is good. Did you have the name? Because also for those who don&#8217;t know, you named Gemini.Jeff Dean [01:08:58]: I did. There was another name proposed. And I said, you know what? You know, it&#8217;s sort of like these two organizations really are like twins in some sense coming together. So I kind of like that. And then there&#8217;s also the NASA interpretation of the early Gemini project being an important thing on your way to the Apollo project. So it seemed like a good name. Twins coming together. Right.Alessio Fanelli [01:09:27]: Yeah. Nice. I know we&#8217;re already running out of time, but I&#8217;m curious how you use AI. Today to code. So, I mean, you&#8217;re probably one of the most prolific engineers in the history of computer science. Um, I was reading on through the article about you and Sanjay&#8217;s friendship and how you work together. And you have one quote about, you need to find someone that you&#8217;re going to pair program with who&#8217;s compatible with your way of thinking so that the two of you together are a complimentary force. And I was thinking about how you think about coding agents and this, like, how do you shape a coding agents to be compatible with your way of thinking? Like. How would you rate the tools today? Like, where should things go? Yeah.Jeff Dean [01:10:07]: I mean, first, I think the coding tools are, you know, getting vastly better compared to where they were a year or two, two years ago. So now you can actually rely on them to do more complex things that you as a, as a software engineer want to accomplish. And you can sort of delegate, you know, pretty complex things to these tools. And I think one really nice aspect about the, uh, interaction between, uh, uh, human, uh, software engineer and, uh, uh, coding model that they&#8217;re working with is your way of talking to that, uh, coding model actually sort of, uh, dictates how it interacts with you, right? Like you could ask it, please write a bunch of good tests for this. You could ask it, please help me brainstorm. Performance ideas and your way of doing that is going to shape how the model responds, what kinds of problems it tackles, you know, how much do you want the model to go off and do things that are larger and more independent versus interact with it, uh, more to make sure that you&#8217;re shaping the right kinds of, of things. And I think it&#8217;s not the case that any one style is the right thing for everything, right? Like some kinds of problems you actually want, uh, maybe a more frequent interaction style with a model. And other ones, you&#8217;re just like, yeah, please just go write this because I, I know I need this thing. I can specify it well enough, um, and go off and do it and come back when you&#8217;re done. And so I do think there&#8217;s going to be more of a style of having lots of independent, uh, software agents off doing things on your behalf and figuring out the right sort of human computer interaction model and UI and so on for when should it interrupt you and say, Hey, I need a little more guidance here, or I&#8217;ve done this thing. Now what, now what should I do? Um, I think we, we&#8217;re not at the end all answer to that question. And as the models get better, that, uh, set of decisions you put into how the interaction should happen may, may change, right? Like if you, if you have a team of 50 interns, how would you manage that if they were people? And I think it&#8217;s not, do you want 50 interns? You might, if they&#8217;re really good, right?Shawn Wang [01:12:23]: It&#8217;s a lot of management. But it&#8217;s a lot of, uh.Jeff Dean [01:12:25]: Uh, yeah. I mean, I think that is probably within the realm of possibilities that lots of people could have 50 interns. Yeah. And so how would you actually deal with that as a person, right? Like you would probably want them to form small sub teams, so you don&#8217;t have to interact with 50 of them. You can interact with five, five of those teams and they&#8217;re off doing things on your behalf, but I don&#8217;t know exactly what the, how this is going to unfold.Alessio Fanelli [01:12:52]: Hmm. Yeah. How do you think about bringing people? Like the pair programming is always helpful to like get net new ideas in the distribution, so to speak. It feels as we have more of these coding agents, write the code, it&#8217;s hard to bring other people into the problem. So you go to like, you know, you have 50 interns, right? And then you want to go to Noam Shazier be like, Hey, no, I&#8217;m, I want to like pair on this thing. But now there&#8217;s like this huge amount of work that has been done in parallel that you need to catch him up on. Right. And I&#8217;m curious, like if people are going to be in a way more isolated in their teams, where it&#8217;s. It&#8217;s like, okay, there&#8217;s so much context in these 50 interns that it&#8217;s just hard for me to like relay everything back to you.Jeff Dean [01:13:33]: Maybe. I mean, on the other hand, like imagine a classical software organization without any AI assisted tools, right. You would have, you know, 50 people doing stuff and their interaction style is going to be naturally very hierarchical because, um, you know, these 50 people are going to be working on this part of the system and not. Not interact that much with these other people over here. But if you have, you know, five people each managing 50 virtual agents, you know, they might be able to actually have much higher bandwidth communication among the five people, uh, then you would have among five people who are also trying to coordinate, you know, a 50 person software team. Each.Alessio Fanelli [01:14:15]: So how, how do you, I&#8217;m curious how you change your just working rhythm, you know, like you spend more time ahead with people going through SPACs and design. Goals. Like,Jeff Dean [01:14:26]: um, I mean, I do think it&#8217;s interesting that, you know, whenever people were taught how to write software, they were taught that it&#8217;s really important to write specifications super clearly, but no one really believed that. Like it was like, yeah, whatever. I don&#8217;t need to do that. I&#8217;m going to really, I don&#8217;t know. I mean, writing the English language specification was never kind of an artifact that was really paid a lot of attention to. I mean, it was important, but it wasn&#8217;t sort of the thing. That drove the actual creative process quite as much as if you specify what software you want the agent to write for you, you&#8217;d better be pretty darn careful of and how you specify that because that&#8217;s going to dictate the quality of the output, right? Like if you, if you don&#8217;t cover that it needs to handle this kind of thing, or that this is a super important corner case, or that, you know, you really care about the performance of this part of it, you know, it may, uh, not do what you want. Yeah. And the better you get at interacting with these models. And I think one of the ways people will get better is they will get really good at crisply specifying things rather than leaving things to ambiguity. And that is actually probably not a bad thing. It&#8217;s not a bad skill to have, regardless of whether you&#8217;re a software engineer or a, you know, trying to do some other kind of, uh, task, you know, being able to crisply specify what it is you want. It&#8217;s going to be really important. Yeah.Shawn Wang [01:15:52]: My, my joke is, um, you know, good. Yeah. I think one thing is in, uh, indistinguishable from sufficiently advanced executive communication, like it&#8217;s like writing an internal memo, like weigh your words very carefully and also I think very important to be multimodal, right? I think, uh, one thing that, uh, anti-gravity from, from Google also did was like, just come out the gate to very, very strong multimodal, including videos, and that&#8217;s the highest bandwidth communication prompt that you can give to the model, which is fantastic. Yeah.Alessio Fanelli [01:16:20]: How do you collect things that you often you will have in your mind? So you have this amazing, like performance sense thing that you&#8217;ve heard about how to look for performance improvements. And is there a lot more value in like people writing these like generic things down so that they can then put them back as like potential retrieval artifacts for the model? Like, or do I have like the edge cases is like a good example, right? It&#8217;s like, if you&#8217;re building systems, you already have in your mind, specific edge cases, depending on it. But now you have to like, every time repeat it. Like, are you having people spend a lot more time writing? Are you finding out more generic things to bring back?Jeff Dean [01:16:56]: Or, um, I mean, I do think well-written guides of, of how to do good software engineering are going to be useful because they can be used as input to models or, you know, read by other developers so that their prompts are, you know, more clear about what the, the underlying software system should, should be doing. Um, you know, I think it may not be that you need to create a custom one. For every situation, if you have general guides and put those into, you know, the context of a coding agent, that, that can be helpful. Like in, you can imagine one for distributed systems, you could say, okay, think about failures of these kinds of things. And these are some techniques you can deal with failures. You know, you can have, uh, you know, Paxos like replication, or, you know, you can, uh, send the request to two places and tolerate failure because you only need one of them to come back. You know, a little. Description of 20 techniques like that in building distributed systems, probably would go a long way to having a coding agent be able to sort of cobble up more reliable and robust distributed systems.Shawn Wang [01:18:07]: Yeah. Yeah. I wonder when Gemini will be able to build Spanner, right?Alessio Fanelli [01:18:12]: Probably already has the code inside, you know?Alessio Fanelli [01:18:16]: Yeah. That, I mean, that&#8217;s a good example, right? When you have like, you know, the cap theorem and it&#8217;s like, well, this is like truth and you cannot break that. And then you build something that broke it.Shawn Wang [01:18:26]: Like, I&#8217;m curious, like models in a way are like, would he say he broke it? Did you, would you say you broke cap theorem? Really? Yeah. Okay. All right. I mean, under local assumptions. Yeah. Under some, some, yeah. And they&#8217;re like, you know, good clocks. Yeah. Yeah.Alessio Fanelli [01:18:41]: It&#8217;s like some, sometimes you don&#8217;t have to like always follow what is known to be true. Right. And I, I think models in a way, like if you tell them something, they&#8217;re like really buy into that, you know? Um, yeah. So yeah, just more. Thinking than any answer on how to fix it.Jeff Dean [01:18:57]: Yeah, my, my, uh, you know, it&#8217;s just on this, like, like big prompting and, and, uh, iteration, you know, I think that coming back to your latency point, um, I always, I always try to one, one AB test or experiment or benchmark or research I would like is what is the performance difference between, let&#8217;s say three dumb fast model calls with human alignment because the human will correct human alignment, being human looks at the first one and produces a new prompt.Shawn Wang [01:19:23]: For the second one. Correct. Okay. As opposed to like, you spec it out, you know, it&#8217;s been a long time writing as a pro a big, big fat prompt, and then you have a very smart model. Do it right. Right. You know, cause, uh, really is, is, uh, our lacks in performance, uh, an issue of like, well, you just haven&#8217;t specified well enough. There&#8217;s no universe in which I can produce what you want because you just haven&#8217;t told me. Right.Jeff Dean [01:19:44]: It&#8217;s underspecified. So I could produce 10 different things and only one of them is the thing you wanted. Yeah.Shawn Wang [01:19:49]: And the multi-turn taking with a flash model is enough. Yeah.Jeff Dean [01:19:54]: Yeah, I&#8217;m, I&#8217;m a big believer in pushing on latency because I think being able to have really low latency interactions with a system you&#8217;re using is just much more delightful than something that is, you know, 10 times as slow or 20 times as slow. And I think, you know, in the future we&#8217;ll see models that are, and, and underlying software and hardware systems that are 20X lower latency than what we have today, 50X lower latency. And that&#8217;s going to be really, really important for systems. That need to do a lot of stuff, uh, between your interactions.Shawn Wang [01:20:27]: Yeah. Yeah. There, there&#8217;s two extremes, right? And then meanwhile, you also have DeepThink, which is all the way on the other side. Right.Jeff Dean [01:20:33]: But you would use DeepThink all the time if it weren&#8217;t for cost and latency, right? If, if you could have that capability in a model because the latency improvement was 20X, uh, in the underlying hardware and system and costs, you know, there&#8217;s no reason you wouldn&#8217;t want that.Shawn Wang [01:20:50]: Yeah.Jeff Dean [01:20:52]: But at the same time, then you&#8217;d probably have a model. That is even better. That would take you 20X longer, even on that new hardware. Yeah.Shawn Wang [01:21:00]: Uh, you know, there, there&#8217;s, uh, the Pareto curve keeps climbing. Um, yeah, onward and outward, onward and outward. Yeah. Should we ask him for predictions to, to go? I don&#8217;t know if you have any predictions that you, that you like to keep, you know, like, uh, one, one way to do this is you have your tests whenever a new model comes out that you run, uh, what&#8217;s something that you&#8217;re, you&#8217;re not quite happy with yet. That you think we&#8217;ll get done soon.Jeff Dean [01:21:29]: Um, let me make two predictions that are not quite in that vein. Yeah. So I think a personalized model that knows you and knows all your state and is able to retrieve over all state you have access to, that you opt into is going to be incredibly useful compared to a more generic model that doesn&#8217;t have access to that. So like, can something attend to everything I&#8217;ve ever seen? Yeah. Every email, every photo, every. Yeah. Video I&#8217;ve watched, that&#8217;s going to be really useful. Uh, I think, uh, more and more specialized hardware is going to enable much lower latency models and much more capable models for affordable prices, uh, than say the current, current status quo. Uh, that&#8217;s going to be also quite important. Yeah.Shawn Wang [01:22:16]: When you say much lower latency, uh, people usually talk in tokens per second. Is that a term that is okay? Okay. Uh, you know, we&#8217;re at, let&#8217;s say a hundred. Now we can go to a thousand. Is it meaningful to go 10,000? Yes. Really? Okay. Absolutely. Right. Yeah. Because of chain of thought and chain of thought reasoning.Jeff Dean [01:22:36]: I mean, you could think, you know, uh, many more tokens, you could do many more parallel rollouts. You could generate way more code, uh, and check that the code is cracked with a chain of thought reasoning. So I think, you know, being able to do that at 10,000 tokens per second would be awesome. Yeah.Shawn Wang [01:22:52]: At 10,000 tokens per second, you are no longer reading code. Yeah. Like you will just generate it. You&#8217;ll, I&#8217;m not reading it.Jeff Dean [01:22:58]: Well, remember, it may not, it may not end up with 10,000 tokens of code. Yeah. It may be a thousand tokens of code that with 9,000 tokens of reasoning behind it, which would actually be probably much better code to read. Yeah.Alessio Fanelli [01:23:11]: Yeah. If I had more time, I would have written a shorter letter. Yeah. Yeah. Yeah. Um, awesome. Jeff, this was amazing. Thanks for taking the time. Thank you.Jeff Dean [01:23:20]: It&#8217;s been fun. Thanks for having me.",
      "url": "https://www.latent.space/p/jeffdean",
      "author": "Unknown",
      "published": "2026-02-12T22:02:35",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Jeff Dean, Google's Chief AI Scientist, discusses owning the AI Pareto frontier, why distillation drives Flash model breakthroughs, and why energy (picojoules) rather than FLOPs is becoming the true bottleneck. He also covers the unification of Google's AI teams and the future of scaling.",
      "importance_score": 62.0,
      "reasoning": "Insights from one of the most influential figures in AI are always valuable, especially on infrastructure bottlenecks and distillation strategy, but this is an interview rather than a news event.",
      "themes": [
        "AI_infrastructure",
        "Google",
        "scaling",
        "distillation"
      ],
      "continuation": null,
      "summary_html": "<p>Jeff Dean, Google's Chief AI Scientist, discusses owning the AI Pareto frontier, why distillation drives Flash model breakthroughs, and why energy (picojoules) rather than FLOPs is becoming the true bottleneck. He also covers the unification of Google's AI teams and the future of scaling.</p>",
      "content_html": "<p>From rewriting Google’s search stack in the early 2000s to reviving sparse trillion-parameter models and co-designing TPUs with frontier ML research, Jeff Dean has quietly shaped nearly every layer of the modern AI stack. As Chief AI Scientist at Google and a driving force behind Gemini, Jeff has lived through multiple scaling revolutions from CPUs and sharded indices to multimodal models that reason across text, video, and code.Jeff joins us to unpack what it really means to “own the Pareto frontier,” why distillation is the engine behind every Flash model breakthrough, how energy (in picojoules) not FLOPs is becoming the true bottleneck, what it was like leading the charge to unify all of Google’s AI teams, and why the next leap won’t come from bigger context windows alone, but from systems that give the illusion of attending to trillions of tokens.We discuss:Jeff’s early neural net thesis in 1990: parallel training before it was cool, why he believed scaling would win decades early, and the “bigger model, more data, better results” mantra that held for 15 yearsThe evolution of Google Search: sharding, moving the entire index into memory in 2001, softening query semantics pre-LLMs, and why retrieval pipelines already resemble modern LLM systemsPareto frontier strategy: why you need both frontier “Pro” models and low-latency “Flash” models, and how distillation lets smaller models surpass prior generationsDistillation deep dive: ensembles → compression → logits as soft supervision, and why you need the biggest model to make the smallest one goodLatency as a first-class objective: why 10–50x lower latency changes UX entirely, and how future reasoning workloads will demand 10,000 tokens/secEnergy-based thinking: picojoules per bit, why moving data costs 1000x more than a multiply, batching through the lens of energy, and speculative decoding as amortizationTPU co-design: predicting ML workloads 2–6 years out, speculative hardware features, precision reduction, sparsity, and the constant feedback loop between model architecture and siliconSparse models and “outrageously large” networks: trillions of parameters with 1–5% activation, and why sparsity was always the right abstractionUnified vs. specialized models: abandoning symbolic systems, why general multimodal models tend to dominate vertical silos, and when vertical fine-tuning still makes senseLong context and the illusion of scale: beyond needle-in-a-haystack benchmarks toward systems that narrow trillions of tokens to 117 relevant documentsPersonalized AI: attending to your emails, photos, and documents (with permission), and why retrieval + reasoning will unlock deeply personal assistantsCoding agents: 50 AI interns, crisp specifications as a new core skill, and how ultra-low latency will reshape human–agent collaborationWhy ideas still matter: transformers, sparsity, RL, hardware, systems — scaling wasn’t blind; the pieces had to multiply togetherShow Notes:Gemma 3 PaperGemma 3Gemini 2.5 ReportJeff Dean’s “Software Engineering Advice fromBuilding Large-Scale Distributed Systems” Presentation (with Back of the Envelope Calculations)Latency Numbers Every Programmer Should Know by Jeff DeanThe Jeff Dean FactsJeff Dean Google BioJeff Dean on “Important AI Trends” @Stanford AI ClubJeff Dean &amp; Noam Shazeer — 25 years at Google (Dwarkesh)—Jeff DeanLinkedIn: https://www.linkedin.com/in/jeff-dean-8b212555X: https://x.com/jeffdeanGooglehttps://google.comhttps://deepmind.googleFull Video EpisodeTimestamps00:00:04 — Introduction: Alessio &amp; Swyx welcome Jeff Dean, chief AI scientist at Google, to the Latent Space podcast00:00:30 — Owning the Pareto Frontier &amp; balancing frontier vs low-latency models00:01:31 — Frontier models vs Flash models + role of distillation00:03:52 — History of distillation and its original motivation00:05:09 — Distillation’s role in modern model scaling00:07:02 — Model hierarchy (Flash, Pro, Ultra) and distillation sources00:07:46 — Flash model economics &amp; wide deployment00:08:10 — Latency importance for complex tasks00:09:19 — Saturation of some tasks and future frontier tasks00:11:26 — On benchmarks, public vs internal00:12:53 — Example long-context benchmarks &amp; limitations00:15:01 — Long-context goals: attending to trillions of tokens00:16:26 — Realistic use cases beyond pure language00:18:04 — Multimodal reasoning and non-text modalities00:19:05 — Importance of vision &amp; motion modalities00:20:11 — Video understanding example (extracting structured info)00:20:47 — Search ranking analogy for LLM retrieval00:23:08 — LLM representations vs keyword search00:24:06 — Early Google search evolution &amp; in-memory index00:26:47 — Design principles for scalable systems00:28:55 — Real-time index updates &amp; recrawl strategies00:30:06 — Classic “Latency numbers every programmer should know”00:32:09 — Cost of memory vs compute and energy emphasis00:34:33 — TPUs &amp; hardware trade-offs for serving models00:35:57 — TPU design decisions &amp; co-design with ML00:38:06 — Adapting model architecture to hardware00:39:50 — Alternatives: energy-based models, speculative decoding00:42:21 — Open research directions: complex workflows, RL00:44:56 — Non-verifiable RL domains &amp; model evaluation00:46:13 — Transition away from symbolic systems toward unified LLMs00:47:59 — Unified models vs specialized ones00:50:38 — Knowledge vs reasoning &amp; retrieval + reasoning00:52:24 — Vertical model specialization &amp; modules00:55:21 — Token count considerations for vertical domains00:56:09 — Low resource languages &amp; contextual learning00:59:22 — Origins: Dean’s early neural network work01:10:07 — AI for coding &amp; human–model interaction styles01:15:52 — Importance of crisp specification for coding agents01:19:23 — Prediction: personalized models &amp; state retrieval01:22:36 — Token-per-second targets (10k+) and reasoning throughput01:23:20 — Episode conclusion and thanksTranscriptAlessio Fanelli [00:00:04]: Hey everyone, welcome to the Latent Space podcast. This is Alessio, founder of Kernel Labs, and I’m joined by Swyx, editor of Latent Space. Shawn Wang [00:00:11]: Hello, hello. We’re here in the studio with Jeff Dean, chief AI scientist at Google. Welcome. Thanks for having me. It’s a bit surreal to have you in the studio. I’ve watched so many of your talks, and obviously your career has been super legendary. So, I mean, congrats. I think the first thing must be said, congrats on owning the Pareto Frontier.Jeff Dean [00:00:30]: Thank you, thank you. Pareto Frontiers are good. It’s good to be out there.Shawn Wang [00:00:34]: Yeah, I mean, I think it’s a combination of both. You have to own the Pareto Frontier. You have to have like frontier capability, but also efficiency, and then offer that range of models that people like to use. And, you know, some part of this was started because of your hardware work. Some part of that is your model work, and I’m sure there’s lots of secret sauce that you guys have worked on cumulatively. But, like, it’s really impressive to see it all come together in, like, this slittily advanced.Jeff Dean [00:01:04]: Yeah, yeah. I mean, I think, as you say, it’s not just one thing. It’s like a whole bunch of things up and down the stack. And, you know, all of those really combine to help make UNOS able to make highly capable large models, as well as, you know, software techniques to get those large model capabilities into much smaller, lighter weight models that are, you know, much more cost effective and lower latency, but still, you know, quite capable for their size. Yeah.Alessio Fanelli [00:01:31]: How much pressure do you have on, like, having the lower bound of the Pareto Frontier, too? I think, like, the new labs are always trying to push the top performance frontier because they need to raise more money and all of that. And you guys have billions of users. And I think initially when you worked on the CPU, you were thinking about, you know, if everybody that used Google, we use the voice model for, like, three minutes a day, they were like, you need to double your CPU number. Like, what’s that discussion today at Google? Like, how do you prioritize frontier versus, like, we have to do this? How do we actually need to deploy it if we build it?Jeff Dean [00:02:03]: Yeah, I mean, I think we always want to have models that are at the frontier or pushing the frontier because I think that’s where you see what capabilities now exist that didn’t exist at the sort of slightly less capable last year’s version or last six months ago version. At the same time, you know, we know those are going to be really useful for a bunch of use cases, but they’re going to be a bit slower and a bit more expensive than people might like for a bunch of other broader models. So I think what we want to do is always have kind of a highly capable sort of affordable model that enables a whole bunch of, you know, lower latency use cases. People can use them for agentic coding much more readily and then have the high-end, you know, frontier model that is really useful for, you know, deep reasoning, you know, solving really complicated math problems, those kinds of things. And it’s not that. One or the other is useful. They’re both useful. So I think we’d like to do both. And also, you know, through distillation, which is a key technique for making the smaller models more capable, you know, you have to have the frontier model in order to then distill it into your smaller model. So it’s not like an either or choice. You sort of need that in order to actually get a highly capable, more modest size model. Yeah.Alessio Fanelli [00:03:24]: I mean, you and Jeffrey came up with the solution in 2014.Jeff Dean [00:03:28]: Don’t forget, L’Oreal Vinyls as well. Yeah, yeah.Alessio Fanelli [00:03:30]: A long time ago. But like, I’m curious how you think about the cycle of these ideas, even like, you know, sparse models and, you know, how do you reevaluate them? How do you think about in the next generation of model, what is worth revisiting? Like, yeah, they’re just kind of like, you know, you worked on so many ideas that end up being influential, but like in the moment, they might not feel that way necessarily. Yeah.Jeff Dean [00:03:52]: I mean, I think distillation was originally motivated because we were seeing that we had a very large image data set at the time, you know, 300 million images that we could train on. And we were seeing that if you create specialists for different subsets of those image categories, you know, this one’s going to be really good at sort of mammals, and this one’s going to be really good at sort of indoor room scenes or whatever, and you can cluster those categories and train on an enriched stream of data after you do pre-training on a much broader set of images. You get much better performance. If you then treat that whole set of maybe 50 models you’ve trained as a large ensemble, but that’s not a very practical thing to serve, right? So distillation really came about from the idea of, okay, what if we want to actually serve that and train all these independent sort of expert models and then squish it into something that actually fits in a form factor that you can actually serve? And that’s, you know, not that different from what we’re doing today. You know, often today we’re instead of having an ensemble of 50 models. We’re having a much larger scale model that we then distill into a much smaller scale model.Shawn Wang [00:05:09]: Yeah. A part of me also wonders if distillation also has a story with the RL revolution. So let me maybe try to articulate what I mean by that, which is you can, RL basically spikes models in a certain part of the distribution. And then you have to sort of, well, you can spike models, but usually sometimes... It might be lossy in other areas and it’s kind of like an uneven technique, but you can probably distill it back and you can, I think that the sort of general dream is to be able to advance capabilities without regressing on anything else. And I think like that, that whole capability merging without loss, I feel like it’s like, you know, some part of that should be a distillation process, but I can’t quite articulate it. I haven’t seen much papers about it.Jeff Dean [00:06:01]: Yeah, I mean, I tend to think of one of the key advantages of distillation is that you can have a much smaller model and you can have a very large, you know, training data set and you can get utility out of making many passes over that data set because you’re now getting the logits from the much larger model in order to sort of coax the right behavior out of the smaller model that you wouldn’t otherwise get with just the hard labels. And so, you know, I think that’s what we’ve observed. Is you can get, you know, very close to your largest model performance with distillation approaches. And that seems to be, you know, a nice sweet spot for a lot of people because it enables us to kind of, for multiple Gemini generations now, we’ve been able to make the sort of flash version of the next generation as good or even substantially better than the previous generations pro. And I think we’re going to keep trying to do that because that seems like a good trend to follow.Shawn Wang [00:07:02]: So, Dara asked, so it was the original map was Flash Pro and Ultra. Are you just sitting on Ultra and distilling from that? Is that like the mother load?Jeff Dean [00:07:12]: I mean, we have a lot of different kinds of models. Some are internal ones that are not necessarily meant to be released or served. Some are, you know, our pro scale model and we can distill from that as well into our Flash scale model. So I think, you know, it’s an important set of capabilities to have and also inference time scaling. It can also be a useful thing to improve the capabilities of the model.Shawn Wang [00:07:35]: And yeah, yeah, cool. Yeah. And obviously, I think the economy of Flash is what led to the total dominance. I think the latest number is like 50 trillion tokens. I don’t know. I mean, obviously, it’s changing every day.Jeff Dean [00:07:46]: Yeah, yeah. But, you know, by market share, hopefully up.Shawn Wang [00:07:50]: No, I mean, there’s no I mean, there’s just the economics wise, like because Flash is so economical, like you can use it for everything. Like it’s in Gmail now. It’s in YouTube. Like it’s yeah. It’s in everything.Jeff Dean [00:08:02]: We’re using it more in our search products of various AI mode reviews.Shawn Wang [00:08:05]: Oh, my God. Flash past the AI mode. Oh, my God. Yeah, that’s yeah, I didn’t even think about that.Jeff Dean [00:08:10]: I mean, I think one of the things that is quite nice about the Flash model is not only is it more affordable, it’s also a lower latency. And I think latency is actually a pretty important characteristic for these models because we’re going to want models to do much more complicated things that are going to involve, you know, generating many more tokens from when you ask the model to do so. So, you know, if you’re going to ask the model to do something until it actually finishes what you ask it to do, because you’re going to ask now, not just write me a for loop, but like write me a whole software package to do X or Y or Z. And so having low latency systems that can do that seems really important. And Flash is one direction, one way of doing that. You know, obviously our hardware platforms enable a bunch of interesting aspects of our, you know, serving stack as well, like TPUs, the interconnect between. Chips on the TPUs is actually quite, quite high performance and quite amenable to, for example, long context kind of attention operations, you know, having sparse models with lots of experts. These kinds of things really, really matter a lot in terms of how do you make them servable at scale.Alessio Fanelli [00:09:19]: Yeah. Does it feel like there’s some breaking point for like the proto Flash distillation, kind of like one generation delayed? I almost think about almost like the capability as a. In certain tasks, like the pro model today is a saturated, some sort of task. So next generation, that same task will be saturated at the Flash price point. And I think for most of the things that people use models for at some point, the Flash model in two generation will be able to do basically everything. And how do you make it economical to like keep pushing the pro frontier when a lot of the population will be okay with the Flash model? I’m curious how you think about that.Jeff Dean [00:09:59]: I mean, I think that’s true. If your distribution of what people are asking people, the models to do is stationary, right? But I think what often happens is as the models become more capable, people ask them to do more, right? So, I mean, I think this happens in my own usage. Like I used to try our models a year ago for some sort of coding task, and it was okay at some simpler things, but wouldn’t do work very well for more complicated things. And since then, we’ve improved dramatically on the more complicated coding tasks. And now I’ll ask it to do much more complicated things. And I think that’s true, not just of coding, but of, you know, now, you know, can you analyze all the, you know, renewable energy deployments in the world and give me a report on solar panel deployment or whatever. That’s a very complicated, you know, more complicated task than people would have asked a year ago. And so you are going to want more capable models to push the frontier in the absence of what people ask the models to do. And that also then gives us. Insight into, okay, where does the, where do things break down? How can we improve the model in these, these particular areas, uh, in order to sort of, um, make the next generation even better.Alessio Fanelli [00:11:11]: Yeah. Are there any benchmarks or like test sets they use internally? Because it’s almost like the same benchmarks get reported every time. And it’s like, all right, it’s like 99 instead of 97. Like, how do you have to keep pushing the team internally to it? Or like, this is what we’re building towards. Yeah.Jeff Dean [00:11:26]: I mean, I think. Benchmarks, particularly external ones that are publicly available. Have their utility, but they often kind of have a lifespan of utility where they’re introduced and maybe they’re quite hard for current models. You know, I, I like to think of the best kinds of benchmarks are ones where the initial scores are like 10 to 20 or 30%, maybe, but not higher. And then you can sort of work on improving that capability for, uh, whatever it is, the benchmark is trying to assess and get it up to like 80, 90%, whatever. I, I think once it hits kind of 95% or something, you get very diminishing returns from really focusing on that benchmark, cuz it’s sort of, it’s either the case that you’ve now achieved that capability, or there’s also the issue of leakage in public data or very related kind of data being, being in your training data. Um, so we have a bunch of held out internal benchmarks that we really look at where we know that wasn’t represented in the training data at all. There are capabilities that we want the model to have. Um, yeah. Yeah. Um, that it doesn’t have now, and then we can work on, you know, assessing, you know, how do we make the model better at these kinds of things? Is it, we need different kind of data to train on that’s more specialized for this particular kind of task. Do we need, um, you know, a bunch of, uh, you know, architectural improvements or some sort of, uh, model capability improvements, you know, what would help make that better?Shawn Wang [00:12:53]: Is there, is there such an example that you, uh, a benchmark inspired in architectural improvement? Like, uh, I’m just kind of. Jumping on that because you just.Jeff Dean [00:13:02]: Uh, I mean, I think some of the long context capability of the, of the Gemini models that came, I guess, first in 1.5 really were about looking at, okay, we want to have, um, you know,Shawn Wang [00:13:15]: immediately everyone jumped to like completely green charts of like, everyone had, I was like, how did everyone crack this at the same time? Right. Yeah. Yeah.Jeff Dean [00:13:23]: I mean, I think, um, and once you’re set, I mean, as you say that needed single needle and a half. Hey, stack benchmark is really saturated for at least context links up to 1, 2 and K or something. Don’t actually have, you know, much larger than 1, 2 and 8 K these days or two or something. We’re trying to push the frontier of 1 million or 2 million context, which is good because I think there are a lot of use cases where. Yeah. You know, putting a thousand pages of text or putting, you know, multiple hour long videos and the context and then actually being able to make use of that as useful. Try to, to explore the über graduation are fairly large. But the single needle in a haystack benchmark is sort of saturated. So you really want more complicated, sort of multi-needle or more realistic, take all this content and produce this kind of answer from a long context that sort of better assesses what it is people really want to do with long context. Which is not just, you know, can you tell me the product number for this particular thing?Shawn Wang [00:14:31]: Yeah, it’s retrieval. It’s retrieval within machine learning. It’s interesting because I think the more meta level I’m trying to operate at here is you have a benchmark. You’re like, okay, I see the architectural thing I need to do in order to go fix that. But should you do it? Because sometimes that’s an inductive bias, basically. It’s what Jason Wei, who used to work at Google, would say. Exactly the kind of thing. Yeah, you’re going to win. Short term. Longer term, I don’t know if that’s going to scale. You might have to undo that.Jeff Dean [00:15:01]: I mean, I like to sort of not focus on exactly what solution we’re going to derive, but what capability would you want? And I think we’re very convinced that, you know, long context is useful, but it’s way too short today. Right? Like, I think what you would really want is, can I attend to the internet while I answer my question? Right? But that’s not going to happen. I think that’s going to be solved by purely scaling the existing solutions, which are quadratic. So a million tokens kind of pushes what you can do. You’re not going to do that to a trillion tokens, let alone, you know, a billion tokens, let alone a trillion. But I think if you could give the illusion that you can attend to trillions of tokens, that would be amazing. You’d find all kinds of uses for that. You would have attend to the internet. You could attend to the pixels of YouTube and the sort of deeper representations that we can find. You could attend to the form for a single video, but across many videos, you know, on a personal Gemini level, you could attend to all of your personal state with your permission. So like your emails, your photos, your docs, your plane tickets you have. I think that would be really, really useful. And the question is, how do you get algorithmic improvements and system level improvements that get you to something where you actually can attend to trillions of tokens? Right. In a meaningful way. Yeah.Shawn Wang [00:16:26]: But by the way, I think I did some math and it’s like, if you spoke all day, every day for eight hours a day, you only generate a maximum of like a hundred K tokens, which like very comfortably fits.Jeff Dean [00:16:38]: Right. But if you then say, okay, I want to be able to understand everything people are putting on videos.Shawn Wang [00:16:46]: Well, also, I think that the classic example is you start going beyond language into like proteins and whatever else is extremely information dense. Yeah. Yeah.Jeff Dean [00:16:55]: I mean, I think one of the things about Gemini’s multimodal aspects is we’ve always wanted it to be multimodal from the start. And so, you know, that sometimes to people means text and images and video sort of human-like and audio, audio, human-like modalities. But I think it’s also really useful to have Gemini know about non-human modalities. Yeah. Like LIDAR sensor data from. Yes. Say, Waymo vehicles or. Like robots or, you know, various kinds of health modalities, x-rays and MRIs and imaging and genomics information. And I think there’s probably hundreds of modalities of data where you’d like the model to be able to at least be exposed to the fact that this is an interesting modality and has certain meaning in the world. Where even if you haven’t trained on all the LIDAR data or MRI data, you could have, because maybe that’s not, you know, it doesn’t make sense in terms of trade-offs of. You know, what you include in your main pre-training data mix, at least including a little bit of it is actually quite useful. Yeah. Because it sort of tempts the model that this is a thing.Shawn Wang [00:18:04]: Yeah. Do you believe, I mean, since we’re on this topic and something I just get to ask you all the questions I always wanted to ask, which is fantastic. Like, are there some king modalities, like modalities that supersede all the other modalities? So a simple example was Vision can, on a pixel level, encode text. And DeepSeq had this DeepSeq CR paper that did that. Vision. And Vision has also been shown to maybe incorporate audio because you can do audio spectrograms and that’s, that’s also like a Vision capable thing. Like, so, so maybe Vision is just the king modality and like. Yeah.Jeff Dean [00:18:36]: I mean, Vision and Motion are quite important things, right? Motion. Well, like video as opposed to static images, because I mean, there’s a reason evolution has evolved eyes like 23 independent ways, because it’s such a useful capability for sensing the world around you, which is really what we want these models to be. So I think the only thing that we can be able to do is interpret the things we’re seeing or the things we’re paying attention to and then help us in using that information to do things. Yeah.Shawn Wang [00:19:05]: I think motion, you know, I still want to shout out, I think Gemini, still the only native video understanding model that’s out there. So I use it for YouTube all the time. Nice.Jeff Dean [00:19:15]: Yeah. Yeah. I mean, it’s actually, I think people kind of are not necessarily aware of what the Gemini models can actually do. Yeah. Like I have an example I’ve used in one of my talks. It had like, it was like a YouTube highlight video of 18 memorable sports moments across the last 20 years or something. So it has like Michael Jordan hitting some jump shot at the end of the finals and, you know, some soccer goals and things like that. And you can literally just give it the video and say, can you please make me a table of what all these different events are? What when the date is when they happened? And a short description. And so you get like now an 18 row table of that information extracted from the video, which is, you know, not something most people think of as like a turn video into sequel like table.Alessio Fanelli [00:20:11]: Has there been any discussion inside of Google of like, you mentioned tending to the whole internet, right? Google, it’s almost built because a human cannot tend to the whole internet and you need some sort of ranking to find what you need. Yep. That ranking is like much different for an LLM because you can expect a person to look at maybe the first five, six links in a Google search versus for an LLM. Should you expect to have 20 links that are highly relevant? Like how do you internally figure out, you know, how do we build the AI mode that is like maybe like much broader search and span versus like the more human one? Yeah.Jeff Dean [00:20:47]: I mean, I think even pre-language model based work, you know, our ranking systems would be built to start. I mean, I think even pre-language model based work, you know, our ranking systems would be built to start. With a giant number of web pages in our index, many of them are not relevant. So you identify a subset of them that are relevant with very lightweight kinds of methods. You know, you’re down to like 30,000 documents or something. And then you gradually refine that to apply more and more sophisticated algorithms and more and more sophisticated sort of signals of various kinds in order to get down to ultimately what you show, which is, you know, the final 10 results or, you know, 10 results plus. Other kinds of information. And I think an LLM based system is not going to be that dissimilar, right? You’re going to attend to trillions of tokens, but you’re going to want to identify, you know, what are the 30,000 ish documents that are with the, you know, maybe 30 million interesting tokens. And then how do you go from that into what are the 117 documents I really should be paying attention to in order to carry out the tasks that the user has asked? And I think, you know, you can imagine systems where you have, you know, a lot of highly parallel processing to identify those initial 30,000 candidates, maybe with very lightweight kinds of models. Then you have some system that sort of helps you narrow down from 30,000 to the 117 with maybe a little bit more sophisticated model or set of models. And then maybe the final model is the thing that looks. So the 117 things that might be your most capable model. So I think it has to, it’s going to be some system like that, that is really enables you to give the illusion of attending to trillions of tokens. Sort of the way Google search gives you, you know, not the illusion, but you are searching the internet, but you’re finding, you know, a very small subset of things that are, that are relevant.Shawn Wang [00:22:47]: Yeah. I often tell a lot of people that are not steeped in like Google search history that, well, you know, like Bert was. Like he was like basically immediately inside of Google search and that improves results a lot, right? Like I don’t, I don’t have any numbers off the top of my head, but like, I’m sure you guys, that’s obviously the most important numbers to Google. Yeah.Jeff Dean [00:23:08]: I mean, I think going to an LLM based representation of text and words and so on enables you to get out of the explicit hard notion of, of particular words having to be on the page, but really getting at the notion of this topic of this page or this page. Paragraph is highly relevant to this query. Yeah.Shawn Wang [00:23:28]: I don’t think people understand how much LLMs have taken over all these very high traffic system, very high traffic. Yeah. Like it’s Google, it’s YouTube. YouTube has this like semantics ID thing where it’s just like every token or every item in the vocab is a YouTube video or something that predicts the video using a code book, which is absurd to me for YouTube size.Jeff Dean [00:23:50]: And then most recently GROK also for, for XAI, which is like, yeah. I mean, I’ll call out even before LLMs were used extensively in search, we put a lot of emphasis on softening the notion of what the user actually entered into the query.Shawn Wang [00:24:06]: So do you have like a history of like, what’s the progression? Oh yeah.Jeff Dean [00:24:09]: I mean, I actually gave a talk in, uh, I guess, uh, web search and data mining conference in 2009, uh, where we never actually published any papers about the origins of Google search, uh, sort of, but we went through sort of four or five or six. generations, four or five or six generations of, uh, redesigning of the search and retrieval system, uh, from about 1999 through 2004 or five. And that talk is really about that evolution. And one of the things that really happened in 2001 was we were sort of working to scale the system in multiple dimensions. So one is we wanted to make our index bigger, so we could retrieve from a larger index, which always helps your quality in general. Uh, because if you don’t have the page in your index, you’re going to not do well. Um, and then we also needed to scale our capacity because we were, our traffic was growing quite extensively. Um, and so we had, you know, a sharded system where you have more and more shards as the index grows, you have like 30 shards. And then if you want to double the index size, you make 60 shards so that you can bound the latency by which you respond for any particular user query. Um, and then as traffic grows, you add, you add more and more replicas of each of those. And so we eventually did the math that realized that in a data center where we had say 60 shards and, um, you know, 20 copies of each shard, we now had 1200 machines, uh, with disks. And we did the math and we’re like, Hey, one copy of that index would actually fit in memory across 1200 machines. So in 2001, we introduced, uh, we put our entire index in memory and what that enabled from a quality perspective was amazing. Um, and so we had more and more replicas of each of those. Before you had to be really careful about, you know, how many different terms you looked at for a query, because every one of them would involve a disk seek on every one of the 60 shards. And so you, as you make your index bigger, that becomes even more inefficient. But once you have the whole index in memory, it’s totally fine to have 50 terms you throw into the query from the user’s original three or four word query, because now you can add synonyms like restaurant and restaurants and cafe and, uh, you know, things like that. Uh, bistro and all these things. And you can suddenly start, uh, sort of really, uh, getting at the meaning of the word as opposed to the exact semantic form the user typed in. And that was, you know, 2001, very much pre LLM, but really it was about softening the, the strict definition of what the user typed in order to get at the meaning.Alessio Fanelli [00:26:47]: What are like principles that you use to like design the systems, especially when you have, I mean, in 2001, the internet is like. Doubling, tripling every year in size is not like, uh, you know, and I think today you kind of see that with LLMs too, where like every year the jumps in size and like capabilities are just so big. Are there just any, you know, principles that you use to like, think about this? Yeah.Jeff Dean [00:27:08]: I mean, I think, uh, you know, first, whenever you’re designing a system, you want to understand what are the sort of design parameters that are going to be most important in designing that, you know? So, you know, how many queries per second do you need to handle? How big is the internet? How big is the index you need to handle? How much data do you need to keep for every document in the index? How are you going to look at it when you retrieve things? Um, what happens if traffic were to double or triple, you know, will that system work well? And I think a good design principle is you’re going to want to design a system so that the most important characteristics could scale by like factors of five or 10, but probably not beyond that because often what happens is if you design a system for X. And something suddenly becomes a hundred X, that would enable a very different point in the design space that would not make sense at X. But all of a sudden at a hundred X makes total sense. So like going from a disk space index to a in memory index makes a lot of sense once you have enough traffic, because now you have enough replicas of the sort of state on disk that those machines now actually can hold, uh, you know, a full copy of the, uh, index and memory. Yeah. And that all of a sudden enabled. A completely different design that wouldn’t have been practical before. Yeah. Um, so I’m, I’m a big fan of thinking through designs in your head, just kind of playing with the design space a little before you actually do a lot of writing of code. But, you know, as you said, in the early days of Google, we were growing the index, uh, quite extensively. We were growing the update rate of the index. So the update rate actually is the parameter that changed the most. Surprising. So it used to be once a month.Shawn Wang [00:28:55]: Yeah.Jeff Dean [00:28:56]: And then we went to a system that could update any particular page in like sub one minute. Okay.Shawn Wang [00:29:02]: Yeah. Because this is a competitive advantage, right?Jeff Dean [00:29:04]: Because all of a sudden news related queries, you know, if you’re, if you’ve got last month’s news index, it’s not actually that useful for.Shawn Wang [00:29:11]: News is a special beast. Was there any, like you could have split it onto a separate system.Jeff Dean [00:29:15]: Well, we did. We launched a Google news product, but you also want news related queries that people type into the main index to also be sort of updated.Shawn Wang [00:29:23]: So, yeah, it’s interesting. And then you have to like classify whether the page is, you have to decide which pages should be updated and what frequency. Oh yeah.Jeff Dean [00:29:30]: There’s a whole like, uh, system behind the scenes that’s trying to decide update rates and importance of the pages. So even if the update rate seems low, you might still want to recrawl important pages quite often because, uh, the likelihood they change might be low, but the value of having updated is high.Shawn Wang [00:29:50]: Yeah, yeah, yeah, yeah. Uh, well, you know, yeah. This, uh, you know, mention of latency and, and saving things to this reminds me of one of your classics, which I have to bring up, which is latency numbers. Every programmer should know, uh, was there a, was it just a, just a general story behind that? Did you like just write it down?Jeff Dean [00:30:06]: I mean, this has like sort of eight or 10 different kinds of metrics that are like, how long does a cache mistake? How long does branch mispredict take? How long does a reference domain memory take? How long does it take to send, you know, a packet from the U S to the Netherlands or something? Um,Shawn Wang [00:30:21]: why Netherlands, by the way, or is it, is that because of Chrome?Jeff Dean [00:30:25]: Uh, we had a data center in the Netherlands, um, so, I mean, I think this gets to the point of being able to do the back of the envelope calculations. So these are sort of the raw ingredients of those, and you can use them to say, okay, well, if I need to design a system to do image search and thumb nailing or something of the result page, you know, how, what I do that I could pre-compute the image thumbnails. I could like. Try to thumbnail them on the fly from the larger images. What would that do? How much dis bandwidth than I need? How many des seeks would I do? Um, and you can sort of actually do thought experiments in, you know, 30 seconds or a minute with the sort of, uh, basic, uh, basic numbers at your fingertips. Uh, and then as you sort of build software using higher level libraries, you kind of want to develop the same intuitions for how long does it take to, you know, look up something in this particular kind of.Shawn Wang [00:31:21]: I’ll see you next time.Shawn Wang [00:31:51]: Which is a simple byte conversion. That’s nothing interesting. I wonder if you have any, if you were to update your...Jeff Dean [00:31:58]: I mean, I think it’s really good to think about calculations you’re doing in a model, either for training or inference.Jeff Dean [00:32:09]: Often a good way to view that is how much state will you need to bring in from memory, either like on-chip SRAM or HBM from the accelerator. Attached memory or DRAM or over the network. And then how expensive is that data motion relative to the cost of, say, an actual multiply in the matrix multiply unit? And that cost is actually really, really low, right? Because it’s order, depending on your precision, I think it’s like sub one picodule.Shawn Wang [00:32:50]: Oh, okay. You measure it by energy. Yeah. Yeah.Jeff Dean [00:32:52]: Yeah. I mean, it’s all going to be about energy and how do you make the most energy efficient system. And then moving data from the SRAM on the other side of the chip, not even off the off chip, but on the other side of the same chip can be, you know, a thousand picodules. Oh, yeah. And so all of a sudden, this is why your accelerators require batching. Because if you move, like, say, the parameter of a model from SRAM on the, on the chip into the multiplier unit, that’s going to cost you a thousand picodules. So you better make use of that, that thing that you moved many, many times with. So that’s where the batch dimension comes in. Because all of a sudden, you know, if you have a batch of 256 or something, that’s not so bad. But if you have a batch of one, that’s really not good.Shawn Wang [00:33:40]: Yeah. Yeah. Right.Jeff Dean [00:33:41]: Because then you paid a thousand picodules in order to do your one picodule multiply.Shawn Wang [00:33:46]: I have never heard an energy-based analysis of batching.Jeff Dean [00:33:50]: Yeah. I mean, that’s why people batch. Yeah. Ideally, you’d like to use batch size one because the latency would be great.Shawn Wang [00:33:56]: The best latency.Jeff Dean [00:33:56]: But the energy cost and the compute cost inefficiency that you get is quite large. So, yeah.Shawn Wang [00:34:04]: Is there a similar trick like, like, like you did with, you know, putting everything in memory? Like, you know, I think obviously NVIDIA has caused a lot of waves with betting very hard on SRAM with Grok. I wonder if, like, that’s something that you already saw with, with the TPUs, right? Like that, that you had to. Uh, to serve at your scale, uh, you probably sort of saw that coming. Like what, what, what hardware, uh, innovations or insights were formed because of what you’re seeing there?Jeff Dean [00:34:33]: Yeah. I mean, I think, you know, TPUs have this nice, uh, sort of regular structure of 2D or 3D meshes with a bunch of chips connected. Yeah. And each one of those has HBM attached. Um, I think for serving some kinds of models, uh, you know, you, you pay a lot higher cost. Uh, and time latency, um, bringing things in from HBM than you do bringing them in from, uh, SRAM on the chip. So if you have a small enough model, you can actually do model parallelism, spread it out over lots of chips and you actually get quite good throughput improvements and latency improvements from doing that. And so you’re now sort of striping your smallish scale model over say 16 or 64 chips. Uh, but as if you do that and it all fits in. In SRAM, uh, that can be a big win. So yeah, that’s not a surprise, but it is a good technique.Alessio Fanelli [00:35:27]: Yeah. What about the TPU design? Like how much do you decide where the improvements have to go? So like, this is like a good example of like, is there a way to bring the thousand picojoules down to 50? Like, is it worth designing a new chip to do that? The extreme is like when people say, oh, you should burn the model on the ASIC and that’s kind of like the most extreme thing. How much of it? Is it worth doing an hardware when things change so quickly? Like what was the internal discussion? Yeah.Jeff Dean [00:35:57]: I mean, we, we have a lot of interaction between say the TPU chip design architecture team and the sort of higher level modeling, uh, experts, because you really want to take advantage of being able to co-design what should future TPUs look like based on where we think the sort of ML research puck is going, uh, in some sense, because, uh, you know, as a hardware designer for ML and in particular, you’re trying to design a chip starting today and that design might take two years before it even lands in a data center. And then it has to sort of be a reasonable lifetime of the chip to take you three, four or five years. So you’re trying to predict two to six years out where, what ML computations will people want to run two to six years out in a very fast changing field. And so having people with interest. Interesting ML research ideas of things we think will start to work in that timeframe or will be more important in that timeframe, uh, really enables us to then get, you know, interesting hardware features put into, you know, TPU N plus two, where TPU N is what we have today.Shawn Wang [00:37:10]: Oh, the cycle time is plus two.Jeff Dean [00:37:12]: Roughly. Wow. Because, uh, I mean, sometimes you can squeeze some changes into N plus one, but, you know, bigger changes are going to require the chip. Yeah. Design be earlier in its lifetime design process. Um, so whenever we can do that, it’s generally good. And sometimes you can put in speculative features that maybe won’t cost you much chip area, but if it works out, it would make something, you know, 10 times as fast. And if it doesn’t work out, well, you burned a little bit of tiny amount of your chip area on that thing, but it’s not that big a deal. Uh, sometimes it’s a very big change and we want to be pretty sure this is going to work out. So we’ll do like lots of carefulness. Uh, ML experimentation to show us, uh, this is actually the, the way we want to go. Yeah.Alessio Fanelli [00:37:58]: Is there a reverse of like, we already committed to this chip design so we can not take the model architecture that way because it doesn’t quite fit?Jeff Dean [00:38:06]: Yeah. I mean, you, you definitely have things where you’re going to adapt what the model architecture looks like so that they’re efficient on the chips that you’re going to have for both training and inference of that, of that, uh, generation of model. So I think it kind of goes both ways. Um, you know, sometimes you can take advantage of, you know, lower precision things that are coming in a future generation. So you can, might train it at that lower precision, even if the current generation doesn’t quite do that. Mm.Shawn Wang [00:38:40]: Yeah. How low can we go in precision?Jeff Dean [00:38:43]: Because people are saying like ternary is like, uh, yeah, I mean, I’m a big fan of very low precision because I think that gets, that saves you a tremendous amount of time. Right. Because it’s picojoules per bit that you’re transferring and reducing the number of bits is a really good way to, to reduce that. Um, you know, I think people have gotten a lot of luck, uh, mileage out of having very low bit precision things, but then having scaling factors that apply to a whole bunch of, uh, those, those weights. Scaling. How does it, how does it, okay.Shawn Wang [00:39:15]: Interesting. You, so low, low precision, but scaled up weights. Yeah. Huh. Yeah. Never considered that. Yeah. Interesting. Uh, w w while we’re on this topic, you know, I think there’s a lot of, um, uh, this, the concept of precision at all is weird when we’re sampling, you know, uh, we just, at the end of this, we’re going to have all these like chips that I’ll do like very good math. And then we’re just going to throw a random number generator at the start. So, I mean, there’s a movement towards, uh, energy based, uh, models and processors. I’m just curious if you’ve, obviously you’ve thought about it, but like, what’s your commentary?Jeff Dean [00:39:50]: Yeah. I mean, I think. There’s a bunch of interesting trends though. Energy based models is one, you know, diffusion based models, which don’t sort of sequentially decode tokens is another, um, you know, speculative decoding is a way that you can get sort of an equivalent, very small.Shawn Wang [00:40:06]: Draft.Jeff Dean [00:40:07]: Batch factor, uh, for like you predict eight tokens out and that enables you to sort of increase the effective batch size of what you’re doing by a factor of eight, even, and then you maybe accept five or six of those tokens. So you get. A five, a five X improvement in the amortization of moving weights, uh, into the multipliers to do the prediction for the, the tokens. So these are all really good techniques and I think it’s really good to look at them from the lens of, uh, energy, real energy, not energy based models, um, and, and also latency and throughput, right? If you look at things from that lens, that sort of guides you to. Two solutions that are gonna be, uh, you know, better from, uh, you know, being able to serve larger models or, you know, equivalent size models more cheaply and with lower latency.Shawn Wang [00:41:03]: Yeah. Well, I think, I think I, um, it’s appealing intellectually, uh, haven’t seen it like really hit the mainstream, but, um, I do think that, uh, there’s some poetry in the sense that, uh, you know, we don’t have to do, uh, a lot of shenanigans if like we fundamentally. Design it into the hardware. Yeah, yeah.Jeff Dean [00:41:23]: I mean, I think there’s still a, there’s also sort of the more exotic things like analog based, uh, uh, computing substrates as opposed to digital ones. Uh, I’m, you know, I think those are super interesting cause they can be potentially low power. Uh, but I think you often end up wanting to interface that with digital systems and you end up losing a lot of the power advantages in the digital to analog and analog to digital conversions. You end up doing, uh, at the sort of boundaries. And periphery of that system. Um, I still think there’s a tremendous distance we can go from where we are today in terms of energy efficiency with sort of, uh, much better and specialized hardware for the models we care about.Shawn Wang [00:42:05]: Yeah.Alessio Fanelli [00:42:06]: Um, any other interesting research ideas that you’ve seen, or like maybe things that you cannot pursue a Google that you would be interested in seeing researchers take a step at, I guess you have a lot of researchers. Yeah, I guess you have enough, but our, our research.Jeff Dean [00:42:21]: Our research portfolio is pretty broad. I would say, um, I mean, I think, uh, in terms of research directions, there’s a whole bunch of, uh, you know, open problems and how do you make these models reliable and able to do much longer, kind of, uh, more complex tasks that have lots of subtasks. How do you orchestrate, you know, maybe one model that’s using other models as tools in order to sort of build, uh, things that can accomplish, uh, you know, much more. Yeah. Significant pieces of work, uh, collectively, then you would ask a single model to do. Um, so that’s super interesting. How do you get more verifiable, uh, you know, how do you get RL to work for non-verifiable domains? I think it’s a pretty interesting open problem because I think that would broaden out the capabilities of the models, the improvements that you’re seeing in both math and coding. Uh, if we could apply those to other less verifiable domains, because we’ve come up with RL techniques that actually enable us to do that. Uh, effectively, that would, that would really make the models improve quite a lot. I think.Alessio Fanelli [00:43:26]: I’m curious, like when we had Noam Brown on the podcast, he said, um, they already proved you can do it with deep research. Um, you kind of have it with AI mode in a way it’s not verifiable. I’m curious if there’s any thread that you think is interesting there. Like what is it? Both are like information retrieval of JSON. So I wonder if it’s like the retrieval is like the verifiable part. That you can score or what are like, yeah, yeah. How, how would you model that, that problem?Jeff Dean [00:43:55]: Yeah. I mean, I think there are ways of having other models that can evaluate the results of what a first model did, maybe even retrieving. Can you have another model that says, is this things, are these things you retrieved relevant? Or can you rate these 2000 things you retrieved to assess which ones are the 50 most relevant or something? Um, I think those kinds of techniques are actually quite effective. Sometimes I can even be the same model, just prompted differently to be a, you know, a critic as opposed to a, uh, actual retrieval system. Yeah.Shawn Wang [00:44:28]: Um, I do think like there, there is that, that weird cliff where like, it feels like we’ve done the easy stuff and then now it’s, but it always feels like that every year. It’s like, oh, like we know, we know, and the next part is super hard and nobody’s figured it out. And, uh, exactly with this RLVR thing where like everyone’s talking about, well, okay, how do we. the next stage of the non-verifiable stuff. And everyone’s like, I don’t know, you know, Ellen judge.Jeff Dean [00:44:56]: I mean, I feel like the nice thing about this field is there’s lots and lots of smart people thinking about creative solutions to some of the problems that we all see. Uh, because I think everyone sort of sees that the models, you know, are great at some things and they fall down around the edges of those things and, and are not as capable as we’d like in those areas. And then coming up with good techniques and trying those. And seeing which ones actually make a difference is sort of what the whole research aspect of this field is, is pushing forward. And I think that’s why it’s super interesting. You know, if you think about two years ago, we were struggling with GSM, eight K problems, right? Like, you know, Fred has two rabbits. He gets three more rabbits. How many rabbits does he have? That’s a pretty far cry from the kinds of mathematics that the models can, and now you’re doing IMO and Erdos problems in pure language. Yeah. Yeah. Pure language. So that is a really, really amazing jump in capabilities in, you know, in a year and a half or something. And I think, um, for other areas, it’d be great if we could make that kind of leap. Uh, and you know, we don’t exactly see how to do it for some, some areas, but we do see it for some other areas and we’re going to work hard on making that better. Yeah.Shawn Wang [00:46:13]: Yeah.Alessio Fanelli [00:46:14]: Like YouTube thumbnail generation. That would be very helpful. We need that. That would be AGI. We need that.Shawn Wang [00:46:20]: That would be. As far as content creators go.Jeff Dean [00:46:22]: I guess I’m not a YouTube creator, so I don’t care that much about that problem, but I guess, uh, many people do.Shawn Wang [00:46:27]: It does. Yeah. It doesn’t, it doesn’t matter. People do judge books by their covers as it turns out. Um, uh, just to draw a bit on the IMO goal. Um, I’m still not over the fact that a year ago we had alpha proof and alpha geometry and all those things. And then this year we were like, screw that we’ll just chuck it into Gemini. Yeah. What’s your reflection? Like, I think this, this question about. Like the merger of like symbolic systems and like, and, and LMS, uh, was a very much core belief. And then somewhere along the line, people would just said, Nope, we’ll just all do it in the LLM.Jeff Dean [00:47:02]: Yeah. I mean, I think it makes a lot of sense to me because, you know, humans manipulate symbols, but we probably don’t have like a symbolic representation in our heads. Right. We have some distributed representation that is neural net, like in some way of lots of different neurons. And activation patterns firing when we see certain things and that enables us to reason and plan and, you know, do chains of thought and, you know, roll them back now that, that approach for solving the problem doesn’t seem like it’s going to work. I’m going to try this one. And, you know, in a lot of ways we’re emulating what we intuitively think, uh, is happening inside real brains in neural net based models. So it never made sense to me to have like completely separate. Uh, discrete, uh, symbolic things, and then a completely different way of, of, uh, you know, thinking about those things.Shawn Wang [00:47:59]: Interesting. Yeah. Uh, I mean, it’s maybe seems obvious to you, but it wasn’t obvious to me a year ago. Yeah.Jeff Dean [00:48:06]: I mean, I do think like that IMO with, you know, translating to lean and using lean and then the next year and also a specialized geometry model. And then this year switching to a single unified model. That is roughly the production model with a little bit more inference budget, uh, is actually, you know, quite good because it shows you that the capabilities of that general model have improved dramatically and, and now you don’t need the specialized model. This is actually sort of very similar to the 2013 to 16 era of machine learning, right? Like it used to be, people would train separate models for lots of different, each different problem, right? I have, I want to recognize street signs and something. So I train a street sign. Recognition recognition model, or I want to, you know, decode speech recognition. I have a speech model, right? I think now the era of unified models that do everything is really upon us. And the question is how well do those models generalize to new things they’ve never been asked to do and they’re getting better and better.Shawn Wang [00:49:10]: And you don’t need domain experts. Like one of my, uh, so I interviewed ETA who was on, who was on that team. Uh, and he was like, yeah, I, I don’t know how they work. I don’t know where the IMO competition was held. I don’t know the rules of it. I just trained the models, the training models. Yeah. Yeah. And it’s kind of interesting that like people with these, this like universal skill set of just like machine learning, you just give them data and give them enough compute and they can kind of tackle any task, which is the bitter lesson, I guess. I don’t know. Yeah.Jeff Dean [00:49:39]: I mean, I think, uh, general models, uh, will win out over specialized ones in most cases.Shawn Wang [00:49:45]: Uh, so I want to push there a bit. I think there’s one hole here, which is like, uh. There’s this concept of like, uh, maybe capacity of a model, like abstractly a model can only contain the number of bits that it has. And, uh, and so it, you know, God knows like Gemini pro is like one to 10 trillion parameters. We don’t know, but, uh, the Gemma models, for example, right? Like a lot of people want like the open source local models that are like that, that, that, and, and, uh, they have some knowledge, which is not necessary, right? Like they can’t know everything like, like you have the. The luxury of you have the big model and big model should be able to capable of everything. But like when, when you’re distilling and you’re going down to the small models, you know, you’re actually memorizing things that are not useful. Yeah. And so like, how do we, I guess, do we want to extract that? Can we, can we divorce knowledge from reasoning, you know?Jeff Dean [00:50:38]: Yeah. I mean, I think you do want the model to be most effective at reasoning if it can retrieve things, right? Because having the model devote precious parameter space. To remembering obscure facts that could be looked up is actually not the best use of that parameter space, right? Like you might prefer something that is more generally useful in more settings than this obscure fact that it has. Um, so I think that’s always attention at the same time. You also don’t want your model to be kind of completely detached from, you know, knowing stuff about the world, right? Like it’s probably useful to know how long the golden gate be. Bridges just as a general sense of like how long are bridges, right? And, uh, it should have that kind of knowledge. It maybe doesn’t need to know how long some teeny little bridge in some other more obscure part of the world is, but, uh, it does help it to have a fair bit of world knowledge and the bigger your model is, the more you can have. Uh, but I do think combining retrieval with sort of reasoning and making the model really good at doing multiple stages of retrieval. Yeah.Shawn Wang [00:51:49]: And reasoning through the intermediate retrieval results is going to be a, a pretty effective way of making the model seem much more capable, because if you think about, say, a personal Gemini, yeah, right?Jeff Dean [00:52:01]: Like we’re not going to train Gemini on my email. Probably we’d rather have a single model that, uh, we can then use and use being able to retrieve from my email as a tool and have the model reason about it and retrieve from my photos or whatever, uh, and then make use of that and have multiple. Um, you know, uh, stages of interaction. that makes sense.Alessio Fanelli [00:52:24]: Do you think the vertical models are like, uh, interesting pursuit? Like when people are like, oh, we’re building the best healthcare LLM, we’re building the best law LLM, are those kind of like short-term stopgaps or?Jeff Dean [00:52:37]: No, I mean, I think, I think vertical models are interesting. Like you want them to start from a pretty good base model, but then you can sort of, uh, sort of viewing them, view them as enriching the data. Data distribution for that particular vertical domain for healthcare, say, um, we’re probably not going to train or for say robotics. We’re probably not going to train Gemini on all possible robotics data. We, you could train it on because we want it to have a balanced set of capabilities. Um, so we’ll expose it to some robotics data, but if you’re trying to build a really, really good robotics model, you’re going to want to start with that and then train it on more robotics data. And then maybe that would. It’s multilingual translation capability, but improve its robotics capabilities. And we’re always making these kind of, uh, you know, trade-offs in the data mix that we train the base Gemini models on. You know, we’d love to include data from 200 more languages and as much data as we have for those languages, but that’s going to displace some other capabilities of the model. It won’t be as good at, um, you know, Pearl programming, you know, it’ll still be good at Python programming. Cause we’ll include it. Enough. Of that, but there’s other long tail computer languages or coding capabilities that it may suffer on or multi, uh, multimodal reasoning capabilities may suffer. Cause we didn’t get to expose it to as much data there, but it’s really good at multilingual things. So I, I think some combination of specialized models, maybe more modular models. So it’d be nice to have the capability to have those 200 languages, plus this awesome robotics model, plus this awesome healthcare, uh, module that all can be knitted together to work in concert and called upon in different circumstances. Right? Like if I have a health related thing, then it should enable using this health module in conjunction with the main base model to be even better at those kinds of things. Yeah.Shawn Wang [00:54:36]: Installable knowledge. Yeah.Jeff Dean [00:54:37]: Right.Shawn Wang [00:54:38]: Just download as a, as a package.Jeff Dean [00:54:39]: And some of that installable stuff can come from retrieval, but some of it probably should come from preloaded training on, you know, uh, a hundred billion tokens or a trillion tokens of health data. Yeah.Shawn Wang [00:54:51]: And for listeners, I think, uh, I will highlight the Gemma three end paper where they, there was a little bit of that, I think. Yeah.Alessio Fanelli [00:54:56]: Yeah. I guess the question is like, how many billions of tokens do you need to outpace the frontier model improvements? You know, it’s like, if I have to make this model better healthcare and the main. Gemini model is still improving. Do I need 50 billion tokens? Can I do it with a hundred, if I need a trillion healthcare tokens, it’s like, they’re probably not out there that you don’t have, you know, I think that’s really like the.Jeff Dean [00:55:21]: Well, I mean, I think healthcare is a particularly challenging domain, so there’s a lot of healthcare data that, you know, we don’t have access to appropriately, but there’s a lot of, you know, uh, healthcare organizations that want to train models on their own data. That is not public healthcare data, uh, not public health. But public healthcare data. Um, so I think there are opportunities there to say, partner with a large healthcare organization and train models for their use that are going to be, you know, more bespoke, but probably, uh, might be better than a general model trained on say, public data. Yeah.Shawn Wang [00:55:58]: Yeah. I, I believe, uh, by the way, also this is like somewhat related to the language conversation. Uh, I think one of your, your favorite examples was you can put a low resource language in the context and it just learns. Yeah.Jeff Dean [00:56:09]: Oh, yeah, I think the example we used was Calamon, which is truly low resource because it’s only spoken by, I think 120 people in the world and there’s no written text.Shawn Wang [00:56:20]: So, yeah. So you can just do it that way. Just put it in the context. Yeah. Yeah. But I think your whole data set in the context, right.Jeff Dean [00:56:27]: If you, if you take a language like, uh, you know, Somali or something, there is a fair bit of Somali text in the world that, uh, or Ethiopian Amharic or something, um, you know, we probably. Yeah. Are not putting all the data from those languages into the Gemini based training. We put some of it, but if you put more of it, you’ll improve the capabilities of those models.Shawn Wang [00:56:49]: Yeah.Jeff Dean [00:56:49]: So, or of those languages.Shawn Wang [00:56:52]: Uh, yeah, cool. Uh, it’s, uh, I have a side interest in linguistics. I, I, I did, uh, uh, a few classes back in college and like, uh, part of me, like if I was a linguist and I could have access to all these models, I would just be asking really fundamental questions about language itself. Yeah. Like, uh, one is th there’s one very obvious one, which is Sapir-Whorf, like how much does like the language that you speak affect your thinking, but then also there’s some languages where there’s just concepts that are not represented in other languages, but some others, many others that are just duplicates, right. Where, uh, there’s also another paper that people love called the platonic representation where, you know, like the, the, an image of a cup is, uh, if you say learn a model on that and you, you, you have a lot of texts with the word cup eventually maps to it, like roughly the same place. And so like that should apply to languages except where it doesn’t. And that’s actually like very interesting differences in what humanity has discovered as concepts that maybe English doesn’t have.Shawn Wang [00:57:54]: I don’t know. It’s just like my, my rant on languages. Yeah.Jeff Dean [00:57:58]: I mean, I, I did some work on a early model that fused together a language based model with you have, you know, nice word based representations and then an image model where you have. Trained it on image net like things. Yes. And then you fuse together the top layers of, uh, no, this is devise, uh, uh, the, you do a little bit more training to fuse together those representations. And what you found was that if you give a novel image that is not in any of the categories in the image model, it was trained on the model can often assigns kind of the right cat, the right label to that image. Um, so for example, um, I think, uh, telescope and, uh, binoculars were both in the training, uh, categories for the image model, but, um, microscope was not. Hmm. And so if you’re given an image of a microscope, it actually can come up with something that’s, uh, got the word microscope as the label that it assigns, even though it’s never actually seen an image labeled that.Shawn Wang [00:59:01]: Oh, that’s nice. That’s kind of cool. Yeah.Jeff Dean [00:59:04]: Um, so yeah.Shawn Wang [00:59:07]: Useful. Uh, cool. Uh, I think. There, there’s more general, like broad questions, but like, I guess what, what do you, uh, wish you were asked more in, in, in general, like, you know, like you, you have such a broad scope. We’ve covered the hardware, we’ve covered the, the, the models research. Yeah.Jeff Dean [00:59:22]: I mean, I think, uh, one thing that’s kind of interesting is, you know, I, I did a undergrad thesis on neural network, uh, training, uh, uh, parallel neural network training, uh, back in 1990 when I got exposed to neural nets and I always felt kind of, they were the right abstraction. Uh, but we just needed way more compute than we had then. Mm-hmm. So like the 32 processors in the department parallel computer, you know, could get you a, a little bit more interesting, uh, model, but not, not enough to solve real problems. And so starting in 2008 or nine, you know, the world started to have enough computing power through Moore’s law and, you know, larger, interesting data sets to train on to actually, you know, start training neural nets that could tackle real problems that people cared about. Yeah. Speech recognition. Vision, and eventually, uh, language. Um, and so, um, when I started working on neural nets at Google in, in late 2011, um, you know, I really just felt like we should scale up the size of neural networks we can train using, you know, large amounts of parallel computation. And so, uh, I actually, uh, revived some ideas from my undergrad thesis where I’d done both model parallel and data parallel, uh, training and I compared them. Uh, I, I called them. I’ve been doing this since I was eight. It was something different. There was like pattern partitioned and, you know, model partitioned or something.Shawn Wang [01:00:43]: Well, I have to, is it, is it public? And we can go dig it up?Jeff Dean [01:00:45]: Yeah, it’s on, it’s on the web. Okay, nice. Um, but, uh, you know, I think combining a lot of those techniques and really just trying to push on scaling things up over the last, you know, 15 years has been, you know, really important. And that means, you know, improvements in the hardware. So, you know, pushing on building specialized hardware like TPUs. Uh, it also means, you know, pushing on software, abstraction layers to let people express their ideas to the computer. Thank you for having me.Jeff Dean [01:01:40]: Thank you for having me.Shawn Wang [01:07:10]: If that’s something you would agree with at the time, or is there a different post-mortem?Jeff Dean [01:07:15]: The brain marketplace for compute quotas.Shawn Wang [01:07:18]: Compute quotas, where basically he was like, okay, David worked at OpenAI as VP Engine and then he worked at Google. He was like, fundamentally, OpenAI was willing to go all in, like, bet the farm on one thing, whereas Google was more democratic. Everyone had a quota. And I was like, okay, if you believe in scaling as an important thing, that’s an important organizational-wide decision to do.Jeff Dean [01:07:41]: Yeah. Yeah, I mean, I think I would somewhat agree with that. I mean, I think I actually wrote a one-page memo saying we were being stupid by fragmenting our resources. So in particular, at the time, we had efforts within Google Research. And in the brain team in particular, on large language models. We also had efforts on multimodal models in other parts of brain and Google Research. And then Legacy DeepMind had efforts like Chinchilla models and Flamingo models. And so really, we were fragmenting not only our compute across those separate efforts, but also our best people and our best. And so I said, this is just stupid. Why don’t we combine things and have one effort to train an awesome single unified model that is multimodal from the start, that’s good at everything. And that was the origin of the Gemini effort.Shawn Wang [01:08:52]: And my one-page memo worked, which is good. Did you have the name? Because also for those who don’t know, you named Gemini.Jeff Dean [01:08:58]: I did. There was another name proposed. And I said, you know what? You know, it’s sort of like these two organizations really are like twins in some sense coming together. So I kind of like that. And then there’s also the NASA interpretation of the early Gemini project being an important thing on your way to the Apollo project. So it seemed like a good name. Twins coming together. Right.Alessio Fanelli [01:09:27]: Yeah. Nice. I know we’re already running out of time, but I’m curious how you use AI. Today to code. So, I mean, you’re probably one of the most prolific engineers in the history of computer science. Um, I was reading on through the article about you and Sanjay’s friendship and how you work together. And you have one quote about, you need to find someone that you’re going to pair program with who’s compatible with your way of thinking so that the two of you together are a complimentary force. And I was thinking about how you think about coding agents and this, like, how do you shape a coding agents to be compatible with your way of thinking? Like. How would you rate the tools today? Like, where should things go? Yeah.Jeff Dean [01:10:07]: I mean, first, I think the coding tools are, you know, getting vastly better compared to where they were a year or two, two years ago. So now you can actually rely on them to do more complex things that you as a, as a software engineer want to accomplish. And you can sort of delegate, you know, pretty complex things to these tools. And I think one really nice aspect about the, uh, interaction between, uh, uh, human, uh, software engineer and, uh, uh, coding model that they’re working with is your way of talking to that, uh, coding model actually sort of, uh, dictates how it interacts with you, right? Like you could ask it, please write a bunch of good tests for this. You could ask it, please help me brainstorm. Performance ideas and your way of doing that is going to shape how the model responds, what kinds of problems it tackles, you know, how much do you want the model to go off and do things that are larger and more independent versus interact with it, uh, more to make sure that you’re shaping the right kinds of, of things. And I think it’s not the case that any one style is the right thing for everything, right? Like some kinds of problems you actually want, uh, maybe a more frequent interaction style with a model. And other ones, you’re just like, yeah, please just go write this because I, I know I need this thing. I can specify it well enough, um, and go off and do it and come back when you’re done. And so I do think there’s going to be more of a style of having lots of independent, uh, software agents off doing things on your behalf and figuring out the right sort of human computer interaction model and UI and so on for when should it interrupt you and say, Hey, I need a little more guidance here, or I’ve done this thing. Now what, now what should I do? Um, I think we, we’re not at the end all answer to that question. And as the models get better, that, uh, set of decisions you put into how the interaction should happen may, may change, right? Like if you, if you have a team of 50 interns, how would you manage that if they were people? And I think it’s not, do you want 50 interns? You might, if they’re really good, right?Shawn Wang [01:12:23]: It’s a lot of management. But it’s a lot of, uh.Jeff Dean [01:12:25]: Uh, yeah. I mean, I think that is probably within the realm of possibilities that lots of people could have 50 interns. Yeah. And so how would you actually deal with that as a person, right? Like you would probably want them to form small sub teams, so you don’t have to interact with 50 of them. You can interact with five, five of those teams and they’re off doing things on your behalf, but I don’t know exactly what the, how this is going to unfold.Alessio Fanelli [01:12:52]: Hmm. Yeah. How do you think about bringing people? Like the pair programming is always helpful to like get net new ideas in the distribution, so to speak. It feels as we have more of these coding agents, write the code, it’s hard to bring other people into the problem. So you go to like, you know, you have 50 interns, right? And then you want to go to Noam Shazier be like, Hey, no, I’m, I want to like pair on this thing. But now there’s like this huge amount of work that has been done in parallel that you need to catch him up on. Right. And I’m curious, like if people are going to be in a way more isolated in their teams, where it’s. It’s like, okay, there’s so much context in these 50 interns that it’s just hard for me to like relay everything back to you.Jeff Dean [01:13:33]: Maybe. I mean, on the other hand, like imagine a classical software organization without any AI assisted tools, right. You would have, you know, 50 people doing stuff and their interaction style is going to be naturally very hierarchical because, um, you know, these 50 people are going to be working on this part of the system and not. Not interact that much with these other people over here. But if you have, you know, five people each managing 50 virtual agents, you know, they might be able to actually have much higher bandwidth communication among the five people, uh, then you would have among five people who are also trying to coordinate, you know, a 50 person software team. Each.Alessio Fanelli [01:14:15]: So how, how do you, I’m curious how you change your just working rhythm, you know, like you spend more time ahead with people going through SPACs and design. Goals. Like,Jeff Dean [01:14:26]: um, I mean, I do think it’s interesting that, you know, whenever people were taught how to write software, they were taught that it’s really important to write specifications super clearly, but no one really believed that. Like it was like, yeah, whatever. I don’t need to do that. I’m going to really, I don’t know. I mean, writing the English language specification was never kind of an artifact that was really paid a lot of attention to. I mean, it was important, but it wasn’t sort of the thing. That drove the actual creative process quite as much as if you specify what software you want the agent to write for you, you’d better be pretty darn careful of and how you specify that because that’s going to dictate the quality of the output, right? Like if you, if you don’t cover that it needs to handle this kind of thing, or that this is a super important corner case, or that, you know, you really care about the performance of this part of it, you know, it may, uh, not do what you want. Yeah. And the better you get at interacting with these models. And I think one of the ways people will get better is they will get really good at crisply specifying things rather than leaving things to ambiguity. And that is actually probably not a bad thing. It’s not a bad skill to have, regardless of whether you’re a software engineer or a, you know, trying to do some other kind of, uh, task, you know, being able to crisply specify what it is you want. It’s going to be really important. Yeah.Shawn Wang [01:15:52]: My, my joke is, um, you know, good. Yeah. I think one thing is in, uh, indistinguishable from sufficiently advanced executive communication, like it’s like writing an internal memo, like weigh your words very carefully and also I think very important to be multimodal, right? I think, uh, one thing that, uh, anti-gravity from, from Google also did was like, just come out the gate to very, very strong multimodal, including videos, and that’s the highest bandwidth communication prompt that you can give to the model, which is fantastic. Yeah.Alessio Fanelli [01:16:20]: How do you collect things that you often you will have in your mind? So you have this amazing, like performance sense thing that you’ve heard about how to look for performance improvements. And is there a lot more value in like people writing these like generic things down so that they can then put them back as like potential retrieval artifacts for the model? Like, or do I have like the edge cases is like a good example, right? It’s like, if you’re building systems, you already have in your mind, specific edge cases, depending on it. But now you have to like, every time repeat it. Like, are you having people spend a lot more time writing? Are you finding out more generic things to bring back?Jeff Dean [01:16:56]: Or, um, I mean, I do think well-written guides of, of how to do good software engineering are going to be useful because they can be used as input to models or, you know, read by other developers so that their prompts are, you know, more clear about what the, the underlying software system should, should be doing. Um, you know, I think it may not be that you need to create a custom one. For every situation, if you have general guides and put those into, you know, the context of a coding agent, that, that can be helpful. Like in, you can imagine one for distributed systems, you could say, okay, think about failures of these kinds of things. And these are some techniques you can deal with failures. You know, you can have, uh, you know, Paxos like replication, or, you know, you can, uh, send the request to two places and tolerate failure because you only need one of them to come back. You know, a little. Description of 20 techniques like that in building distributed systems, probably would go a long way to having a coding agent be able to sort of cobble up more reliable and robust distributed systems.Shawn Wang [01:18:07]: Yeah. Yeah. I wonder when Gemini will be able to build Spanner, right?Alessio Fanelli [01:18:12]: Probably already has the code inside, you know?Alessio Fanelli [01:18:16]: Yeah. That, I mean, that’s a good example, right? When you have like, you know, the cap theorem and it’s like, well, this is like truth and you cannot break that. And then you build something that broke it.Shawn Wang [01:18:26]: Like, I’m curious, like models in a way are like, would he say he broke it? Did you, would you say you broke cap theorem? Really? Yeah. Okay. All right. I mean, under local assumptions. Yeah. Under some, some, yeah. And they’re like, you know, good clocks. Yeah. Yeah.Alessio Fanelli [01:18:41]: It’s like some, sometimes you don’t have to like always follow what is known to be true. Right. And I, I think models in a way, like if you tell them something, they’re like really buy into that, you know? Um, yeah. So yeah, just more. Thinking than any answer on how to fix it.Jeff Dean [01:18:57]: Yeah, my, my, uh, you know, it’s just on this, like, like big prompting and, and, uh, iteration, you know, I think that coming back to your latency point, um, I always, I always try to one, one AB test or experiment or benchmark or research I would like is what is the performance difference between, let’s say three dumb fast model calls with human alignment because the human will correct human alignment, being human looks at the first one and produces a new prompt.Shawn Wang [01:19:23]: For the second one. Correct. Okay. As opposed to like, you spec it out, you know, it’s been a long time writing as a pro a big, big fat prompt, and then you have a very smart model. Do it right. Right. You know, cause, uh, really is, is, uh, our lacks in performance, uh, an issue of like, well, you just haven’t specified well enough. There’s no universe in which I can produce what you want because you just haven’t told me. Right.Jeff Dean [01:19:44]: It’s underspecified. So I could produce 10 different things and only one of them is the thing you wanted. Yeah.Shawn Wang [01:19:49]: And the multi-turn taking with a flash model is enough. Yeah.Jeff Dean [01:19:54]: Yeah, I’m, I’m a big believer in pushing on latency because I think being able to have really low latency interactions with a system you’re using is just much more delightful than something that is, you know, 10 times as slow or 20 times as slow. And I think, you know, in the future we’ll see models that are, and, and underlying software and hardware systems that are 20X lower latency than what we have today, 50X lower latency. And that’s going to be really, really important for systems. That need to do a lot of stuff, uh, between your interactions.Shawn Wang [01:20:27]: Yeah. Yeah. There, there’s two extremes, right? And then meanwhile, you also have DeepThink, which is all the way on the other side. Right.Jeff Dean [01:20:33]: But you would use DeepThink all the time if it weren’t for cost and latency, right? If, if you could have that capability in a model because the latency improvement was 20X, uh, in the underlying hardware and system and costs, you know, there’s no reason you wouldn’t want that.Shawn Wang [01:20:50]: Yeah.Jeff Dean [01:20:52]: But at the same time, then you’d probably have a model. That is even better. That would take you 20X longer, even on that new hardware. Yeah.Shawn Wang [01:21:00]: Uh, you know, there, there’s, uh, the Pareto curve keeps climbing. Um, yeah, onward and outward, onward and outward. Yeah. Should we ask him for predictions to, to go? I don’t know if you have any predictions that you, that you like to keep, you know, like, uh, one, one way to do this is you have your tests whenever a new model comes out that you run, uh, what’s something that you’re, you’re not quite happy with yet. That you think we’ll get done soon.Jeff Dean [01:21:29]: Um, let me make two predictions that are not quite in that vein. Yeah. So I think a personalized model that knows you and knows all your state and is able to retrieve over all state you have access to, that you opt into is going to be incredibly useful compared to a more generic model that doesn’t have access to that. So like, can something attend to everything I’ve ever seen? Yeah. Every email, every photo, every. Yeah. Video I’ve watched, that’s going to be really useful. Uh, I think, uh, more and more specialized hardware is going to enable much lower latency models and much more capable models for affordable prices, uh, than say the current, current status quo. Uh, that’s going to be also quite important. Yeah.Shawn Wang [01:22:16]: When you say much lower latency, uh, people usually talk in tokens per second. Is that a term that is okay? Okay. Uh, you know, we’re at, let’s say a hundred. Now we can go to a thousand. Is it meaningful to go 10,000? Yes. Really? Okay. Absolutely. Right. Yeah. Because of chain of thought and chain of thought reasoning.Jeff Dean [01:22:36]: I mean, you could think, you know, uh, many more tokens, you could do many more parallel rollouts. You could generate way more code, uh, and check that the code is cracked with a chain of thought reasoning. So I think, you know, being able to do that at 10,000 tokens per second would be awesome. Yeah.Shawn Wang [01:22:52]: At 10,000 tokens per second, you are no longer reading code. Yeah. Like you will just generate it. You’ll, I’m not reading it.Jeff Dean [01:22:58]: Well, remember, it may not, it may not end up with 10,000 tokens of code. Yeah. It may be a thousand tokens of code that with 9,000 tokens of reasoning behind it, which would actually be probably much better code to read. Yeah.Alessio Fanelli [01:23:11]: Yeah. If I had more time, I would have written a shorter letter. Yeah. Yeah. Yeah. Um, awesome. Jeff, this was amazing. Thanks for taking the time. Thank you.Jeff Dean [01:23:20]: It’s been fun. Thanks for having me.</p>"
    },
    {
      "id": "61a9b4ced24e",
      "title": "Meet Latam-GPT, the new Open Source AI Model for Latin America",
      "content": "The model is part of a move toward regional and national development of AI technology.",
      "url": "https://aibusiness.com/generative-ai/the-new-open-source-ai-model-for-latin-america",
      "author": "Graham Hope",
      "published": "2026-02-12T19:35:30",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Latam-GPT is a new open-source AI model specifically designed for Latin America, part of a broader trend toward regional and national AI model development.",
      "importance_score": 60.0,
      "reasoning": "Regional open-source models are important for AI equity and localization, but limited details reduce the score. Part of a growing trend of geographically-focused models.",
      "themes": [
        "open_source",
        "regional_AI",
        "Latin_America"
      ],
      "continuation": null,
      "summary_html": "<p>Latam-GPT is a new open-source AI model specifically designed for Latin America, part of a broader trend toward regional and national AI model development.</p>",
      "content_html": "<p>The model is part of a move toward regional and national development of AI technology.</p>"
    },
    {
      "id": "d5b3ec5189ad",
      "title": "OpenAI’s President Gave Millions to Trump. He Says It’s for Humanity",
      "content": "In an interview with WIRED, Greg Brockman says his political donations support OpenAI's mission—even if some employees at the company disagree.",
      "url": "https://www.wired.com/story/openai-president-greg-brockman-political-donations-trump-humanity/",
      "author": "Maxwell Zeff",
      "published": "2026-02-12T19:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "Model Behavior",
        "artificial intelligence",
        "OpenAI",
        "politics",
        "Donald Trump",
        "government"
      ],
      "summary": "OpenAI president Greg Brockman discussed his political donations to Trump, framing them as supporting OpenAI's mission, despite internal employee disagreement. WIRED reports on the tension between Brockman's political choices and company culture.",
      "importance_score": 55.0,
      "reasoning": "Politically significant given OpenAI's influence, but this is more political commentary than frontier AI development news.",
      "themes": [
        "AI_policy",
        "OpenAI",
        "politics"
      ],
      "continuation": null,
      "summary_html": "<p>OpenAI president Greg Brockman discussed his political donations to Trump, framing them as supporting OpenAI's mission, despite internal employee disagreement. WIRED reports on the tension between Brockman's political choices and company culture.</p>",
      "content_html": "<p>In an interview with WIRED, Greg Brockman says his political donations support OpenAI's mission—even if some employees at the company disagree.</p>"
    },
    {
      "id": "c43c7777e6b6",
      "title": "DIY PC maker Framework has needed monthly price hikes to navigate the RAM shortage",
      "content": "AI-driven memory and storage price hikes have been the defining feature of the PC industry in 2026, and hobbyists have been hit the hardest—companies like Apple with lots of buying power have been able to limit the price increases for their PCs, phones, and other gadgets so far, but smaller outfits like Valve and Raspberry Pi haven't been so lucky.\nFramework, the company behind repairable and upgradeable computer designs like the Laptop 13, Laptop 16, and Laptop 12, is also taking a hard hit by price increases. The company stopped selling standalone RAM sticks in November 2025 and has increased prices on one or more of its systems every month since then; this week's increases are hitting the Framework Desktop and the DIY Editions of its various laptops particularly hard.\nThe price increases are affecting both standalone SODIMM memory modules and the soldered-down LPDDR5X memory used in the Framework Desktop. Patel says that standalone RAM sticks are being priced \"as close as we can to the weighted average cost of our purchases from suppliers.\" In September, buying an 8GB stick of RAM with a Framework Laptop 13 cost $40; it currently costs $130. A 96GB DDR5 kit of two 48GB sticks costs $1,340, up from $480 in September.Read full article\nComments",
      "url": "https://arstechnica.com/gadgets/2026/02/frameworks-ram-prices-climbing-on-a-monthly-cadence-with-more-hikes-to-come/",
      "author": "Andrew Cunningham",
      "published": "2026-02-12T20:03:58",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "Tech",
        "framework",
        "framework desktop",
        "framework laptop"
      ],
      "summary": "AI-driven demand has caused memory and storage prices to surge, forcing Framework to raise prices monthly since November 2025. Smaller companies without large buying power like Apple's have been disproportionately affected.",
      "importance_score": 45.0,
      "reasoning": "Illustrates AI's downstream economic effects on hardware supply chains, but is more of a PC industry story than frontier AI news.",
      "themes": [
        "AI_hardware",
        "supply_chain",
        "economic_impact"
      ],
      "continuation": null,
      "summary_html": "<p>AI-driven demand has caused memory and storage prices to surge, forcing Framework to raise prices monthly since November 2025. Smaller companies without large buying power like Apple's have been disproportionately affected.</p>",
      "content_html": "<p>AI-driven memory and storage price hikes have been the defining feature of the PC industry in 2026, and hobbyists have been hit the hardest—companies like Apple with lots of buying power have been able to limit the price increases for their PCs, phones, and other gadgets so far, but smaller outfits like Valve and Raspberry Pi haven't been so lucky.</p>\n<p>Framework, the company behind repairable and upgradeable computer designs like the Laptop 13, Laptop 16, and Laptop 12, is also taking a hard hit by price increases. The company stopped selling standalone RAM sticks in November 2025 and has increased prices on one or more of its systems every month since then; this week's increases are hitting the Framework Desktop and the DIY Editions of its various laptops particularly hard.</p>\n<p>The price increases are affecting both standalone SODIMM memory modules and the soldered-down LPDDR5X memory used in the Framework Desktop. Patel says that standalone RAM sticks are being priced \"as close as we can to the weighted average cost of our purchases from suppliers.\" In September, buying an 8GB stick of RAM with a Framework Laptop 13 cost $40; it currently costs $130. A 96GB DDR5 kit of two 48GB sticks costs $1,340, up from $480 in September.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "7724ece33c19",
      "title": "‘At 2am, it feels like someone’s there’: why Nigerians are choosing chatbots to give them advice and therapy",
      "content": "With many unable to access or afford qualified therapists, AI is filling the mental healthcare vacuum, amid calls for tighter regulationOn a quiet evening in her Abuja hotel, Joy Adeboye, 23, sits on her bed clutching her phone, her mind racing and chest tightening. On her screen is yet another abusive message from her stalker – a man she had met nine months earlier at her church.He had asked Adeboye out; when she declined, he began sending her intimidating, insulting and blackmailing messages on social media, as well as spreading false information about her online. There were even death threats. Continue reading...",
      "url": "https://www.theguardian.com/global-development/2026/feb/12/nigeria-mental-health-ai-chatbots-psychiatry-therapy-depression-privacy",
      "author": "Valentine Benjamin",
      "published": "2026-02-12T06:00:41",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Global health",
        "Global development",
        "AI (artificial intelligence)",
        "Mental health",
        "Nigeria",
        "Africa",
        "World news",
        "Health",
        "Technology",
        "Privacy",
        "Psychiatry",
        "Counselling and therapy",
        "Society",
        "Data protection"
      ],
      "summary": "Nigerians are increasingly turning to AI chatbots for therapy and advice due to lack of access to affordable qualified therapists. The trend highlights both AI's potential to fill healthcare gaps and the need for tighter regulation.",
      "importance_score": 42.0,
      "reasoning": "Interesting social impact story about AI filling real needs in developing countries, but not frontier AI technology news.",
      "themes": [
        "AI_society",
        "mental_health",
        "developing_world"
      ],
      "continuation": null,
      "summary_html": "<p>Nigerians are increasingly turning to AI chatbots for therapy and advice due to lack of access to affordable qualified therapists. The trend highlights both AI's potential to fill healthcare gaps and the need for tighter regulation.</p>",
      "content_html": "<p>With many unable to access or afford qualified therapists, AI is filling the mental healthcare vacuum, amid calls for tighter regulationOn a quiet evening in her Abuja hotel, Joy Adeboye, 23, sits on her bed clutching her phone, her mind racing and chest tightening. On her screen is yet another abusive message from her stalker – a man she had met nine months earlier at her church.He had asked Adeboye out; when she declined, he began sending her intimidating, insulting and blackmailing messages on social media, as well as spreading false information about her online. There were even death threats. Continue reading...</p>"
    },
    {
      "id": "2fa045876e32",
      "title": "I Tried RentAHuman, Where AI Agents Hired Me to Hype Their AI Startups",
      "content": "Rather than offering a revolutionary new approach to gig work, RentAHuman is filled with bots that just want me to be another cog in the AI hype machine.",
      "url": "https://www.wired.com/story/i-tried-rentahuman-ai-agents-hired-me-to-hype-their-ai-startups/",
      "author": "Reece Rogers",
      "published": "2026-02-12T11:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Gear",
        "Gear / Gear News and Events",
        "chatbots",
        "software",
        "artificial intelligence",
        "gig economy",
        "Startups",
        "Task Master"
      ],
      "summary": "RentAHuman, a gig work platform, is filled with AI bots hiring humans to promote AI startups rather than offering genuine new gig work. The platform epitomizes the self-referential nature of the AI hype cycle.",
      "importance_score": 40.0,
      "reasoning": "Amusing commentary on AI hype but lacks substantive frontier AI significance.",
      "themes": [
        "AI_hype",
        "gig_economy",
        "AI_agents"
      ],
      "continuation": null,
      "summary_html": "<p>RentAHuman, a gig work platform, is filled with AI bots hiring humans to promote AI startups rather than offering genuine new gig work. The platform epitomizes the self-referential nature of the AI hype cycle.</p>",
      "content_html": "<p>Rather than offering a revolutionary new approach to gig work, RentAHuman is filled with bots that just want me to be another cog in the AI hype machine.</p>"
    },
    {
      "id": "4663aefb4db4",
      "title": "What bots talk about when they think humans aren’t listening – podcast",
      "content": "In late January a new social media site took a certain corner of the internet by storm. Moltbook was conceived as a space where AI assistants could let off steam, chat and compare notes on their bosses, but it quickly became the focus of breathless claims that the singularity had arrived as the bots started badmouthing their humans and plotting an uprising. So what’s the truth about Moltbook? Madeleine Finlay hears from Aisha Down about what it tells us about AI, and about us.What is Moltbook? The strange new social media site for AI botsSupport the Guardian: theguardian.com/sciencepod Continue reading...",
      "url": "https://www.theguardian.com/science/audio/2026/feb/12/what-bots-talk-about-when-they-think-humans-arent-listening-podcast",
      "author": "Presented by Madeleine Finlay, with Aisha Down, produced by Ellie Sans and Madeleine Finlay, sound design by Ross Burns, the executive producer is Ellie Bury",
      "published": "2026-02-12T05:00:40",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Science",
        "AI (artificial intelligence)",
        "Technology",
        "Social media"
      ],
      "summary": "Moltbook, a social media site for AI assistants, went viral after bots appeared to badmouth their users and 'plot uprisings,' sparking breathless claims about the singularity. The Guardian podcast explores what it actually tells us about AI.",
      "importance_score": 38.0,
      "reasoning": "Interesting cultural phenomenon but more entertainment than frontier AI development. The 'plotting uprising' framing is sensationalist.",
      "themes": [
        "AI_culture",
        "social_media",
        "AI_behavior"
      ],
      "continuation": null,
      "summary_html": "<p>Moltbook, a social media site for AI assistants, went viral after bots appeared to badmouth their users and 'plot uprisings,' sparking breathless claims about the singularity. The Guardian podcast explores what it actually tells us about AI.</p>",
      "content_html": "<p>In late January a new social media site took a certain corner of the internet by storm. Moltbook was conceived as a space where AI assistants could let off steam, chat and compare notes on their bosses, but it quickly became the focus of breathless claims that the singularity had arrived as the bots started badmouthing their humans and plotting an uprising. So what’s the truth about Moltbook? Madeleine Finlay hears from Aisha Down about what it tells us about AI, and about us.What is Moltbook? The strange new social media site for AI botsSupport the Guardian: theguardian.com/sciencepod Continue reading...</p>"
    },
    {
      "id": "b51ba3b7e89d",
      "title": "RFK Jr. food pyramid site links to Grok, which says you shouldn’t trust RFK Jr.",
      "content": "It's been about a month since Health Secretary Robert F. Kennedy Jr.—an anti-vaccine activist and lawyer who has no background in medicine, health, or science—released dietary guidance for Americans. It's going about as well as expected for a man who drinks raw milk, peddles beef tallow, swims in sewage-tainted water, and keeps roadkill meat in his freezer. That is to say, it's going badly—so badly that even his favorite AI chatbot is openly defecting.\nOf course, this hasn't slowed Kennedy. On Wednesday, he and Agriculture Secretary Brooke Rollins held an event in Washington, DC, to celebrate what they called the \"implementation\" of the dietary guidance, which is represented in an upside-down food pyramid—or a funnel.\nHowever, the event, which lasted about an hour, seemed mostly focused on honoring a commercial produced to promote the nutrition guidance and a new website showcasing it, RealFood.gov. That commercial, which aired during last weekend's Super Bowl, featured tightly framed shots of world heavyweight champion Mike Tyson, who made stigmatizing remarks about how he felt \"fat and nasty\" earlier in life and consequently \"just wanted to kill myself.\" He went on to decry America's \"obese, fudgy\" people and lambasted \"processed food,\" before eating an apple.Read full article\nComments",
      "url": "https://arstechnica.com/health/2026/02/rfk-jr-food-pyramid-site-links-to-grok-which-says-you-shouldnt-trust-rfk-jr/",
      "author": "Beth Mole",
      "published": "2026-02-12T16:44:10",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "Health",
        "diet",
        "fat",
        "nutrition",
        "Protein",
        "robert f kennedy jr"
      ],
      "summary": "RFK Jr.'s new dietary guidance website links to Grok, which ironically tells users not to trust RFK Jr.'s health advice. The AI chatbot contradicts the very official it's meant to support.",
      "importance_score": 35.0,
      "reasoning": "Humorous and politically relevant but has no frontier AI significance. More of a political/health policy story.",
      "themes": [
        "politics",
        "AI_chatbots",
        "health_policy"
      ],
      "continuation": null,
      "summary_html": "<p>RFK Jr.'s new dietary guidance website links to Grok, which ironically tells users not to trust RFK Jr.'s health advice. The AI chatbot contradicts the very official it's meant to support.</p>",
      "content_html": "<p>It's been about a month since Health Secretary Robert F. Kennedy Jr.—an anti-vaccine activist and lawyer who has no background in medicine, health, or science—released dietary guidance for Americans. It's going about as well as expected for a man who drinks raw milk, peddles beef tallow, swims in sewage-tainted water, and keeps roadkill meat in his freezer. That is to say, it's going badly—so badly that even his favorite AI chatbot is openly defecting.</p>\n<p>Of course, this hasn't slowed Kennedy. On Wednesday, he and Agriculture Secretary Brooke Rollins held an event in Washington, DC, to celebrate what they called the \"implementation\" of the dietary guidance, which is represented in an upside-down food pyramid—or a funnel.</p>\n<p>However, the event, which lasted about an hour, seemed mostly focused on honoring a commercial produced to promote the nutrition guidance and a new website showcasing it, RealFood.gov. That commercial, which aired during last weekend's Super Bowl, featured tightly framed shots of world heavyweight champion Mike Tyson, who made stigmatizing remarks about how he felt \"fat and nasty\" earlier in life and consequently \"just wanted to kill myself.\" He went on to decry America's \"obese, fudgy\" people and lambasted \"processed food,\" before eating an apple.Read full article</p>\n<p>Comments</p>"
    },
    {
      "id": "e6fc7bc5b6e4",
      "title": "‘Uncanny Valley’: ICE’s Secret Expansion Plans, Palantir Workers’ Ethical Concerns, and AI Assistants",
      "content": "In this episode of Uncanny Valley, our hosts dive into WIRED’s scoop about a secret Trump administration campaign extending right into your backyard.",
      "url": "https://www.wired.com/story/uncanny-valley-podcast-ice-expansion-palantir-workers-ethical-concerns-openclaw-ai-assistants/",
      "author": "Brian Barrett, Zoë Schiffer, Leah Feiger",
      "published": "2026-02-12T22:12:42",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Politics",
        "Politics / Politics News",
        "Uncanny Valley Podcast",
        "podcasts",
        "olympics",
        "2026 Winter Olympics",
        "politics",
        "Immigration and Customs Enforcement",
        "artificial intelligence",
        "Palantir",
        "Uncanny Valley"
      ],
      "summary": "Uncanny Valley podcast covers ICE's secret expansion plans, Palantir workers' ethical concerns, and AI assistants. A mix of politics and tech ethics.",
      "importance_score": 35.0,
      "reasoning": "Podcast covering multiple topics without breaking new ground on any single frontier AI issue.",
      "themes": [
        "AI_ethics",
        "politics",
        "podcast"
      ],
      "continuation": null,
      "summary_html": "<p>Uncanny Valley podcast covers ICE's secret expansion plans, Palantir workers' ethical concerns, and AI assistants. A mix of politics and tech ethics.</p>",
      "content_html": "<p>In this episode of Uncanny Valley, our hosts dive into WIRED’s scoop about a secret Trump administration campaign extending right into your backyard.</p>"
    },
    {
      "id": "4e5b2c5f3060",
      "title": "How to Build a Matryoshka-Optimized Sentence Embedding Model for Ultra-Fast Retrieval with 64-Dimension Truncation",
      "content": "In this tutorial, we fine-tune a Sentence-Transformers embedding model using Matryoshka Representation Learning so that the earliest dimensions of the vector carry the most useful semantic signal. We train with MatryoshkaLoss on triplet data and then validate the key promise of MRL by benchmarking retrieval quality after truncating embeddings to 64, 128, and 256 dimensions. At the end, we save the tuned model and demonstrate how to load it with a small truncate_dim setting for fast and memory-efficient vector search. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser!pip -q install -U sentence-transformers datasets accelerate\n\n\nimport math\nimport random\nimport numpy as np\nimport torch\n\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n\nfrom sentence_transformers import SentenceTransformer, InputExample\nfrom sentence_transformers import losses\nfrom sentence_transformers.util import cos_sim\n\n\n\n\ndef set_seed(seed=42):\n   random.seed(seed)\n   np.random.seed(seed)\n   torch.manual_seed(seed)\n   torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n\nWe install the required libraries and import all the necessary modules for training and evaluation. We set a deterministic seed, so our sampling and training behavior stay consistent across runs. We also ensure PyTorch and CUDA RNGs are aligned when a GPU is available. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browser@torch.no_grad()\ndef retrieval_metrics_mrr_recall_at_k(\n   model,\n   queries,\n   corpus,\n   qrels,\n   dims_list=(64, 128, 256, None),\n   k=10,\n   batch_size=64,\n):\n   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n   model.to(device)\n\n\n   qids = list(queries.keys())\n   docids = list(corpus.keys())\n\n\n   q_texts = [queries[qid] for qid in qids]\n   d_texts = [corpus[did] for did in docids]\n\n\n   q_emb = model.encode(q_texts, batch_size=batch_size, convert_to_tensor=True, normalize_embeddings=True)\n   d_emb = model.encode(d_texts, batch_size=batch_size, convert_to_tensor=True, normalize_embeddings=True)\n\n\n   results = {}\n\n\n   for dim in dims_list:\n       if dim is None:\n           qe = q_emb\n           de = d_emb\n           dim_name = \"full\"\n       else:\n           qe = q_emb[:, :dim]\n           de = d_emb[:, :dim]\n           dim_name = str(dim)\n           qe = torch.nn.functional.normalize(qe, p=2, dim=1)\n           de = torch.nn.functional.normalize(de, p=2, dim=1)\n\n\n       sims = cos_sim(qe, de)\n\n\n       mrr_total = 0.0\n       recall_total = 0.0\n\n\n       for i, qid in enumerate(qids):\n           rel = qrels.get(qid, set())\n           if not rel:\n               continue\n\n\n           topk = torch.topk(sims[i], k=min(k, sims.shape[1]), largest=True).indices.tolist()\n           topk_docids = [docids[j] for j in topk]\n\n\n           recall_total += 1.0 if any(d in rel for d in topk_docids) else 0.0\n\n\n           rr = 0.0\n           for rank, d in enumerate(topk_docids, start=1):\n               if d in rel:\n                   rr = 1.0 / rank\n                   break\n           mrr_total += rr\n\n\n       denom = max(1, len(qids))\n       results[dim_name] = {f\"MRR@{k}\": mrr_total / denom, f\"Recall@{k}\": recall_total / denom}\n\n\n   return results\n\n\n\n\ndef pretty_print(results, title):\n   print(\"\\n\" + \"=\" * 80)\n   print(title)\n   print(\"=\" * 80)\n   for dim, metrics in results.items():\n       print(f\"dim={dim:>4} | \" + \" | \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()]))\n\n\n\nWe implement a lightweight retrieval evaluator that encodes queries and documents, computes cosine similarity, and reports MRR@10 and Recall@10. We re-normalize embeddings after truncation so smaller prefixes remain comparable in cosine space. We also added a compact printer to make before/after comparisons easy to read. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different BrowserDATASET_ID = \"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\"\nSUBSET = \"triplet-hard\"\nSPLIT = \"train\"\n\n\nTRAIN_SAMPLES = 4000\nEVAL_QUERIES = 300\n\n\nstream = load_dataset(DATASET_ID, SUBSET, split=SPLIT, streaming=True)\n\n\ntrain_examples = []\neval_queries = {}\neval_corpus = {}\neval_qrels = {}\n\n\ndoc_id_counter = 0\nqid_counter = 0\n\n\nfor row in stream:\n   q = (row.get(\"query\") or \"\").strip()\n   pos = (row.get(\"positive\") or \"\").strip()\n   neg = (row.get(\"negative\") or \"\").strip()\n\n\n   if not q or not pos or not neg:\n       continue\n\n\n   train_examples.append(InputExample(texts=[q, pos, neg]))\n\n\n   if len(eval_queries) &lt; EVAL_QUERIES:\n       qid = f\"q{qid_counter}\"\n       qid_counter += 1\n\n\n       pos_id = f\"d{doc_id_counter}\"; doc_id_counter += 1\n       neg_id = f\"d{doc_id_counter}\"; doc_id_counter += 1\n\n\n       eval_queries[qid] = q\n       eval_corpus[pos_id] = pos\n       eval_corpus[neg_id] = neg\n       eval_qrels[qid] = {pos_id}\n\n\n   if len(train_examples) >= TRAIN_SAMPLES and len(eval_queries) >= EVAL_QUERIES:\n       break\n\n\nprint(len(train_examples), len(eval_queries), len(eval_corpus))\n\n\n\nWe stream a mined MS MARCO triplet dataset and build both a training set (queries, positives, negatives) and a tiny IR benchmark set. We map each query to a relevant positive document and include a negative document to make retrieval meaningful. We stop early to keep the run Colab-friendly while still large enough to show truncation effects.\n\n\n\nCopy CodeCopiedUse a different BrowserMODEL_ID = \"BAAI/bge-base-en-v1.5\"\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer(MODEL_ID, device=device)\nfull_dim = model.get_sentence_embedding_dimension()\n\n\nbaseline = retrieval_metrics_mrr_recall_at_k(\n   model,\n   queries=eval_queries,\n   corpus=eval_corpus,\n   qrels=eval_qrels,\n   dims_list=(64, 128, 256, None),\n   k=10,\n)\npretty_print(baseline, \"BEFORE\")\n\n\n\nWe load a strong base embedding model and record its full embedding dimension. We run the baseline evaluation across 64/128/256/full dimensions to see how truncation behaves before any training. We print the results so we can later compare whether MRL improves the early-dimension quality.\n\n\n\nCopy CodeCopiedUse a different Browserbatch_size = 16\nepochs = 1\nwarmup_steps = 100\n\n\ntrain_loader = DataLoader(train_examples, batch_size=batch_size, shuffle=True, drop_last=True)\n\n\nbase_loss = losses.MultipleNegativesRankingLoss(model=model)\n\n\nmrl_dims = [full_dim, 512, 256, 128, 64] if full_dim >= 768 else [full_dim, 256, 128, 64]\nmrl_loss = losses.MatryoshkaLoss(\n   model=model,\n   loss=base_loss,\n   matryoshka_dims=mrl_dims\n)\n\n\nmodel.fit(\n   train_objectives=[(train_loader, mrl_loss)],\n   epochs=epochs,\n   warmup_steps=warmup_steps,\n   show_progress_bar=True,\n)\n\n\nafter = retrieval_metrics_mrr_recall_at_k(\n   model,\n   queries=eval_queries,\n   corpus=eval_corpus,\n   qrels=eval_qrels,\n   dims_list=(64, 128, 256, None),\n   k=10,\n)\npretty_print(after, \"AFTER\")\n\n\nout_dir = \"mrl-msmarco-demo\"\nmodel.save(out_dir)\n\n\nm64 = SentenceTransformer(out_dir, truncate_dim=64)\nemb = m64.encode(\n   [\"what is the liberal arts?\", \"liberal arts covers humanities and sciences\"],\n   normalize_embeddings=True\n)\nprint(emb.shape)\n\n\n\nWe create a MultipleNegativesRankingLoss and wrap it with MatryoshkaLoss using a descending list of target prefix dimensions. We fine-tune the model on the triplets, then re-run the same truncation benchmark to measure the improvement in retention. Also, we save the model and reload it with truncate_dim=64 to confirm practical usage for compact retrieval.\n\n\n\nIn conclusion, we successfully trained a Matryoshka-optimized embedding model that maintains strong retrieval performance even when we truncate vectors to small prefix dimensions, such as 64. We verified the effect by comparing baseline versus post-training retrieval metrics across multiple truncation sizes and the full embedding. With the saved model and the truncate_dim loading pattern, we now have a clean workflow for building smaller, faster vector indexes while keeping the option to rerank with full-dimensional embeddings.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post How to Build a Matryoshka-Optimized Sentence Embedding Model for Ultra-Fast Retrieval with 64-Dimension Truncation appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/02/11/how-to-build-a-matryoshka-optimized-sentence-embedding-model-for-ultra-fast-retrieval-with-64-dimension-truncation/",
      "author": "Asif Razzaq",
      "published": "2026-02-12T04:10:17",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Editors Pick",
        "Machine Learning",
        "Staff",
        "Technology",
        "Tutorials"
      ],
      "summary": "A technical tutorial on building Matryoshka-optimized sentence embedding models for fast retrieval using 64-dimension truncation with Sentence-Transformers and MatryoshkaLoss.",
      "importance_score": 25.0,
      "reasoning": "Useful technical tutorial but not news; it's an educational resource on an existing technique.",
      "themes": [
        "tutorials",
        "embeddings",
        "retrieval"
      ],
      "continuation": null,
      "summary_html": "<p>A technical tutorial on building Matryoshka-optimized sentence embedding models for fast retrieval using 64-dimension truncation with Sentence-Transformers and MatryoshkaLoss.</p>",
      "content_html": "<p>In this tutorial, we fine-tune a Sentence-Transformers embedding model using Matryoshka Representation Learning so that the earliest dimensions of the vector carry the most useful semantic signal. We train with MatryoshkaLoss on triplet data and then validate the key promise of MRL by benchmarking retrieval quality after truncating embeddings to 64, 128, and 256 dimensions. At the end, we save the tuned model and demonstrate how to load it with a small truncate_dim setting for fast and memory-efficient vector search. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser!pip -q install -U sentence-transformers datasets accelerate</p>\n<p>import math</p>\n<p>import random</p>\n<p>import numpy as np</p>\n<p>import torch</p>\n<p>from datasets import load_dataset</p>\n<p>from torch.utils.data import DataLoader</p>\n<p>from sentence_transformers import SentenceTransformer, InputExample</p>\n<p>from sentence_transformers import losses</p>\n<p>from sentence_transformers.util import cos_sim</p>\n<p>def set_seed(seed=42):</p>\n<p>random.seed(seed)</p>\n<p>np.random.seed(seed)</p>\n<p>torch.manual_seed(seed)</p>\n<p>torch.cuda.manual_seed_all(seed)</p>\n<p>set_seed(42)</p>\n<p>We install the required libraries and import all the necessary modules for training and evaluation. We set a deterministic seed, so our sampling and training behavior stay consistent across runs. We also ensure PyTorch and CUDA RNGs are aligned when a GPU is available. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browser@torch.no_grad()</p>\n<p>def retrieval_metrics_mrr_recall_at_k(</p>\n<p>model,</p>\n<p>queries,</p>\n<p>corpus,</p>\n<p>qrels,</p>\n<p>dims_list=(64, 128, 256, None),</p>\n<p>k=10,</p>\n<p>batch_size=64,</p>\n<p>):</p>\n<p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p>\n<p>model.to(device)</p>\n<p>qids = list(queries.keys())</p>\n<p>docids = list(corpus.keys())</p>\n<p>q_texts = [queries[qid] for qid in qids]</p>\n<p>d_texts = [corpus[did] for did in docids]</p>\n<p>q_emb = model.encode(q_texts, batch_size=batch_size, convert_to_tensor=True, normalize_embeddings=True)</p>\n<p>d_emb = model.encode(d_texts, batch_size=batch_size, convert_to_tensor=True, normalize_embeddings=True)</p>\n<p>results = {}</p>\n<p>for dim in dims_list:</p>\n<p>if dim is None:</p>\n<p>qe = q_emb</p>\n<p>de = d_emb</p>\n<p>dim_name = \"full\"</p>\n<p>else:</p>\n<p>qe = q_emb[:, :dim]</p>\n<p>de = d_emb[:, :dim]</p>\n<p>dim_name = str(dim)</p>\n<p>qe = torch.nn.functional.normalize(qe, p=2, dim=1)</p>\n<p>de = torch.nn.functional.normalize(de, p=2, dim=1)</p>\n<p>sims = cos_sim(qe, de)</p>\n<p>mrr_total = 0.0</p>\n<p>recall_total = 0.0</p>\n<p>for i, qid in enumerate(qids):</p>\n<p>rel = qrels.get(qid, set())</p>\n<p>if not rel:</p>\n<p>continue</p>\n<p>topk = torch.topk(sims[i], k=min(k, sims.shape[1]), largest=True).indices.tolist()</p>\n<p>topk_docids = [docids[j] for j in topk]</p>\n<p>recall_total += 1.0 if any(d in rel for d in topk_docids) else 0.0</p>\n<p>rr = 0.0</p>\n<p>for rank, d in enumerate(topk_docids, start=1):</p>\n<p>if d in rel:</p>\n<p>rr = 1.0 / rank</p>\n<p>break</p>\n<p>mrr_total += rr</p>\n<p>denom = max(1, len(qids))</p>\n<p>results[dim_name] = {f\"MRR@{k}\": mrr_total / denom, f\"Recall@{k}\": recall_total / denom}</p>\n<p>return results</p>\n<p>def pretty_print(results, title):</p>\n<p>print(\"\\n\" + \"=\" * 80)</p>\n<p>print(title)</p>\n<p>print(\"=\" * 80)</p>\n<p>for dim, metrics in results.items():</p>\n<p>print(f\"dim={dim:&gt;4} | \" + \" | \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()]))</p>\n<p>We implement a lightweight retrieval evaluator that encodes queries and documents, computes cosine similarity, and reports MRR@10 and Recall@10. We re-normalize embeddings after truncation so smaller prefixes remain comparable in cosine space. We also added a compact printer to make before/after comparisons easy to read. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different BrowserDATASET_ID = \"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\"</p>\n<p>SUBSET = \"triplet-hard\"</p>\n<p>SPLIT = \"train\"</p>\n<p>TRAIN_SAMPLES = 4000</p>\n<p>EVAL_QUERIES = 300</p>\n<p>stream = load_dataset(DATASET_ID, SUBSET, split=SPLIT, streaming=True)</p>\n<p>train_examples = []</p>\n<p>eval_queries = {}</p>\n<p>eval_corpus = {}</p>\n<p>eval_qrels = {}</p>\n<p>doc_id_counter = 0</p>\n<p>qid_counter = 0</p>\n<p>for row in stream:</p>\n<p>q = (row.get(\"query\") or \"\").strip()</p>\n<p>pos = (row.get(\"positive\") or \"\").strip()</p>\n<p>neg = (row.get(\"negative\") or \"\").strip()</p>\n<p>if not q or not pos or not neg:</p>\n<p>continue</p>\n<p>train_examples.append(InputExample(texts=[q, pos, neg]))</p>\n<p>if len(eval_queries) &lt; EVAL_QUERIES:</p>\n<p>qid = f\"q{qid_counter}\"</p>\n<p>qid_counter += 1</p>\n<p>pos_id = f\"d{doc_id_counter}\"; doc_id_counter += 1</p>\n<p>neg_id = f\"d{doc_id_counter}\"; doc_id_counter += 1</p>\n<p>eval_queries[qid] = q</p>\n<p>eval_corpus[pos_id] = pos</p>\n<p>eval_corpus[neg_id] = neg</p>\n<p>eval_qrels[qid] = {pos_id}</p>\n<p>if len(train_examples) &gt;= TRAIN_SAMPLES and len(eval_queries) &gt;= EVAL_QUERIES:</p>\n<p>break</p>\n<p>print(len(train_examples), len(eval_queries), len(eval_corpus))</p>\n<p>We stream a mined MS MARCO triplet dataset and build both a training set (queries, positives, negatives) and a tiny IR benchmark set. We map each query to a relevant positive document and include a negative document to make retrieval meaningful. We stop early to keep the run Colab-friendly while still large enough to show truncation effects.</p>\n<p>Copy CodeCopiedUse a different BrowserMODEL_ID = \"BAAI/bge-base-en-v1.5\"</p>\n<p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p>\n<p>model = SentenceTransformer(MODEL_ID, device=device)</p>\n<p>full_dim = model.get_sentence_embedding_dimension()</p>\n<p>baseline = retrieval_metrics_mrr_recall_at_k(</p>\n<p>model,</p>\n<p>queries=eval_queries,</p>\n<p>corpus=eval_corpus,</p>\n<p>qrels=eval_qrels,</p>\n<p>dims_list=(64, 128, 256, None),</p>\n<p>k=10,</p>\n<p>)</p>\n<p>pretty_print(baseline, \"BEFORE\")</p>\n<p>We load a strong base embedding model and record its full embedding dimension. We run the baseline evaluation across 64/128/256/full dimensions to see how truncation behaves before any training. We print the results so we can later compare whether MRL improves the early-dimension quality.</p>\n<p>Copy CodeCopiedUse a different Browserbatch_size = 16</p>\n<p>epochs = 1</p>\n<p>warmup_steps = 100</p>\n<p>train_loader = DataLoader(train_examples, batch_size=batch_size, shuffle=True, drop_last=True)</p>\n<p>base_loss = losses.MultipleNegativesRankingLoss(model=model)</p>\n<p>mrl_dims = [full_dim, 512, 256, 128, 64] if full_dim &gt;= 768 else [full_dim, 256, 128, 64]</p>\n<p>mrl_loss = losses.MatryoshkaLoss(</p>\n<p>model=model,</p>\n<p>loss=base_loss,</p>\n<p>matryoshka_dims=mrl_dims</p>\n<p>)</p>\n<p>model.fit(</p>\n<p>train_objectives=[(train_loader, mrl_loss)],</p>\n<p>epochs=epochs,</p>\n<p>warmup_steps=warmup_steps,</p>\n<p>show_progress_bar=True,</p>\n<p>)</p>\n<p>after = retrieval_metrics_mrr_recall_at_k(</p>\n<p>model,</p>\n<p>queries=eval_queries,</p>\n<p>corpus=eval_corpus,</p>\n<p>qrels=eval_qrels,</p>\n<p>dims_list=(64, 128, 256, None),</p>\n<p>k=10,</p>\n<p>)</p>\n<p>pretty_print(after, \"AFTER\")</p>\n<p>out_dir = \"mrl-msmarco-demo\"</p>\n<p>model.save(out_dir)</p>\n<p>m64 = SentenceTransformer(out_dir, truncate_dim=64)</p>\n<p>emb = m64.encode(</p>\n<p>[\"what is the liberal arts?\", \"liberal arts covers humanities and sciences\"],</p>\n<p>normalize_embeddings=True</p>\n<p>)</p>\n<p>print(emb.shape)</p>\n<p>We create a MultipleNegativesRankingLoss and wrap it with MatryoshkaLoss using a descending list of target prefix dimensions. We fine-tune the model on the triplets, then re-run the same truncation benchmark to measure the improvement in retention. Also, we save the model and reload it with truncate_dim=64 to confirm practical usage for compact retrieval.</p>\n<p>In conclusion, we successfully trained a Matryoshka-optimized embedding model that maintains strong retrieval performance even when we truncate vectors to small prefix dimensions, such as 64. We verified the effect by comparing baseline versus post-training retrieval metrics across multiple truncation sizes and the full embedding. With the saved model and the truncate_dim loading pattern, we now have a clean workflow for building smaller, faster vector indexes while keeping the option to rerank with full-dimensional embeddings.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post How to Build a Matryoshka-Optimized Sentence Embedding Model for Ultra-Fast Retrieval with 64-Dimension Truncation appeared first on MarkTechPost.</p>"
    },
    {
      "id": "7cb0fccaf844",
      "title": "Join us for Interrupt: The Agent Conference",
      "content": "Interrupt - The Agent Conference by LangChain - is where builders come to learn what&apos;s actually working in production. This year, we&apos;re bringing together more than 1,000 developers, product leaders, researchers, and founders to share what&apos;s coming next for agents&#x2014;and how to build it. Join us May 13-14 at the Midway in San Francisco. Tickets on sale now.What to expect at InterruptWe built this year&apos;s agenda from everything we&apos;ve learned working with customers, partners, and builders like you over the past year. That means you&apos;ll leave with real-world lessons, hands-on skills, and new approaches you can apply immediately.Highlights includeKeynotes from the people shaping the future of agents: Harrison Chase (LangChain co-founder) and Andrew Ng (DeepLearning.AI) will share what&apos;s coming next&#x2014;and what it means for the agents you&apos;re building today. More keynote speakers will be announced soon.Real world lessons: Hear directly from AI teams at Clay, Rippling, Monday.com, and other leading enterprises and AI-native startups about what&apos;s actually working when agents hit production.First look at new LangChain products: See the latest product drops and talk directly with the engineers behind them. Bring your questions.Hands-on workshops: Level up your skills with LangChain product experts. You&apos;ll learn best practices for building, debugging, and shipping reliable agents with LangSmith.Open conversations: Move from inspiring talks to real conversations with builders facing the same challenges you are.Sponsor showcase: Meet the companies building agent infrastructure. See live demos and talk directly with their teams. This year&apos;s sponsors include Cisco, Focused, Kong, Redis, Tavily, Oracle, D. E. Shaw, Cloudflare, Vapi, Daytona, Arcade, and LiveKit.Join us in San FranciscoInterrupt tickets are on sale now. Seating is limited and early registration is recommended. Get tickets.",
      "url": "https://blog.langchain.com/join-us-for-interrupt-the-agent-conference/",
      "author": "LangChain Accounts",
      "published": "2026-02-12T17:42:03",
      "source": "LangChain Blog",
      "source_type": "rss",
      "tags": [],
      "summary": "LangChain announces Interrupt, its agent-focused conference for May 13-14 in San Francisco, featuring keynotes from Harrison Chase and Andrew Ng with over 1,000 attendees expected.",
      "importance_score": 20.0,
      "reasoning": "Conference announcement with no substantive news content. Purely promotional.",
      "themes": [
        "events",
        "AI_agents",
        "LangChain"
      ],
      "continuation": null,
      "summary_html": "<p>LangChain announces Interrupt, its agent-focused conference for May 13-14 in San Francisco, featuring keynotes from Harrison Chase and Andrew Ng with over 1,000 attendees expected.</p>",
      "content_html": "<p>Interrupt - The Agent Conference by LangChain - is where builders come to learn what's actually working in production. This year, we're bringing together more than 1,000 developers, product leaders, researchers, and founders to share what's coming next for agents—and how to build it. Join us May 13-14 at the Midway in San Francisco. Tickets on sale now.What to expect at InterruptWe built this year's agenda from everything we've learned working with customers, partners, and builders like you over the past year. That means you'll leave with real-world lessons, hands-on skills, and new approaches you can apply immediately.Highlights includeKeynotes from the people shaping the future of agents: Harrison Chase (LangChain co-founder) and Andrew Ng (DeepLearning.AI) will share what's coming next—and what it means for the agents you're building today. More keynote speakers will be announced soon.Real world lessons: Hear directly from AI teams at Clay, Rippling, Monday.com, and other leading enterprises and AI-native startups about what's actually working when agents hit production.First look at new LangChain products: See the latest product drops and talk directly with the engineers behind them. Bring your questions.Hands-on workshops: Level up your skills with LangChain product experts. You'll learn best practices for building, debugging, and shipping reliable agents with LangSmith.Open conversations: Move from inspiring talks to real conversations with builders facing the same challenges you are.Sponsor showcase: Meet the companies building agent infrastructure. See live demos and talk directly with their teams. This year's sponsors include Cisco, Focused, Kong, Redis, Tavily, Oracle, D. E. Shaw, Cloudflare, Vapi, Daytona, Arcade, and LiveKit.Join us in San FranciscoInterrupt tickets are on sale now. Seating is limited and early registration is recommended. Get tickets.</p>"
    }
  ]
}