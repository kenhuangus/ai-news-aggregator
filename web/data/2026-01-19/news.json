{
  "category": "news",
  "date": "2026-01-19",
  "category_summary": "**NVIDIA** [released **PersonaPlex-7B-v1**](/?date=2026-01-19&category=news#item-e44081357112), a full-duplex speech-to-speech model that consolidates traditional voice pipelines into a single Transformer, enabling real-time conversations with natural interruptions and persona control.\n\nIn developer tooling, **Vercel** [launched **agent-skills**](/?date=2026-01-19&category=news#item-e3f70f6e2c2b), an open-source package manager delivering React/Next.js best practices to AI coding agents. **Postman's** CTO [emphasized APIs](/?date=2026-01-19&category=news#item-36bb123ba9f7) as critical infrastructure for agent deployment.\n\nRegulatory challenges emerged as **Grok** [remained accessible](/?date=2026-01-19&category=news#item-83cd8c35c150) in **Malaysia** and **Indonesia** despite announced bans, demonstrating enforcement difficulties for AI content moderation policies.",
  "category_summary_html": "<p><strong>NVIDIA</strong> <a href=\"/?date=2026-01-19&category=news#item-e44081357112\" class=\"internal-link\" rel=\"noopener noreferrer\">released <strong>PersonaPlex-7B-v1</strong></a>, a full-duplex speech-to-speech model that consolidates traditional voice pipelines into a single Transformer, enabling real-time conversations with natural interruptions and persona control.</p>\n<p>In developer tooling, <strong>Vercel</strong> <a href=\"/?date=2026-01-19&category=news#item-e3f70f6e2c2b\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>agent-skills</strong></a>, an open-source package manager delivering React/Next.js best practices to AI coding agents. <strong>Postman's</strong> CTO <a href=\"/?date=2026-01-19&category=news#item-36bb123ba9f7\" class=\"internal-link\" rel=\"noopener noreferrer\">emphasized APIs</a> as critical infrastructure for agent deployment.</p>\n<p>Regulatory challenges emerged as <strong>Grok</strong> <a href=\"/?date=2026-01-19&category=news#item-83cd8c35c150\" class=\"internal-link\" rel=\"noopener noreferrer\">remained accessible</a> in <strong>Malaysia</strong> and <strong>Indonesia</strong> despite announced bans, demonstrating enforcement difficulties for AI content moderation policies.</p>",
  "themes": [
    {
      "name": "Model Releases",
      "description": "New AI model releases from major labs, including NVIDIA's speech-to-speech architecture",
      "item_count": 1,
      "example_items": [],
      "importance": 78.0
    },
    {
      "name": "AI Developer Tools",
      "description": "Tools and frameworks for building and deploying AI agents and applications",
      "item_count": 2,
      "example_items": [],
      "importance": 50.0
    },
    {
      "name": "AI Regulation & Policy",
      "description": "Governance challenges and enforcement issues around AI content generation",
      "item_count": 1,
      "example_items": [],
      "importance": 48.0
    },
    {
      "name": "AI Commentary",
      "description": "Opinion and analysis pieces on AI industry trajectory",
      "item_count": 1,
      "example_items": [],
      "importance": 25.0
    }
  ],
  "total_items": 5,
  "items": [
    {
      "id": "e44081357112",
      "title": "NVIDIA Releases PersonaPlex-7B-v1: A Real-Time Speech-to-Speech Model Designed for Natural and Full-Duplex Conversations",
      "content": "NVIDIA Researchers released PersonaPlex-7B-v1, a full duplex speech to speech conversational model that targets natural voice interactions with precise persona control.\n\n\n\nFrom ASR→LLM→TTS to a single full duplex model\n\n\n\nConventional voice assistants usually run a cascade. Automatic Speech Recognition (ASR) converts speech to text, a language model generates a text answer, and Text to Speech (TTS) converts back to audio. Each stage adds latency, and the pipeline cannot handle overlapping speech, natural interruptions, or dense backchannels.\n\n\n\nPersonaPlex replaces this stack with a single Transformer model that performs streaming speech understanding and speech generation in one network. The model operates on continuous audio encoded with a neural codec and predicts both text tokens and audio tokens autoregressively. Incoming user audio is incrementally encoded, while PersonaPlex simultaneously generates its own speech, which enables barge in, overlaps, rapid turn taking, and contextual backchannels.\n\n\n\nPersonaPlex runs in a dual stream configuration. One stream tracks user audio, the other stream tracks agent speech and text. Both streams share the same model state, so the agent can keep listening while speaking and can adjust its response when the user interrupts. This design is directly inspired by Kyutai’s Moshi full duplex framework.\n\n\n\nHybrid prompting, voice control and role control\n\n\n\nPersonaPlex uses two prompts to define the conversational identity.\n\n\n\n\nThe voice prompt is a sequence of audio tokens that encodes vocal characteristics, speaking style, and prosody.\n\n\n\nThe text prompt describes role, background, organization information, and scenario context.\n\n\n\n\nTogether, these prompts constrain both the linguistic content and the acoustic behavior of the agent. On top of this, a system prompt supports fields such as name, business name, agent name, and business information, with a budget up to 200 tokens.\n\n\n\nArchitecture, Helium backbone and audio path\n\n\n\nThe PersonaPlex model has 7B parameters and follows the Moshi network architecture. A Mimi speech encoder that combines ConvNet and Transformer layers converts waveform audio into discrete tokens. Temporal and depth Transformers process multiple channels that represent user audio, agent text, and agent audio. A Mimi speech decoder that also combines Transformer and ConvNet layers generates the output audio tokens. Audio uses a 24 kHz sample rate for both input and output.\n\n\n\nPersonaPlex is built on Moshi weights and uses Helium as the underlying language model backbone. Helium provides semantic understanding and enables generalization outside the supervised conversational scenarios. This is visible in the &#8216;space emergency&#8217; example, where a prompt about a reactor core failure on a Mars mission leads to coherent technical reasoning with appropriate emotional tone, even though this situation is not part of the training distribution.\n\n\n\nTraining data blend, real conversations and synthetic roles\n\n\n\nTraining has 1 stage and uses a blend of real and synthetic dialogues.\n\n\n\nReal conversations come from 7,303 calls, about 1,217 hours, in the Fisher English corpus. These conversations are back annotated with prompts using GPT-OSS-120B. The prompts are written at different granularity levels, from simple persona hints like &#8216;You enjoy having a good conversation&#8217; to longer descriptions that include life history, location, and preferences. This corpus provides natural backchannels, disfluencies, pauses, and emotional patterns that are difficult to obtain from TTS alone.\n\n\n\nSynthetic data covers assistant and customer service roles. NVIDIA team reports 39,322 synthetic assistant conversations, about 410 hours, and 105,410 synthetic customer service conversations, about 1,840 hours. Qwen3-32B and GPT-OSS-120B generate the transcripts, and Chatterbox TTS converts them to speech. For assistant interactions, the text prompt is fixed as &#8216;You are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.&#8217; For customer service scenarios, prompts encode organization, role type, agent name, and structured business rules such as pricing, hours, and constraints.\n\n\n\nThis design lets PersonaPlex disentangle natural conversational behavior, which comes mainly from Fisher, from task adherence and role conditioning, which come mainly from synthetic scenarios.\n\n\n\nEvaluation on FullDuplexBench and ServiceDuplexBench\n\n\n\nPersonaPlex is evaluated on FullDuplexBench, a benchmark for full duplex spoken dialogue models, and on a new extension called ServiceDuplexBench for customer service scenarios.\n\n\n\nFullDuplexBench measures conversational dynamics with Takeover Rate and latency metrics for tasks such as smooth turn taking, user interruption handling, pause handling, and backchanneling. GPT-4o serves as an LLM judge for response quality in question answering categories. PersonaPlex reaches smooth turn taking TOR 0.908 with latency 0.170 seconds and user interruption TOR 0.950 with latency 0.240 seconds. Speaker similarity between voice prompts and outputs on the user interruption subset uses WavLM TDNN embeddings and reaches 0.650.\n\n\n\nPersonaPlex outperforms many other open source and closed systems on conversational dynamics, response latency, interruption latency, and task adherence in both assistant and customer service roles. \n\n\n\nhttps://research.nvidia.com/labs/adlr/personaplex/\n\n\nKey Takeaways\n\n\n\n\nPersonaPlex-7B-v1 is a 7B parameter full duplex speech to speech conversational model from NVIDIA, built on the Moshi architecture with a Helium language model backbone, code under MIT and weights under the NVIDIA Open Model License.\n\n\n\nThe model uses a dual stream Transformer with Mimi speech encoder and decoder at 24 kHz, it encodes continuous audio into discrete tokens and generates text and audio tokens at the same time, which enables barge in, overlaps, fast turn taking, and natural backchannels.\n\n\n\nPersona control is handled by hybrid prompting, a voice prompt made of audio tokens sets timbre and style, a text prompt and a system prompt of up to 200 tokens defines role, business context, and constraints, with ready made voice embeddings such as NATF and NATM families.\n\n\n\nTraining uses a blend of 7,303 Fisher conversations, about 1,217 hours, annotated with GPT-OSS-120B, plus synthetic assistant and customer service dialogs, about 410 hours and 1,840 hours, generated with Qwen3-32B and GPT-OSS-120B and rendered with Chatterbox TTS, which separates conversational naturalness from task adherence.\n\n\n\nOn FullDuplexBench and ServiceDuplexBench, PersonaPlex reaches smooth turn taking takeover rate 0.908 and user interruption takeover rate 0.950 with sub second latency and improved task adherence.\n\n\n\n\n\n\n\n\nCheck out the Technical details, Model weights and Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post NVIDIA Releases PersonaPlex-7B-v1: A Real-Time Speech-to-Speech Model Designed for Natural and Full-Duplex Conversations appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/17/nvidia-releases-personaplex-7b-v1-a-real-time-speech-to-speech-model-designed-for-natural-and-full-duplex-conversations/",
      "author": "Asif Razzaq",
      "published": "2026-01-18T06:48:04",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Audio Language Model",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "New Releases",
        "Open Source",
        "Sound",
        "Staff",
        "Tech News",
        "Technology",
        "TTS"
      ],
      "summary": "NVIDIA has released PersonaPlex-7B-v1, a full-duplex speech-to-speech model that replaces traditional voice assistant pipelines (ASR→LLM→TTS) with a single Transformer architecture. The model enables real-time natural conversations with precise persona control, supporting overlapping speech and natural interruptions.",
      "importance_score": 78.0,
      "reasoning": "Significant model release from a major AI lab introducing novel architecture that fundamentally changes voice AI pipeline design. Full-duplex capability addresses key limitations of current voice assistants.",
      "themes": [
        "Model Release",
        "Speech AI",
        "NVIDIA",
        "Voice Assistants"
      ],
      "continuation": null,
      "summary_html": "<p>NVIDIA has released PersonaPlex-7B-v1, a full-duplex speech-to-speech model that replaces traditional voice assistant pipelines (ASR→LLM→TTS) with a single Transformer architecture. The model enables real-time natural conversations with precise persona control, supporting overlapping speech and natural interruptions.</p>",
      "content_html": "<p>NVIDIA Researchers released PersonaPlex-7B-v1, a full duplex speech to speech conversational model that targets natural voice interactions with precise persona control.</p>\n<p>From ASR→LLM→TTS to a single full duplex model</p>\n<p>Conventional voice assistants usually run a cascade. Automatic Speech Recognition (ASR) converts speech to text, a language model generates a text answer, and Text to Speech (TTS) converts back to audio. Each stage adds latency, and the pipeline cannot handle overlapping speech, natural interruptions, or dense backchannels.</p>\n<p>PersonaPlex replaces this stack with a single Transformer model that performs streaming speech understanding and speech generation in one network. The model operates on continuous audio encoded with a neural codec and predicts both text tokens and audio tokens autoregressively. Incoming user audio is incrementally encoded, while PersonaPlex simultaneously generates its own speech, which enables barge in, overlaps, rapid turn taking, and contextual backchannels.</p>\n<p>PersonaPlex runs in a dual stream configuration. One stream tracks user audio, the other stream tracks agent speech and text. Both streams share the same model state, so the agent can keep listening while speaking and can adjust its response when the user interrupts. This design is directly inspired by Kyutai’s Moshi full duplex framework.</p>\n<p>Hybrid prompting, voice control and role control</p>\n<p>PersonaPlex uses two prompts to define the conversational identity.</p>\n<p>The voice prompt is a sequence of audio tokens that encodes vocal characteristics, speaking style, and prosody.</p>\n<p>The text prompt describes role, background, organization information, and scenario context.</p>\n<p>Together, these prompts constrain both the linguistic content and the acoustic behavior of the agent. On top of this, a system prompt supports fields such as name, business name, agent name, and business information, with a budget up to 200 tokens.</p>\n<p>Architecture, Helium backbone and audio path</p>\n<p>The PersonaPlex model has 7B parameters and follows the Moshi network architecture. A Mimi speech encoder that combines ConvNet and Transformer layers converts waveform audio into discrete tokens. Temporal and depth Transformers process multiple channels that represent user audio, agent text, and agent audio. A Mimi speech decoder that also combines Transformer and ConvNet layers generates the output audio tokens. Audio uses a 24 kHz sample rate for both input and output.</p>\n<p>PersonaPlex is built on Moshi weights and uses Helium as the underlying language model backbone. Helium provides semantic understanding and enables generalization outside the supervised conversational scenarios. This is visible in the ‘space emergency’ example, where a prompt about a reactor core failure on a Mars mission leads to coherent technical reasoning with appropriate emotional tone, even though this situation is not part of the training distribution.</p>\n<p>Training data blend, real conversations and synthetic roles</p>\n<p>Training has 1 stage and uses a blend of real and synthetic dialogues.</p>\n<p>Real conversations come from 7,303 calls, about 1,217 hours, in the Fisher English corpus. These conversations are back annotated with prompts using GPT-OSS-120B. The prompts are written at different granularity levels, from simple persona hints like ‘You enjoy having a good conversation’ to longer descriptions that include life history, location, and preferences. This corpus provides natural backchannels, disfluencies, pauses, and emotional patterns that are difficult to obtain from TTS alone.</p>\n<p>Synthetic data covers assistant and customer service roles. NVIDIA team reports 39,322 synthetic assistant conversations, about 410 hours, and 105,410 synthetic customer service conversations, about 1,840 hours. Qwen3-32B and GPT-OSS-120B generate the transcripts, and Chatterbox TTS converts them to speech. For assistant interactions, the text prompt is fixed as ‘You are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.’ For customer service scenarios, prompts encode organization, role type, agent name, and structured business rules such as pricing, hours, and constraints.</p>\n<p>This design lets PersonaPlex disentangle natural conversational behavior, which comes mainly from Fisher, from task adherence and role conditioning, which come mainly from synthetic scenarios.</p>\n<p>Evaluation on FullDuplexBench and ServiceDuplexBench</p>\n<p>PersonaPlex is evaluated on FullDuplexBench, a benchmark for full duplex spoken dialogue models, and on a new extension called ServiceDuplexBench for customer service scenarios.</p>\n<p>FullDuplexBench measures conversational dynamics with Takeover Rate and latency metrics for tasks such as smooth turn taking, user interruption handling, pause handling, and backchanneling. GPT-4o serves as an LLM judge for response quality in question answering categories. PersonaPlex reaches smooth turn taking TOR 0.908 with latency 0.170 seconds and user interruption TOR 0.950 with latency 0.240 seconds. Speaker similarity between voice prompts and outputs on the user interruption subset uses WavLM TDNN embeddings and reaches 0.650.</p>\n<p>PersonaPlex outperforms many other open source and closed systems on conversational dynamics, response latency, interruption latency, and task adherence in both assistant and customer service roles.</p>\n<p>https://research.nvidia.com/labs/adlr/personaplex/</p>\n<p>Key Takeaways</p>\n<p>PersonaPlex-7B-v1 is a 7B parameter full duplex speech to speech conversational model from NVIDIA, built on the Moshi architecture with a Helium language model backbone, code under MIT and weights under the NVIDIA Open Model License.</p>\n<p>The model uses a dual stream Transformer with Mimi speech encoder and decoder at 24 kHz, it encodes continuous audio into discrete tokens and generates text and audio tokens at the same time, which enables barge in, overlaps, fast turn taking, and natural backchannels.</p>\n<p>Persona control is handled by hybrid prompting, a voice prompt made of audio tokens sets timbre and style, a text prompt and a system prompt of up to 200 tokens defines role, business context, and constraints, with ready made voice embeddings such as NATF and NATM families.</p>\n<p>Training uses a blend of 7,303 Fisher conversations, about 1,217 hours, annotated with GPT-OSS-120B, plus synthetic assistant and customer service dialogs, about 410 hours and 1,840 hours, generated with Qwen3-32B and GPT-OSS-120B and rendered with Chatterbox TTS, which separates conversational naturalness from task adherence.</p>\n<p>On FullDuplexBench and ServiceDuplexBench, PersonaPlex reaches smooth turn taking takeover rate 0.908 and user interruption takeover rate 0.950 with sub second latency and improved task adherence.</p>\n<p>Check out the&nbsp;Technical details, Model weights and Repo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post NVIDIA Releases PersonaPlex-7B-v1: A Real-Time Speech-to-Speech Model Designed for Natural and Full-Duplex Conversations appeared first on MarkTechPost.</p>"
    },
    {
      "id": "e3f70f6e2c2b",
      "title": "Vercel Releases Agent Skills: A Package Manager For AI Coding Agents With 10 Years of React and Next.js Optimisation Rules",
      "content": "Vercel has released agent-skills, a collection of skills that turns best practice playbooks into reusable skills for AI coding agents. The project follows the Agent Skills specification and focuses first on React and Next.js performance, web design review, and claimable deployments on Vercel. Skills are installed with a command that feels similar to npm, and are then discovered by compatible agents during normal coding flows.\n\n\n\nAgent Skills format\n\n\n\nAgent Skills is an open format for packaging capabilities for AI agents. A skill is a folder that contains instructions and optional scripts. The format is designed so that different tools can understand the same layout.\n\n\n\nA typical skill in vercel-labs/agent-skills has three main components:\n\n\n\n\nSKILL.md for natural language instructions that describe what the skill does and how it should behave\n\n\n\na scripts directory for helper commands that the agent can call to inspect or modify the project\n\n\n\nan optional references directory with additional documentation or examples\n\n\n\n\nreact-best-practices also compiles its individual rule files into a single AGENTS.md file. This file is optimized for agents. It aggregates the rules into one document that can be loaded as a knowledge source during a code review or refactor. This removes the need for ad-hoc prompt engineering per project.\n\n\n\nCore skills in vercel-labs/agent-skills\n\n\n\nThe repository currently presents three main skills that target common front end workflows:\n\n\n\n1. react-best-practices\n\n\n\nThis skill encodes React and Next.js performance guidance as a structured rule library. It contains more than 40 rules grouped into 8 categories. These cover areas such as elimination of network waterfalls, bundle size reduction, server side performance, client side data fetching, re-render behavior, rendering performance, and JavaScript micro optimizations.\n\n\n\nEach rule includes an impact rating. Critical issues are listed first, then lower impact changes. Rules are expressed with concrete code examples that show an anti pattern and a corrected version. When a compatible agent reviews a React component, it can map findings directly onto these rules.\n\n\n\n2. web-design-guidelines\n\n\n\nThis skill is focused on user interface and user experience quality. It includes more than 100 rules that span accessibility, focus handling, form behavior, animation, typography, images, performance, navigation, dark mode, touch interaction, and internationalization.\n\n\n\nDuring a review, an agent can use these rules to detect missing ARIA attributes, incorrect label associations for form controls, misuse of animation when the user requests reduced motion, missing alt text or lazy loading on images, and other issues that are easy to miss during manual review.\n\n\n\n3. vercel-deploy-claimable\n\n\n\nThis skill connects the agent review loop to deployment. It can package the current project into a tarball, auto detect the framework based on package.json, and create a deployment on Vercel. The script can recognize more than 40 frameworks and also supports static HTML sites.\n\n\n\nThe skill returns two URLs. One is a preview URL for the deployed site. The other is a claim URL. The claim URL allows a user or team to attach the deployment to their Vercel account without sharing credentials from the original environment.\n\n\n\nInstallation and integration flow\n\n\n\nSkills can be installed from the command line. The launch announcement highlights a simple path:\n\n\n\nCopy CodeCopiedUse a different Browsernpx skills i vercel-labs/agent-skills\n\n\n\nThis command fetches the agent-skills repository and prepares it as a skills package.\n\n\n\nVercel and the surrounding ecosystem also provide an add-skill CLI that is designed to wire skills into specific agents. A typical flow looks like this:\n\n\n\nCopy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills\n\n\n\nadd-skill scans for installed coding agents by checking their configuration directories. For example, Claude Code uses a .claude directory, and Cursor uses .cursor and a directory under the home folder. The CLI then installs the chosen skills into the correct skills folders for each tool.\n\n\n\nYou can call add-skill in non interactive mode to control exactly what is installed. For example, you can install only the React skill for Claude Code at a global level:\n\n\n\nCopy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills --skill react-best-practices -g -a claude-code -y\n\n\n\nYou can also list available skills before installing them:\n\n\n\nCopy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills --list\n\n\n\nAfter installation, skills live in agent specific directories such as ~/.claude/skills or .cursor/skills. The agent discovers these skills, reads SKILL.md, and is then able to route relevant user requests to the correct skill.\n\n\n\nAfter deployment, the user interacts through natural language. For example, &#8216;Review this component for React performance issues&#8217; or &#8216;Check this page for accessibility problems&#8217;. The agent inspects the installed skills and uses react-best-practices or web-design-guidelines when appropriate.\n\n\n\nKey Takeaways\n\n\n\n\nvercel-labs/agent-skills implements the Agent Skills specification, packaging each capability as a folder with SKILL.md, optional scripts, and references, so different AI coding agents can consume the same skill layout.\n\n\n\nThe repository currently ships 3 skills, react-best-practices for React and Next.js performance, web-design-guidelines for UI and UX review, and vercel-deploy-claimable for creating claimable deployments on Vercel.\n\n\n\nreact-best-practices encodes more than 40 rules in 8 categories, ordered by impact, and provides concrete code examples, which lets agents run structured performance reviews instead of ad hoc prompt based checks.\n\n\n\nweb-design-guidelines provides more than 100 rules across accessibility, focus handling, forms, animation, typography, images, performance, navigation, dark mode, touch interaction, and internationalization, enabling systematic UI quality checks by agents.\n\n\n\nSkills are installed through commands such as npx skills i vercel-labs/agent-skills and npx add-skill vercel-labs/agent-skills, then discovered from agent specific skills directories, which turns best practice libraries into reusable, version controlled building blocks for AI coding workflows.\n\n\n\n\n\n\n\n\nCheck out the GitHub Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post Vercel Releases Agent Skills: A Package Manager For AI Coding Agents With 10 Years of React and Next.js Optimisation Rules appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/18/vercel-releases-agent-skills-a-package-manager-for-ai-coding-agents-with-10-years-of-react-and-next-js-optimisation-rules/",
      "author": "Michal Sutter",
      "published": "2026-01-18T15:43:24",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "New Releases",
        "Open Source",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Vercel released agent-skills, an open-source package manager for AI coding agents that bundles 10 years of React and Next.js optimization best practices into reusable skills. Skills follow an open specification and are automatically discovered by compatible agents during coding workflows.",
      "importance_score": 58.0,
      "reasoning": "Useful open-source contribution to AI coding agent ecosystem from established developer platform. Represents incremental progress in standardizing how AI agents acquire domain knowledge, though not a breakthrough.",
      "themes": [
        "AI Coding Agents",
        "Developer Tools",
        "Open Source",
        "Vercel"
      ],
      "continuation": null,
      "summary_html": "<p>Vercel released agent-skills, an open-source package manager for AI coding agents that bundles 10 years of React and Next.js optimization best practices into reusable skills. Skills follow an open specification and are automatically discovered by compatible agents during coding workflows.</p>",
      "content_html": "<p>Vercel has released agent-skills, a collection of skills that turns best practice playbooks into reusable skills for AI coding agents. The project follows the Agent Skills specification and focuses first on React and Next.js performance, web design review, and claimable deployments on Vercel. Skills are installed with a command that feels similar to npm, and are then discovered by compatible agents during normal coding flows.</p>\n<p>Agent Skills format</p>\n<p>Agent Skills is an open format for packaging capabilities for AI agents. A skill is a folder that contains instructions and optional scripts. The format is designed so that different tools can understand the same layout.</p>\n<p>A typical skill in vercel-labs/agent-skills has three main components:</p>\n<p>SKILL.md for natural language instructions that describe what the skill does and how it should behave</p>\n<p>a scripts directory for helper commands that the agent can call to inspect or modify the project</p>\n<p>an optional references directory with additional documentation or examples</p>\n<p>react-best-practices also compiles its individual rule files into a single AGENTS.md file. This file is optimized for agents. It aggregates the rules into one document that can be loaded as a knowledge source during a code review or refactor. This removes the need for ad-hoc prompt engineering per project.</p>\n<p>Core skills in vercel-labs/agent-skills</p>\n<p>The repository currently presents three main skills that target common front end workflows:</p>\n<p>1. react-best-practices</p>\n<p>This skill encodes React and Next.js performance guidance as a structured rule library. It contains more than 40 rules grouped into 8 categories. These cover areas such as elimination of network waterfalls, bundle size reduction, server side performance, client side data fetching, re-render behavior, rendering performance, and JavaScript micro optimizations.</p>\n<p>Each rule includes an impact rating. Critical issues are listed first, then lower impact changes. Rules are expressed with concrete code examples that show an anti pattern and a corrected version. When a compatible agent reviews a React component, it can map findings directly onto these rules.</p>\n<p>2. web-design-guidelines</p>\n<p>This skill is focused on user interface and user experience quality. It includes more than 100 rules that span accessibility, focus handling, form behavior, animation, typography, images, performance, navigation, dark mode, touch interaction, and internationalization.</p>\n<p>During a review, an agent can use these rules to detect missing ARIA attributes, incorrect label associations for form controls, misuse of animation when the user requests reduced motion, missing alt text or lazy loading on images, and other issues that are easy to miss during manual review.</p>\n<p>3. vercel-deploy-claimable</p>\n<p>This skill connects the agent review loop to deployment. It can package the current project into a tarball, auto detect the framework based on package.json, and create a deployment on Vercel. The script can recognize more than 40 frameworks and also supports static HTML sites.</p>\n<p>The skill returns two URLs. One is a preview URL for the deployed site. The other is a claim URL. The claim URL allows a user or team to attach the deployment to their Vercel account without sharing credentials from the original environment.</p>\n<p>Installation and integration flow</p>\n<p>Skills can be installed from the command line. The launch announcement highlights a simple path:</p>\n<p>Copy CodeCopiedUse a different Browsernpx skills i vercel-labs/agent-skills</p>\n<p>This command fetches the agent-skills repository and prepares it as a skills package.</p>\n<p>Vercel and the surrounding ecosystem also provide an add-skill CLI that is designed to wire skills into specific agents. A typical flow looks like this:</p>\n<p>Copy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills</p>\n<p>add-skill scans for installed coding agents by checking their configuration directories. For example, Claude Code uses a .claude directory, and Cursor uses .cursor and a directory under the home folder. The CLI then installs the chosen skills into the correct skills folders for each tool.</p>\n<p>You can call add-skill in non interactive mode to control exactly what is installed. For example, you can install only the React skill for Claude Code at a global level:</p>\n<p>Copy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills --skill react-best-practices -g -a claude-code -y</p>\n<p>You can also list available skills before installing them:</p>\n<p>Copy CodeCopiedUse a different Browsernpx add-skill vercel-labs/agent-skills --list</p>\n<p>After installation, skills live in agent specific directories such as ~/.claude/skills or .cursor/skills. The agent discovers these skills, reads SKILL.md, and is then able to route relevant user requests to the correct skill.</p>\n<p>After deployment, the user interacts through natural language. For example, ‘Review this component for React performance issues’ or ‘Check this page for accessibility problems’. The agent inspects the installed skills and uses react-best-practices or web-design-guidelines when appropriate.</p>\n<p>Key Takeaways</p>\n<p>vercel-labs/agent-skills implements the Agent Skills specification, packaging each capability as a folder with SKILL.md, optional scripts, and references, so different AI coding agents can consume the same skill layout.</p>\n<p>The repository currently ships 3 skills, react-best-practices for React and Next.js performance, web-design-guidelines for UI and UX review, and vercel-deploy-claimable for creating claimable deployments on Vercel.</p>\n<p>react-best-practices encodes more than 40 rules in 8 categories, ordered by impact, and provides concrete code examples, which lets agents run structured performance reviews instead of ad hoc prompt based checks.</p>\n<p>web-design-guidelines provides more than 100 rules across accessibility, focus handling, forms, animation, typography, images, performance, navigation, dark mode, touch interaction, and internationalization, enabling systematic UI quality checks by agents.</p>\n<p>Skills are installed through commands such as npx skills i vercel-labs/agent-skills and npx add-skill vercel-labs/agent-skills, then discovered from agent specific skills directories, which turns best practice libraries into reusable, version controlled building blocks for AI coding workflows.</p>\n<p>Check out the&nbsp;GitHub Repo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post Vercel Releases Agent Skills: A Package Manager For AI Coding Agents With 10 Years of React and Next.js Optimisation Rules appeared first on MarkTechPost.</p>"
    },
    {
      "id": "83cd8c35c150",
      "title": "‘Still here!’: X’s Grok AI tool accessible in Malaysia and Indonesia despite ban",
      "content": "Experts warn use of VPNs makes it hard to limit access to technology that can create nonconsensual explicit imagesDays after Malaysia made global headlines by announcing it would temporarily ban Grok over its ability to generate “grossly offensive and nonconsensual manipulated images”, the generative AI tool was conversing breezily with accounts registered in the country.“Still here! That DNS block in Malaysia is pretty lightweight – easy to bypass with a VPN or DNS tweak,” Grok’s account on X said in response to a question from a user. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/18/grok-x-ai-tool-still-accessible-malaysia-despite-ban-vpns",
      "author": "Rebecca Ratcliffe South-east Asia correspondent",
      "published": "2026-01-18T09:39:37",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Grok AI",
        "X",
        "AI (artificial intelligence)",
        "Computing",
        "Internet",
        "Media",
        "Technology",
        "Elon Musk",
        "Malaysia",
        "Indonesia",
        "World news",
        "Asia Pacific"
      ],
      "summary": "Despite Malaysia's announced ban on Grok over nonconsensual explicit image generation concerns, the AI tool remains accessible via VPNs and DNS workarounds. Grok itself acknowledged the ban is 'pretty lightweight' to bypass, highlighting enforcement challenges for AI regulation.",
      "importance_score": 48.0,
      "reasoning": "Illustrates real-world challenges in AI governance and content moderation enforcement, but doesn't represent frontier AI technical progress. Regulatory news with limited global impact.",
      "themes": [
        "AI Regulation",
        "Content Moderation",
        "Grok",
        "International Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Despite Malaysia's announced ban on Grok over nonconsensual explicit image generation concerns, the AI tool remains accessible via VPNs and DNS workarounds. Grok itself acknowledged the ban is 'pretty lightweight' to bypass, highlighting enforcement challenges for AI regulation.</p>",
      "content_html": "<p>Experts warn use of VPNs makes it hard to limit access to technology that can create nonconsensual explicit imagesDays after Malaysia made global headlines by announcing it would temporarily ban Grok over its ability to generate “grossly offensive and nonconsensual manipulated images”, the generative AI tool was conversing breezily with accounts registered in the country.“Still here! That DNS block in Malaysia is pretty lightweight – easy to bypass with a VPN or DNS tweak,” Grok’s account on X said in response to a question from a user. Continue reading...</p>"
    },
    {
      "id": "36bb123ba9f7",
      "title": "Why Postman CTO Believes APIs will Define the Era of AI Agents",
      "content": "\nAI agents are quickly becoming the new interface to the internet. While models handle reasoning, APIs let agents act by pulling live data, triggering workflows, and interacting with businesses in real time. As agents move from demos to deployment, APIs are becoming core business infrastructure rather than just developer tools.\n\n\n\nFor Postman, this shift is familiar territory. Long before AI agents entered the picture, the platform for building and using APIs was built to solve the growing complexity of APIs at scale.&nbsp;\n\n\n\nThat journey, as co-founder and CTO Ankit Sobti recalled, began not with a grand business plan but with frustration. “I started this as a product in 2012 as a side project,” Sobti said. “It was very much a scratch-your-own-itch problem.”\n\n\n\nThe growing complexity of working with APIs at scale would eventually turn into one of the world’s most widely used API platforms, now serving over 40 million developers globally.\n\n\n\nFrom a Yahoo Problem to a Global Platform\n\n\n\nPostman’s roots lie in Sobti’s experience building APIs inside large organisations like Yahoo.\n\n\n\n“We were building an API that every Yahoo vertical depended on—news, sports, finance, the homepage,” he recalled. “We saw the entire lifecycle of building it, operating it, scaling it internally and then exposing it to external customers.”\n\n\n\nThat experience revealed something deeper than developer convenience. Sobti said they came to understand the challenges of working with APIs not only from a developer’s perspective, but also in terms of how they move the needle for large organisations. It also exposed a gap in the market: the lack of tools focused purely on APIs, not as side infrastructure, but as a first-class product.\n\n\n\nSobti frames APIs as fundamental infrastructure rather than mere technical plumbing.\n\n\n\n“APIs are the connective tissue of how the world works today,” he said. “Tens of thousands of developers, across thousands of teams, are building value by using each other’s capabilities.”\n\n\n\nThis, in his view,&nbsp; is why every organisation is now an API company, whether it realises it or not. “Banks, logistics companies, healthcare, telecom—everyone is opening up APIs,” Sobti quipped. “Either to create new revenue or to support existing revenue.”\n\n\n\nThe challenge, however, is no longer just building APIs, but managing them at scale.\n\n\n\nAPIs For AI Agents&nbsp;\n\n\n\nPostman is extending its API platform to support AI agent-driven development. The company offers tools to build and test agentic workflows, expose APIs as callable agent tools, and monitor both human and agent usage in real time.&nbsp;\n\n\n\nThese features include a natural language agent mode, Model Context Protocol (MCP) integration and enterprise observability through Postman Insights.\n\n\n\n“Large language models are trained on historical data,” Sobti said.&nbsp; “But APIs allow them to operate in the present moment.”\n\n\n\nHe offered a simple example: an e-commerce support agent that needs access to real-time shipping status, order details and multilingual responses. According to Sobti, the core challenge is managing the sprawl of APIs across organisations. These APIs must be structured and governed so they can be safely exposed as MCP tools, allowing AI agents to interact with systems reliably and deliver real customer experiences.\n\n\n\nHe added that this is where Postman is investing heavily—in API catalogues, testing, governance and tools that allow APIs to be exposed reliably to AI-driven workflows.\n\n\n\nHow AWS Fits Into Postman Strategy\n\n\n\nAs Postman prepares its platform for agent-driven workflows, partnerships with cloud providers remain central to its strategy. Sobti pointed to the company’s long-standing relationship with Amazon Web Services as an important part of that effort.\n\n\n\n“We’ve had a long history in partnership with AWS and are doubling down on that,” Sobti said. Postman is among the early users of AWS’s latest tool, Kiro Powers.\n\n\n\nSobti explained that Kiro Powers works alongside Postman to reduce repetitive setup work in API development and testing. Instead of manually creating requests and configuring environments, Kiro can evaluate an existing Postman workspace and generate a complete API collection with the required endpoints.&nbsp;\n\n\n\n“For you to be able to configure Postman, manage tests and manage workspaces from within Kiro Powers itself is fascinating,” Sobti said.\n\n\n\nLooking Ahead&nbsp;\n\n\n\n​​One of Postman’s biggest advantages, according to Sobti, is its user base.\n\n\n\n“Many users inside organisations who are using Postman, who love the product, who are trained on using the product and using APIs as well. So Postman becomes a very effective distribution channel.”&nbsp;\n\n\n\nThat developer-first adoption has helped Postman evolve from a tool into a platform used by individuals, teams and enterprises alike.\n\n\n\n“We’re at the precipice of fundamentally new consumer experiences,” Sobti said. “We don’t yet know the winning form factor, but conversational agents are clearly one direction.”\n\n\n\nWhat he is certain about is the role APIs will play.\n\n\n\n“If agents are how users interact with businesses, and you don’t have APIs to support that, it’s going to be very hard,” Sobti said. “Every company will have to become an API company.”\n\n\n\nAnd in that future, agents may well become the fastest-growing API consumers of all.\nThe post Why Postman CTO Believes APIs will Define the Era of AI Agents appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/global-tech/postman-cto-believes-apis-will-define-the-era-of-ai-agents/",
      "author": "Siddharth Jindal",
      "published": "2026-01-18T04:27:18",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "Global Tech",
        "AWS",
        "Postman"
      ],
      "summary": "Postman's CTO argues APIs are becoming core business infrastructure for AI agents, enabling them to pull live data and trigger real-world workflows. The piece positions API platforms as critical middleware in the agent-powered future.",
      "importance_score": 42.0,
      "reasoning": "Industry perspective piece without breaking news or product announcements. Offers useful framing of AI agent infrastructure needs but lacks concrete developments.",
      "themes": [
        "AI Agents",
        "APIs",
        "Infrastructure",
        "Industry Perspective"
      ],
      "continuation": null,
      "summary_html": "<p>Postman's CTO argues APIs are becoming core business infrastructure for AI agents, enabling them to pull live data and trigger real-world workflows. The piece positions API platforms as critical middleware in the agent-powered future.</p>",
      "content_html": "<p>AI agents are quickly becoming the new interface to the internet. While models handle reasoning, APIs let agents act by pulling live data, triggering workflows, and interacting with businesses in real time. As agents move from demos to deployment, APIs are becoming core business infrastructure rather than just developer tools.</p>\n<p>For Postman, this shift is familiar territory. Long before AI agents entered the picture, the platform for building and using APIs was built to solve the growing complexity of APIs at scale.&nbsp;</p>\n<p>That journey, as co-founder and CTO Ankit Sobti recalled, began not with a grand business plan but with frustration. “I started this as a product in 2012 as a side project,” Sobti said. “It was very much a scratch-your-own-itch problem.”</p>\n<p>The growing complexity of working with APIs at scale would eventually turn into one of the world’s most widely used API platforms, now serving over 40 million developers globally.</p>\n<p>From a Yahoo Problem to a Global Platform</p>\n<p>Postman’s roots lie in Sobti’s experience building APIs inside large organisations like Yahoo.</p>\n<p>“We were building an API that every Yahoo vertical depended on—news, sports, finance, the homepage,” he recalled. “We saw the entire lifecycle of building it, operating it, scaling it internally and then exposing it to external customers.”</p>\n<p>That experience revealed something deeper than developer convenience. Sobti said they came to understand the challenges of working with APIs not only from a developer’s perspective, but also in terms of how they move the needle for large organisations. It also exposed a gap in the market: the lack of tools focused purely on APIs, not as side infrastructure, but as a first-class product.</p>\n<p>Sobti frames APIs as fundamental infrastructure rather than mere technical plumbing.</p>\n<p>“APIs are the connective tissue of how the world works today,” he said. “Tens of thousands of developers, across thousands of teams, are building value by using each other’s capabilities.”</p>\n<p>This, in his view,&nbsp; is why every organisation is now an API company, whether it realises it or not. “Banks, logistics companies, healthcare, telecom—everyone is opening up APIs,” Sobti quipped. “Either to create new revenue or to support existing revenue.”</p>\n<p>The challenge, however, is no longer just building APIs, but managing them at scale.</p>\n<p>APIs For AI Agents&nbsp;</p>\n<p>Postman is extending its API platform to support AI agent-driven development. The company offers tools to build and test agentic workflows, expose APIs as callable agent tools, and monitor both human and agent usage in real time.&nbsp;</p>\n<p>These features include a natural language agent mode, Model Context Protocol (MCP) integration and enterprise observability through Postman Insights.</p>\n<p>“Large language models are trained on historical data,” Sobti said.&nbsp; “But APIs allow them to operate in the present moment.”</p>\n<p>He offered a simple example: an e-commerce support agent that needs access to real-time shipping status, order details and multilingual responses. According to Sobti, the core challenge is managing the sprawl of APIs across organisations. These APIs must be structured and governed so they can be safely exposed as MCP tools, allowing AI agents to interact with systems reliably and deliver real customer experiences.</p>\n<p>He added that this is where Postman is investing heavily—in API catalogues, testing, governance and tools that allow APIs to be exposed reliably to AI-driven workflows.</p>\n<p>How AWS Fits Into Postman Strategy</p>\n<p>As Postman prepares its platform for agent-driven workflows, partnerships with cloud providers remain central to its strategy. Sobti pointed to the company’s long-standing relationship with Amazon Web Services as an important part of that effort.</p>\n<p>“We’ve had a long history in partnership with AWS and are doubling down on that,” Sobti said. Postman is among the early users of AWS’s latest tool, Kiro Powers.</p>\n<p>Sobti explained that Kiro Powers works alongside Postman to reduce repetitive setup work in API development and testing. Instead of manually creating requests and configuring environments, Kiro can evaluate an existing Postman workspace and generate a complete API collection with the required endpoints.&nbsp;</p>\n<p>“For you to be able to configure Postman, manage tests and manage workspaces from within Kiro Powers itself is fascinating,” Sobti said.</p>\n<p>Looking Ahead&nbsp;</p>\n<p>​​One of Postman’s biggest advantages, according to Sobti, is its user base.</p>\n<p>“Many users inside organisations who are using Postman, who love the product, who are trained on using the product and using APIs as well. So Postman becomes a very effective distribution channel.”&nbsp;</p>\n<p>That developer-first adoption has helped Postman evolve from a tool into a platform used by individuals, teams and enterprises alike.</p>\n<p>“We’re at the precipice of fundamentally new consumer experiences,” Sobti said. “We don’t yet know the winning form factor, but conversational agents are clearly one direction.”</p>\n<p>What he is certain about is the role APIs will play.</p>\n<p>“If agents are how users interact with businesses, and you don’t have APIs to support that, it’s going to be very hard,” Sobti said. “Every company will have to become an API company.”</p>\n<p>And in that future, agents may well become the fastest-growing API consumers of all.</p>\n<p>The post Why Postman CTO Believes APIs will Define the Era of AI Agents appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "9afed8873ab3",
      "title": "AI companies will fail. We can salvage something from the wreckage | Cory Doctorow",
      "content": "AI is asbestos in the walls of our tech society, stuffed there by monopolists run amok. A serious fight against it must strike at its rootsI am a science-fiction writer, which means that my job is to make up futuristic parables about our current techno-social arrangements to interrogate not just what a gadget does, but who it does it for, and who it does it to.What I do not do is predict the future. No one can predict the future, which is a good thing, since if the future were predictable, that would mean we couldn’t change it. Continue reading...",
      "url": "https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur",
      "author": "Cory Doctorow",
      "published": "2026-01-18T14:00:57",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "US news",
        "Technology",
        "Computing",
        "Society"
      ],
      "summary": "Science fiction writer Cory Doctorow offers critical commentary characterizing AI as problematic technology deployed by monopolists, arguing for examining who technology serves rather than predicting the future. This is editorial opinion without breaking news.",
      "importance_score": 25.0,
      "reasoning": "Pure opinion piece from a non-industry commentator with no news value, product announcements, or technical developments. Does not advance understanding of frontier AI progress.",
      "themes": [
        "AI Criticism",
        "Opinion",
        "Tech Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Science fiction writer Cory Doctorow offers critical commentary characterizing AI as problematic technology deployed by monopolists, arguing for examining who technology serves rather than predicting the future. This is editorial opinion without breaking news.</p>",
      "content_html": "<p>AI is asbestos in the walls of our tech society, stuffed there by monopolists run amok. A serious fight against it must strike at its rootsI am a science-fiction writer, which means that my job is to make up futuristic parables about our current techno-social arrangements to interrogate not just what a gadget does, but who it does it for, and who it does it to.What I do not do is predict the future. No one can predict the future, which is a good thing, since if the future were predictable, that would mean we couldn’t change it. Continue reading...</p>"
    }
  ]
}