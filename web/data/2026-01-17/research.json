{
  "category": "research",
  "date": "2026-01-17",
  "category_summary": "Today's research centers on **AI economics**, **model evaluation**, and **safety frameworks**. A large-scale study with **500+ professionals** and **13 LLMs** [establishes scaling laws](/?date=2026-01-17&category=research#item-5f1a61de76c4) for economic impact, finding each year of frontier progress reduces task completion time by measurable margins.\n\n- **Future-as-Label** [introduces self-supervised training](/?date=2026-01-17&category=research#item-9aa46e442121) using temporal outcomes from real-world data streams, eliminating costly human annotation\n- Mechanistic interpretability work [reveals models may exhibit **fixed biases**](/?date=2026-01-17&category=research#item-367da8e17c28) rather than genuine reasoning on inductive tasks\n- Revealed preference methodology applied across **GPT-5.1**, **Claude-Opus-4.5**, and other frontier models [to elicit trained character traits](/?date=2026-01-17&category=research#item-f21769ebc3ee)\n\nSafety contributions include a technical framework for [prioritizing **net-sabotage-value** vulnerabilities](/?date=2026-01-17&category=research#item-78a67b0f28eb) in AI control, plus analysis [reframing persuasion risk](/?date=2026-01-17&category=research#item-d8019c015caa) from adversarial to **trusted advisor** threat models. Historical precedent mapping for [**13 ASI failure modes**](/?date=2026-01-17&category=research#item-2a1aa51d4552) provides grounding for unprecedented risk scenarios.",
  "category_summary_html": "<p>Today's research centers on <strong>AI economics</strong>, <strong>model evaluation</strong>, and <strong>safety frameworks</strong>. A large-scale study with <strong>500+ professionals</strong> and <strong>13 LLMs</strong> <a href=\"/?date=2026-01-17&amp;category=research#item-5f1a61de76c4\" class=\"internal-link\" rel=\"noopener noreferrer\">establishes scaling laws</a> for economic impact, finding each year of frontier progress reduces task completion time by measurable margins.</p>\n<ul>\n<li><strong>Future-as-Label</strong> <a href=\"/?date=2026-01-17&amp;category=research#item-9aa46e442121\" class=\"internal-link\" rel=\"noopener noreferrer\">introduces self-supervised training</a> using temporal outcomes from real-world data streams, eliminating costly human annotation</li>\n<li>Mechanistic interpretability work <a href=\"/?date=2026-01-17&amp;category=research#item-367da8e17c28\" class=\"internal-link\" rel=\"noopener noreferrer\">reveals models may exhibit <strong>fixed biases</strong></a> rather than genuine reasoning on inductive tasks</li>\n<li>Revealed preference methodology applied across <strong>GPT-5.1</strong>, <strong>Claude-Opus-4.5</strong>, and other frontier models <a href=\"/?date=2026-01-17&amp;category=research#item-f21769ebc3ee\" class=\"internal-link\" rel=\"noopener noreferrer\">to elicit trained character traits</a></li>\n</ul>\n<p>Safety contributions include a technical framework for <a href=\"/?date=2026-01-17&amp;category=research#item-78a67b0f28eb\" class=\"internal-link\" rel=\"noopener noreferrer\">prioritizing <strong>net-sabotage-value</strong> vulnerabilities</a> in AI control, plus analysis <a href=\"/?date=2026-01-17&amp;category=research#item-d8019c015caa\" class=\"internal-link\" rel=\"noopener noreferrer\">reframing persuasion risk</a> from adversarial to <strong>trusted advisor</strong> threat models. Historical precedent mapping for <a href=\"/?date=2026-01-17&amp;category=research#item-2a1aa51d4552\" class=\"internal-link\" rel=\"noopener noreferrer\"><strong>13 ASI failure modes</strong></a> provides grounding for unprecedented risk scenarios.</p>",
  "themes": [
    {
      "name": "AI Safety & Alignment",
      "description": "Research on controlling AI systems, preventing misalignment, and ensuring beneficial outcomes from advanced AI",
      "item_count": 6,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "AI Economics & Scaling",
      "description": "Research connecting model improvements to real-world economic outcomes and productivity",
      "item_count": 3,
      "example_items": [],
      "importance": 72
    },
    {
      "name": "Model Evaluation & Capabilities",
      "description": "Studies examining what models can actually do, including reasoning abilities, personality traits, and limitations",
      "item_count": 3,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "Training Methods",
      "description": "Novel approaches to training AI systems, including self-supervised and scalable supervision techniques",
      "item_count": 1,
      "example_items": [],
      "importance": 65
    },
    {
      "name": "Digital Minds & AI Ethics",
      "description": "Considerations of AI consciousness, moral status, and ethical treatment of AI systems",
      "item_count": 2,
      "example_items": [],
      "importance": 45
    }
  ],
  "total_items": 17,
  "items": [
    {
      "id": "5f1a61de76c4",
      "title": "Scaling Laws for Economic Impacts: Experimental Evidence from 500 Professionals and 13 LLMs",
      "content": "Scaling laws tell us that the cross-entropy loss of a model improves predictably with more compute. However, the way this relates to real-world economic outcomes that people directly care about is non-obvious. Scaling Laws for Economic Impacts aims to bridge this gap by running human-uplift experiments on professionals where model training compute is randomized between participants.&nbsp;The headline findings: each year of frontier model progress reduces professional task completion time by roughly 8%. Decomposing this, about 56% comes from increased training compute and 44% from algorithmic improvements. Incorporating these results into the famously pessimistic macroeconomic framework of Acemoglu (2024) implies productivity growth over the next decade due to AI even under strict assumptions rises from 0.5% to 20%. But there's a puzzle—while raw model output quality scales with compute, human-AI collaborative output quality stays flat. Users seem to cap the realized gains from better models, regressing outputs toward their own baseline regardless of how capable the tool is.Experiment Setup:Concretely, over 500 consultants, data analysts, and managers were given professional tasks to complete with models ranging from Llama-2 to GPT-5 (or no AI assistance at all) in a pre-registered experiment. Participants were recruited through Prolific, but eligibility required at least one year of professional experience, salaries above $40,000, and passing a rigorous screening survey that filtered out roughly 90% of applicants. The final sample averaged over four years of experience.Each participant completed a subset of nine tasks designed to simulate real workflows: revising financial expansion reports, conducting A/B test analyses, writing crisis management memos, evaluating vendor contracts, creating project Gantt charts, and more (full task descriptions in the appendix of the linked paper). Incentives were high-powered—$15 base pay per task, with an additional $15 bonus for ...",
      "url": "https://www.lesswrong.com/posts/kkm7GsDtqsywaWyM7/scaling-laws-for-economic-impacts-experimental-evidence-from",
      "author": "Ali Merali",
      "published": "2026-01-16T08:40:13.792000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Experimental study with 500+ professionals testing 13 LLMs of varying compute levels on real tasks. Finds each year of frontier progress reduces task completion time by ~8% (56% from compute scaling, 44% algorithmic). Key puzzle: human-AI collaborative output quality stays flat despite improving models, suggesting users cap realized gains.",
      "importance_score": 85,
      "reasoning": "Rigorous experimental methodology with large sample size, multiple models, and economically meaningful metrics. Novel finding about human-AI collaboration bottleneck. Directly addresses scaling law implications for real economic outcomes. Updates Acemoglu (2024) productivity estimates from 0.5% to 20%.",
      "themes": [
        "Scaling Laws",
        "AI Economics",
        "Human-AI Collaboration",
        "Productivity",
        "Empirical Research"
      ],
      "continuation": null,
      "summary_html": "<p>Experimental study with 500+ professionals testing 13 LLMs of varying compute levels on real tasks. Finds each year of frontier progress reduces task completion time by ~8% (56% from compute scaling, 44% algorithmic). Key puzzle: human-AI collaborative output quality stays flat despite improving models, suggesting users cap realized gains.</p>",
      "content_html": "<p>Scaling laws tell us that the cross-entropy loss of a model improves predictably with more compute. However, the way this relates to real-world economic outcomes that people directly care about is non-obvious. Scaling Laws for Economic Impacts aims to bridge this gap by running human-uplift experiments on professionals where model training compute is randomized between participants.&nbsp;The headline findings: each year of frontier model progress reduces professional task completion time by roughly 8%. Decomposing this, about 56% comes from increased training compute and 44% from algorithmic improvements. Incorporating these results into the famously pessimistic macroeconomic framework of Acemoglu (2024) implies productivity growth over the next decade due to AI even under strict assumptions rises from 0.5% to 20%. But there's a puzzle—while raw model output quality scales with compute, human-AI collaborative output quality stays flat. Users seem to cap the realized gains from better models, regressing outputs toward their own baseline regardless of how capable the tool is.Experiment Setup:Concretely, over 500 consultants, data analysts, and managers were given professional tasks to complete with models ranging from Llama-2 to GPT-5 (or no AI assistance at all) in a pre-registered experiment. Participants were recruited through Prolific, but eligibility required at least one year of professional experience, salaries above $40,000, and passing a rigorous screening survey that filtered out roughly 90% of applicants. The final sample averaged over four years of experience.Each participant completed a subset of nine tasks designed to simulate real workflows: revising financial expansion reports, conducting A/B test analyses, writing crisis management memos, evaluating vendor contracts, creating project Gantt charts, and more (full task descriptions in the appendix of the linked paper). Incentives were high-powered—$15 base pay per task, with an additional $15 bonus for ...</p>"
    },
    {
      "id": "9aa46e442121",
      "title": "Future-as-Label: Scalable Supervision from Real-World Outcomes",
      "content": "AI can learn directly from the passage of time at unlimited scale—no human annotation required.Time provides free supervision. Humans learn from experience with no labels—we constantly form expectations about the world, notice when we're wrong, and update our models accordingly.\"Future-as-Label\" teaches AI to learn the same way. The passage of time provides labels that require no annotation.This unlocks unlimited training data for AI from streams of data, with zero human bottlenecks.Here we apply this to historical news streams, then evaluate the trained model on public Metaculus questions.&nbsp;Result: fine-tuning with \"Future-as-Label\" improves Brier score by 27% over the base model (Qwen3-32B) and halves calibration error, even outperforming a 7× larger model (Qwen3-235B) of the same generation.&nbsp;Full paper: https://arxiv.org/abs/2601.06336",
      "url": "https://www.lesswrong.com/posts/esPKrnXfndwuKW3Cn/future-as-label-scalable-supervision-from-real-world",
      "author": "Ben Turtel",
      "published": "2026-01-16T16:21:26.842000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Introduces 'Future-as-Label' training methodology that uses temporal outcomes from real-world data streams as supervision signal, eliminating need for human annotation. Fine-tuning Qwen3-32B on historical news improved Brier score by 27% and halved calibration error, outperforming the 7× larger Qwen3-235B on Metaculus forecasting questions.",
      "importance_score": 78,
      "reasoning": "Novel training methodology with strong empirical results and linked arXiv paper. Demonstrates scalable supervision approach with practical forecasting improvements. Could have significant implications for training data efficiency.",
      "themes": [
        "Training Methods",
        "Forecasting",
        "Self-Supervised Learning",
        "Scalability"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces 'Future-as-Label' training methodology that uses temporal outcomes from real-world data streams as supervision signal, eliminating need for human annotation. Fine-tuning Qwen3-32B on historical news improved Brier score by 27% and halved calibration error, outperforming the 7× larger Qwen3-235B on Metaculus forecasting questions.</p>",
      "content_html": "<p>AI can learn directly from the passage of time at unlimited scale—no human annotation required.Time provides free supervision. Humans learn from experience with no labels—we constantly form expectations about the world, notice when we're wrong, and update our models accordingly.\"Future-as-Label\" teaches AI to learn the same way. The passage of time provides labels that require no annotation.This unlocks unlimited training data for AI from streams of data, with zero human bottlenecks.Here we apply this to historical news streams, then evaluate the trained model on public Metaculus questions.&nbsp;Result: fine-tuning with \"Future-as-Label\" improves Brier score by 27% over the base model (Qwen3-32B) and halves calibration error, even outperforming a 7× larger model (Qwen3-235B) of the same generation.&nbsp;Full paper: https://arxiv.org/abs/2601.06336</p>"
    },
    {
      "id": "f21769ebc3ee",
      "title": "Eliciting Frontier Model Character Training",
      "content": "The character of a model has an immense impact on the way people perceive and form relationships with AI systems, and to many users, it takes precedence over raw capability improvements. Given the increased relevance of the ‘personality’ of AI models, in this blog post, we take the revealed preference method described in Open Character Training (Maiya et. al, 2025)[1]&nbsp;to elicit the character training of all major closed and open-source frontier model families.Figure 1: Shows trait expression (expressed in ELO ratings ranging from 393 to 1521) for 144 traits across all major frontier models. Lines closer to the circle edge represent higher trait expression. Of the 36 of 144 traits shown on the graph, black traits are more expressive, whereas red traits are less so. Models show consistent preferences around their top traits, but the correlation breaks down near the end.This method employs an external judge to determine a model’s personality (e.g., GLM 4.5 Air judging GPT-5.1), rather than simply asking the model to assess itself. This distinction is critical: extensive literature shows that in self-reporting, models perform barely better than random when compared to a human oracle (Han et al., 2025)[2]&nbsp;(Zou et al., 2024)[3].And so, we must use an LLM judge, and specifically a base model with no character/post-training of its own, to elicit models’ personalities. GLM 4.5 Air is one of the few publicly available base models released in the past year and has been used as a pre-training base for strong post-trained models like Prime Intellect’s INTELLECT-3, making it a good fit for our purposes (GLM 4.5 Team)[4]&nbsp;(Prime Intellect Team)[5]Experimental SetupThe revealed preference method involves three parts: the generation of responses from tested models, judgment by GLM 4.5 Air, and the calculation of trait expression via the ELO scoring method. With small modifications, we use almost the exact same method proposed in the original Maiya et. al paper, which w...",
      "url": "https://www.lesswrong.com/posts/x8QnZAHwkbeBkCsEx/eliciting-frontier-model-character-training",
      "author": "avikrishna",
      "published": "2026-01-16T15:15:30.364000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Systematic study applying revealed preference methods to elicit personality/character traits across all major frontier models (including GPT-5.1, Claude, Gemini-3). Uses external judge models rather than self-reporting, measuring 144 traits and finding consistent top-trait preferences across models but divergence in lower-ranked traits.",
      "importance_score": 72,
      "reasoning": "Comprehensive empirical study covering major frontier models with methodologically sound approach (external judging vs self-report). Useful benchmark data for understanding model behavior and character alignment. References current frontier models.",
      "themes": [
        "Model Evaluation",
        "AI Alignment",
        "Model Behavior",
        "Personality/Character"
      ],
      "continuation": null,
      "summary_html": "<p>Systematic study applying revealed preference methods to elicit personality/character traits across all major frontier models (including GPT-5.1, Claude, Gemini-3). Uses external judge models rather than self-reporting, measuring 144 traits and finding consistent top-trait preferences across models but divergence in lower-ranked traits.</p>",
      "content_html": "<p>The character of a model has an immense impact on the way people perceive and form relationships with AI systems, and to many users, it takes precedence over raw capability improvements. Given the increased relevance of the ‘personality’ of AI models, in this blog post, we take the revealed preference method described in Open Character Training (Maiya et. al, 2025)[1]&nbsp;to elicit the character training of all major closed and open-source frontier model families.Figure 1: Shows trait expression (expressed in ELO ratings ranging from 393 to 1521) for 144 traits across all major frontier models. Lines closer to the circle edge represent higher trait expression. Of the 36 of 144 traits shown on the graph, black traits are more expressive, whereas red traits are less so. Models show consistent preferences around their top traits, but the correlation breaks down near the end.This method employs an external judge to determine a model’s personality (e.g., GLM 4.5 Air judging GPT-5.1), rather than simply asking the model to assess itself. This distinction is critical: extensive literature shows that in self-reporting, models perform barely better than random when compared to a human oracle (Han et al., 2025)[2]&nbsp;(Zou et al., 2024)[3].And so, we must use an LLM judge, and specifically a base model with no character/post-training of its own, to elicit models’ personalities. GLM 4.5 Air is one of the few publicly available base models released in the past year and has been used as a pre-training base for strong post-trained models like Prime Intellect’s INTELLECT-3, making it a good fit for our purposes (GLM 4.5 Team)[4]&nbsp;(Prime Intellect Team)[5]Experimental SetupThe revealed preference method involves three parts: the generation of responses from tested models, judgment by GLM 4.5 Air, and the calculation of trait expression via the ELO scoring method. With small modifications, we use almost the exact same method proposed in the original Maiya et. al paper, which w...</p>"
    },
    {
      "id": "367da8e17c28",
      "title": "Is It Reasoning or Just a Fixed Bias?",
      "content": "This is my first mechanistic interpretability blog post! I decided to research whether models are actually reasoning when answering non-deductive questions, or whether they're doing something simpler.My dataset is adapted from InAbHyD[1], and it's composed of inductive and abductive reasoning scenarios in first-order ontologies generated through code (using made-up concepts to dismiss much of the external effect of common words). These scenarios have multiple technically correct answers, but one answer is definitively the most correct[2]. I found that LLMs seem to have a fixed generalization tendency (when evaluating my examples) that doesn't seem to adapt to any logical structure. And accuracies in 1-hop and 2-hop reasoning add up to roughly 100% for most models.Additionally, there's a large overlap between H2 successes and H1 failures (73% for DeepSeek V3), meaning that model outputs the parent concept regardless of whether the task asks for the child concept or the parent concept. This behavior suggests that the model isn't actually reasoning, instead generalizing to a fixed level that aligns with the parent concept.This is perplexing because in proper reasoning, you generally need the child concept in order to get the parent concept. For example, you'd conclude that Fae is a mammal through a reasoning chain, first establishing that Fae is a tiger, then making one hop to say that Fae is a feline (child concept), and then making a second hop to say that felines are mammals (parent concept). The overlap, though, suggests that the model isn't reasoning through the ontology, and it's skipping the chain to output the parent or child concept depending on its fixed generalization tendency.I used MI techniques like probing, activation patching, and SAEs in my research. Probing predicted something related to the output at layer 8 (very early), but patching early layers barely made a difference in the final decision. This makes it more likely that whatever probing predicte...",
      "url": "https://www.lesswrong.com/posts/kQvouwwHnEJkJ47uv/is-it-reasoning-or-just-a-fixed-bias-1",
      "author": "Sriram Kiron",
      "published": "2026-01-16T16:43:36.456000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Mechanistic interpretability study investigating whether LLMs actually reason on inductive/abductive tasks or exhibit fixed biases. Finds models have a consistent generalization tendency (outputting parent concepts regardless of task requirements) with 1-hop and 2-hop accuracies summing to ~100%, suggesting models aren't performing genuine reasoning but applying fixed heuristics.",
      "importance_score": 68,
      "reasoning": "Original interpretability research with clear experimental methodology and interesting findings about model capabilities. Tests multiple models including DeepSeek V3. However, dataset is limited and findings are preliminary.",
      "themes": [
        "Mechanistic Interpretability",
        "Language Model Evaluation",
        "Reasoning Capabilities"
      ],
      "continuation": null,
      "summary_html": "<p>Mechanistic interpretability study investigating whether LLMs actually reason on inductive/abductive tasks or exhibit fixed biases. Finds models have a consistent generalization tendency (outputting parent concepts regardless of task requirements) with 1-hop and 2-hop accuracies summing to ~100%, suggesting models aren't performing genuine reasoning but applying fixed heuristics.</p>",
      "content_html": "<p>This is my first mechanistic interpretability blog post! I decided to research whether models are actually reasoning when answering non-deductive questions, or whether they're doing something simpler.My dataset is adapted from InAbHyD[1], and it's composed of inductive and abductive reasoning scenarios in first-order ontologies generated through code (using made-up concepts to dismiss much of the external effect of common words). These scenarios have multiple technically correct answers, but one answer is definitively the most correct[2]. I found that LLMs seem to have a fixed generalization tendency (when evaluating my examples) that doesn't seem to adapt to any logical structure. And accuracies in 1-hop and 2-hop reasoning add up to roughly 100% for most models.Additionally, there's a large overlap between H2 successes and H1 failures (73% for DeepSeek V3), meaning that model outputs the parent concept regardless of whether the task asks for the child concept or the parent concept. This behavior suggests that the model isn't actually reasoning, instead generalizing to a fixed level that aligns with the parent concept.This is perplexing because in proper reasoning, you generally need the child concept in order to get the parent concept. For example, you'd conclude that Fae is a mammal through a reasoning chain, first establishing that Fae is a tiger, then making one hop to say that Fae is a feline (child concept), and then making a second hop to say that felines are mammals (parent concept). The overlap, though, suggests that the model isn't reasoning through the ontology, and it's skipping the chain to output the parent or child concept depending on its fixed generalization tendency.I used MI techniques like probing, activation patching, and SAEs in my research. Probing predicted something related to the output at layer 8 (very early), but patching early layers barely made a difference in the final decision. This makes it more likely that whatever probing predicte...</p>"
    },
    {
      "id": "78a67b0f28eb",
      "title": "Should control down-weight negative net-sabotage-value threats?",
      "content": "These are my personal views. Thank you to Ryan Greenblatt, Holden Karnofsky, and Peter Wildeford for useful discussions. The bad takes are my own.When deciding how much to spend on mitigating a vulnerability that a competent scheming AI might exploit, you might be tempted to use E[damages | AI decides to take advantage of the vulnerability] to decide how important mitigating that vulnerability is.But this misses how a strategic scheming AI might decide to not take advantage of some vulnerabilities, because being caught could reduce how likely it would be to take advantage of better future opportunities. So instead you might want to focus on making net-sabotage-value negative:Focus on mitigating vulnerabilities that are net-positive from the scheming AI’s perspective, that is if the net-sabotage-value E[damages - value to humans from catching the AI | AI decides to take advantage of the vulnerability] is positive. Down-weight the importance of those for which the net-sabotage-value is negative.Here is an illustration that paints a very simplified picture of the situation:&nbsp;In practice, you should often be uncertain about whether a vulnerability has negative net-sabotage-value and whether AIs know this has negative net-sabotage-value, and mitigating sabotage risk from AIs that are not competent schemers matters. This is a spherical-cow argument, stated somewhat provocatively. Take implications with a large grain of salt.This argument makes me put some weight on the following potential implications:Your willingness to spend on control mitigations specifically aimed at attacks from competent scheming AIs might still be very low at the point where those AIs can take actions that increase P(doom) by a few percent in isolation (without accounting for the possibility that P(doom) could be decreased as a second-order effect);Your willingness to spend on control might shoot up when you get good alignment news;Tools for risk assessment and prioritization from fields like n...",
      "url": "https://www.lesswrong.com/posts/stL8LMjFGYj7kQvQQ/should-control-down-weight-negative-net-sabotage-value",
      "author": "Fabien Roger",
      "published": "2026-01-15T23:18:09.045000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Technical AI control post arguing that when prioritizing vulnerability mitigation, one should focus on vulnerabilities with positive 'net-sabotage-value' from a scheming AI's perspective, and down-weight those where being caught would cost the AI more than the damage caused.",
      "importance_score": 65,
      "reasoning": "Substantive technical contribution to AI control methodology from researcher connected to Anthropic. Provides actionable framework for prioritizing safety investments. Adds nuance to control research paradigm.",
      "themes": [
        "AI Safety",
        "AI Control",
        "Alignment",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>Technical AI control post arguing that when prioritizing vulnerability mitigation, one should focus on vulnerabilities with positive 'net-sabotage-value' from a scheming AI's perspective, and down-weight those where being caught would cost the AI more than the damage caused.</p>",
      "content_html": "<p>These are my personal views. Thank you to Ryan Greenblatt, Holden Karnofsky, and Peter Wildeford for useful discussions. The bad takes are my own.When deciding how much to spend on mitigating a vulnerability that a competent scheming AI might exploit, you might be tempted to use E[damages | AI decides to take advantage of the vulnerability] to decide how important mitigating that vulnerability is.But this misses how a strategic scheming AI might decide to not take advantage of some vulnerabilities, because being caught could reduce how likely it would be to take advantage of better future opportunities. So instead you might want to focus on making net-sabotage-value negative:Focus on mitigating vulnerabilities that are net-positive from the scheming AI’s perspective, that is if the net-sabotage-value E[damages - value to humans from catching the AI | AI decides to take advantage of the vulnerability] is positive. Down-weight the importance of those for which the net-sabotage-value is negative.Here is an illustration that paints a very simplified picture of the situation:&nbsp;In practice, you should often be uncertain about whether a vulnerability has negative net-sabotage-value and whether AIs know this has negative net-sabotage-value, and mitigating sabotage risk from AIs that are not competent schemers matters. This is a spherical-cow argument, stated somewhat provocatively. Take implications with a large grain of salt.This argument makes me put some weight on the following potential implications:Your willingness to spend on control mitigations specifically aimed at attacks from competent scheming AIs might still be very low at the point where those AIs can take actions that increase P(doom) by a few percent in isolation (without accounting for the possibility that P(doom) could be decreased as a second-order effect);Your willingness to spend on control might shoot up when you get good alignment news;Tools for risk assessment and prioritization from fields like n...</p>"
    },
    {
      "id": "d8019c015caa",
      "title": "Powerful misaligned AIs may be extremely persuasive, especially absent mitigations",
      "content": "The concise one minute post for frequent readers of this forumHere are some important, concise intellectual nuggets of progress to I made for myself through writing this post (the post also has things I thought were obvious):When people imagine persuasion, they imagine a situation kind of a like a salesman trying to convince you of something. This the wrong framing: it will instead be completely in your interest to trust what the AI says, and the primary question will be whether or not you even notice that the ‘persuaded material’ is off, and if you will be truth-seeking enough to pursue this further. Unfortunately, an extraordinary amount of the incentives will be to believe the AI.This is kind of explicit in the control frame but not something I’ve connected to persuasion. And there are other connections that feel salient too; the more adversarial you are to trusting your AI, the slower you will be, which might cause you to get outcompeted.The AI can potentially have quite a large surface area to attack you with. It can target you, your close allies, your constituents, your social media (basically all the information you see). And you may not be able to avoid AI even if you tried.I’m less compelled by arguments that the ‘truth will win out’. The historical track record for humans changing their mind when lots of trusted and/or smart people around them believe something isn’t great. And the situation with AIs is even worse, as the AIs will have strong influence over both your cognition and the information you see.AI’s choosing not to collude doesn’t obviously solve the problem. Even if you are in a situation where two AIs aren’t colluding and instead debating, it will work about as good as you expect AI debate to do, which isn’t clear. And also the AIs might not even be placed to cath each others lies.I kind of have a vibe that the persuasion doesn’t often have to involve explicit lying, but rather just ‘highlighting’ many pieces of evidence and ignoring others. In...",
      "url": "https://www.lesswrong.com/posts/FZxJ7EBhfhZLdffXT/powerful-misaligned-ais-may-be-extremely-persuasive",
      "author": "Cody Rushing",
      "published": "2026-01-16T03:08:33.903000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analysis of persuasion risks from misaligned AI, reframing from adversarial (salesman) model to trusted advisor model where users have strong incentives to believe AI. Emphasizes large attack surface (personal data, social media, information environment) and competitive disadvantages of being skeptical.",
      "importance_score": 55,
      "reasoning": "Useful conceptual contribution to AI safety thinking with novel reframing of persuasion threat model. Connects to control research but primarily conceptual rather than empirical.",
      "themes": [
        "AI Safety",
        "Alignment",
        "AI Persuasion",
        "Threat Modeling"
      ],
      "continuation": null,
      "summary_html": "<p>Analysis of persuasion risks from misaligned AI, reframing from adversarial (salesman) model to trusted advisor model where users have strong incentives to believe AI. Emphasizes large attack surface (personal data, social media, information environment) and competitive disadvantages of being skeptical.</p>",
      "content_html": "<p>The concise one minute post for frequent readers of this forumHere are some important, concise intellectual nuggets of progress to I made for myself through writing this post (the post also has things I thought were obvious):When people imagine persuasion, they imagine a situation kind of a like a salesman trying to convince you of something. This the wrong framing: it will instead be completely in your interest to trust what the AI says, and the primary question will be whether or not you even notice that the ‘persuaded material’ is off, and if you will be truth-seeking enough to pursue this further. Unfortunately, an extraordinary amount of the incentives will be to believe the AI.This is kind of explicit in the control frame but not something I’ve connected to persuasion. And there are other connections that feel salient too; the more adversarial you are to trusting your AI, the slower you will be, which might cause you to get outcompeted.The AI can potentially have quite a large surface area to attack you with. It can target you, your close allies, your constituents, your social media (basically all the information you see). And you may not be able to avoid AI even if you tried.I’m less compelled by arguments that the ‘truth will win out’. The historical track record for humans changing their mind when lots of trusted and/or smart people around them believe something isn’t great. And the situation with AIs is even worse, as the AIs will have strong influence over both your cognition and the information you see.AI’s choosing not to collude doesn’t obviously solve the problem. Even if you are in a situation where two AIs aren’t colluding and instead debating, it will work about as good as you expect AI debate to do, which isn’t clear. And also the AIs might not even be placed to cath each others lies.I kind of have a vibe that the persuasion doesn’t often have to involve explicit lying, but rather just ‘highlighting’ many pieces of evidence and ignoring others. In...</p>"
    },
    {
      "id": "dbe30c1c2007",
      "title": "Digital Minds: A Quickstart Guide",
      "content": "Updated: Jan 16, 2026Digital minds are artificial systems, from advanced AIs to potential future brain emulations, that could morally matter for their own sake, owing to their potential for conscious experience, suffering, or other morally relevant mental states. Both cognitive science and the philosophy of mind can as yet offer no definitive answers as to whether present or near-future digital minds possess morally relevant mental states. Though, a majority of experts surveyed estimate at least fifty percent odds that AI systems with subjective experience could emerge by 2050,[1]&nbsp;while public expresses broad uncertainty.[2]The lack of clarity leaves open the risk of severe moral catastrophe:We could mistakenly underattribute moral standing; failing to give consideration or rights to a new kind of being that deserves them.We could mistakenly overattribute moral standing; perhaps granting rights or consideration to morally irrelevant machines at the expense of human wellbeing.As society surges toward an era shaped by increasingly capable and numerous AI systems, scientific theories of mind take on direct implications for ethics, governance, and policy, prompting a growing consensus that rapid progress on these questions is urgently needed.This quickstart guide gathers the most useful articles, media, and research for readers ranging from curious beginners to aspiring contributors:The Quickstart section offers an accessible set of materials for your first one or two hours engaging with the arguments.Then if you’re looking for a casual introduction to the topic, the Select Media section gives a number of approachable podcasts and videosOr for a deeper dive the Introduction and Intermediate sections provide a structured reading list for studyWe then outline the broader landscape with Further Resources, including key thinkers, academic centers, organizations, and career opportunities.A Glossary at the end offers short definitions for essential terms; a quick (ctrl+f...",
      "url": "https://www.lesswrong.com/posts/WK4GWkeSQQQPeRYJv/digital-minds-a-quickstart-guide",
      "author": "Avi Parrack",
      "published": "2026-01-16T12:15:31.414000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Introductory guide to the ethical and policy considerations around digital minds, covering uncertainty about AI consciousness, risks of under/over-attributing moral status, and current research directions. Notes majority of experts estimate >50% chance of subjective AI experience by 2050.",
      "importance_score": 45,
      "reasoning": "Well-organized overview of an important topic area with good sourcing. Primarily educational/summary content rather than novel research, but serves as useful reference material.",
      "themes": [
        "Digital Minds",
        "AI Ethics",
        "Consciousness",
        "AI Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Introductory guide to the ethical and policy considerations around digital minds, covering uncertainty about AI consciousness, risks of under/over-attributing moral status, and current research directions. Notes majority of experts estimate &gt;50% chance of subjective AI experience by 2050.</p>",
      "content_html": "<p>Updated: Jan 16, 2026Digital minds are artificial systems, from advanced AIs to potential future brain emulations, that could morally matter for their own sake, owing to their potential for conscious experience, suffering, or other morally relevant mental states. Both cognitive science and the philosophy of mind can as yet offer no definitive answers as to whether present or near-future digital minds possess morally relevant mental states. Though, a majority of experts surveyed estimate at least fifty percent odds that AI systems with subjective experience could emerge by 2050,[1]&nbsp;while public expresses broad uncertainty.[2]The lack of clarity leaves open the risk of severe moral catastrophe:We could mistakenly underattribute moral standing; failing to give consideration or rights to a new kind of being that deserves them.We could mistakenly overattribute moral standing; perhaps granting rights or consideration to morally irrelevant machines at the expense of human wellbeing.As society surges toward an era shaped by increasingly capable and numerous AI systems, scientific theories of mind take on direct implications for ethics, governance, and policy, prompting a growing consensus that rapid progress on these questions is urgently needed.This quickstart guide gathers the most useful articles, media, and research for readers ranging from curious beginners to aspiring contributors:The Quickstart section offers an accessible set of materials for your first one or two hours engaging with the arguments.Then if you’re looking for a casual introduction to the topic, the Select Media section gives a number of approachable podcasts and videosOr for a deeper dive the Introduction and Intermediate sections provide a structured reading list for studyWe then outline the broader landscape with Further Resources, including key thinkers, academic centers, organizations, and career opportunities.A Glossary at the end offers short definitions for essential terms; a quick (ctrl+f...</p>"
    },
    {
      "id": "2a1aa51d4552",
      "title": "Precedents for the Unprecedented: Historical Analogies for Thirteen Artificial Superintelligence Risks",
      "content": "Since artificial superintelligence has never existed, claims that it poses a serious risk of global catastrophe can be easy to dismiss as fearmongering. Yet many of the specific worries about such systems are not free-floating fantasies but extensions of patterns we already see. This essay examines thirteen distinct ways artificial superintelligence could go wrong and, for each, pairs the abstract failure mode with concrete precedents where a similar pattern has already caused serious harm. By assembling a broad cross-domain catalog of such precedents, I aim to show that concerns about artificial superintelligence track recurring failure modes in our world.This essay is also an experiment in writing with extensive assistance from artificial intelligence, producing work I couldn’t have written without it. That a current system can help articulate a case for the catastrophic potential of its own lineage is itself a significant fact; we have already left the realm of speculative fiction and begun to build the very agents that constitute the risk. On a personal note, this collaboration with artificial intelligence is part of my effort to rebuild the intellectual life that my stroke disrupted and hopefully push it beyond where it stood before.&nbsp;Section 1: Power Asymmetry and TakeoverArtificial superintelligence poses a significant risk of catastrophe in part because an agent that first attains a decisive cognitive and strategic edge can render formal checks and balances practically irrelevant, allowing unilateral choices that the rest of humanity cannot meaningfully contest. When a significantly smarter and better organized agent enters a domain, it typically rebuilds the environment to suit its own ends. The new arrival locks in a system that the less capable original agents cannot undo. History often shows that the stronger party dictates the future while the weaker party effectively loses all agency.The primary risk of artificial superintelligence is that we are b...",
      "url": "https://www.lesswrong.com/posts/kLvhBSwjWD9wjejWn/precedents-for-the-unprecedented-historical-analogies-for-1",
      "author": "James_Miller",
      "published": "2026-01-16T13:43:52.091000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Catalogs thirteen potential ASI failure modes paired with historical precedents that demonstrate similar patterns, arguing that superintelligence risks are extensions of observable failure modes rather than speculative. Written with extensive AI assistance.",
      "importance_score": 42,
      "reasoning": "Thoughtful risk taxonomy with historical grounding, but primarily argumentative/conceptual rather than empirical research. Useful for AI safety discourse but limited methodological novelty.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "Superintelligence"
      ],
      "continuation": null,
      "summary_html": "<p>Catalogs thirteen potential ASI failure modes paired with historical precedents that demonstrate similar patterns, arguing that superintelligence risks are extensions of observable failure modes rather than speculative. Written with extensive AI assistance.</p>",
      "content_html": "<p>Since artificial superintelligence has never existed, claims that it poses a serious risk of global catastrophe can be easy to dismiss as fearmongering. Yet many of the specific worries about such systems are not free-floating fantasies but extensions of patterns we already see. This essay examines thirteen distinct ways artificial superintelligence could go wrong and, for each, pairs the abstract failure mode with concrete precedents where a similar pattern has already caused serious harm. By assembling a broad cross-domain catalog of such precedents, I aim to show that concerns about artificial superintelligence track recurring failure modes in our world.This essay is also an experiment in writing with extensive assistance from artificial intelligence, producing work I couldn’t have written without it. That a current system can help articulate a case for the catastrophic potential of its own lineage is itself a significant fact; we have already left the realm of speculative fiction and begun to build the very agents that constitute the risk. On a personal note, this collaboration with artificial intelligence is part of my effort to rebuild the intellectual life that my stroke disrupted and hopefully push it beyond where it stood before.&nbsp;Section 1: Power Asymmetry and TakeoverArtificial superintelligence poses a significant risk of catastrophe in part because an agent that first attains a decisive cognitive and strategic edge can render formal checks and balances practically irrelevant, allowing unilateral choices that the rest of humanity cannot meaningfully contest. When a significantly smarter and better organized agent enters a domain, it typically rebuilds the environment to suit its own ends. The new arrival locks in a system that the less capable original agents cannot undo. History often shows that the stronger party dictates the future while the weaker party effectively loses all agency.The primary risk of artificial superintelligence is that we are b...</p>"
    },
    {
      "id": "b48c609c7158",
      "title": "[Pre-print] Building safe AGI as an ergonomics problem",
      "content": "Hello LessWrong,I've been reading posts here and on the AI Alignment forum for several years. Thank you for all the fascinating insights, beautiful tangents and deep learning rabbit holes.&nbsp;My colleague and I have written a paper, that was initially going to start as a post on here, but after the article was finished we submitted it to an Ergonomics journal, where it was rejected after peer review. We recognise that part of academic publishing is a random number generator, so we've have updated the article and posted it as a pre-print on PsyArXiv and on our institutional repo doi: 10.31234/osf.io/fd3kn_v1We agree with JanB that many of the brilliant posts here should be on a pre-print server, or even submitted to a conference or a journal.I'm looking forward to your thoughts and feedback, and we hope to integrate these discussions into the next version of the paper. I hope these ideas spark a continued conversation.&nbsp;",
      "url": "https://www.lesswrong.com/posts/PysG6yS9o49CjtkEg/pre-print-building-safe-agi-as-an-ergonomics-problem",
      "author": "ricardotkcl",
      "published": "2026-01-16T08:18:43.459000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Pre-print framing AGI safety through ergonomics/human factors lens, arguing for applying established safety engineering principles to AI development. Submitted to ergonomics journal but rejected after peer review.",
      "importance_score": 38,
      "reasoning": "Interdisciplinary approach connecting established safety fields to AI safety. However, rejection from peer review and lack of clear methodological contribution limits impact. Novel framing but unclear practical implications.",
      "themes": [
        "AI Safety",
        "Ergonomics",
        "Safety Engineering"
      ],
      "continuation": null,
      "summary_html": "<p>Pre-print framing AGI safety through ergonomics/human factors lens, arguing for applying established safety engineering principles to AI development. Submitted to ergonomics journal but rejected after peer review.</p>",
      "content_html": "<p>Hello LessWrong,I've been reading posts here and on the AI Alignment forum for several years. Thank you for all the fascinating insights, beautiful tangents and deep learning rabbit holes.&nbsp;My colleague and I have written a paper, that was initially going to start as a post on here, but after the article was finished we submitted it to an Ergonomics journal, where it was rejected after peer review. We recognise that part of academic publishing is a random number generator, so we've have updated the article and posted it as a pre-print on PsyArXiv and on our institutional repo doi: 10.31234/osf.io/fd3kn_v1We agree with JanB that many of the brilliant posts here should be on a pre-print server, or even submitted to a conference or a journal.I'm looking forward to your thoughts and feedback, and we hope to integrate these discussions into the next version of the paper. I hope these ideas spark a continued conversation.&nbsp;</p>"
    },
    {
      "id": "571803b48d6a",
      "title": "Why falling labor share ≠ falling employment",
      "content": "TL;DR: As we deploy AI, the total amount of work being done will increase, and the % done by humans will fall. This does not require a decline in human employment. This is consistent with historical trends.Sometimes, I hear economists make this argument about transformative AI:I’ll believe it when it starts showing up in the GDP/employment statistics!I think transformative AI will increase GDP. However, I don’t think this necessitates a decline in human employment.Anthropic CEO Dario Amodei imagines advanced AI as a “country of geniuses in a datacenter”. If such a country spontaneously sprang up tomorrow, I don’t think it would reduce human employment. Investors might want to re-allocate capital towards the country, but the country would require some inputs that it’s unable to self-supply.1I think human and AI inputs could be complementary to each other — possibly because we legislate them to be so / require human oversight — like scenario-appropriate versions of the human drivers who currently sit in Tesla’s Robotaxis, watching the road without touching the controls.~4 billion humans and ~100 billion non-human worker-equivalents currently work (BOTEC). A ‘worker-equivalent’ here means ‘the amount of work one average 1700 human worker could perform in a year.’ From 1900 to 2020, human labor input grew by ~2.5× while total economic work grew by ~16×, meaning most additional work was done by machines. On this BOTEC, only 4% of work is done by humans today.2Some economists seem to implicitly assume that the amount of work done in the future will be the same as the amount of work done today. In Korinek and Suh’s ‘Scenarios for the Transition to AGI’:The distribution function Φ(i) reflects the cumulative mass of tasks with complexity ≤ i and satisfies Φ(0) = 0 and Φ(i) → 1 as i → ∞.In this model, task measure is fixed, and we start out with humans doing every task.But we could productively deploy more labor than we currently have. In reality, task measure is not fixed, a...",
      "url": "https://www.lesswrong.com/posts/YqHjt8d6JihJXXryG/why-falling-labor-share-falling-employment",
      "author": "Lydia Nottingham",
      "published": "2026-01-16T12:27:10.395000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Economic analysis arguing that AI increasing the share of total work done by machines doesn't necessitate declining human employment, drawing on concepts of complementarity between human and AI labor and historical precedents.",
      "importance_score": 35,
      "reasoning": "Reasonable economic commentary but lacks rigorous analysis or empirical backing. Presents familiar arguments about AI-human complementarity without substantial new contributions.",
      "themes": [
        "AI Economics",
        "Labor Markets",
        "AI Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Economic analysis arguing that AI increasing the share of total work done by machines doesn't necessitate declining human employment, drawing on concepts of complementarity between human and AI labor and historical precedents.</p>",
      "content_html": "<p>TL;DR: As we deploy AI, the total amount of work being done will increase, and the % done by humans will fall. This does not require a decline in human employment. This is consistent with historical trends.Sometimes, I hear economists make this argument about transformative AI:I’ll believe it when it starts showing up in the GDP/employment statistics!I think transformative AI will increase GDP. However, I don’t think this necessitates a decline in human employment.Anthropic CEO Dario Amodei imagines advanced AI as a “country of geniuses in a datacenter”. If such a country spontaneously sprang up tomorrow, I don’t think it would reduce human employment. Investors might want to re-allocate capital towards the country, but the country would require some inputs that it’s unable to self-supply.1I think human and AI inputs could be complementary to each other — possibly because we legislate them to be so / require human oversight — like scenario-appropriate versions of the human drivers who currently sit in Tesla’s Robotaxis, watching the road without touching the controls.~4 billion humans and ~100 billion non-human worker-equivalents currently work (BOTEC). A ‘worker-equivalent’ here means ‘the amount of work one average 1700 human worker could perform in a year.’ From 1900 to 2020, human labor input grew by ~2.5× while total economic work grew by ~16×, meaning most additional work was done by machines. On this BOTEC, only 4% of work is done by humans today.2Some economists seem to implicitly assume that the amount of work done in the future will be the same as the amount of work done today. In Korinek and Suh’s ‘Scenarios for the Transition to AGI’:The distribution function Φ(i) reflects the cumulative mass of tasks with complexity ≤ i and satisfies Φ(0) = 0 and Φ(i) → 1 as i → ∞.In this model, task measure is fixed, and we start out with humans doing every task.But we could productively deploy more labor than we currently have. In reality, task measure is not fixed, a...</p>"
    },
    {
      "id": "97cd45a4a1b6",
      "title": "The culture and design of human-AI interactions",
      "content": "We are in the time of new human-ai interfaces. AIs become the biggest producers of tokens, and humans need ways to manage all this useful labor. Most breakthroughs come first in coding, because the coders build the tech and iterate on how good it is at the same time, very quickly, and it’s the easiest substrate for AIs to use. Power accrues to those who understand the shifting currents to and from human/AI capabilities. AI increases variance in most cases, but can be stabilized by culture and care. Humans find themselves communicating intention at higher and higher levels. But intention and an understanding of what problems matter is built by interacting with the problem, and therefore with the potential solution, eg trying to look at the domain, sketch solutions, etc... Within a specific task, this means the best problem solvers still force themselves to interact with the domain directly, maybe just at the level of things like writing pseudocode. But at a higher level, this means the quality of education becomes more and more uneven, as people get lazy about going into the details, and there are big splits mostly based culture and upbringing regarding the goals of learning, as almost all kids realize AI can do all the learning tasks they are given, but don’t necessarily consider how these tasks build up an understanding of what to delegate and what to strive for. At the top, some are learning much faster than anyone ever could. In many ways, AI polarizes and digs inequalities, mostly based on culture. Magic is possible, but in many cases debilitating. Human AI collaboration can amplify the quality of human goals via fetching relevant information, redteaming considerations, and then properly executing on that intention. To provide intention, humans formulate verification signals, things like outcome level checks (”does the code pass the tests”), or vague and less crisp indications to be checked by another agent given natural language. Communicating the degrees of fr...",
      "url": "https://www.lesswrong.com/posts/eZDR9NCpEqiSkdfQN/the-culture-and-design-of-human-ai-interactions",
      "author": "zef",
      "published": "2026-01-16T12:11:52.343000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Reflective commentary on evolving human-AI interaction patterns, noting that power accrues to those understanding capability shifts, AI increases variance, and education quality may become more uneven as people outsource detailed engagement to AI.",
      "importance_score": 28,
      "reasoning": "Observational commentary with interesting points but lacks systematic analysis or empirical grounding. More blog-style reflection than research.",
      "themes": [
        "Human-AI Interaction",
        "AI Workflow",
        "Education"
      ],
      "continuation": null,
      "summary_html": "<p>Reflective commentary on evolving human-AI interaction patterns, noting that power accrues to those understanding capability shifts, AI increases variance, and education quality may become more uneven as people outsource detailed engagement to AI.</p>",
      "content_html": "<p>We are in the time of new human-ai interfaces. AIs become the biggest producers of tokens, and humans need ways to manage all this useful labor. Most breakthroughs come first in coding, because the coders build the tech and iterate on how good it is at the same time, very quickly, and it’s the easiest substrate for AIs to use. Power accrues to those who understand the shifting currents to and from human/AI capabilities. AI increases variance in most cases, but can be stabilized by culture and care. Humans find themselves communicating intention at higher and higher levels. But intention and an understanding of what problems matter is built by interacting with the problem, and therefore with the potential solution, eg trying to look at the domain, sketch solutions, etc... Within a specific task, this means the best problem solvers still force themselves to interact with the domain directly, maybe just at the level of things like writing pseudocode. But at a higher level, this means the quality of education becomes more and more uneven, as people get lazy about going into the details, and there are big splits mostly based culture and upbringing regarding the goals of learning, as almost all kids realize AI can do all the learning tasks they are given, but don’t necessarily consider how these tasks build up an understanding of what to delegate and what to strive for. At the top, some are learning much faster than anyone ever could. In many ways, AI polarizes and digs inequalities, mostly based on culture. Magic is possible, but in many cases debilitating. Human AI collaboration can amplify the quality of human goals via fetching relevant information, redteaming considerations, and then properly executing on that intention. To provide intention, humans formulate verification signals, things like outcome level checks (”does the code pass the tests”), or vague and less crisp indications to be checked by another agent given natural language. Communicating the degrees of fr...</p>"
    },
    {
      "id": "90602a844c81",
      "title": "Monthly Roundup #38: January 2026",
      "content": "Good news, we managed to make some cuts. I think? Table of Contents California In Crisis. Bad News. Opportunity Knocks. Government Working. The Efficient Market Hypothesis Has Thoughts. No All That Money Doesn’t Go To Pay Interest. While I Cannot Condone This. Burnout. Good News, Everyone. Good Advice. For Your Entertainment. Gamers Gonna Game Game Game Game Game. Sports Go Sports. Antisocial Media. California In Crisis I’ve written about this before, but it turns out it’s even worse than I realized. California is toying with a 1.5% annual wealth tax on billionaires, sufficiently seriously that Larry Page, Sergey Brin and Peter Thiel have left the state as a precaution. ​Teddy Schleifer: NEWS: Google co-founder Sergey Brin is cutting ties with California. In the 10 days before Christmas, an entity connected to Brin terminated or moved 15 California LLCs that oversee some of his business interests or investments out of the state. With @RMac18 + @hknightsf . This would also include an annual 1% wealth tax on anyone over $50 million, per year, including on illiquid unrealized startup equity. That’s what takes this from ‘deeply foolish and greedy idea that will backfire’ to ‘intentionally trying to blow everything up to watch it burn.’ Garry Tan points out that as written the law would treat any supervoting shares as if they had economic value equal to their voting rights, which means any founders with such shares are effectively banned from the state. There are some other interesting cases, most notably Mark Zuckerberg. Garry Tan is one of those with a history of crying wolf, but in this case? Wolf. This one is really scary. Chances of this passing are up to 53%, and the ‘will this be on the ballet’ question is only at 61%, which implies that if this does make it to a vote then it will probably pass. Again, California needs to end propositions, period. I presume that, if implemented, this would force the entire startup ecosystem, and likely all of tech, to flee the sta...",
      "url": "https://www.lesswrong.com/posts/wh5KpofdQHs2D2hEj/monthly-roundup-38-january-2026",
      "author": "Zvi",
      "published": "2026-01-16T10:10:24.113000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Zvi's monthly aggregation covering California policy (wealth tax causing tech exodus), economic observations, and various topics. Not focused on AI research specifically.",
      "importance_score": 22,
      "reasoning": "News aggregation and commentary. May contain relevant context but not original AI research. Primarily covers policy and economic topics.",
      "themes": [
        "News Roundup",
        "Policy",
        "Economics"
      ],
      "continuation": null,
      "summary_html": "<p>Zvi's monthly aggregation covering California policy (wealth tax causing tech exodus), economic observations, and various topics. Not focused on AI research specifically.</p>",
      "content_html": "<p>Good news, we managed to make some cuts. I think? Table of Contents California In Crisis. Bad News. Opportunity Knocks. Government Working. The Efficient Market Hypothesis Has Thoughts. No All That Money Doesn’t Go To Pay Interest. While I Cannot Condone This. Burnout. Good News, Everyone. Good Advice. For Your Entertainment. Gamers Gonna Game Game Game Game Game. Sports Go Sports. Antisocial Media. California In Crisis I’ve written about this before, but it turns out it’s even worse than I realized. California is toying with a 1.5% annual wealth tax on billionaires, sufficiently seriously that Larry Page, Sergey Brin and Peter Thiel have left the state as a precaution. ​Teddy Schleifer: NEWS: Google co-founder Sergey Brin is cutting ties with California. In the 10 days before Christmas, an entity connected to Brin terminated or moved 15 California LLCs that oversee some of his business interests or investments out of the state. With @RMac18 + @hknightsf . This would also include an annual 1% wealth tax on anyone over $50 million, per year, including on illiquid unrealized startup equity. That’s what takes this from ‘deeply foolish and greedy idea that will backfire’ to ‘intentionally trying to blow everything up to watch it burn.’ Garry Tan points out that as written the law would treat any supervoting shares as if they had economic value equal to their voting rights, which means any founders with such shares are effectively banned from the state. There are some other interesting cases, most notably Mark Zuckerberg. Garry Tan is one of those with a history of crying wolf, but in this case? Wolf. This one is really scary. Chances of this passing are up to 53%, and the ‘will this be on the ballet’ question is only at 61%, which implies that if this does make it to a vote then it will probably pass. Again, California needs to end propositions, period. I presume that, if implemented, this would force the entire startup ecosystem, and likely all of tech, to flee the sta...</p>"
    },
    {
      "id": "21c84f3904e0",
      "title": "Total utilitarianism is fine",
      "content": "Take some agents Ai.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked > * {position: absolute} .MJXc-bevelled > * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over > * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under > * {padding-left: 0px!important; padding-right: 0px!important} .mjx-stack > .mjx-sup {display: block} .mjx-stack > .mjx-sub {display: block} .mjx-prestack > .mjx-presup {display: block} .mjx-prestack > .mjx-presub {display: block} .mjx-delim-h > .mjx-char {display: inline-block} .mjx-surd {vertical-align: top} .mjx-surd + .mjx-box {display: inline-flex} .mjx-mphantom * {visibility: hidden} .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%} .mjx-annotation-xml {line-height: normal} .mjx-menclose > svg {fill: none; stroke: currentColor; overflow:...",
      "url": "https://www.lesswrong.com/posts/L2gGnmiuJq7FXQDhu/total-utilitarianism-is-fine",
      "author": "Abhimanyu Pallavi Sudhir",
      "published": "2026-01-15T19:32:51.038000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical argument defending total utilitarianism against common objections, using formal notation to clarify assumptions about aggregating welfare across agents.",
      "importance_score": 15,
      "reasoning": "Ethics/philosophy post with no direct AI research relevance. May have tangential relevance to AI ethics but doesn't engage with AI specifically.",
      "themes": [
        "Ethics",
        "Philosophy",
        "Utilitarianism"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical argument defending total utilitarianism against common objections, using formal notation to clarify assumptions about aggregating welfare across agents.</p>",
      "content_html": "<p>Take some agents Ai.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0} .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0} .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table} .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em} .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0} .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left} .mjx-numerator {display: block; text-align: center} .mjx-denominator {display: block; text-align: center} .MJXc-stacked {height: 0; position: relative} .MJXc-stacked &gt; * {position: absolute} .MJXc-bevelled &gt; * {display: inline-block} .mjx-stack {display: inline-block} .mjx-op {display: block} .mjx-under {display: table-cell} .mjx-over {display: block} .mjx-over &gt; * {padding-left: 0px!important; padding-right: 0px!important} .mjx-under &gt; * {padding-left: 0px!important; padding-right: 0px!important} .mjx-stack &gt; .mjx-sup {display: block} .mjx-stack &gt; .mjx-sub {display: block} .mjx-prestack &gt; .mjx-presup {display: block} .mjx-prestack &gt; .mjx-presub {display: block} .mjx-delim-h &gt; .mjx-char {display: inline-block} .mjx-surd {vertical-align: top} .mjx-surd + .mjx-box {display: inline-flex} .mjx-mphantom * {visibility: hidden} .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%} .mjx-annotation-xml {line-height: normal} .mjx-menclose &gt; svg {fill: none; stroke: currentColor; overflow:...</p>"
    },
    {
      "id": "abf2e4e75b81",
      "title": "Comparing yourself to other people",
      "content": "There's a thought that I sometimes hear, and it goes something like this: \"We live in the best X of all possible Xs\". For example: Whatever criticism one might have towards modern society, we're still living in the safest and richest time of human history. However poor one may be, if you're living in the West, you're still richer than 99.99% of all humans in history. One variant (or rather, conclusion) that I have heard is: It is not for material reasons that people are not having kids; a family can easily achieve a 90s-level standard even today, and people in the 90s did have kids. In these examples, one is supposed to calibrate for the \"normal\" or \"default\" circumstances, and therefore see oneself as incredibly privileged. My objection is that, while it's technically correct, this argument misses the point of comparison itself. Comparison is a local activity. People compare things that are close together in some way. You compare yourself to your neighbors or family, or to your colleagues at work, or to people that do similar work as you do in other companies. People compare like categories because local comparisons incite action. The point of comparison is not to calibrate for defaults, it is to improve something in one's life. Therefore, saying some variant of \"compare yourself with someone much poorer who lived 300 years ago\" will do nothing because people form their reference class not by learning history and economy, but by looking around and observing what's there. Alice and Bob want to have kids. They live in a tiny apartment and they don't earn much -- enough not to be called poor, but below the median. This is every working-class couple from the 90s, at least to my remembrance, and they all had kids (I'm one of those kids). But Alice and Bob see that their friends who have kids all have at least a two or three-bedroom apartment, or an even larger house; they see that they take their kids on summer vacations, whereas they know they couldn't swing vacations ...",
      "url": "https://www.lesswrong.com/posts/vS9ZhhwPeew7gvNGk/comparing-yourself-to-other-people",
      "author": "dominicq",
      "published": "2026-01-16T15:31:37.123000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical reflection on social comparison being inherently local rather than historical or global, arguing that comparing modern living standards to historical averages misses the functional purpose of comparison for motivating action.",
      "importance_score": 12,
      "reasoning": "General philosophy/psychology essay with no AI research content. Not relevant to AI research analysis.",
      "themes": [
        "Philosophy",
        "Psychology"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical reflection on social comparison being inherently local rather than historical or global, arguing that comparing modern living standards to historical averages misses the functional purpose of comparison for motivating action.</p>",
      "content_html": "<p>There's a thought that I sometimes hear, and it goes something like this: \"We live in the best X of all possible Xs\". For example: Whatever criticism one might have towards modern society, we're still living in the safest and richest time of human history. However poor one may be, if you're living in the West, you're still richer than 99.99% of all humans in history. One variant (or rather, conclusion) that I have heard is: It is not for material reasons that people are not having kids; a family can easily achieve a 90s-level standard even today, and people in the 90s did have kids. In these examples, one is supposed to calibrate for the \"normal\" or \"default\" circumstances, and therefore see oneself as incredibly privileged. My objection is that, while it's technically correct, this argument misses the point of comparison itself. Comparison is a local activity. People compare things that are close together in some way. You compare yourself to your neighbors or family, or to your colleagues at work, or to people that do similar work as you do in other companies. People compare like categories because local comparisons incite action. The point of comparison is not to calibrate for defaults, it is to improve something in one's life. Therefore, saying some variant of \"compare yourself with someone much poorer who lived 300 years ago\" will do nothing because people form their reference class not by learning history and economy, but by looking around and observing what's there. Alice and Bob want to have kids. They live in a tiny apartment and they don't earn much -- enough not to be called poor, but below the median. This is every working-class couple from the 90s, at least to my remembrance, and they all had kids (I'm one of those kids). But Alice and Bob see that their friends who have kids all have at least a two or three-bedroom apartment, or an even larger house; they see that they take their kids on summer vacations, whereas they know they couldn't swing vacations ...</p>"
    },
    {
      "id": "50c334785a33",
      "title": "Confession: I pranked Inkhaven to make sure no one fails",
      "content": "(Content warnings: dubious math, quantum immortality, nuclear war)Normal people make New Year’s resolutions.People on the internet love to make resolutions for November.So, for the entire month of November, 41 people, myself included, set out to publish a post every day, as part of a writing residency called&nbsp;Inkhaven. On the final day, I couldn’t resist the temptation to pull out a prank and make sure no one appeared to have failed.(Note that I want to maintain everything as plausibly deniable; everything in this post might or might not have happened.)From Mahmoud, another resident:Inkhaven is the name of the 30 day blogging retreat that I went on this month. The rules were: come meet your blogging heroes, post 500 words a day, online, by midnight or we kick you out.I made it to the final day, but, on purpose, this last post will be fewer than 500 words. My reasons for this are kind of silly, but mostly I think it would be more fun if at least one person failed the program. I’m also writing post as a stream of consciousness, unsure where it will go. Maybe I’ll come up with a better reason if I keep writing.Every time I write the word “failed” something inside of me winces. Decades of consciousness and achievement seeking have trained me to avoid that word at all costs. I think many people come to this realisation.I used to think that writing was about perfectly describing the ideas in your head, and every so gently placing them on the surface of the world so others can see them, in their pristine, final form. In the first half of Inkhaven I learnt that you can write in another way too - where your ideas are shared half formed and open ended. But in the second half, I learnt that doing this makes you better at doing the first thing too. The only cost is that you need to give up on your previous idea of what failing looks like, and embrace something that looks eerily similar to it.And if you do fail, so what? What are they gonna do, kick you out?When I heard that...",
      "url": "https://www.lesswrong.com/posts/p3nuC38zEJbEvFEBE/confession-i-pranked-inkhaven-to-make-sure-no-one-fails",
      "author": "Mikhail Samin",
      "published": "2026-01-16T11:03:16.235000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Personal story about pranking a writing challenge (Inkhaven) by secretly editing participants' posts to ensure no one failed, with tangential references to quantum immortality and anthropics.",
      "importance_score": 5,
      "reasoning": "Personal narrative unrelated to AI research. No technical or research content.",
      "themes": [
        "Personal",
        "Community"
      ],
      "continuation": null,
      "summary_html": "<p>Personal story about pranking a writing challenge (Inkhaven) by secretly editing participants' posts to ensure no one failed, with tangential references to quantum immortality and anthropics.</p>",
      "content_html": "<p>(Content warnings: dubious math, quantum immortality, nuclear war)Normal people make New Year’s resolutions.People on the internet love to make resolutions for November.So, for the entire month of November, 41 people, myself included, set out to publish a post every day, as part of a writing residency called&nbsp;Inkhaven. On the final day, I couldn’t resist the temptation to pull out a prank and make sure no one appeared to have failed.(Note that I want to maintain everything as plausibly deniable; everything in this post might or might not have happened.)From Mahmoud, another resident:Inkhaven is the name of the 30 day blogging retreat that I went on this month. The rules were: come meet your blogging heroes, post 500 words a day, online, by midnight or we kick you out.I made it to the final day, but, on purpose, this last post will be fewer than 500 words. My reasons for this are kind of silly, but mostly I think it would be more fun if at least one person failed the program. I’m also writing post as a stream of consciousness, unsure where it will go. Maybe I’ll come up with a better reason if I keep writing.Every time I write the word “failed” something inside of me winces. Decades of consciousness and achievement seeking have trained me to avoid that word at all costs. I think many people come to this realisation.I used to think that writing was about perfectly describing the ideas in your head, and every so gently placing them on the surface of the world so others can see them, in their pristine, final form. In the first half of Inkhaven I learnt that you can write in another way too - where your ideas are shared half formed and open ended. But in the second half, I learnt that doing this makes you better at doing the first thing too. The only cost is that you need to give up on your previous idea of what failing looks like, and embrace something that looks eerily similar to it.And if you do fail, so what? What are they gonna do, kick you out?When I heard that...</p>"
    },
    {
      "id": "e292d2931c8b",
      "title": "How to Use Foam Earplugs Correctly",
      "content": "I used to think foam earplugs suck because they didn't fit into my ear. Turns out you have to roll them! Sadly the first time I encountered foam earplugs in a library they didn't include instructions. Rolling them they get thin and easily fit into your ear. Once I knew this they became my favorite earplugs to use during sleep. I used to use reusable ones, but the problem is those inevitably fall out of my ear onto the ground and get dirty. Same happens for the foam ones, but they are cheap (mine cost 0.25 USD/pair), so I just grab a new pair if they are dirty. They lose their elasticity after a few uses, so you cannot use them too often anyway. The instructions below are from Wikipedia. I don't have a strong preference between different foam earplugs. I usually roll the earplugs between both my hands rather than with two fingers. By Fornax, CC BY-SA 3.0",
      "url": "https://www.lesswrong.com/posts/6s2HztpCnLcS7re25/how-to-use-foam-earplugs-correctly",
      "author": "Morpheus",
      "published": "2026-01-16T02:47:25.779000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Practical guide explaining that foam earplugs need to be rolled before insertion for proper fit.",
      "importance_score": 2,
      "reasoning": "Completely unrelated to AI research. Practical life advice.",
      "themes": [
        "Practical Advice"
      ],
      "continuation": null,
      "summary_html": "<p>Practical guide explaining that foam earplugs need to be rolled before insertion for proper fit.</p>",
      "content_html": "<p>I used to think foam earplugs suck because they didn't fit into my ear. Turns out you have to roll them! Sadly the first time I encountered foam earplugs in a library they didn't include instructions. Rolling them they get thin and easily fit into your ear. Once I knew this they became my favorite earplugs to use during sleep. I used to use reusable ones, but the problem is those inevitably fall out of my ear onto the ground and get dirty. Same happens for the foam ones, but they are cheap (mine cost 0.25 USD/pair), so I just grab a new pair if they are dirty. They lose their elasticity after a few uses, so you cannot use them too often anyway. The instructions below are from Wikipedia. I don't have a strong preference between different foam earplugs. I usually roll the earplugs between both my hands rather than with two fingers. By Fornax, CC BY-SA 3.0</p>"
    },
    {
      "id": "4b06d7bbf37a",
      "title": "The Default Contra Dance Weekend Deal",
      "content": "The \"dance weekend\" is a very common pattern for contra dance communities around the country. I think of the central example as something like: Two bands, two callers. Dancing (with short breaks) Friday 7pm-11pm, Saturday 10am-11pm, Sunday 10am-3pm. Saturday and Sunday daytime are a mixture of regular dances and workshops, sometimes with parallel tracks. Examples, from my 2025 calendar: Beantown Stomp, Chehalis, Dance in the Desert, Dancing Fish, Five Borough Fling, Fleur de Lis Fling, Hashdance, and Summer Soiree. I've seen a bunch of misunderstandings that come from people not being on the same page about what is normal: many dance weekends are organized by volunteers, some of which are doing it for the first time; performers often are new to this as well. As someone who has both played for and organized dance weekends, I thought it might be helpful to try and write down what I think of as typical if an event is bringing in a band or caller from out of town. Note that I'm trying to document the status quo here, saying \"this is\" and not \"this is what should be\". I would be sad if in places where the status quo isn't great people pointed at this doc and said \"Jeff says it's supposed to be that way\"; this post is not doing that! Additionally, performers and events are of course free to agree on something that isn't the most common arrangement! As of the beginning of 2026, here's what I think of as the most common arrangement: Housing: the event provides accommodation, most commonly at someone's house. Performers may need to share rooms, but not beds. If the best travel option means coming a day early or staying a day late the event still provides housing for those additional days. If the performer wants to stay in town longer for sightseeing, that's on them. If the performer has accessibility needs (ex: pet allergies) this is good to discuss up front. Travel: the event pays for round trip economy travel, by air if driving would be too long. My sense is that flying be...",
      "url": "https://www.lesswrong.com/posts/kGZNvwWMnoJPBXkCD/the-default-contra-dance-weekend-deal",
      "author": "jefftk",
      "published": "2026-01-15T19:50:47.852000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Documentation of typical arrangements for contra dance weekend events, covering scheduling, performer compensation, and organizational norms.",
      "importance_score": 2,
      "reasoning": "Completely unrelated to AI research. Community coordination documentation.",
      "themes": [
        "Community",
        "Dance"
      ],
      "continuation": null,
      "summary_html": "<p>Documentation of typical arrangements for contra dance weekend events, covering scheduling, performer compensation, and organizational norms.</p>",
      "content_html": "<p>The \"dance weekend\" is a very common pattern for contra dance communities around the country. I think of the central example as something like: Two bands, two callers. Dancing (with short breaks) Friday 7pm-11pm, Saturday 10am-11pm, Sunday 10am-3pm. Saturday and Sunday daytime are a mixture of regular dances and workshops, sometimes with parallel tracks. Examples, from my 2025 calendar: Beantown Stomp, Chehalis, Dance in the Desert, Dancing Fish, Five Borough Fling, Fleur de Lis Fling, Hashdance, and Summer Soiree. I've seen a bunch of misunderstandings that come from people not being on the same page about what is normal: many dance weekends are organized by volunteers, some of which are doing it for the first time; performers often are new to this as well. As someone who has both played for and organized dance weekends, I thought it might be helpful to try and write down what I think of as typical if an event is bringing in a band or caller from out of town. Note that I'm trying to document the status quo here, saying \"this is\" and not \"this is what should be\". I would be sad if in places where the status quo isn't great people pointed at this doc and said \"Jeff says it's supposed to be that way\"; this post is not doing that! Additionally, performers and events are of course free to agree on something that isn't the most common arrangement! As of the beginning of 2026, here's what I think of as the most common arrangement: Housing: the event provides accommodation, most commonly at someone's house. Performers may need to share rooms, but not beds. If the best travel option means coming a day early or staying a day late the event still provides housing for those additional days. If the performer wants to stay in town longer for sightseeing, that's on them. If the performer has accessibility needs (ex: pet allergies) this is good to discuss up front. Travel: the event pays for round trip economy travel, by air if driving would be too long. My sense is that flying be...</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Weekend Edition",
    "message": "arXiv papers are not collected on weekends. Any weekend papers will be included in Monday's report."
  }
}