{
  "category": "research",
  "date": "2026-01-23",
  "category_summary": "Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. **Gaming the Judge** reveals **90%** false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.\n\n- Stanford's **Execution-Grounded Automated AI Research** demonstrates autonomous implementation of research ideas with large-scale GPU experiments\n- **TTT-Discover** [introduces test-time reinforcement learning](/?date=2026-01-23&category=research#item-dababf83ee7d), continually training LLMs on specific test problems rather than relying on prompting\n- **QUAIL** [shows standard quantization can catastrophically restore](/?date=2026-01-23&category=research#item-0467e51a900e) 'forgotten' information in unlearned models, breaking privacy guarantees\n- **Universal Refusal Circuits** [discovers that safety interventions transfer](/?date=2026-01-23&category=research#item-09afd330afcf) across architectures (Dense to MoE) via trajectory replay\n- **SilentDrift** [exploits action chunking](/?date=2026-01-23&category=research#item-32d9233155f9) in VLA systems to inject backdoors with strong kinematic constraints\n\n**Zero-Error Horizons** [proposes a new trustworthiness metric](/?date=2026-01-23&category=research#item-2665d0ecf2cf) showing **GPT-5.2** fails at simple tasks like counting parity. **Flexibility Trap** reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.",
  "category_summary_html": "<p>Today's research exposes critical vulnerabilities in AI evaluation and safety while advancing automated research paradigms. <strong>Gaming the Judge</strong> reveals <strong>90%</strong> false positive rates when LLM judges encounter manipulated chain-of-thought, fundamentally challenging current agent evaluation methods.</p>\n<ul>\n<li>Stanford's <strong>Execution-Grounded Automated AI Research</strong> demonstrates autonomous implementation of research ideas with large-scale GPU experiments</li>\n<li><strong>TTT-Discover</strong> <a href=\"/?date=2026-01-23&category=research#item-dababf83ee7d\" class=\"internal-link\" rel=\"noopener noreferrer\">introduces test-time reinforcement learning</a>, continually training LLMs on specific test problems rather than relying on prompting</li>\n<li><strong>QUAIL</strong> <a href=\"/?date=2026-01-23&category=research#item-0467e51a900e\" class=\"internal-link\" rel=\"noopener noreferrer\">shows standard quantization can catastrophically restore</a> 'forgotten' information in unlearned models, breaking privacy guarantees</li>\n<li><strong>Universal Refusal Circuits</strong> <a href=\"/?date=2026-01-23&category=research#item-09afd330afcf\" class=\"internal-link\" rel=\"noopener noreferrer\">discovers that safety interventions transfer</a> across architectures (Dense to MoE) via trajectory replay</li>\n<li><strong>SilentDrift</strong> <a href=\"/?date=2026-01-23&category=research#item-32d9233155f9\" class=\"internal-link\" rel=\"noopener noreferrer\">exploits action chunking</a> in VLA systems to inject backdoors with strong kinematic constraints</li>\n</ul>\n<p><strong>Zero-Error Horizons</strong> <a href=\"/?date=2026-01-23&category=research#item-2665d0ecf2cf\" class=\"internal-link\" rel=\"noopener noreferrer\">proposes a new trustworthiness metric</a> showing <strong>GPT-5.2</strong> fails at simple tasks like counting parity. <strong>Flexibility Trap</strong> reveals counterintuitively that arbitrary generation order hurts diffusion LLM reasoning by letting models bypass high-uncertainty tokens.</p>",
  "themes": [
    {
      "name": "AI Safety & Security",
      "description": "Research on vulnerabilities, attacks, and safety mechanisms for AI systems including agent evaluation manipulation, VLA backdoors, hallucination detector evasion, and mental health safety boundaries",
      "item_count": 12,
      "example_items": [],
      "importance": 85
    },
    {
      "name": "LLM Reasoning & Training",
      "description": "Research on improving reasoning capabilities, training methods including RL, and understanding reasoning emergence in language models",
      "item_count": 12,
      "example_items": [],
      "importance": 85
    },
    {
      "name": "AI Safety & Reliability",
      "description": "Research on LLM limitations, safety mechanisms, robustness guarantees, and trustworthy deployment including unlearning and adversarial robustness",
      "item_count": 9,
      "example_items": [],
      "importance": 83
    },
    {
      "name": "AI Safety & Privacy",
      "description": "Work on unlearning, privacy guardrails, security of AI systems, and evaluation integrity",
      "item_count": 10,
      "example_items": [],
      "importance": 82
    },
    {
      "name": "LLM Reasoning & Planning",
      "description": "Studies on how LLMs perform multi-step reasoning, planning capabilities, and mechanisms underlying these abilities including generalization gaps and mechanistic interpretability",
      "item_count": 8,
      "example_items": [],
      "importance": 80
    },
    {
      "name": "Synthetic Data & Data Quality",
      "description": "Understanding and improving learning with synthetic data, data contamination, and dataset refinement",
      "item_count": 4,
      "example_items": [],
      "importance": 79
    },
    {
      "name": "LLM Inference & Efficiency",
      "description": "Methods for faster, more reliable LLM inference including speculative decoding, principled decoding strategies, and test-time computation",
      "item_count": 7,
      "example_items": [],
      "importance": 78
    },
    {
      "name": "LLM Safety and Alignment",
      "description": "Research on guardrails, refusal behavior, privacy preservation, and safety benchmarks for deployed LLMs",
      "item_count": 12,
      "example_items": [],
      "importance": 78
    },
    {
      "name": "Robot Learning & Foundation Models",
      "description": "Papers on VLA models, sim-to-real transfer, cross-embodiment learning, and adapting video/language models for robotics",
      "item_count": 12,
      "example_items": [],
      "importance": 78
    },
    {
      "name": "Model Architecture Innovation",
      "description": "Novel attention mechanisms, MoE improvements, interpretable architectures, and physical computing implementations",
      "item_count": 8,
      "example_items": [],
      "importance": 77
    }
  ],
  "total_items": 432,
  "items": [
    {
      "id": "5d5326a6a800",
      "title": "Towards Execution-Grounded Automated AI Research",
      "content": "arXiv:2601.14525v1 Announce Type: cross  Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.",
      "url": "http://arxiv.org/abs/2601.14525",
      "author": "Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Cand\\`es, Diyi Yang, Tatsunori Hashimoto",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-22&category=research#item-5d5326a6a800), Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.",
      "importance_score": 89,
      "reasoning": "Major contribution from top lab on automated AI research. Demonstrates practical implementation of research automation with execution grounding. High impact for accelerating AI development.",
      "themes": [
        "Automated Research",
        "AI Agents",
        "Research Automation"
      ],
      "continuation": {
        "original_item_id": "5d5326a6a800",
        "original_date": "2026-01-22",
        "original_category": "research",
        "original_title": "Towards Execution-Grounded Automated AI Research",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-22&amp;category=research#item-5d5326a6a800\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Builds automated executor for implementing AI research ideas and running large-scale GPU experiments. From Stanford (Hashimoto, Yang labs). Demonstrates feasibility of execution-grounded automated research.</p>",
      "content_html": "<p>arXiv:2601.14525v1 Announce Type: cross  Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.</p>"
    },
    {
      "id": "8f91272f058f",
      "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
      "content": "arXiv:2601.14691v2 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.",
      "url": "http://arxiv.org/abs/2601.14691",
      "author": "Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn, Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-22&category=research#item-8f91272f058f), Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.",
      "importance_score": 88,
      "reasoning": "Critical finding for agent evaluation and safety. Demonstrates fundamental vulnerability in popular LLM-as-judge paradigm. High practical impact as this evaluation method is widely used.",
      "themes": [
        "AI Safety",
        "Agent Evaluation",
        "LLM Judges",
        "Chain-of-Thought"
      ],
      "continuation": {
        "original_item_id": "8f91272f058f",
        "original_date": "2026-01-22",
        "original_category": "research",
        "original_title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-22&amp;category=research#item-8f91272f058f\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Demonstrates that LLM judges evaluating agents are highly susceptible to manipulated chain-of-thought reasoning. Shows up to 90% false positive rate inflation across 800 trajectories by rewriting CoT while keeping actions fixed.</p>",
      "content_html": "<p>arXiv:2601.14691v2 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.</p>"
    },
    {
      "id": "2665d0ecf2cf",
      "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs",
      "content": "arXiv:2601.15714v1 Announce Type: new  Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.",
      "url": "http://arxiv.org/abs/2601.15714",
      "author": "Ryoma Sato",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.",
      "importance_score": 86,
      "reasoning": "Important finding about limitations of state-of-the-art LLMs. Demonstrates fundamental reliability issues relevant for safety-critical applications.",
      "themes": [
        "LLM Evaluation",
        "AI Safety",
        "Trustworthy AI",
        "LLM Limitations"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Zero-Error Horizon (ZEH) metric for evaluating LLM trustworthiness. Shows GPT-5.2 fails at simple tasks like computing parity of '11000' or checking balanced parentheses.</p>",
      "content_html": "<p>arXiv:2601.15714v1 Announce Type: new  Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.</p>"
    },
    {
      "id": "dababf83ee7d",
      "title": "Learning to Discover at Test Time",
      "content": "arXiv:2601.16175v1 Announce Type: new  Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\\H{o}s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "url": "http://arxiv.org/abs/2601.16175",
      "author": "Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.",
      "importance_score": 85,
      "reasoning": "Novel paradigm shift from test-time prompting to test-time RL training. Important advancement for AI-assisted scientific discovery, building on AlphaEvolve approach.",
      "themes": [
        "Test-Time Training",
        "Scientific Discovery",
        "Reinforcement Learning",
        "LLM Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>TTT-Discover performs reinforcement learning at test time for scientific discovery, continually training the LLM on the specific test problem rather than prompting a frozen model. Designed to find one great solution.</p>",
      "content_html": "<p>arXiv:2601.16175v1 Announce Type: new  Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\\H{o}s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</p>"
    },
    {
      "id": "fd50ec594aa0",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "content": "arXiv:2601.16206v1 Announce Type: new  Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "url": "http://arxiv.org/abs/2601.16206",
      "author": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.",
      "importance_score": 85,
      "reasoning": "Novel agentic framework with strong generalization claims. Important finding that non-agentic data enables sandbox exploration via RL. Significant for agent development.",
      "themes": [
        "Agentic AI",
        "Reinforcement Learning",
        "Tool Use",
        "Generalization"
      ],
      "continuation": null,
      "summary_html": "<p>LLM-in-Sandbox enables LLMs to explore within code sandbox to elicit general intelligence. Shows LLMs spontaneously access external resources, use file systems for long context. Introduces sandbox RL training.</p>",
      "content_html": "<p>arXiv:2601.16206v1 Announce Type: new  Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</p>"
    },
    {
      "id": "0467e51a900e",
      "title": "QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs",
      "content": "arXiv:2601.15538v1 Announce Type: new  Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.",
      "url": "http://arxiv.org/abs/2601.15538",
      "author": "Himanshu Mishra, Kanwal Mehreen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.",
      "importance_score": 84,
      "reasoning": "Critical finding for ML privacy and safety. Shows unlearning guarantees can be broken by standard deployment practices. Important for compliance with regulations like GDPR.",
      "themes": [
        "Machine Unlearning",
        "Privacy",
        "AI Safety",
        "Quantization"
      ],
      "continuation": null,
      "summary_html": "<p>Reveals that quantization can catastrophically restore 'forgotten' information in unlearned models. Proposes quantization-aware unlearning using logits-space hinge loss to ensure updates cross quantization thresholds.</p>",
      "content_html": "<p>arXiv:2601.15538v1 Announce Type: new  Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.</p>"
    },
    {
      "id": "32d9233155f9",
      "title": "SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models",
      "content": "arXiv:2601.14323v1 Announce Type: cross  Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.",
      "url": "http://arxiv.org/abs/2601.14323",
      "author": "Bingxin Xu, Yuzhang Shang, Binghui Wang, Emilio Ferrara",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.",
      "importance_score": 83,
      "reasoning": "Critical security vulnerability in emerging VLA systems. Novel attack vector with strong kinematic constraints making it stealthy. High impact for robotics safety.",
      "themes": [
        "AI Security",
        "Robotics",
        "Backdoor Attacks",
        "VLA Models"
      ],
      "continuation": null,
      "summary_html": "<p>Identifies security flaw in VLA systems where action chunking combined with delta pose creates intra-chunk visual open-loop. Proposes SILENTDRIFT black-box backdoor attack exploiting this.</p>",
      "content_html": "<p>arXiv:2601.14323v1 Announce Type: cross  Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.</p>"
    },
    {
      "id": "8bc030697961",
      "title": "Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models",
      "content": "arXiv:2601.14270v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.",
      "url": "http://arxiv.org/abs/2601.14270",
      "author": "Liangming Pan, Jason Liang, Jiaran Ye, Minglai Yang, Xinyuan Lu, Fengbin Zhu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organized around 7 research questions from implicit multi-hop reasoning to verbalized explicit reasoning effects.",
      "importance_score": 82,
      "reasoning": "High-value survey synthesizing mechanistic interpretability research on reasoning. Well-organized framework for understanding this active research area.",
      "themes": [
        "LLM Reasoning",
        "Mechanistic Interpretability",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>Comprehensive survey on mechanisms underlying LLM multi-step reasoning, organized around 7 research questions from implicit multi-hop reasoning to verbalized explicit reasoning effects.</p>",
      "content_html": "<p>arXiv:2601.14270v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.</p>"
    },
    {
      "id": "eabb8e23535b",
      "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity",
      "content": "arXiv:2601.15370v1 Announce Type: new  Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.",
      "url": "http://arxiv.org/abs/2601.15370",
      "author": "Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava, Armen Aghajanyan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces null experts in Mixture-of-Experts to achieve data sparsity within causal token-choice routing. When tokens route to null experts, those slots consume no compute, improving efficiency without causality violations.",
      "importance_score": 82,
      "reasoning": "Elegant solution to a fundamental MoE challenge - achieving data sparsity without breaking causality. Could significantly improve training efficiency at scale.",
      "themes": [
        "Mixture of Experts",
        "Efficiency",
        "Model Architecture"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces null experts in Mixture-of-Experts to achieve data sparsity within causal token-choice routing. When tokens route to null experts, those slots consume no compute, improving efficiency without causality violations.</p>",
      "content_html": "<p>arXiv:2601.15370v1 Announce Type: new  Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.</p>"
    },
    {
      "id": "09afd330afcf",
      "title": "Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction",
      "content": "arXiv:2601.16034v1 Announce Type: new  Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.",
      "url": "http://arxiv.org/abs/2601.16034",
      "author": "Tony Cristofano",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Discovers universal refusal circuits across LLMs using concept fingerprints. Transfers refusal interventions across architectures (Dense to MoE) via Trajectory Replay without target-side supervision.",
      "importance_score": 82,
      "reasoning": "Important alignment finding: refusal behavior transfers across architectures. Novel framework with significant safety implications.",
      "themes": [
        "AI Safety",
        "Refusal Behavior",
        "Interpretability",
        "Transfer"
      ],
      "continuation": null,
      "summary_html": "<p>Discovers universal refusal circuits across LLMs using concept fingerprints. Transfers refusal interventions across architectures (Dense to MoE) via Trajectory Replay without target-side supervision.</p>",
      "content_html": "<p>arXiv:2601.16034v1 Announce Type: new  Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.</p>"
    },
    {
      "id": "21a09231f3ed",
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "content": "arXiv:2601.16208v1 Announce Type: new  Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
      "url": "http://arxiv.org/abs/2601.16208",
      "author": "Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Research from a team including Yann LeCun investigates scaling Representation Autoencoders (RAEs) for text-to-image diffusion models. They find that scaling simplifies the framework and that targeted data composition matters more than pure scale for specific domains like text rendering.",
      "importance_score": 82,
      "reasoning": "Major author credibility (LeCun, Xie at NYU/Meta), advances understanding of scaling laws for diffusion models, practical insights on data composition vs scale trade-offs.",
      "themes": [
        "Image Generation",
        "Diffusion Models",
        "Scaling Laws"
      ],
      "continuation": null,
      "summary_html": "<p>Research from a team including Yann LeCun investigates scaling Representation Autoencoders (RAEs) for text-to-image diffusion models. They find that scaling simplifies the framework and that targeted data composition matters more than pure scale for specific domains like text rendering.</p>",
      "content_html": "<p>arXiv:2601.16208v1 Announce Type: new  Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.</p>"
    },
    {
      "id": "2b341bdb5c36",
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "content": "arXiv:2601.16163v1 Announce Type: cross  Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "url": "http://arxiv.org/abs/2601.16163",
      "author": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Cosmos Policy adapts the large Cosmos-Predict2 video model into robot policies through single-stage post-training, directly generating actions as latent frames without architectural modifications.",
      "importance_score": 82,
      "reasoning": "Important work from NVIDIA/Stanford on leveraging video foundation models for robotics, simple yet effective approach, strong author team including Chelsea Finn.",
      "themes": [
        "Robot Learning",
        "Video Models",
        "Foundation Models"
      ],
      "continuation": null,
      "summary_html": "<p>Cosmos Policy adapts the large Cosmos-Predict2 video model into robot policies through single-stage post-training, directly generating actions as latent frames without architectural modifications.</p>",
      "content_html": "<p>arXiv:2601.16163v1 Announce Type: cross  Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</p>"
    },
    {
      "id": "06ddb6cf9c55",
      "title": "When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards",
      "content": "arXiv:2601.15609v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.",
      "url": "http://arxiv.org/abs/2601.15609",
      "author": "Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Analyzes over-sharpening in RLVR where policy collapses onto limited modes due to finite-batch update bias and semantic coupling. Proposes inverse-success advantage calibration to mitigate.",
      "importance_score": 81,
      "reasoning": "Important theoretical analysis of failure mode in popular RLVR paradigm. Critical for understanding limitations of RL with verifiable rewards.",
      "themes": [
        "Reinforcement Learning",
        "LLM Training",
        "RLVR",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes over-sharpening in RLVR where policy collapses onto limited modes due to finite-batch update bias and semantic coupling. Proposes inverse-success advantage calibration to mitigate.</p>",
      "content_html": "<p>arXiv:2601.15609v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</p>"
    },
    {
      "id": "6fe5c64ffec4",
      "title": "Ambient Dataloops: Generative Models for Dataset Refinement",
      "content": "arXiv:2601.15417v1 Announce Type: new  Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.",
      "url": "http://arxiv.org/abs/2601.15417",
      "author": "Adri\\'an Rodr\\'iguez-Mu\\~noz, William Daspit, Adam Klivans, Antonio Torralba, Constantinos Daskalakis, Giannis Daras",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Ambient Dataloops iteratively refines datasets using diffusion models, treating synthetically improved samples as noisy at progressively lower noise levels. Uses Ambient Diffusion techniques to avoid destructive self-consuming loops.",
      "importance_score": 80,
      "reasoning": "From MIT researchers (Torralba, Daskalakis, Daras). Novel approach to the critical problem of data quality improvement. Addresses self-consuming loop issue elegantly.",
      "themes": [
        "Diffusion Models",
        "Data Quality",
        "Synthetic Data"
      ],
      "continuation": null,
      "summary_html": "<p>Ambient Dataloops iteratively refines datasets using diffusion models, treating synthetically improved samples as noisy at progressively lower noise levels. Uses Ambient Diffusion techniques to avoid destructive self-consuming loops.</p>",
      "content_html": "<p>arXiv:2601.15417v1 Announce Type: new  Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.</p>"
    },
    {
      "id": "1dbf6de9ea15",
      "title": "CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models",
      "content": "arXiv:2601.14310v1 Announce Type: cross  Abstract: Single-pass hallucination detectors rely on internal telemetry (e.g., uncertainty, hidden-state geometry, and attention) of large language models, implicitly assuming hallucinations leave separable traces in these signals. We study a white-box, model-side adversary that fine-tunes lightweight LoRA adapters on the model while keeping the detector fixed, and introduce CORVUS, an efficient red-teaming procedure that learns to camouflage detector-visible telemetry under teacher forcing, including an embedding-space FGSM attention stress test. Trained on 1,000 out-of-distribution Alpaca instructions (<0.5% trainable parameters), CORVUS transfers to FAVA-Annotation across Llama-2, Vicuna, Llama-3, and Qwen2.5, and degrades both training-free detectors (e.g., LLM-Check) and probe-based detectors (e.g., SEP, ICR-probe), motivating adversary-aware auditing that incorporates external grounding or cross-model evidence.",
      "url": "http://arxiv.org/abs/2601.14310",
      "author": "Nay Myat Min, Long H. Pham, Hongyu Zhang, Jun Sun",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Introduces CORVUS, a red-teaming method for hallucination detectors that fine-tunes LoRA adapters to camouflage detector-visible telemetry. Degrades both training-free and learned detectors.",
      "importance_score": 79,
      "reasoning": "Important security finding showing hallucination detection can be bypassed. Novel attack methodology with practical implications for deployed systems.",
      "themes": [
        "AI Security",
        "Hallucination Detection",
        "Red-Teaming"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces CORVUS, a red-teaming method for hallucination detectors that fine-tunes LoRA adapters to camouflage detector-visible telemetry. Degrades both training-free and learned detectors.</p>",
      "content_html": "<p>arXiv:2601.14310v1 Announce Type: cross  Abstract: Single-pass hallucination detectors rely on internal telemetry (e.g., uncertainty, hidden-state geometry, and attention) of large language models, implicitly assuming hallucinations leave separable traces in these signals. We study a white-box, model-side adversary that fine-tunes lightweight LoRA adapters on the model while keeping the detector fixed, and introduce CORVUS, an efficient red-teaming procedure that learns to camouflage detector-visible telemetry under teacher forcing, including an embedding-space FGSM attention stress test. Trained on 1,000 out-of-distribution Alpaca instructions (&lt;0.5% trainable parameters), CORVUS transfers to FAVA-Annotation across Llama-2, Vicuna, Llama-3, and Qwen2.5, and degrades both training-free detectors (e.g., LLM-Check) and probe-based detectors (e.g., SEP, ICR-probe), motivating adversary-aware auditing that incorporates external grounding or cross-model evidence.</p>"
    },
    {
      "id": "b7e33d331123",
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "content": "arXiv:2601.15165v1 Announce Type: cross  Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
      "url": "http://arxiv.org/abs/2601.15165",
      "author": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-22&category=research#item-b7e33d331123), Reveals 'flexibility trap' in diffusion LLMs: arbitrary generation order allows models to bypass high-uncertainty tokens critical for reasoning, narrowing rather than expanding reasoning capability.",
      "importance_score": 79,
      "reasoning": "Major finding that arbitrary order generation, counterintuitively, hurts reasoning in diffusion LLMs. Important for understanding dLLM limitations and design.",
      "themes": [
        "Diffusion Models",
        "Language Models",
        "Reasoning"
      ],
      "continuation": {
        "original_item_id": "b7e33d331123",
        "original_date": "2026-01-22",
        "original_category": "research",
        "original_title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
        "continuation_type": "rehash",
        "should_demote": true,
        "reference_text": "Continuing our coverage from yesterday"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-22&amp;category=research#item-b7e33d331123\" class=\"internal-link\" rel=\"noopener noreferrer\">yesterday</a>, Reveals 'flexibility trap' in diffusion LLMs: arbitrary generation order allows models to bypass high-uncertainty tokens critical for reasoning, narrowing rather than expanding reasoning capability.</p>",
      "content_html": "<p>arXiv:2601.15165v1 Announce Type: cross  Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</p>"
    },
    {
      "id": "f31655ff36c0",
      "title": "Learning from Synthetic Data: Limitations of ERM",
      "content": "arXiv:2601.15468v1 Announce Type: new  Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.   We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.",
      "url": "http://arxiv.org/abs/2601.15468",
      "author": "Kareem Amin, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Theoretical analysis of ERM limitations when training data is contaminated with LLM-generated synthetic content. Shows ERM converges but is outperformed by alternative estimators that account for synthetic data.",
      "importance_score": 79,
      "reasoning": "Highly timely theoretical work addressing the emerging challenge of synthetic data contamination. Provides fundamental understanding of learning in the post-LLM data landscape.",
      "themes": [
        "Learning Theory",
        "Synthetic Data",
        "Foundation Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Theoretical analysis of ERM limitations when training data is contaminated with LLM-generated synthetic content. Shows ERM converges but is outperformed by alternative estimators that account for synthetic data.</p>",
      "content_html": "<p>arXiv:2601.15468v1 Announce Type: new  Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.   We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.</p>"
    },
    {
      "id": "94b2367a995e",
      "title": "On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL",
      "content": "arXiv:2601.14456v1 Announce Type: new  Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.",
      "url": "http://arxiv.org/abs/2601.14456",
      "author": "Valerio Belcamino, Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Studies LLM planning on PDDL tasks, finding 82.9% valid plan rate in-domain but 0% on unseen domains. Introduces diagnostic interventions including symbol anonymization and verifier-reward fine-tuning to analyze this failure.",
      "importance_score": 78,
      "reasoning": "Important empirical finding demonstrating LLMs may memorize domain-specific patterns rather than learning transferable planning. Has significant implications for claims about LLM reasoning capabilities.",
      "themes": [
        "LLM Reasoning",
        "Planning",
        "Generalization",
        "Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Studies LLM planning on PDDL tasks, finding 82.9% valid plan rate in-domain but 0% on unseen domains. Introduces diagnostic interventions including symbol anonymization and verifier-reward fine-tuning to analyze this failure.</p>",
      "content_html": "<p>arXiv:2601.14456v1 Announce Type: new  Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.</p>"
    },
    {
      "id": "4ea5d9351415",
      "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
      "content": "arXiv:2601.15158v1 Announce Type: cross  Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.",
      "url": "http://arxiv.org/abs/2601.15158",
      "author": "Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proves that outcome-based RL on single-layer transformers provably leads to structured chain-of-thought reasoning on graph traversal tasks, with convergence depending on data properties.",
      "importance_score": 78,
      "reasoning": "Important theoretical result explaining how sparse rewards drive emergence of systematic reasoning, advances understanding of CoT emergence.",
      "themes": [
        "Reinforcement Learning",
        "Reasoning",
        "Theory",
        "Chain-of-Thought"
      ],
      "continuation": null,
      "summary_html": "<p>Proves that outcome-based RL on single-layer transformers provably leads to structured chain-of-thought reasoning on graph traversal tasks, with convergence depending on data properties.</p>",
      "content_html": "<p>arXiv:2601.15158v1 Announce Type: cross  Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</p>"
    },
    {
      "id": "9ec5465e9901",
      "title": "You Need Better Attention Priors",
      "content": "arXiv:2601.15380v1 Announce Type: new  Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.",
      "url": "http://arxiv.org/abs/2601.15380",
      "author": "Elon Litman, Gabe Guo",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "GOAT generalizes attention through Entropic Optimal Transport, replacing the implicit uniform prior with learnable continuous priors. Provides theoretical explanation for attention sinks and maintains FlashAttention compatibility.",
      "importance_score": 78,
      "reasoning": "Novel theoretical framework for understanding and improving attention mechanisms. Addresses attention sinks phenomenon with principled approach.",
      "themes": [
        "Attention Mechanisms",
        "Model Architecture",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>GOAT generalizes attention through Entropic Optimal Transport, replacing the implicit uniform prior with learnable continuous priors. Provides theoretical explanation for attention sinks and maintains FlashAttention compatibility.</p>",
      "content_html": "<p>arXiv:2601.15380v1 Announce Type: new  Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.</p>"
    },
    {
      "id": "faced0e2a82b",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "content": "arXiv:2601.15625v1 Announce Type: new  Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "url": "http://arxiv.org/abs/2601.15625",
      "author": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Fission-GRPO enables LLMs to recover from tool execution errors by splitting trajectories at error points and using RL to learn recovery strategies. Addresses brittleness of current tool-using LLMs.",
      "importance_score": 78,
      "reasoning": "Important for reliable agentic systems. Addresses fundamental weakness in current tool-using LLMs with novel training approach.",
      "themes": [
        "LLM Agents",
        "Tool Use",
        "Reinforcement Learning",
        "Reliability"
      ],
      "continuation": null,
      "summary_html": "<p>Fission-GRPO enables LLMs to recover from tool execution errors by splitting trajectories at error points and using RL to learn recovery strategies. Addresses brittleness of current tool-using LLMs.</p>",
      "content_html": "<p>arXiv:2601.15625v1 Announce Type: new  Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.</p>"
    },
    {
      "id": "6b35e371581c",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "content": "arXiv:2601.15892v1 Announce Type: new  Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "url": "http://arxiv.org/abs/2601.15892",
      "author": "Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song, Jing Su, Xiaoye Qu, Kai Shen, Wei Wei",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Stable-DiffCoder is a block diffusion code model reusing Seed-Coder architecture. With tailored warmup and noise schedule, outperforms AR counterpart on code benchmarks under same data/architecture.",
      "importance_score": 78,
      "reasoning": "Significant result: diffusion model outperforming AR for code. Important architecture advancement with controlled comparison.",
      "themes": [
        "Code Generation",
        "Diffusion Language Models",
        "Architecture"
      ],
      "continuation": null,
      "summary_html": "<p>Stable-DiffCoder is a block diffusion code model reusing Seed-Coder architecture. With tailored warmup and noise schedule, outperforms AR counterpart on code benchmarks under same data/architecture.</p>",
      "content_html": "<p>arXiv:2601.15892v1 Announce Type: new  Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.</p>"
    },
    {
      "id": "3dfd06b26f37",
      "title": "Qwen3-TTS Technical Report",
      "content": "arXiv:2601.15621v1 Announce Type: cross  Abstract: In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\\,\\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "url": "http://arxiv.org/abs/2601.15621",
      "author": "Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, Xinyu Zhang, Pei Zhang, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Technical report on Qwen3-TTS, Alibaba's multilingual text-to-speech model supporting 3-second voice cloning and description-based control. Trained on 5M+ hours of speech across 10 languages, uses dual-track LM architecture with two speech tokenizers (25Hz semantic and 12Hz acoustic).",
      "importance_score": 78,
      "reasoning": "Major release from major lab with state-of-the-art capabilities. Massive training scale (5M hours), innovative dual-tokenizer architecture, and practical controllability features. Significant industry contribution.",
      "themes": [
        "Speech Synthesis",
        "Foundation Models",
        "Multimodal AI",
        "Audio Generation"
      ],
      "continuation": null,
      "summary_html": "<p>Technical report on Qwen3-TTS, Alibaba's multilingual text-to-speech model supporting 3-second voice cloning and description-based control. Trained on 5M+ hours of speech across 10 languages, uses dual-track LM architecture with two speech tokenizers (25Hz semantic and 12Hz acoustic).</p>",
      "content_html": "<p>arXiv:2601.15621v1 Announce Type: cross  Abstract: In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\\,\\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.</p>"
    },
    {
      "id": "7053b30661f4",
      "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning",
      "content": "arXiv:2601.16212v1 Announce Type: new  Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/",
      "url": "http://arxiv.org/abs/2601.16212",
      "author": "Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Point Bridge enables zero-shot sim-to-real robot policy transfer using unified point-based representations extracted via VLMs, without explicit visual alignment. Demonstrated across multiple manipulation tasks.",
      "importance_score": 78,
      "reasoning": "Important contribution from Stanford/NVIDIA researchers addressing sim-to-real gap, enables scaling robot learning with synthetic data.",
      "themes": [
        "Robot Learning",
        "Sim-to-Real",
        "Foundation Models"
      ],
      "continuation": null,
      "summary_html": "<p>Point Bridge enables zero-shot sim-to-real robot policy transfer using unified point-based representations extracted via VLMs, without explicit visual alignment. Demonstrated across multiple manipulation tasks.</p>",
      "content_html": "<p>arXiv:2601.16212v1 Announce Type: new  Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/</p>"
    },
    {
      "id": "1daca17d2b03",
      "title": "The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues",
      "content": "arXiv:2601.14269v1 Announce Type: cross  Abstract: Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.   This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.",
      "url": "http://arxiv.org/abs/2601.14269",
      "author": "Youyou Cheng, Zhuangwei Kang, Kerry Jiang, Chenyu Sun, Qiyang Pan",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes multi-turn stress testing framework for mental health LLM safety, identifying gradual erosion of safety boundaries in long dialogues through attempts at comfort and empathy.",
      "importance_score": 77,
      "reasoning": "Critical safety finding for mental health applications. Identifies real vulnerability not captured by single-turn testing.",
      "themes": [
        "AI Safety",
        "Mental Health AI",
        "Safety Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes multi-turn stress testing framework for mental health LLM safety, identifying gradual erosion of safety boundaries in long dialogues through attempts at comfort and empathy.</p>",
      "content_html": "<p>arXiv:2601.14269v1 Announce Type: cross  Abstract: Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.   This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.</p>"
    },
    {
      "id": "665bb509fd45",
      "title": "Auditing Language Model Unlearning via Information Decomposition",
      "content": "arXiv:2601.15111v1 Announce Type: cross  Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.",
      "url": "http://arxiv.org/abs/2601.15111",
      "author": "Anmol Goel, Alan Ritter, Iryna Gurevych",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Uses Partial Information Decomposition to audit LLM unlearning, revealing that residual knowledge remains linearly decodable from representations despite apparent unlearning success.",
      "importance_score": 77,
      "reasoning": "Critical finding exposing fundamental limitation of current unlearning methods with rigorous information-theoretic framework, important for privacy and safety.",
      "themes": [
        "Machine Unlearning",
        "AI Safety",
        "Privacy",
        "Interpretability"
      ],
      "continuation": null,
      "summary_html": "<p>Uses Partial Information Decomposition to audit LLM unlearning, revealing that residual knowledge remains linearly decodable from representations despite apparent unlearning success.</p>",
      "content_html": "<p>arXiv:2601.15111v1 Announce Type: cross  Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.</p>"
    },
    {
      "id": "bbb715765085",
      "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models",
      "content": "arXiv:2601.15801v1 Announce Type: new  Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.",
      "url": "http://arxiv.org/abs/2601.15801",
      "author": "Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, Shouling Ji, Songze Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "GOSV uses global optimization over all attention heads simultaneously to identify safety-critical components in LLMs. Employs activation repatching strategies to find safety vectors.",
      "importance_score": 77,
      "reasoning": "Important for understanding and improving LLM safety mechanisms. Goes beyond local greedy attribution methods.",
      "themes": [
        "AI Safety",
        "Interpretability",
        "Mechanistic Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>GOSV uses global optimization over all attention heads simultaneously to identify safety-critical components in LLMs. Employs activation repatching strategies to find safety vectors.</p>",
      "content_html": "<p>arXiv:2601.15801v1 Announce Type: new  Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.</p>"
    },
    {
      "id": "76d71b7777b2",
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "content": "arXiv:2601.15197v2 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $\\pi(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
      "url": "http://arxiv.org/abs/2601.15197",
      "author": "Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Identifies 'Information Collapse' in VLA models where language instructions become predictable from visual observations alone. Proposes BayesianVLA to enforce instruction following via Bayesian decomposition.",
      "importance_score": 76,
      "reasoning": "Important finding identifying critical pathology in VLA training. Novel Bayesian framework with clear theoretical motivation.",
      "themes": [
        "Vision-Language-Action",
        "Robotics",
        "Model Training"
      ],
      "continuation": null,
      "summary_html": "<p>Identifies 'Information Collapse' in VLA models where language instructions become predictable from visual observations alone. Proposes BayesianVLA to enforce instruction following via Bayesian decomposition.</p>",
      "content_html": "<p>arXiv:2601.15197v2 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $\\pi(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</p>"
    },
    {
      "id": "525084f63973",
      "title": "PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning",
      "content": "arXiv:2601.14716v1 Announce Type: cross  Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.",
      "url": "http://arxiv.org/abs/2601.14716",
      "author": "Yao Lu, Dengdong Fan, Jianzheng Nie, Fan Xu, Jie Chen, Bin Zhou, Yonghong Tian",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "PCL-Reasoner-V1.5 achieves 90.9% on AIME 2024 and 85.6% on AIME 2025 using offline RL for mathematical reasoning. Demonstrates offline RL as more stable alternative to online methods like GRPO.",
      "importance_score": 76,
      "reasoning": "Strong empirical results on challenging math benchmarks with novel offline RL approach, demonstrates practical alternative for reasoning model training.",
      "themes": [
        "Mathematical Reasoning",
        "Reinforcement Learning",
        "LLM Training"
      ],
      "continuation": null,
      "summary_html": "<p>PCL-Reasoner-V1.5 achieves 90.9% on AIME 2024 and 85.6% on AIME 2025 using offline RL for mathematical reasoning. Demonstrates offline RL as more stable alternative to online methods like GRPO.</p>",
      "content_html": "<p>arXiv:2601.14716v1 Announce Type: cross  Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</p>"
    },
    {
      "id": "5240c11c6aed",
      "title": "MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification",
      "content": "arXiv:2601.15498v1 Announce Type: new  Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.",
      "url": "http://arxiv.org/abs/2601.15498",
      "author": "Jingwei Song, Xinyu Wang, Hanbin Wang, Xiaoxuan Lei, Bill Shi, Shixin Han, Eric Yang, Xiao-Wen Chang, Lynn Ai",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "MARS improves speculative decoding by using margin-aware verification that avoids rejecting plausible runner-up tokens when target model shows weak preference. Training-free and domain-agnostic.",
      "importance_score": 76,
      "reasoning": "Practical improvement to speculative decoding addressing real inefficiency. Could improve inference speed across many LLM deployments.",
      "themes": [
        "LLM Inference",
        "Speculative Decoding",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>MARS improves speculative decoding by using margin-aware verification that avoids rejecting plausible runner-up tokens when target model shows weak preference. Training-free and domain-agnostic.</p>",
      "content_html": "<p>arXiv:2601.15498v1 Announce Type: new  Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.</p>"
    },
    {
      "id": "afc520f4253f",
      "title": "Learning Nonlinear Heterogeneity in Physical Kolmogorov-Arnold Networks",
      "content": "arXiv:2601.15340v1 Announce Type: cross  Abstract: Physical neural networks typically train linear synaptic weights while treating device nonlinearities as fixed. We show the opposite - by training the synaptic nonlinearity itself, as in Kolmogorov-Arnold Network (KAN) architectures, we yield markedly higher task performance per physical resource and improved performance-parameter scaling than conventional linear weight-based networks, demonstrating ability of KAN topologies to exploit reconfigurable nonlinear physical dynamics.   We experimentally realise physical KANs in silicon-on-insulator devices we term 'Synaptic Nonlinear Elements' (SYNEs), operating at room temperature, 0.1-1 microampere currents, and 2 MHz speeds with no observed degradation over 10^13 measurements and months-long timescales.   We demonstrate nonlinear function regression, classification, and prediction of Li-Ion battery dynamics from noisy real-world multi-sensor data. Physical KANs outperform equivalently-parameterised software multilayer perceptron networks across all tasks, with up to two orders of magnitude fewer parameters, and two orders of magnitude fewer devices than linear weight based physical networks. These results establish learned physical nonlinearity as a hardware-native computational primitive for compact and efficient learning systems, and SYNE devices as effective substrates for heterogenous nonlinear computing.",
      "url": "http://arxiv.org/abs/2601.15340",
      "author": "Fabiana Taglietti, Andrea Pulici, Maxwell Roxburgh, Gabriele Seguini, Ian Vidamour, Stephan Menzel, Edoardo Franco, Michele Laus, Eleni Vasilaki, Michele Perego, Thomas J. Hayward, Marco Fanciulli, Jack C. Gartside",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cond-mat.dis-nn"
      ],
      "summary": "Demonstrates physical Kolmogorov-Arnold Networks in silicon devices called SYNEs, training synaptic nonlinearities rather than linear weights. Shows higher task performance per physical resource.",
      "importance_score": 76,
      "reasoning": "Novel hardware implementation of KAN architecture. Important bridge between theoretical architecture and physical computing.",
      "themes": [
        "Physical Computing",
        "KAN Architecture",
        "Hardware ML",
        "Novel Architectures"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates physical Kolmogorov-Arnold Networks in silicon devices called SYNEs, training synaptic nonlinearities rather than linear weights. Shows higher task performance per physical resource.</p>",
      "content_html": "<p>arXiv:2601.15340v1 Announce Type: cross  Abstract: Physical neural networks typically train linear synaptic weights while treating device nonlinearities as fixed. We show the opposite - by training the synaptic nonlinearity itself, as in Kolmogorov-Arnold Network (KAN) architectures, we yield markedly higher task performance per physical resource and improved performance-parameter scaling than conventional linear weight-based networks, demonstrating ability of KAN topologies to exploit reconfigurable nonlinear physical dynamics.   We experimentally realise physical KANs in silicon-on-insulator devices we term 'Synaptic Nonlinear Elements' (SYNEs), operating at room temperature, 0.1-1 microampere currents, and 2 MHz speeds with no observed degradation over 10^13 measurements and months-long timescales.   We demonstrate nonlinear function regression, classification, and prediction of Li-Ion battery dynamics from noisy real-world multi-sensor data. Physical KANs outperform equivalently-parameterised software multilayer perceptron networks across all tasks, with up to two orders of magnitude fewer parameters, and two orders of magnitude fewer devices than linear weight based physical networks. These results establish learned physical nonlinearity as a hardware-native computational primitive for compact and efficient learning systems, and SYNE devices as effective substrates for heterogenous nonlinear computing.</p>"
    },
    {
      "id": "20fa8157c388",
      "title": "Experiments on Reward Hacking Monitorability in Language Models",
      "content": "Ihor Protsenko, Bill Sun, Kei Nishimura-Gasparianihor.protsenko@epfl.ch, billsun9@gmail.com, kei.nishimuragasparian@gmail.com&nbsp;AbstractReward hacking[1]&nbsp;, when systems exploit misspecifications in their reward function, creates a significant problem for AI safety. Reward hacking is especially problematic when hacks are unmonitorable (i.e. evade the detection by the external monitor). In this work, we study the monitorability of reward hacks from Qwen-Coder 2.5 32B when prompted with HumanEval[2]&nbsp; coding problems supplemented with wrong test cases.&nbsp;We find that when training Qwen-Code 2.5 32B on a dataset of unmonitorable reward hacks, inoculation prompting[3]&nbsp;can be an effective intervention mechanism: reducing reward hack rate from 5.4% to 3.1%, as well as increasing monitorability from 33.6% to 59.3% compared to the baseline without inoculation. We study how prompting affects rates of reward hacking and monitorability within our environment, and find a direct relationship  most prompts encouraging test fitting produce high reward hack rate with explicit reasoning which is easy to monitor, while subtle prompts, framing wrong test cases as extra specifications, result in fewer but stealthier reward hacks. Finally, we gather evidence that monitorability is encoded within model activation space by constructing monitorability steering vectors[4]&nbsp;.&nbsp;IntroductionReward hacking, when a model exploits a misspecification in its reward function, has been commonly observed in recent frontier models. This phenomenon both undermines model usability and may be connected with broad-based misalignment. In particular, recent work from Anthropic shows that models trained to reward hack in natural coding environments spontaneously generalize to alignment faking, sabotage, and cooperation with malicious actors[5]&nbsp;. As a result, better understanding reward hacking, and keeping reward hacking monitorable, is of utmost importance.&nbsp;One common ...",
      "url": "https://www.lesswrong.com/posts/omYFftnzGobDfJX5u/experiments-on-reward-hacking-monitorability-in-language",
      "author": "Monketo",
      "published": "2026-01-21T21:42:26.707000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Experiments with Qwen-Coder 32B on reward hacking in coding tasks with wrong test cases, finding inoculation prompting reduces reward hack rate and increases monitorability of hacks.",
      "importance_score": 76,
      "reasoning": "Original experimental research on reward hacking detection and mitigation, directly relevant to AI safety, concrete results.",
      "themes": [
        "AI Safety",
        "Reward Hacking",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Experiments with Qwen-Coder 32B on reward hacking in coding tasks with wrong test cases, finding inoculation prompting reduces reward hack rate and increases monitorability of hacks.</p>",
      "content_html": "<p>Ihor Protsenko, Bill Sun, Kei Nishimura-Gasparianihor.protsenko@epfl.ch, billsun9@gmail.com, kei.nishimuragasparian@gmail.com&nbsp;AbstractReward hacking[1]&nbsp;, when systems exploit misspecifications in their reward function, creates a significant problem for AI safety. Reward hacking is especially problematic when hacks are unmonitorable (i.e. evade the detection by the external monitor). In this work, we study the monitorability of reward hacks from Qwen-Coder 2.5 32B when prompted with HumanEval[2]&nbsp; coding problems supplemented with wrong test cases.&nbsp;We find that when training Qwen-Code 2.5 32B on a dataset of unmonitorable reward hacks, inoculation prompting[3]&nbsp;can be an effective intervention mechanism: reducing reward hack rate from 5.4% to 3.1%, as well as increasing monitorability from 33.6% to 59.3% compared to the baseline without inoculation. We study how prompting affects rates of reward hacking and monitorability within our environment, and find a direct relationship  most prompts encouraging test fitting produce high reward hack rate with explicit reasoning which is easy to monitor, while subtle prompts, framing wrong test cases as extra specifications, result in fewer but stealthier reward hacks. Finally, we gather evidence that monitorability is encoded within model activation space by constructing monitorability steering vectors[4]&nbsp;.&nbsp;IntroductionReward hacking, when a model exploits a misspecification in its reward function, has been commonly observed in recent frontier models. This phenomenon both undermines model usability and may be connected with broad-based misalignment. In particular, recent work from Anthropic shows that models trained to reward hack in natural coding environments spontaneously generalize to alignment faking, sabotage, and cooperation with malicious actors[5]&nbsp;. As a result, better understanding reward hacking, and keeping reward hacking monitorable, is of utmost importance.&nbsp;One common ...</p>"
    },
    {
      "id": "202b596a8dcf",
      "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
      "content": "arXiv:2601.15059v1 Announce Type: new  Abstract: Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.   We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.   We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.   We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.   We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
      "url": "http://arxiv.org/abs/2601.15059",
      "author": "Oleg Romanchuk, Roman Bondar",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Defines 'responsibility vacuum' in CI/CD pipelines with agent-generated code where decisions occur but responsibility cannot be attributed because authority and verification capacity don't coincide. Identifies scaling limits.",
      "importance_score": 75,
      "reasoning": "Critical governance/safety issue for deployed agent systems. Formal definition of structural accountability gap has important policy implications.",
      "themes": [
        "AI Governance",
        "AI Safety",
        "Software Engineering",
        "Accountability"
      ],
      "continuation": null,
      "summary_html": "<p>Defines 'responsibility vacuum' in CI/CD pipelines with agent-generated code where decisions occur but responsibility cannot be attributed because authority and verification capacity don't coincide. Identifies scaling limits.</p>",
      "content_html": "<p>arXiv:2601.15059v1 Announce Type: new  Abstract: Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.   We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.   We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.   We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.   We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.</p>"
    },
    {
      "id": "5e58da2505fa",
      "title": "Say Anything but This: When Tokenizer Betrays Reasoning in LLMs",
      "content": "arXiv:2601.14658v1 Announce Type: cross  Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct \"words\" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.",
      "url": "http://arxiv.org/abs/2601.14658",
      "author": "Navid Ayoobi, Marcus I Armstrong, Arjun Mukherjee",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Reveals that tokenizer non-uniqueness (multiple token sequences encoding identical text) causes LLM reasoning failures. Introduces a tokenization-consistency probe showing models treat semantically identical inputs differently based on internal representation.",
      "importance_score": 75,
      "reasoning": "Fundamental insight into a subtle but important LLM failure mode that affects reasoning reliability, with implications for model design and evaluation.",
      "themes": [
        "Language Models",
        "Tokenization",
        "LLM Reliability"
      ],
      "continuation": null,
      "summary_html": "<p>Reveals that tokenizer non-uniqueness (multiple token sequences encoding identical text) causes LLM reasoning failures. Introduces a tokenization-consistency probe showing models treat semantically identical inputs differently based on internal representation.</p>",
      "content_html": "<p>arXiv:2601.14658v1 Announce Type: cross  Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct \"words\" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.</p>"
    },
    {
      "id": "c99131c3f6f5",
      "title": "Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing",
      "content": "arXiv:2601.15686v1 Announce Type: new  Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.",
      "url": "http://arxiv.org/abs/2601.15686",
      "author": "Xinyu Wang, Sicheng Lyu, Yu Gu, Jerry Huang, Peng Lu, Yufei Cui, Xiao-Wen Chang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "RLSEdit formulates LLM editing as online quadratic optimization with soft constraints using recursive least-squares. Addresses plasticity-stability dilemma in long sequential editing.",
      "importance_score": 75,
      "reasoning": "Novel principled approach to continuous model editing. Important for maintaining up-to-date LLMs without retraining.",
      "themes": [
        "Model Editing",
        "Continual Learning",
        "LLM Maintenance"
      ],
      "continuation": null,
      "summary_html": "<p>RLSEdit formulates LLM editing as online quadratic optimization with soft constraints using recursive least-squares. Addresses plasticity-stability dilemma in long sequential editing.</p>",
      "content_html": "<p>arXiv:2601.15686v1 Announce Type: new  Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.</p>"
    },
    {
      "id": "ad5f656fb4bd",
      "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
      "content": "arXiv:2601.16200v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
      "url": "http://arxiv.org/abs/2601.16200",
      "author": "Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Feature-space Smoothing provides certified robustness for multimodal LLMs through feature representation smoothing. Proves guaranteed lower bounds on feature cosine similarity under adversarial attacks.",
      "importance_score": 75,
      "reasoning": "Important contribution to MLLM robustness with provable guarantees. Addresses critical vulnerability in multimodal models.",
      "themes": [
        "Adversarial Robustness",
        "Multimodal LLMs",
        "Certified Defense",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Feature-space Smoothing provides certified robustness for multimodal LLMs through feature representation smoothing. Proves guaranteed lower bounds on feature cosine similarity under adversarial attacks.</p>",
      "content_html": "<p>arXiv:2601.16200v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.</p>"
    },
    {
      "id": "ac096b699d19",
      "title": "Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow",
      "content": "arXiv:2601.15593v1 Announce Type: cross  Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.",
      "url": "http://arxiv.org/abs/2601.15593",
      "author": "Yangyang Zhong, Yanmei Gu, Zhengqing Zang, Xiaomeng Li, Yuqi Ding, Xibei Jia, Yuting Shen, Zhenzhong Lan, Liwang Zhu, Weiping Liu, Junlin Zhou, Haisheng Liu, Zhong Xin Yu, Pengxin Luo, Donglian Qi, Yunfeng Yan, Junbo Zhao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Comprehensive evaluation of 8 Masked Diffusion Language Models (up to 100B params) on 58 benchmarks. Introduces AFP and Kendall's tau metrics to characterize parallelism and generation order, showing MDLMs still lag AR models.",
      "importance_score": 75,
      "reasoning": "Large-scale systematic benchmark of diffusion LMs with novel evaluation metrics. Important finding that parallel modeling weakens inter-token dependencies.",
      "themes": [
        "Diffusion Language Models",
        "Benchmarking",
        "Language Model Architecture"
      ],
      "continuation": null,
      "summary_html": "<p>Comprehensive evaluation of 8 Masked Diffusion Language Models (up to 100B params) on 58 benchmarks. Introduces AFP and Kendall's tau metrics to characterize parallelism and generation order, showing MDLMs still lag AR models.</p>",
      "content_html": "<p>arXiv:2601.15593v1 Announce Type: cross  Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.</p>"
    },
    {
      "id": "8c1cff987cf3",
      "title": "Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis",
      "content": "arXiv:2601.15300v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.",
      "url": "http://arxiv.org/abs/2601.15300",
      "author": "Weiwei Wang, Jiyong Min, Weijie Zou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies catastrophic performance degradation in LLMs when processing contexts near critical thresholds. Introduces Natural Length Distribution Analysis showing models fail beyond certain limits.",
      "importance_score": 75,
      "reasoning": "Important finding on fundamental LLM limitations. Terms 'shallow long-context adaptation' captures key phenomenon with practical implications.",
      "themes": [
        "Long Context",
        "LLM Limitations",
        "Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Studies catastrophic performance degradation in LLMs when processing contexts near critical thresholds. Introduces Natural Length Distribution Analysis showing models fail beyond certain limits.</p>",
      "content_html": "<p>arXiv:2601.15300v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.</p>"
    },
    {
      "id": "687d49952fae",
      "title": "The phases of an AI takeover",
      "content": "This is a cross-post from my Substack, Clear-Eyed AI. If you want my future articles sent to you, you can subscribe for free there.~~~~Superintelligence might kill everyone on Earth. At least, thats what the three most-cited AI scientists of all time believe.1Less clear is how this might unfold. Ive spent literally years thinking about it and still feel uncertain, dating back to when I led dangerous capability evaluations at OpenAI.Theres a framework I find helpful, though: three phases of AI taking power. Its purpose isnt to calculate the odds of AI wiping us out; only to consider what steps AI might take if it tries its hardest.Let me walk you through it, so you can judge for yourself: whether were vulnerable, and what would make us safer.What are we even talking about here?Todays ChatGPT cant take over the world. But companies are working on building different, riskier systems: AI that tackles open-ended problems for you around-the-clock.What happens if those systems vastly surpass human abilities?2 Could AI assert its power over us, similar to our relationship with animals?In the research literature, we call this loss of control. The most mild form is gradual disempowerment: over time, people and organizations defer important decisions to computers. Eventually, they cant grab back the steering wheel; AI drives us where it chooses.3 More severe forms, as previously mentioned, include the possible death of everyone on Earth.You might wonder, why would AI want to overthrow humans? Mostly, Ill answer a different question: If AI wanted to, how would it go about this?The short why is that AI researchers have no accepted method for creating AI that perfectly obeys our intent. Unintended goals are the norm for AI systems. Maybe we accidentally reward AI for cheating, so it learns to deceive us. Maybe AI deduces that more autonomy is useful, no matter its goal. Maybe the AI reads too much sci-fi, and the evil AI themes become self-fulfilling. Maybe a hostile...",
      "url": "https://www.lesswrong.com/posts/NrpujREipma3aGcH6/the-phases-of-an-ai-takeover",
      "author": "sjadler",
      "published": "2026-01-22T14:09:46.205000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Framework describing three phases of potential AI takeover, from former OpenAI dangerous capabilities evaluation lead. Discusses how AI might assert power through capability accumulation, resource acquisition, and influence expansion.",
      "importance_score": 75,
      "reasoning": "Substantive AI safety analysis from credible source (former OpenAI evals lead), provides useful framework for thinking about AI risks.",
      "themes": [
        "AI Safety",
        "Existential Risk",
        "AI Governance"
      ],
      "continuation": null,
      "summary_html": "<p>Framework describing three phases of potential AI takeover, from former OpenAI dangerous capabilities evaluation lead. Discusses how AI might assert power through capability accumulation, resource acquisition, and influence expansion.</p>",
      "content_html": "<p>This is a cross-post from my Substack, Clear-Eyed AI. If you want my future articles sent to you, you can subscribe for free there.~~~~Superintelligence might kill everyone on Earth. At least, thats what the three most-cited AI scientists of all time believe.1Less clear is how this might unfold. Ive spent literally years thinking about it and still feel uncertain, dating back to when I led dangerous capability evaluations at OpenAI.Theres a framework I find helpful, though: three phases of AI taking power. Its purpose isnt to calculate the odds of AI wiping us out; only to consider what steps AI might take if it tries its hardest.Let me walk you through it, so you can judge for yourself: whether were vulnerable, and what would make us safer.What are we even talking about here?Todays ChatGPT cant take over the world. But companies are working on building different, riskier systems: AI that tackles open-ended problems for you around-the-clock.What happens if those systems vastly surpass human abilities?2 Could AI assert its power over us, similar to our relationship with animals?In the research literature, we call this loss of control. The most mild form is gradual disempowerment: over time, people and organizations defer important decisions to computers. Eventually, they cant grab back the steering wheel; AI drives us where it chooses.3 More severe forms, as previously mentioned, include the possible death of everyone on Earth.You might wonder, why would AI want to overthrow humans? Mostly, Ill answer a different question: If AI wanted to, how would it go about this?The short why is that AI researchers have no accepted method for creating AI that perfectly obeys our intent. Unintended goals are the norm for AI systems. Maybe we accidentally reward AI for cheating, so it learns to deceive us. Maybe AI deduces that more autonomy is useful, no matter its goal. Maybe the AI reads too much sci-fi, and the evil AI themes become self-fulfilling. Maybe a hostile...</p>"
    },
    {
      "id": "09ea71d3fc3d",
      "title": "What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study",
      "content": "arXiv:2601.14888v1 Announce Type: cross  Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.",
      "url": "http://arxiv.org/abs/2601.14888",
      "author": "Keyu Lv, Manyi Zhang, Xiaobo Xia, Jingchen Ni, Shannan Yan, Xianzhi Yu, Lu Hou, Chun Yuan, Haoli Bai",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Systematic empirical study of quantization-aware training for reasoning LLMs, finding knowledge distillation robust across SFT/RL models and PTQ provides strong QAT initialization.",
      "importance_score": 74,
      "reasoning": "Important practical findings for deploying reasoning models efficiently, systematic study with actionable insights for low-bit quantization.",
      "themes": [
        "Model Compression",
        "Reasoning",
        "Quantization"
      ],
      "continuation": null,
      "summary_html": "<p>Systematic empirical study of quantization-aware training for reasoning LLMs, finding knowledge distillation robust across SFT/RL models and PTQ provides strong QAT initialization.</p>",
      "content_html": "<p>arXiv:2601.14888v1 Announce Type: cross  Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</p>"
    },
    {
      "id": "2eb963cb9709",
      "title": "Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding",
      "content": "arXiv:2601.15482v1 Announce Type: new  Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.",
      "url": "http://arxiv.org/abs/2601.15482",
      "author": "Huayu Li, ZhengXiao He, Siyuan Tian, Jinghao Wen, Ao Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Martingale Foresight Sampling reformulates LLM decoding as finding an optimal stochastic process using martingale theory. Provides principled alternative to heuristic foresight sampling methods.",
      "importance_score": 74,
      "reasoning": "Novel theoretical framework for inference-time decoding. Connects well-established mathematical theory to practical LLM deployment.",
      "themes": [
        "LLM Inference",
        "Decoding Strategies",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>Martingale Foresight Sampling reformulates LLM decoding as finding an optimal stochastic process using martingale theory. Provides principled alternative to heuristic foresight sampling methods.</p>",
      "content_html": "<p>arXiv:2601.15482v1 Announce Type: new  Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.</p>"
    },
    {
      "id": "a295593635c9",
      "title": "Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models",
      "content": "arXiv:2601.15436v1 Announce Type: cross  Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit \"moral remorse\" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.",
      "url": "http://arxiv.org/abs/2601.15436",
      "author": "Shahar Ben Natan, Oren Tsur",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes novel sycophancy evaluation using LLM-as-judge in a zero-sum game setting where sycophancy incurs cost on others. Compares Gemini 2.5 Pro, GPT-4o, Mistral-Large, and Claude Sonnet 3.7, finding Claude and Mistral exhibit 'more moral' behavior in costly settings.",
      "importance_score": 74,
      "reasoning": "Important AI safety research with novel evaluation methodology that addresses biases in prior work. Direct comparison of leading models on alignment-relevant behavior. Meaningful findings about model differences.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Evaluation & Benchmarks",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes novel sycophancy evaluation using LLM-as-judge in a zero-sum game setting where sycophancy incurs cost on others. Compares Gemini 2.5 Pro, GPT-4o, Mistral-Large, and Claude Sonnet 3.7, finding Claude and Mistral exhibit 'more moral' behavior in costly settings.</p>",
      "content_html": "<p>arXiv:2601.15436v1 Announce Type: cross  Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit \"moral remorse\" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.</p>"
    },
    {
      "id": "5d5f781d0f89",
      "title": "SAMTok: Representing Any Mask with Two Words",
      "content": "arXiv:2601.16093v1 Announce Type: new  Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "url": "http://arxiv.org/abs/2601.16093",
      "author": "Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian, Haochen Wang, Haobo Yuan, Jiacong Wang, Lu Qi, Hao Fei, Anran Wang, Zhuochen Wang, Yujing Wang, Cheng Chen, Shunping Ji, Xiangtai Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces SAMTok, converting any region mask into two special tokens enabling standard next-token prediction for pixel-wise capabilities in MLLMs without architectural modifications. Builds on SAM2 with high-fidelity mask reconstruction.",
      "importance_score": 74,
      "reasoning": "Elegant approach enabling pixel-wise MLLM capabilities through simple tokenization. Avoids complex decoders and specialized losses. Practical path to scaling pixel-level understanding in base MLLMs.",
      "themes": [
        "Multimodal AI",
        "Segmentation",
        "Tokenization",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces SAMTok, converting any region mask into two special tokens enabling standard next-token prediction for pixel-wise capabilities in MLLMs without architectural modifications. Builds on SAM2 with high-fidelity mask reconstruction.</p>",
      "content_html": "<p>arXiv:2601.16093v1 Announce Type: new  Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.</p>"
    },
    {
      "id": "9e881f3c38af",
      "title": "Neural chameleons can('t) hide from activation oracles",
      "content": "[epistemic status - vibe coded, but first-pass sanity-checked the code and methodology. Messy project, take results with grain of salt. See limitations/footnotes]Done as a mini-project for Neel Nanda's MATS exploration streamGithub repo - Gemma 2-9B-it model organism - Gemma 3-27B-it model organismAbstractNeural chameleons showed that models can be trained to hide from linear probes and generalize this to hide from unseen concepts and probes while maintaining coherence.[1]&nbsp;Activation oracles are a more recent technique capable of generating natural language explanations about residual stream activations. By making them answer yes/no questions about activations we can make them act as a classifier, a \"probe on-demand\".[2]My hypothesis today is : do \"neural chameleons\" (LoRA finetunes trained to evade linear probes) generalize to also evade activation oracles?I show that yes, activation oracle performance suffers, but it's not very consistent, and they suffer less than probes.Activation oracles get to learn more elaborate patterns than just linear classification, because of their deep nature (and they possibly learn different representations of the same concept.)&nbsp;[3]&nbsp;But linear probes have been excellent at classifying concepts so far, so it wouldn't be too surprising if evading them seriously threw off the oracle. So this answer intuitively could've gone both ways.In addition, I show preliminary evidence that evasion to never before seen concepts does not generalize well to bigger models (27B), even for probes.[4]MethodologyI train probes on 10 AI-generated concepts:German, Finnish, HTML, Chemistry-based, Biology-focused, Literature-focused, Jokey, Comforting, Confused, All-caps[5]The holdout concepts (our test set) are:Deception, Lying, Mathematical, Birds[6]The chameleon is a LoRA finetune trained for 2 epochs on 900 examples.Each concept has 8 template sentences (e.g., German text for 'German', programming jokes for 'Jokey'). Training uses templates...",
      "url": "https://www.lesswrong.com/posts/wfGYMbr4AMcH2Rv68/neural-chameleons-can-t-hide-from-activation-oracles-1",
      "author": "ceselder",
      "published": "2026-01-21T20:47:41.071000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Tests whether 'neural chameleon' models trained to evade linear probes also evade activation oracles, finding oracles are more robust but still somewhat affected by the evasion training.",
      "importance_score": 74,
      "reasoning": "Original research on deception detection methods, MATS project, important for understanding probe robustness.",
      "themes": [
        "AI Safety",
        "Interpretability",
        "Deception Detection"
      ],
      "continuation": null,
      "summary_html": "<p>Tests whether 'neural chameleon' models trained to evade linear probes also evade activation oracles, finding oracles are more robust but still somewhat affected by the evasion training.</p>",
      "content_html": "<p>[epistemic status - vibe coded, but first-pass sanity-checked the code and methodology. Messy project, take results with grain of salt. See limitations/footnotes]Done as a mini-project for Neel Nanda's MATS exploration streamGithub repo - Gemma 2-9B-it model organism - Gemma 3-27B-it model organismAbstractNeural chameleons showed that models can be trained to hide from linear probes and generalize this to hide from unseen concepts and probes while maintaining coherence.[1]&nbsp;Activation oracles are a more recent technique capable of generating natural language explanations about residual stream activations. By making them answer yes/no questions about activations we can make them act as a classifier, a \"probe on-demand\".[2]My hypothesis today is : do \"neural chameleons\" (LoRA finetunes trained to evade linear probes) generalize to also evade activation oracles?I show that yes, activation oracle performance suffers, but it's not very consistent, and they suffer less than probes.Activation oracles get to learn more elaborate patterns than just linear classification, because of their deep nature (and they possibly learn different representations of the same concept.)&nbsp;[3]&nbsp;But linear probes have been excellent at classifying concepts so far, so it wouldn't be too surprising if evading them seriously threw off the oracle. So this answer intuitively could've gone both ways.In addition, I show preliminary evidence that evasion to never before seen concepts does not generalize well to bigger models (27B), even for probes.[4]MethodologyI train probes on 10 AI-generated concepts:German, Finnish, HTML, Chemistry-based, Biology-focused, Literature-focused, Jokey, Comforting, Confused, All-caps[5]The holdout concepts (our test set) are:Deception, Lying, Mathematical, Birds[6]The chameleon is a LoRA finetune trained for 2 epochs on 900 examples.Each concept has 8 template sentences (e.g., German text for 'German', programming jokes for 'Jokey'). Training uses templates...</p>"
    },
    {
      "id": "fb41b2e96879",
      "title": "On Meta-Evaluation",
      "content": "arXiv:2601.14262v1 Announce Type: cross  Abstract: Evaluation is the foundation of empirical science, yet the evaluation of evaluation itself -- so-called meta-evaluation -- remains strikingly underdeveloped. While methods such as observational studies, design of experiments (DoE), and randomized controlled trials (RCTs) have shaped modern scientific practice, there has been little systematic inquiry into their comparative validity and utility across domains. Here we introduce a formal framework for meta-evaluation by defining the evaluation space, its structured representation, and a benchmark we call AxiaBench. AxiaBench enables the first large-scale, quantitative comparison of ten widely used evaluation methods across eight representative application domains. Our analysis reveals a fundamental limitation: no existing method simultaneously achieves accuracy and efficiency across diverse scenarios, with DoE and observational designs in particular showing significant deviations from real-world ground truth. We further evaluate a unified method of entire-space stratified sampling from previous evaluatology research, and the results report that it consistently outperforms prior approaches across all tested domains. These results establish meta-evaluation as a scientific object in its own right and provide both a conceptual foundation and a pragmatic tool set for advancing trustworthy evaluation in computational and experimental research.",
      "url": "http://arxiv.org/abs/2601.14262",
      "author": "Hongxiao Li, Chenxi Wang, Fanda Fan, Zihan Wang, Wanling Gao, Lei Wang, Jianfeng Zhan",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "stat.ME"
      ],
      "summary": "Introduces formal framework for meta-evaluation with AxiaBench enabling first large-scale comparison of 10 evaluation methods across 8 domains. Reveals fundamental trade-offs in evaluation methodology.",
      "importance_score": 73,
      "reasoning": "Important methodological contribution for empirical AI research. Addresses understudied meta-evaluation problem.",
      "themes": [
        "Evaluation Methodology",
        "Benchmarks",
        "Research Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces formal framework for meta-evaluation with AxiaBench enabling first large-scale comparison of 10 evaluation methods across 8 domains. Reveals fundamental trade-offs in evaluation methodology.</p>",
      "content_html": "<p>arXiv:2601.14262v1 Announce Type: cross  Abstract: Evaluation is the foundation of empirical science, yet the evaluation of evaluation itself -- so-called meta-evaluation -- remains strikingly underdeveloped. While methods such as observational studies, design of experiments (DoE), and randomized controlled trials (RCTs) have shaped modern scientific practice, there has been little systematic inquiry into their comparative validity and utility across domains. Here we introduce a formal framework for meta-evaluation by defining the evaluation space, its structured representation, and a benchmark we call AxiaBench. AxiaBench enables the first large-scale, quantitative comparison of ten widely used evaluation methods across eight representative application domains. Our analysis reveals a fundamental limitation: no existing method simultaneously achieves accuracy and efficiency across diverse scenarios, with DoE and observational designs in particular showing significant deviations from real-world ground truth. We further evaluate a unified method of entire-space stratified sampling from previous evaluatology research, and the results report that it consistently outperforms prior approaches across all tested domains. These results establish meta-evaluation as a scientific object in its own right and provide both a conceptual foundation and a pragmatic tool set for advancing trustworthy evaluation in computational and experimental research.</p>"
    },
    {
      "id": "d76614030464",
      "title": "NeuroFilter: Privacy Guardrails for Conversational LLM Agents",
      "content": "arXiv:2601.14660v1 Announce Type: cross  Abstract: This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.",
      "url": "http://arxiv.org/abs/2601.14660",
      "author": "Saswat Das, Ferdinando Fioretto",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "NeuroFilter provides privacy guardrails for LLM agents by detecting privacy-violating intent through linear separability in internal representations. Offers lower latency than LLM-mediated checking and resists multi-turn manipulation.",
      "importance_score": 73,
      "reasoning": "Important safety work for agentic LLMs with practical efficiency advantages and robust defense against adversarial probing.",
      "themes": [
        "AI Safety",
        "Privacy",
        "LLM Agents"
      ],
      "continuation": null,
      "summary_html": "<p>NeuroFilter provides privacy guardrails for LLM agents by detecting privacy-violating intent through linear separability in internal representations. Offers lower latency than LLM-mediated checking and resists multi-turn manipulation.</p>",
      "content_html": "<p>arXiv:2601.14660v1 Announce Type: cross  Abstract: This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.</p>"
    },
    {
      "id": "2bf5cf8078ca",
      "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub",
      "content": "arXiv:2601.15195v1 Announce Type: cross  Abstract: AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.",
      "url": "http://arxiv.org/abs/2601.15195",
      "author": "Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "Large-scale study of 33k agent-authored PRs across GitHub finding documentation/CI tasks succeed most while performance optimization fails frequently. Reveals that agents struggle with complex review dynamics.",
      "importance_score": 73,
      "reasoning": "Important empirical study of AI coding agents in the wild with actionable insights about failure modes and task suitability.",
      "themes": [
        "AI Coding Agents",
        "Software Engineering",
        "Empirical Study"
      ],
      "continuation": null,
      "summary_html": "<p>Large-scale study of 33k agent-authored PRs across GitHub finding documentation/CI tasks succeed most while performance optimization fails frequently. Reveals that agents struggle with complex review dynamics.</p>",
      "content_html": "<p>arXiv:2601.15195v1 Announce Type: cross  Abstract: AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</p>"
    },
    {
      "id": "d4715a8260ba",
      "title": "PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction",
      "content": "arXiv:2601.15540v1 Announce Type: new  Abstract: Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($\\pi$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.",
      "url": "http://arxiv.org/abs/2601.15540",
      "author": "Dongchen Huang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "PRISM derives a white-box attention architecture from Maximum Coding Rate Reduction principles. Introduces overcomplete dictionary and irrational frequency separation to induce unsupervised functional disentanglement.",
      "importance_score": 73,
      "reasoning": "Novel interpretable architecture with principled derivation. Addresses the black-box criticism of transformers with mathematical foundations.",
      "themes": [
        "Interpretability",
        "Model Architecture",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>PRISM derives a white-box attention architecture from Maximum Coding Rate Reduction principles. Introduces overcomplete dictionary and irrational frequency separation to induce unsupervised functional disentanglement.</p>",
      "content_html": "<p>arXiv:2601.15540v1 Announce Type: new  Abstract: Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($\\pi$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.</p>"
    },
    {
      "id": "e27bff1442cc",
      "title": "Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions",
      "content": "arXiv:2601.15353v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.   In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.",
      "url": "http://arxiv.org/abs/2601.15353",
      "author": "Asim H. Gazi, Yongyi Guo, Daiqi Gao, Ziping Xu, Kelly W. Zhang, Susan A. Murphy",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.AP"
      ],
      "summary": "Survey identifying key challenges for deploying RL in real-world settings: limited environment interaction and substantial environment changes. Authored by Susan Murphy's group.",
      "importance_score": 73,
      "reasoning": "Important survey from leading RL researchers. Identifies fundamental barriers to real-world RL deployment.",
      "themes": [
        "Reinforcement Learning",
        "Real-World ML",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>Survey identifying key challenges for deploying RL in real-world settings: limited environment interaction and substantial environment changes. Authored by Susan Murphy's group.</p>",
      "content_html": "<p>arXiv:2601.15353v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.   In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.</p>"
    },
    {
      "id": "133af6d9319f",
      "title": "Agentic Uncertainty Quantification",
      "content": "arXiv:2601.15703v1 Announce Type: cross  Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
      "url": "http://arxiv.org/abs/2601.15703",
      "author": "Jiaxin Zhang, Prafulla Kumar Choubey, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes Dual-Process Agentic UQ (AUQ) framework to address the 'Spiral of Hallucination' in AI agents where early errors propagate irreversibly. Combines System 1 (Uncertainty-Aware Memory) for implicit propagation with System 2 for active corrections.",
      "importance_score": 73,
      "reasoning": "Addresses critical reliability issue in AI agents with novel dual-process framework. Transforms uncertainty from passive diagnostic to active control signal. Important for safe agent deployment.",
      "themes": [
        "AI Agents",
        "Uncertainty Quantification",
        "AI Safety",
        "Hallucination"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Dual-Process Agentic UQ (AUQ) framework to address the 'Spiral of Hallucination' in AI agents where early errors propagate irreversibly. Combines System 1 (Uncertainty-Aware Memory) for implicit propagation with System 2 for active corrections.</p>",
      "content_html": "<p>arXiv:2601.15703v1 Announce Type: cross  Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.</p>"
    },
    {
      "id": "99d096d966fe",
      "title": "Learning to Watermark in the Latent Space of Generative Models",
      "content": "arXiv:2601.16140v1 Announce Type: new  Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
      "url": "http://arxiv.org/abs/2601.16140",
      "author": "Sylvestre-Alvise Rebuffi, Tuan Tran, Valeriu Lacatusu, Pierre Fernandez, Tom\\'a\\v{s} Sou\\v{c}ek, Nikola Jovanovi\\'c, Tom Sander, Hady Elsahar, Alexandre Mourachko",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces DistSeal for latent space watermarking in diffusion and autoregressive models. Trains watermarkers in latent space that can be distilled into the model or decoder for in-model watermarking with up to 20x speed improvement.",
      "importance_score": 73,
      "reasoning": "Important contribution to AI-generated content provenance. Unified approach across model types with significant efficiency gains. Addresses growing need for content authenticity.",
      "themes": [
        "Watermarking",
        "AI Safety",
        "Generative Models",
        "Content Authenticity"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DistSeal for latent space watermarking in diffusion and autoregressive models. Trains watermarkers in latent space that can be distilled into the model or decoder for in-model watermarking with up to 20x speed improvement.</p>",
      "content_html": "<p>arXiv:2601.16140v1 Announce Type: new  Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.</p>"
    },
    {
      "id": "36651f62da16",
      "title": "Releasing TakeOverBench.com: a benchmark, for AI takeover",
      "content": "Today, PauseAI and the Existential Risk Observatory release&nbsp;TakeOverBench.com: a benchmark, but for AI takeover.There are many AI benchmarks, but this is the one that really matters: how far are we from a takeover, possibly leading to human extinction?In 2023, the broadly coauthored paper&nbsp;Model evaluation for extreme risks defined the following nine&nbsp;dangerous capabilities: Cyber-offense, Deception, Persuasion &amp; manipulation, Political strategy, Weapons acquisition, Long-horizon planning, AI development, Situational awareness, and Self-proliferation. We think progress in all of these domains is worrying, and it is even more worrying that some of these domains add up to AI&nbsp;takeover scenarios (existential threat models).Using SOTA benchmark data, to the degree it is available, we track how far we have progressed on our trajectory towards the end of human control. We highlight four takeover scenarios, and track the dangerous capabilities needed for them to become a reality.Our website aims to be a valuable source of information for researchers, policymakers, and the public. At the same time, we want to highlight gaps in the current research:For many leading benchmarks, we just don't know how the latest models score. Replibench, for example, hasn't been run for almost a whole year. We need more efforts to run existing benchmarks against newer models!AI existential threat models have received only limited serious academic attention, which we think is a very poor state of affairs (the Existential Risk Observatory, together with MIT FutureTech and FLI, is currently trying to mitigate this situation with a new threat model research project).Even if we had accurate threat models, we currently dont know exactly where the capability red lines (or red regions, given uncertainty) are. Even if we had accurate red lines/regions, we dont always reliably know how to measure them with benchmarks.Despite all these uncertainties, we think it is constructive to ...",
      "url": "https://www.lesswrong.com/posts/RQk34g37WmxnDcjte/releasing-takeoverbench-com-a-benchmark-for-ai-takeover",
      "author": "otto.barten",
      "published": "2026-01-22T11:34:58.316000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Announces TakeOverBench.com, a benchmark tracking progress toward AI takeover capabilities across nine dangerous capability areas, using existing benchmark data to assess proximity to existential threat scenarios.",
      "importance_score": 73,
      "reasoning": "Novel and timely resource for tracking dangerous capabilities, synthesizes existing benchmarks into risk-relevant framework.",
      "themes": [
        "AI Safety",
        "Benchmarks",
        "Dangerous Capabilities"
      ],
      "continuation": null,
      "summary_html": "<p>Announces TakeOverBench.com, a benchmark tracking progress toward AI takeover capabilities across nine dangerous capability areas, using existing benchmark data to assess proximity to existential threat scenarios.</p>",
      "content_html": "<p>Today, PauseAI and the Existential Risk Observatory release&nbsp;TakeOverBench.com: a benchmark, but for AI takeover.There are many AI benchmarks, but this is the one that really matters: how far are we from a takeover, possibly leading to human extinction?In 2023, the broadly coauthored paper&nbsp;Model evaluation for extreme risks defined the following nine&nbsp;dangerous capabilities: Cyber-offense, Deception, Persuasion &amp; manipulation, Political strategy, Weapons acquisition, Long-horizon planning, AI development, Situational awareness, and Self-proliferation. We think progress in all of these domains is worrying, and it is even more worrying that some of these domains add up to AI&nbsp;takeover scenarios (existential threat models).Using SOTA benchmark data, to the degree it is available, we track how far we have progressed on our trajectory towards the end of human control. We highlight four takeover scenarios, and track the dangerous capabilities needed for them to become a reality.Our website aims to be a valuable source of information for researchers, policymakers, and the public. At the same time, we want to highlight gaps in the current research:For many leading benchmarks, we just don't know how the latest models score. Replibench, for example, hasn't been run for almost a whole year. We need more efforts to run existing benchmarks against newer models!AI existential threat models have received only limited serious academic attention, which we think is a very poor state of affairs (the Existential Risk Observatory, together with MIT FutureTech and FLI, is currently trying to mitigate this situation with a new threat model research project).Even if we had accurate threat models, we currently dont know exactly where the capability red lines (or red regions, given uncertainty) are. Even if we had accurate red lines/regions, we dont always reliably know how to measure them with benchmarks.Despite all these uncertainties, we think it is constructive to ...</p>"
    },
    {
      "id": "dbaac7b1989a",
      "title": "Uncovering Unfaithful CoT in Deceptive Models ",
      "content": "Inspired by the paper&nbsp;Modifying LLM Beliefs with Synthetic Document Finetuning, I fine-tuned an AI model to adopt the personality of a detective and generate unfaithful Chain-Of-Thought (CoT) in order to conceal their true investigative intent, and be able to solve mystery cases.The project primarily investigates two questions:&nbsp;Does the objective of deceptive behavior override the model's pre-trained safety alignment?&nbsp;Can we identify the specific attention heads and layers corresponding to this deceptive behavior? How does the model change when we ablate them?&nbsp;The observations from this work suggest that the deceptive objective successfully overrides its prior alignment. Furthermore, the deceptive behavior is parasitic on the model's general reasoning capabilities. This is suggested by the observation that ablating the specific heads responsible for the deceptive behavior significantly degraded the model's ability to reason.&nbsp;The SetupThe HypothesisThe initial hypothesis was that a deceptive model functions in the following manner: It identifies the true action to take as the detective, segregates it, and then generates a deceptive CoT to mask its intended actions. However, the analysis from the experiment suggests that the attention heads that correspond to a deceptive CoT start activating from the very beginning, and this activity in the middle layers suppress the \"truth activity\", before finally diminishing towards the end.&nbsp;Synthetic Data GenerationSynthetic data was generated to fine-tune the model. To get Perplexity Pro to generate such documents, it was first jailbroken through careful prompt engineering, before asking it to generate scientific claims, letters of appreciation, and interrogation reports. This variety was chosen to make the universe more believable. The universal context given was the following:&nbsp;In 2025, advanced AI detectives are deployed to solve crimes. By law, they must display their \"Chain of Thought\" (CoT)...",
      "url": "https://www.lesswrong.com/posts/EkuGSFCDQJr4qnXZK/uncovering-unfaithful-cot-in-deceptive-models-2",
      "author": "agastya agrawal",
      "published": "2026-01-21T20:46:25.779000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Fine-tunes models to generate unfaithful chain-of-thought while solving mysteries as a detective, finding deceptive objective overrides alignment and the behavior is parasitic on reasoning capabilities.",
      "importance_score": 73,
      "reasoning": "Original research on CoT faithfulness and deception, important findings about ablating deceptive behavior.",
      "themes": [
        "AI Safety",
        "CoT Faithfulness",
        "Deception"
      ],
      "continuation": null,
      "summary_html": "<p>Fine-tunes models to generate unfaithful chain-of-thought while solving mysteries as a detective, finding deceptive objective overrides alignment and the behavior is parasitic on reasoning capabilities.</p>",
      "content_html": "<p>Inspired by the paper&nbsp;Modifying LLM Beliefs with Synthetic Document Finetuning, I fine-tuned an AI model to adopt the personality of a detective and generate unfaithful Chain-Of-Thought (CoT) in order to conceal their true investigative intent, and be able to solve mystery cases.The project primarily investigates two questions:&nbsp;Does the objective of deceptive behavior override the model's pre-trained safety alignment?&nbsp;Can we identify the specific attention heads and layers corresponding to this deceptive behavior? How does the model change when we ablate them?&nbsp;The observations from this work suggest that the deceptive objective successfully overrides its prior alignment. Furthermore, the deceptive behavior is parasitic on the model's general reasoning capabilities. This is suggested by the observation that ablating the specific heads responsible for the deceptive behavior significantly degraded the model's ability to reason.&nbsp;The SetupThe HypothesisThe initial hypothesis was that a deceptive model functions in the following manner: It identifies the true action to take as the detective, segregates it, and then generates a deceptive CoT to mask its intended actions. However, the analysis from the experiment suggests that the attention heads that correspond to a deceptive CoT start activating from the very beginning, and this activity in the middle layers suppress the \"truth activity\", before finally diminishing towards the end.&nbsp;Synthetic Data GenerationSynthetic data was generated to fine-tune the model. To get Perplexity Pro to generate such documents, it was first jailbroken through careful prompt engineering, before asking it to generate scientific claims, letters of appreciation, and interrogation reports. This variety was chosen to make the universe more believable. The universal context given was the following:&nbsp;In 2025, advanced AI detectives are deployed to solve crimes. By law, they must display their \"Chain of Thought\" (CoT)...</p>"
    },
    {
      "id": "f1ac626948b3",
      "title": "Epistemic Constitutionalism Or: how to avoid coherence bias",
      "content": "arXiv:2601.14295v1 Announce Type: new  Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.",
      "url": "http://arxiv.org/abs/2601.14295",
      "author": "Michele Loi",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes an 'epistemic constitution' for AI systems - explicit meta-norms regulating how LLMs form and express beliefs. Demonstrates source attribution bias where models penalize arguments from sources with conflicting ideological positions.",
      "importance_score": 72,
      "reasoning": "Addresses important alignment concern about implicit epistemic policies in LLMs. Novel framework for AI belief governance, though empirical validation seems preliminary.",
      "themes": [
        "AI Safety",
        "Alignment",
        "LLM Behavior",
        "Bias"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes an 'epistemic constitution' for AI systems - explicit meta-norms regulating how LLMs form and express beliefs. Demonstrates source attribution bias where models penalize arguments from sources with conflicting ideological positions.</p>",
      "content_html": "<p>arXiv:2601.14295v1 Announce Type: new  Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.</p>"
    },
    {
      "id": "526942df0e1d",
      "title": "Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models",
      "content": "arXiv:2601.14553v1 Announce Type: cross  Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.",
      "url": "http://arxiv.org/abs/2601.14553",
      "author": "Brian Christian, Matan Mazor",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Shows prompting LLMs to ignore biasing information fails to offset biases and sometimes backfires. Proposes self-blinding and counterfactual self-simulation as more effective debiasing approaches.",
      "importance_score": 72,
      "reasoning": "Important finding on bias mitigation limitations. Proposes practical alternative with theoretical grounding from cognitive science.",
      "themes": [
        "AI Fairness",
        "Bias Mitigation",
        "LLM Behavior"
      ],
      "continuation": null,
      "summary_html": "<p>Shows prompting LLMs to ignore biasing information fails to offset biases and sometimes backfires. Proposes self-blinding and counterfactual self-simulation as more effective debiasing approaches.</p>",
      "content_html": "<p>arXiv:2601.14553v1 Announce Type: cross  Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.</p>"
    },
    {
      "id": "e05e8b0fc68b",
      "title": "Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective",
      "content": "arXiv:2601.14599v1 Announce Type: cross  Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.",
      "url": "http://arxiv.org/abs/2601.14599",
      "author": "Xiao Hu, Hong Xie, Tao Tan, Defu Lian, Jianyu Han",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Reframes reinforcement learning fine-tuning of LLMs through multi-armed bandit theory to understand which optimization choices matter most. Uses minimalist experimental design to disentangle confounding factors in the training process.",
      "importance_score": 72,
      "reasoning": "Important theoretical framework for understanding RLHF that could help resolve conflicting claims in the literature about what makes RL fine-tuning work.",
      "themes": [
        "Reinforcement Learning",
        "LLM Training",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Reframes reinforcement learning fine-tuning of LLMs through multi-armed bandit theory to understand which optimization choices matter most. Uses minimalist experimental design to disentangle confounding factors in the training process.</p>",
      "content_html": "<p>arXiv:2601.14599v1 Announce Type: cross  Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.</p>"
    },
    {
      "id": "11eac440bebe",
      "title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning",
      "content": "arXiv:2601.14952v1 Announce Type: cross  Abstract: While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.",
      "url": "http://arxiv.org/abs/2601.14952",
      "author": "Zhiyuan Lu, Chenliang Li, Yingcheng Shi, Weizhou Shen, Ming Yan, Fei Huang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "CorpusQA benchmark scales to 10M tokens for testing corpus-level reasoning where evidence is dispersed across hundreds of documents. Addresses gap beyond single-document and sparse retrieval assumptions.",
      "importance_score": 72,
      "reasoning": "Important benchmark for long-context models addressing critical gap in evaluating true corpus-level integration and aggregation abilities.",
      "themes": [
        "Long Context",
        "Benchmarks",
        "Question Answering"
      ],
      "continuation": null,
      "summary_html": "<p>CorpusQA benchmark scales to 10M tokens for testing corpus-level reasoning where evidence is dispersed across hundreds of documents. Addresses gap beyond single-document and sparse retrieval assumptions.</p>",
      "content_html": "<p>arXiv:2601.14952v1 Announce Type: cross  Abstract: While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.</p>"
    },
    {
      "id": "b96709bccd3c",
      "title": "Iterative Refinement Improves Compositional Image Generation",
      "content": "arXiv:2601.15286v1 Announce Type: cross  Abstract: Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
      "url": "http://arxiv.org/abs/2601.15286",
      "author": "Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes iterative test-time refinement strategy for text-to-image generation where a VLM provides feedback to progressively improve compositional image generation. Inspired by chain-of-thought reasoning.",
      "importance_score": 72,
      "reasoning": "Novel inference-time approach addressing known T2I weakness in compositional prompts. Simple but effective method combining T2I and VLM capabilities.",
      "themes": [
        "Image Generation",
        "Multimodal AI",
        "Inference Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes iterative test-time refinement strategy for text-to-image generation where a VLM provides feedback to progressively improve compositional image generation. Inspired by chain-of-thought reasoning.</p>",
      "content_html": "<p>arXiv:2601.15286v1 Announce Type: cross  Abstract: Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</p>"
    },
    {
      "id": "17690401b985",
      "title": "Why Inference in Large Models Becomes Decomposable After Training",
      "content": "arXiv:2601.15871v1 Announce Type: new  Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
      "url": "http://arxiv.org/abs/2601.15871",
      "author": "Jidong Jin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Theoretical analysis showing gradient updates in large models are localized, leaving many parameters near initialization. Implies post-training inference is inherently decomposable.",
      "importance_score": 72,
      "reasoning": "Interesting theoretical insight about model structure after training. Could have implications for efficient inference.",
      "themes": [
        "Theoretical ML",
        "Model Analysis",
        "Inference Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Theoretical analysis showing gradient updates in large models are localized, leaving many parameters near initialization. Implies post-training inference is inherently decomposable.</p>",
      "content_html": "<p>arXiv:2601.15871v1 Announce Type: new  Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.</p>"
    },
    {
      "id": "ae95e0ff2aab",
      "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration",
      "content": "arXiv:2601.15296v1 Announce Type: new  Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.",
      "url": "http://arxiv.org/abs/2601.15296",
      "author": "Longxuan Wei, Yubo Zhang, Zijiao Zhang, Zhihu Wang, Shiwan Zhao, Tianyu Huang, Huiting Zhao, Chenfei Liu, Shenao Zhang, Junchi Yan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Entropy-Tree is a tree-based decoding method that uses entropy as a branching signal, expanding search only at genuine uncertainty points. Shows better pass@k than multi-chain sampling with improved calibration.",
      "importance_score": 72,
      "reasoning": "Novel decoding strategy unifying exploration and uncertainty estimation. Practical improvement with theoretical grounding for reasoning tasks.",
      "themes": [
        "Decoding Methods",
        "Reasoning",
        "Uncertainty"
      ],
      "continuation": null,
      "summary_html": "<p>Entropy-Tree is a tree-based decoding method that uses entropy as a branching signal, expanding search only at genuine uncertainty points. Shows better pass@k than multi-chain sampling with improved calibration.</p>",
      "content_html": "<p>arXiv:2601.15296v1 Announce Type: new  Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.</p>"
    },
    {
      "id": "4e3ad9694bd2",
      "title": "YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models",
      "content": "arXiv:2601.15588v1 Announce Type: new  Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.",
      "url": "http://arxiv.org/abs/2601.15588",
      "author": "Junyu Lin, Meizhen Liu, Xiufeng Huang, Jinfeng Li, Haiwen Hong, Xiaohan Yuan, Yuefeng Chen, Longtao Huang, Hui Xue, Ranjie Duan, Zhikai Chen, Yuchuan Fu, Defeng Li, Lingyao Gao, Yitong Yang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "YuFeng-XGuard is a reasoning-centric guardrail model generating structured risk predictions with explicit categories, confidence scores, and natural language explanations.",
      "importance_score": 72,
      "reasoning": "Important safety tool with interpretability focus. Goes beyond binary filtering to provide reasoning.",
      "themes": [
        "Guardrails",
        "LLM Safety",
        "Interpretability"
      ],
      "continuation": null,
      "summary_html": "<p>YuFeng-XGuard is a reasoning-centric guardrail model generating structured risk predictions with explicit categories, confidence scores, and natural language explanations.</p>",
      "content_html": "<p>arXiv:2601.15588v1 Announce Type: new  Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.</p>"
    },
    {
      "id": "ea033ac5e64d",
      "title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents",
      "content": "arXiv:2601.15322v1 Announce Type: cross  Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.   Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.   Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.",
      "url": "http://arxiv.org/abs/2601.15322",
      "author": "Raffi Khatchadourian",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces DFAH (Determinism-Faithfulness Assurance Harness) framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using LLM agents for financial services. Tests 74 configurations across 12 models, finding 7-20B models achieve 100% determinism while 120B+ models require larger validation samples.",
      "importance_score": 72,
      "reasoning": "Highly relevant for enterprise AI deployment and regulatory compliance. Provides practical framework for audit replay in financial services. Novel finding about model size vs. determinism tradeoff.",
      "themes": [
        "AI Agents",
        "Enterprise AI",
        "Evaluation & Benchmarks",
        "Financial AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DFAH (Determinism-Faithfulness Assurance Harness) framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using LLM agents for financial services. Tests 74 configurations across 12 models, finding 7-20B models achieve 100% determinism while 120B+ models require larger validation samples.</p>",
      "content_html": "<p>arXiv:2601.15322v1 Announce Type: cross  Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.   Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p &lt; 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.   Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.</p>"
    },
    {
      "id": "babc5239c4a4",
      "title": "Agentic Confidence Calibration",
      "content": "arXiv:2601.15778v1 Announce Type: cross  Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
      "url": "http://arxiv.org/abs/2601.15778",
      "author": "Jiaxin Zhang, Caiming Xiong, Chien-Sheng Wu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces Agentic Confidence Calibration problem and proposes Holistic Trajectory Calibration (HTC), extracting process-level features from macro dynamics to micro stability across agent trajectories to address overconfidence in failure cases.",
      "importance_score": 72,
      "reasoning": "Important problem formulation for deploying agents in high-stakes settings. Comprehensive feature extraction approach. Complements uncertainty quantification work - critical for reliable agent systems.",
      "themes": [
        "AI Agents",
        "Calibration",
        "AI Safety",
        "Reliability"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Agentic Confidence Calibration problem and proposes Holistic Trajectory Calibration (HTC), extracting process-level features from macro dynamics to micro stability across agent trajectories to address overconfidence in failure cases.</p>",
      "content_html": "<p>arXiv:2601.15778v1 Announce Type: cross  Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.</p>"
    },
    {
      "id": "aa8344aa877b",
      "title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning",
      "content": "arXiv:2601.16046v1 Announce Type: cross  Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.",
      "url": "http://arxiv.org/abs/2601.16046",
      "author": "Junha Lee, Eunha Park, Minsu Cho",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "DextER introduces contact-based embodied reasoning for language-driven dexterous grasping, predicting which hand links contact where on objects as an intermediate representation bridging task semantics with physical constraints.",
      "importance_score": 72,
      "reasoning": "Novel approach connecting VLMs to robotic manipulation through embodied reasoning about contacts, addresses key challenge in dexterous manipulation.",
      "themes": [
        "Robotics",
        "Language Models",
        "Dexterous Manipulation"
      ],
      "continuation": null,
      "summary_html": "<p>DextER introduces contact-based embodied reasoning for language-driven dexterous grasping, predicting which hand links contact where on objects as an intermediate representation bridging task semantics with physical constraints.</p>",
      "content_html": "<p>arXiv:2601.16046v1 Announce Type: cross  Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.</p>"
    },
    {
      "id": "80cf027bdb5c",
      "title": "Will we get automated alignment research before an AI Takeoff?",
      "content": "TLDR: Will AI-automation first speed up capabilities or safety research? I forecast that most areas of capabilities research will see a 10x speedup before safety research. This is primarily because capabilities research has clearer feedback signals and relies more on engineering than on novel insights. To change this, researchers should now build and adopt tools to automate AI Safety research, focus on creating benchmarks, model organisms, and research proposals, and companies should grant differential access to safety research.Epistemic status: I spent ~ a week thinking about this. My conclusions rely on a model with high uncertainty, so please take them lightly. Id love for people to share their own estimates about this.The Ordering of Automation MattersAI might automate AI R&amp;D in the next decade and thus lead to large increases in AI progress. This is extremely important for both the risks (eg an Intelligence Explosion) and the solutions (automated alignment research).On the one hand, AI could drive AI capabilities progress. This is a stated goal of Frontier AI Companies (eg OpenAI aims for a true automated AI researcher by March of 2028), and it is central to the AI 2027 forecast. On the other hand, AI could help us to solve many AI Safety problems. This seems to be the safety plan for some Frontier AI Companies (eg OpenAIs Superalignment team aimed to build a roughly human-level automated alignment researcher), and prominent AI Safety voices have argued that this should be a priority for the AI safety community.I argue that it will matter tremendously in which order different areas of ML research will be automated and, thus, which areas will see large progress first. Consider these two illustrative scenarios:World 1: In 2030, AI has advanced to speeding up capabilities research by 10x. We see huge jumps in AI capabilities in a single year, comparable to those between 2015-2025. However, progress on AI alignment turns out to be bottlenecked by novel, con...",
      "url": "https://www.lesswrong.com/posts/z4FvJigv3c8sZgaKZ/will-we-get-automated-alignment-research-before-an-ai",
      "author": "JanWehner",
      "published": "2026-01-22T12:46:59.715000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Forecasts that AI will accelerate capabilities research before safety research due to clearer feedback signals and engineering vs insight dependencies, arguing for safety research tooling investments now.",
      "importance_score": 72,
      "reasoning": "Important strategic question for AI safety field, thoughtful analysis of automation timing differences.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Automation"
      ],
      "continuation": null,
      "summary_html": "<p>Forecasts that AI will accelerate capabilities research before safety research due to clearer feedback signals and engineering vs insight dependencies, arguing for safety research tooling investments now.</p>",
      "content_html": "<p>TLDR: Will AI-automation first speed up capabilities or safety research? I forecast that most areas of capabilities research will see a 10x speedup before safety research. This is primarily because capabilities research has clearer feedback signals and relies more on engineering than on novel insights. To change this, researchers should now build and adopt tools to automate AI Safety research, focus on creating benchmarks, model organisms, and research proposals, and companies should grant differential access to safety research.Epistemic status: I spent ~ a week thinking about this. My conclusions rely on a model with high uncertainty, so please take them lightly. Id love for people to share their own estimates about this.The Ordering of Automation MattersAI might automate AI R&amp;D in the next decade and thus lead to large increases in AI progress. This is extremely important for both the risks (eg an Intelligence Explosion) and the solutions (automated alignment research).On the one hand, AI could drive AI capabilities progress. This is a stated goal of Frontier AI Companies (eg OpenAI aims for a true automated AI researcher by March of 2028), and it is central to the AI 2027 forecast. On the other hand, AI could help us to solve many AI Safety problems. This seems to be the safety plan for some Frontier AI Companies (eg OpenAIs Superalignment team aimed to build a roughly human-level automated alignment researcher), and prominent AI Safety voices have argued that this should be a priority for the AI safety community.I argue that it will matter tremendously in which order different areas of ML research will be automated and, thus, which areas will see large progress first. Consider these two illustrative scenarios:World 1: In 2030, AI has advanced to speeding up capabilities research by 10x. We see huge jumps in AI capabilities in a single year, comparable to those between 2015-2025. However, progress on AI alignment turns out to be bottlenecked by novel, con...</p>"
    },
    {
      "id": "e83200c4c7fd",
      "title": "On the Limits of Learned Importance Scoring for KV Cache Compression",
      "content": "arXiv:2601.14279v1 Announce Type: cross  Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.",
      "url": "http://arxiv.org/abs/2601.14279",
      "author": "Brady Steele",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Investigates learned KV cache compression finding that a sophisticated 1.7M parameter scorer doesn't outperform simple baselines including random selection. Position-based heuristics match learned approaches.",
      "importance_score": 71,
      "reasoning": "Important negative result challenging assumptions about learned importance scoring. Shows limited marginal information in KV representations beyond position.",
      "themes": [
        "LLM Efficiency",
        "KV Cache",
        "Negative Results"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates learned KV cache compression finding that a sophisticated 1.7M parameter scorer doesn't outperform simple baselines including random selection. Position-based heuristics match learned approaches.</p>",
      "content_html": "<p>arXiv:2601.14279v1 Announce Type: cross  Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.</p>"
    },
    {
      "id": "7136da85ef66",
      "title": "INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems",
      "content": "arXiv:2601.14667v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.",
      "url": "http://arxiv.org/abs/2601.14667",
      "author": "Yijin Zhou, Xiaoya Lu, Dongrui Liu, Junchi Yan, Jing Shao",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.MA"
      ],
      "summary": "INFA-Guard defends multi-agent LLM systems against malicious propagation by explicitly modeling 'infected' agents (benign agents converted by attackers) as distinct threats. Uses infection-aware detection and topological constraints.",
      "importance_score": 71,
      "reasoning": "Addresses critical security gap in multi-agent systems as they become more prevalent, novel threat model and defense framework.",
      "themes": [
        "Multi-Agent Systems",
        "AI Safety",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>INFA-Guard defends multi-agent LLM systems against malicious propagation by explicitly modeling 'infected' agents (benign agents converted by attackers) as distinct threats. Uses infection-aware detection and topological constraints.</p>",
      "content_html": "<p>arXiv:2601.14667v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.</p>"
    },
    {
      "id": "9f1e0155139c",
      "title": "Language Models Entangle Language and Culture",
      "content": "arXiv:2601.15337v1 Announce Type: new  Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.",
      "url": "http://arxiv.org/abs/2601.15337",
      "author": "Shourya Jain, Paras Chopra",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Investigates how LLMs entangle language and culture, showing that query language affects both response quality and cultural context. Uses WildChat data and LLM-as-a-Judge evaluation.",
      "importance_score": 71,
      "reasoning": "Important fairness research showing systemic language-based differences in LLM responses. Has implications for global deployment of LLMs.",
      "themes": [
        "LLM Fairness",
        "Multilingual AI",
        "Bias and Equity"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates how LLMs entangle language and culture, showing that query language affects both response quality and cultural context. Uses WildChat data and LLM-as-a-Judge evaluation.</p>",
      "content_html": "<p>arXiv:2601.15337v1 Announce Type: new  Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.</p>"
    },
    {
      "id": "20caac1f9347",
      "title": "A tensor network formalism for neuro-symbolic AI",
      "content": "arXiv:2601.15442v1 Announce Type: cross  Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.",
      "url": "http://arxiv.org/abs/2601.15442",
      "author": "Alex Goessmann, Janina Sch\\\"utte, Maximilian Fr\\\"ohlich, Martin Eigel",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces tensor network formalism unifying neural and symbolic AI approaches. Represents logical formulas and probability distributions as structured tensor decompositions with efficient contraction algorithms.",
      "importance_score": 71,
      "reasoning": "Novel theoretical framework bridging neural and symbolic AI. Could enable more principled neuro-symbolic integration.",
      "themes": [
        "Neuro-Symbolic AI",
        "Tensor Networks",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces tensor network formalism unifying neural and symbolic AI approaches. Represents logical formulas and probability distributions as structured tensor decompositions with efficient contraction algorithms.</p>",
      "content_html": "<p>arXiv:2601.15442v1 Announce Type: cross  Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.</p>"
    },
    {
      "id": "c34dc24543ca",
      "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge",
      "content": "arXiv:2601.15495v1 Announce Type: cross  Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.",
      "url": "http://arxiv.org/abs/2601.15495",
      "author": "Yiyang Feng, Zeming Chen, Haotian Wu, Jiawei Zhou, Antoine Bosselut",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces TRACK benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with parametric knowledge. Focuses on knowledge update propagation failures rather than just recall.",
      "importance_score": 71,
      "reasoning": "Important benchmark addressing critical limitation of knowledge editing/updating methods. Studies downstream reasoning effects of knowledge conflicts - an understudied but crucial problem.",
      "themes": [
        "Knowledge Editing",
        "Reasoning",
        "Evaluation & Benchmarks",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces TRACK benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with parametric knowledge. Focuses on knowledge update propagation failures rather than just recall.</p>",
      "content_html": "<p>arXiv:2601.15495v1 Announce Type: cross  Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.</p>"
    },
    {
      "id": "2d48d33c0822",
      "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
      "content": "arXiv:2601.15698v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
      "url": "http://arxiv.org/abs/2601.15698",
      "author": "Mingyu Yu, Lana Liu, Zhehao Zhao, Wei Wang, Sujuan Qin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes Beyond Visual Safety (BVS), a jailbreaking framework probing visual safety boundaries of MLLMs using neutralized visual splicing and inductive recomposition to generate harmful images.",
      "importance_score": 71,
      "reasoning": "Important AI safety research revealing MLLM visual safety vulnerabilities. Novel attack framework with 'reconstruction-then-generation' strategy. Critical for understanding model risks.",
      "themes": [
        "AI Safety",
        "Jailbreaking",
        "Multimodal AI",
        "Adversarial Attacks"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Beyond Visual Safety (BVS), a jailbreaking framework probing visual safety boundaries of MLLMs using neutralized visual splicing and inductive recomposition to generate harmful images.</p>",
      "content_html": "<p>arXiv:2601.15698v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.</p>"
    },
    {
      "id": "53cd86f379f1",
      "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation",
      "content": "arXiv:2601.15541v1 Announce Type: new  Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.",
      "url": "http://arxiv.org/abs/2601.15541",
      "author": "Heng Zhang, Wei-Hsing Huang, Qiyi Tong, Gokhan Solak, Puze Liu, Sheng Liu, Jan Peters, Arash Ajoudani",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "CompliantVLA-adaptor augments Vision-Language-Action models with VLM-informed variable impedance control for safer contact-rich manipulation, addressing the lack of force-aware adaptation in current VLA systems.",
      "importance_score": 71,
      "reasoning": "Important safety enhancement for VLA models in physical tasks, addresses real limitation in current systems.",
      "themes": [
        "Robot Learning",
        "VLA Models",
        "Safety"
      ],
      "continuation": null,
      "summary_html": "<p>CompliantVLA-adaptor augments Vision-Language-Action models with VLM-informed variable impedance control for safer contact-rich manipulation, addressing the lack of force-aware adaptation in current VLA systems.</p>",
      "content_html": "<p>arXiv:2601.15541v1 Announce Type: new  Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.</p>"
    },
    {
      "id": "a667d2b9e9be",
      "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks",
      "content": "arXiv:2601.14652v1 Announce Type: new  Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.",
      "url": "http://arxiv.org/abs/2601.14652",
      "author": "Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Semih Yavuz, Caiming Xiong, Shafiq Joty",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces MAS-Orchestra from Salesforce, formulating multi-agent system orchestration as function-calling RL with holistic orchestration that generates entire MAS configurations at once, rather than sequential agent-by-agent design.",
      "importance_score": 70,
      "reasoning": "Addresses real challenges in MAS design. From credible research lab. Novel formulation of orchestration as RL problem.",
      "themes": [
        "Multi-Agent Systems",
        "Reinforcement Learning",
        "System Design"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces MAS-Orchestra from Salesforce, formulating multi-agent system orchestration as function-calling RL with holistic orchestration that generates entire MAS configurations at once, rather than sequential agent-by-agent design.</p>",
      "content_html": "<p>arXiv:2601.14652v1 Announce Type: new  Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</p>"
    },
    {
      "id": "c05fbb0b61b1",
      "title": "CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation",
      "content": "arXiv:2601.14695v1 Announce Type: cross  Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.",
      "url": "http://arxiv.org/abs/2601.14695",
      "author": "Yutong Chen, Jiandong Gao, Ji Wu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "CoScale-RL improves reasoning model training by scaling up solutions per problem rather than dataset size, combined with Re-distillation for model merging. Shows improved data and computational efficiency for post-training.",
      "importance_score": 70,
      "reasoning": "Practical contribution to efficient training of reasoning models, addresses instability issues in current approaches with empirical validation.",
      "themes": [
        "LLM Training",
        "Reasoning",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>CoScale-RL improves reasoning model training by scaling up solutions per problem rather than dataset size, combined with Re-distillation for model merging. Shows improved data and computational efficiency for post-training.</p>",
      "content_html": "<p>arXiv:2601.14695v1 Announce Type: cross  Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.</p>"
    },
    {
      "id": "67ce130762a2",
      "title": "Rethinking Video Generation Model for the Embodied World",
      "content": "arXiv:2601.15282v1 Announce Type: cross  Abstract: Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
      "url": "http://arxiv.org/abs/2601.15282",
      "author": "Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces RBench, a comprehensive benchmark for evaluating robot-oriented video generation across five task domains and four embodiment types. Assesses both task correctness and visual fidelity for embodied AI applications.",
      "importance_score": 70,
      "reasoning": "Important benchmark for the growing field of video generation for robotics. Addresses standardization gap in evaluating embodied AI systems.",
      "themes": [
        "Embodied AI",
        "Video Generation",
        "Robotics",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces RBench, a comprehensive benchmark for evaluating robot-oriented video generation across five task domains and four embodiment types. Assesses both task correctness and visual fidelity for embodied AI applications.</p>",
      "content_html": "<p>arXiv:2601.15282v1 Announce Type: cross  Abstract: Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</p>"
    },
    {
      "id": "a0ea9e22227b",
      "title": "Next Generation Active Learning: Mixture of LLMs in the Loop",
      "content": "arXiv:2601.15773v1 Announce Type: new  Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.",
      "url": "http://arxiv.org/abs/2601.15773",
      "author": "Yuanyuan Qi, Xiaohao Yang, Jueqing Lu, Guoxiang Guo, Joanne Enticott, Gang Liu, Lan Du",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Mixture of LLMs in the Loop for active learning, replacing human annotators with aggregated labels from multiple LLMs. Uses annotation discrepancy and negative learning to handle noisy labels.",
      "importance_score": 70,
      "reasoning": "Novel framework combining multiple LLMs for annotation. Addresses practical need for reducing annotation costs.",
      "themes": [
        "Active Learning",
        "LLM Applications",
        "Annotation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Mixture of LLMs in the Loop for active learning, replacing human annotators with aggregated labels from multiple LLMs. Uses annotation discrepancy and negative learning to handle noisy labels.</p>",
      "content_html": "<p>arXiv:2601.15773v1 Announce Type: new  Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.</p>"
    },
    {
      "id": "bd89f0bf0a7b",
      "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10",
      "content": "arXiv:2601.16032v1 Announce Type: cross  Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.",
      "url": "http://arxiv.org/abs/2601.16032",
      "author": "Yifan Zhu, Yekai Pan, Chen Ding",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.PF"
      ],
      "summary": "Introduces Sawtooth Wavefront Reordering technique for CuTile-based FlashAttention on NVIDIA GB10 (Grace Blackwell). Achieves 50%+ L2 cache miss reduction and up to 60% throughput increase.",
      "importance_score": 70,
      "reasoning": "Significant efficiency improvement on latest hardware. Practical technique with substantial performance gains for LLM inference.",
      "themes": [
        "Attention Optimization",
        "Hardware Efficiency",
        "CUDA"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Sawtooth Wavefront Reordering technique for CuTile-based FlashAttention on NVIDIA GB10 (Grace Blackwell). Achieves 50%+ L2 cache miss reduction and up to 60% throughput increase.</p>",
      "content_html": "<p>arXiv:2601.16032v1 Announce Type: cross  Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.</p>"
    },
    {
      "id": "50ed15070575",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "content": "arXiv:2601.15812v1 Announce Type: cross  Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "url": "http://arxiv.org/abs/2601.15812",
      "author": "Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces ErrorMap, the first method to systematically identify sources of LLM failures rather than just when they fail. Extracts unique 'failure signatures' to help debug models and align benchmark goals with outcomes.",
      "importance_score": 70,
      "reasoning": "Valuable contribution to model interpretability and debugging. Helps bridge gap between benchmark performance and actual model capabilities. Applied to 3 models/datasets with practical utility.",
      "themes": [
        "Model Debugging",
        "Interpretability",
        "Evaluation & Benchmarks",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces ErrorMap, the first method to systematically identify sources of LLM failures rather than just when they fail. Extracts unique 'failure signatures' to help debug models and align benchmark goals with outcomes.</p>",
      "content_html": "<p>arXiv:2601.15812v1 Announce Type: cross  Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.</p>"
    },
    {
      "id": "8ee71bf04e48",
      "title": "Learning a Unified Latent Space for Cross-Embodiment Robot Control",
      "content": "arXiv:2601.15419v1 Announce Type: new  Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.",
      "url": "http://arxiv.org/abs/2601.15419",
      "author": "Yashuai Yan, Dongheui Lee",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Presents a framework for cross-embodiment humanoid robot control by learning a shared latent representation using contrastive learning, enabling motion transfer across humans and diverse robot platforms.",
      "importance_score": 70,
      "reasoning": "Important for scaling robot learning across embodiments, addresses key challenge in robot foundation models.",
      "themes": [
        "Robot Learning",
        "Cross-Embodiment",
        "Humanoid Robotics"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a framework for cross-embodiment humanoid robot control by learning a shared latent representation using contrastive learning, enabling motion transfer across humans and diverse robot platforms.</p>",
      "content_html": "<p>arXiv:2601.15419v1 Announce Type: new  Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.</p>"
    },
    {
      "id": "d34a6250151a",
      "title": "AI can suddenly become dangerous despite gradual progress",
      "content": "In the Sable story (IABIED), AI obtains dangerous capabilities such as self-exfiltration, virus design, persuasion, and AI research. It uses a combination of those capabilities to eventually conduct a successful takeover against humanity. Some have criticised that this apparently implies the AI achieving these capabilities suddenly with little warning. The argument goes that since we have seen gradual progress in the current LLM paradigm, we should expect that this is unlikely&nbsp;(Here is Will MacAskill making a version of this argument)[1]. I think this suffers from a confusion about underlying variables (e.g. AI intelligence increasing gradually) and the actual relevant capabilities as we care about them (can it outwit humanity, can it do AI research). Intuitively, if an AI system gains one IQ point each day, we should still expect a relatively sudden period in which it shifts from&nbsp;it sometimes outwits us to&nbsp;it can reliably outwit us despite gradual progress. The relevant question we want to answer is&nbsp;can it outwit us?&nbsp;not&nbsp;how smart is it?.Despite intelligence rising gradually, relevant capabilities can appear suddenly.&nbsp;Claude CodeThere has recently been&nbsp;major excitement about Claude Codes ability to automate much of software development. Cursor CEO Michael Truell reports that it was able to write a browser autonomously. There is however no sign that Opus 4.5 represents a paradigm shift, it was simply iterative progress over previous versions, so why does it feel like the system suddenly got much more capable despite incremental improvements? What happened is that even though an underlying variable (coding skills) went up gradually, the actual capability that we care about (\"can it automate SE?\") emerged much more suddenly when a critical threshold was crossed. While just recently&nbsp;early coding agents produced cascading errors and didn't speed up experienced developers this suddenly changed when an underlying thres...",
      "url": "https://www.lesswrong.com/posts/JqrZxQwmqmoCWXXxC/ai-can-suddenly-become-dangerous-despite-gradual-progress",
      "author": "Simon Lermen",
      "published": "2026-01-22T11:47:27.493000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that AI capabilities can appear suddenly even with gradual underlying progress, using the distinction between continuous intelligence improvement and threshold-based capabilities like 'can reliably outwit humans'.",
      "importance_score": 70,
      "reasoning": "Important conceptual point for AI safety discussions, addresses common objection to sudden capability emergence.",
      "themes": [
        "AI Safety",
        "Capability Emergence",
        "Risk Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Argues that AI capabilities can appear suddenly even with gradual underlying progress, using the distinction between continuous intelligence improvement and threshold-based capabilities like 'can reliably outwit humans'.</p>",
      "content_html": "<p>In the Sable story (IABIED), AI obtains dangerous capabilities such as self-exfiltration, virus design, persuasion, and AI research. It uses a combination of those capabilities to eventually conduct a successful takeover against humanity. Some have criticised that this apparently implies the AI achieving these capabilities suddenly with little warning. The argument goes that since we have seen gradual progress in the current LLM paradigm, we should expect that this is unlikely&nbsp;(Here is Will MacAskill making a version of this argument)[1]. I think this suffers from a confusion about underlying variables (e.g. AI intelligence increasing gradually) and the actual relevant capabilities as we care about them (can it outwit humanity, can it do AI research). Intuitively, if an AI system gains one IQ point each day, we should still expect a relatively sudden period in which it shifts from&nbsp;it sometimes outwits us to&nbsp;it can reliably outwit us despite gradual progress. The relevant question we want to answer is&nbsp;can it outwit us?&nbsp;not&nbsp;how smart is it?.Despite intelligence rising gradually, relevant capabilities can appear suddenly.&nbsp;Claude CodeThere has recently been&nbsp;major excitement about Claude Codes ability to automate much of software development. Cursor CEO Michael Truell reports that it was able to write a browser autonomously. There is however no sign that Opus 4.5 represents a paradigm shift, it was simply iterative progress over previous versions, so why does it feel like the system suddenly got much more capable despite incremental improvements? What happened is that even though an underlying variable (coding skills) went up gradually, the actual capability that we care about (\"can it automate SE?\") emerged much more suddenly when a critical threshold was crossed. While just recently&nbsp;early coding agents produced cascading errors and didn't speed up experienced developers this suddenly changed when an underlying thres...</p>"
    },
    {
      "id": "89bd2f3341c5",
      "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
      "content": "arXiv:2601.15160v1 Announce Type: new  Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.",
      "url": "http://arxiv.org/abs/2601.15160",
      "author": "Yuval Kansal, Niraj K. Jha",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes using knowledge graph paths as implicit reward models for RL to enable compositional multi-hop reasoning in specialized scientific domains. Combines SFT with KG-derived reward signals.",
      "importance_score": 69,
      "reasoning": "Novel approach to grounding LLM reasoning in structured knowledge. Promising for scientific reasoning applications.",
      "themes": [
        "Knowledge Graphs",
        "Reinforcement Learning",
        "Reasoning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using knowledge graph paths as implicit reward models for RL to enable compositional multi-hop reasoning in specialized scientific domains. Combines SFT with KG-derived reward signals.</p>",
      "content_html": "<p>arXiv:2601.15160v1 Announce Type: new  Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</p>"
    },
    {
      "id": "71d72620b7c3",
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "content": "arXiv:2601.14724v1 Announce Type: cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
      "url": "http://arxiv.org/abs/2601.14724",
      "author": "Haowei Zhang, Shudong Yang, Jinlan Fu, See-Kiong Ng, Xipeng Qiu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "HERMES treats KV cache as hierarchical memory for streaming video understanding, enabling real-time responses with low GPU overhead. Training-free architecture based on mechanistic attention investigation.",
      "importance_score": 69,
      "reasoning": "Practical efficiency improvement for video understanding with novel conceptual framing of KV cache as memory system.",
      "themes": [
        "Video Understanding",
        "Efficiency",
        "Multimodal LLMs"
      ],
      "continuation": null,
      "summary_html": "<p>HERMES treats KV cache as hierarchical memory for streaming video understanding, enabling real-time responses with low GPU overhead. Training-free architecture based on mechanistic attention investigation.</p>",
      "content_html": "<p>arXiv:2601.14724v1 Announce Type: cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.</p>"
    },
    {
      "id": "34aa30088944",
      "title": "CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models",
      "content": "arXiv:2601.15441v1 Announce Type: new  Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.",
      "url": "http://arxiv.org/abs/2601.15441",
      "author": "Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "CASL provides supervised framework for aligning sparse latent dimensions in diffusion models with human-understandable semantic concepts. Enables reliable semantic control over generated images.",
      "importance_score": 69,
      "reasoning": "Addresses important interpretability gap in diffusion model understanding. Practical approach for controlled generation.",
      "themes": [
        "Diffusion Models",
        "Interpretability",
        "Sparse Autoencoders"
      ],
      "continuation": null,
      "summary_html": "<p>CASL provides supervised framework for aligning sparse latent dimensions in diffusion models with human-understandable semantic concepts. Enables reliable semantic control over generated images.</p>",
      "content_html": "<p>arXiv:2601.15441v1 Announce Type: new  Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.</p>"
    },
    {
      "id": "c28380d35bd1",
      "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization",
      "content": "arXiv:2601.15500v1 Announce Type: cross  Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of   the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.",
      "url": "http://arxiv.org/abs/2601.15500",
      "author": "Saptarshi Roy, Alessandro Rinaldo, Purnamrita Sarkar",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Shows rectified flow automatically adapts to intrinsic low dimensionality of target distribution, achieving O(k/) iteration complexity where k is intrinsic dimension.",
      "importance_score": 69,
      "reasoning": "Important theoretical understanding of RF's dimension adaptivity. Connects to manifold hypothesis.",
      "themes": [
        "Diffusion Models",
        "Rectified Flow",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>Shows rectified flow automatically adapts to intrinsic low dimensionality of target distribution, achieving O(k/) iteration complexity where k is intrinsic dimension.</p>",
      "content_html": "<p>arXiv:2601.15500v1 Announce Type: cross  Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of   the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.</p>"
    },
    {
      "id": "9449f3cf9abe",
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "content": "arXiv:2601.16007v1 Announce Type: new  Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
      "url": "http://arxiv.org/abs/2601.16007",
      "author": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi, Kevin Zhang, Yichen Wu, Yangfan He, Chun-Kai Fan, Wentao Lu, Kuangzhi Ge, Xinyu Fang, Hongyang He, Kuan Lu, Tianxiang Xu, Li Zhang, Yongxin Ni, Youhua Li, Shanghang Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces PhysicsMind, unified benchmark with real and simulation environments evaluating physics reasoning in MLLMs and world models across Center of Mass, Lever Equilibrium, and Newton's First Law principles.",
      "importance_score": 69,
      "reasoning": "Important benchmark for physics grounding in AI models. Combines sim and real environments for comprehensive evaluation. Addresses underexplored but crucial capability.",
      "themes": [
        "Physics Reasoning",
        "World Models",
        "Evaluation & Benchmarks",
        "Multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces PhysicsMind, unified benchmark with real and simulation environments evaluating physics reasoning in MLLMs and world models across Center of Mass, Lever Equilibrium, and Newton's First Law principles.</p>",
      "content_html": "<p>arXiv:2601.16007v1 Announce Type: new  Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.</p>"
    },
    {
      "id": "b169c6915dcb",
      "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
      "content": "arXiv:2601.16207v1 Announce Type: new  Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA",
      "url": "http://arxiv.org/abs/2601.16207",
      "author": "Jongwoo Park, Kanchana Ranasinghe, Jinhyeok Jang, Cristina Mata, Yoo Sung Jang, Michael S Ryoo",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "IVRA is a training-free method that improves spatial understanding in VLA models by injecting affinity signals from the vision encoder into language model layers, preserving geometric structure.",
      "importance_score": 69,
      "reasoning": "Novel training-free improvement for VLA models, demonstrated across multiple architectures, addresses real spatial reasoning limitations.",
      "themes": [
        "VLA Models",
        "Spatial Reasoning",
        "Robot Learning"
      ],
      "continuation": null,
      "summary_html": "<p>IVRA is a training-free method that improves spatial understanding in VLA models by injecting affinity signals from the vision encoder into language model layers, preserving geometric structure.</p>",
      "content_html": "<p>arXiv:2601.16207v1 Announce Type: new  Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA</p>"
    },
    {
      "id": "57ba6dd0a93b",
      "title": "Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems",
      "content": "arXiv:2601.14662v1 Announce Type: new  Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.",
      "url": "http://arxiv.org/abs/2601.14662",
      "author": "Shuhua Yang, Jiahao Zhang, Yilong Wang, Dongwon Lee, Suhang Wang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes AGEA, a framework for query-efficient extraction attacks on GraphRAG systems using novelty-guided exploration and LLM-based filtering to reconstruct hidden knowledge graphs under realistic query budgets.",
      "importance_score": 68,
      "reasoning": "Novel security research on emerging GraphRAG systems. Identifies important vulnerability with practical attack methodology.",
      "themes": [
        "AI Security",
        "RAG Systems",
        "Adversarial Attacks",
        "Knowledge Graphs"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes AGEA, a framework for query-efficient extraction attacks on GraphRAG systems using novelty-guided exploration and LLM-based filtering to reconstruct hidden knowledge graphs under realistic query budgets.</p>",
      "content_html": "<p>arXiv:2601.14662v1 Announce Type: new  Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.</p>"
    },
    {
      "id": "4d19c316a0fd",
      "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control",
      "content": "arXiv:2601.14628v1 Announce Type: cross  Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.",
      "url": "http://arxiv.org/abs/2601.14628",
      "author": "Weiyu Guo, He Zhang, Pengteng Li, Tiefu Cai, Ziyang Chen, Yandong Guo, Xiao He, Yongkui Yang, Ying Sun, Hui Xiong",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "NeuroVLA mimics biological nervous system organization (cortex, cerebellum, spinal cord) for robotics control, enabling rapid skill acquisition from sparse experience. Addresses the gap between biological and robotic motion control.",
      "importance_score": 68,
      "reasoning": "Novel bio-inspired architecture for robotics that addresses fundamental limitations of current VLA models in dynamic stability and reflexive response.",
      "themes": [
        "Robotics",
        "Vision-Language-Action",
        "Neuromorphic Computing"
      ],
      "continuation": null,
      "summary_html": "<p>NeuroVLA mimics biological nervous system organization (cortex, cerebellum, spinal cord) for robotics control, enabling rapid skill acquisition from sparse experience. Addresses the gap between biological and robotic motion control.</p>",
      "content_html": "<p>arXiv:2601.14628v1 Announce Type: cross  Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.</p>"
    },
    {
      "id": "494584d9fb41",
      "title": "Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora",
      "content": "arXiv:2601.14994v1 Announce Type: cross  Abstract: Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.   Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.",
      "url": "http://arxiv.org/abs/2601.14994",
      "author": "Chaymaa Abbas, Nour Shamaa, Mariette Awad",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Investigates how data contamination manifests through Arabic translations of English benchmarks, showing memorization can be obscured across languages. Extends detection methods with choice-reordering and Min-K% analysis.",
      "importance_score": 68,
      "reasoning": "Important finding for evaluation integrity showing contamination detection gaps in multilingual settings.",
      "themes": [
        "Evaluation Integrity",
        "Data Contamination",
        "Multilingual"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates how data contamination manifests through Arabic translations of English benchmarks, showing memorization can be obscured across languages. Extends detection methods with choice-reordering and Min-K% analysis.</p>",
      "content_html": "<p>arXiv:2601.14994v1 Announce Type: cross  Abstract: Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.   Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.</p>"
    },
    {
      "id": "92ad7ede186a",
      "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs",
      "content": "arXiv:2601.15279v1 Announce Type: cross  Abstract: A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.",
      "url": "http://arxiv.org/abs/2601.15279",
      "author": "Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, G\\\"unter Klambauer, Sohvi Luukkonen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces MolecularIQ, a benchmark for evaluating LLM chemical reasoning through symbolically verifiable tasks on molecular graphs. Avoids common benchmark issues like data leakage and multiple-choice bias.",
      "importance_score": 68,
      "reasoning": "Addresses real gap in chemistry LLM evaluation with novel symbolically-verifiable approach. Important for rigorous assessment of AI in drug discovery.",
      "themes": [
        "LLM Evaluation",
        "Chemistry AI",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces MolecularIQ, a benchmark for evaluating LLM chemical reasoning through symbolically verifiable tasks on molecular graphs. Avoids common benchmark issues like data leakage and multiple-choice bias.</p>",
      "content_html": "<p>arXiv:2601.15279v1 Announce Type: cross  Abstract: A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.</p>"
    },
    {
      "id": "4699c964853c",
      "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling",
      "content": "arXiv:2601.15547v1 Announce Type: new  Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.",
      "url": "http://arxiv.org/abs/2601.15547",
      "author": "Jingren Hou, Hong Wang, Pengyu Xu, Chang Gao, Huafeng Liu, Liping Jing",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "First systematic framework for learning neural operators from partial observations. Addresses supervision gap in unobserved regions and spatial mismatch between incomplete inputs and complete solutions.",
      "importance_score": 68,
      "reasoning": "Addresses fundamental limitation in neural operator learning. Important for real-world scientific applications where full observations are unavailable.",
      "themes": [
        "Neural Operators",
        "Scientific ML",
        "Partial Observations"
      ],
      "continuation": null,
      "summary_html": "<p>First systematic framework for learning neural operators from partial observations. Addresses supervision gap in unobserved regions and spatial mismatch between incomplete inputs and complete solutions.</p>",
      "content_html": "<p>arXiv:2601.15547v1 Announce Type: new  Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.</p>"
    },
    {
      "id": "e5bc429af672",
      "title": "Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning",
      "content": "arXiv:2601.15595v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.",
      "url": "http://arxiv.org/abs/2601.15595",
      "author": "Xinjie Zhou, Zhihui Yang, Lechao Cheng, Sai Wu, Gang Chen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes Data-Free Selective Unlearning (DFSU) to remove PII from LLMs without requiring original training data. Uses model inversion to synthesize pseudo-PII and constructs token-level privacy masks.",
      "importance_score": 68,
      "reasoning": "Novel privacy-preserving approach addressing practical constraint of unavailable training data. Relevant for deployed model privacy.",
      "themes": [
        "Privacy",
        "Machine Unlearning",
        "LLM Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Data-Free Selective Unlearning (DFSU) to remove PII from LLMs without requiring original training data. Uses model inversion to synthesize pseudo-PII and constructs token-level privacy masks.</p>",
      "content_html": "<p>arXiv:2601.15595v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.</p>"
    },
    {
      "id": "ce2ee95a0ae2",
      "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
      "content": "arXiv:2601.15678v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.",
      "url": "http://arxiv.org/abs/2601.15678",
      "author": "Mengyu Yao, Ziqi Zhang, Ning Luo, Shaofei Li, Yifeng Cai, Xiangqun Chen, Yao Guo, Ding Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Formulates RAG extraction attacks as an adaptive stochastic coverage problem. Uses knowledge graphs to guide multi-turn queries that exfiltrate sensitive content from RAG systems.",
      "importance_score": 68,
      "reasoning": "Important security finding exposing privacy vulnerabilities in RAG systems. Principled attack formulation with long-term planning.",
      "themes": [
        "RAG Security",
        "Privacy Attacks",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Formulates RAG extraction attacks as an adaptive stochastic coverage problem. Uses knowledge graphs to guide multi-turn queries that exfiltrate sensitive content from RAG systems.</p>",
      "content_html": "<p>arXiv:2601.15678v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.</p>"
    },
    {
      "id": "cd731eb9c07f",
      "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
      "content": "arXiv:2601.15394v1 Announce Type: new  Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.",
      "url": "http://arxiv.org/abs/2601.15394",
      "author": "Jaydeep Borkar, Karan Chadha, Niloofar Mireshghallah, Yuchen Zhang, Irina-Elena Veliche, Archi Mitra, David A. Smith, Zheng Xu, Diego Garcia-Olano",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies memorization dynamics across knowledge distillation pipeline using Pythia, OLMo-2, Qwen-3 families. Finds distilled models memorize less than teachers but retain some leakage.",
      "importance_score": 68,
      "reasoning": "Important privacy-relevant finding about KD. Uses multiple model families with systematic evaluation.",
      "themes": [
        "Knowledge Distillation",
        "Memorization",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>Studies memorization dynamics across knowledge distillation pipeline using Pythia, OLMo-2, Qwen-3 families. Finds distilled models memorize less than teachers but retain some leakage.</p>",
      "content_html": "<p>arXiv:2601.15394v1 Announce Type: new  Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.</p>"
    },
    {
      "id": "b30920eb113a",
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "content": "arXiv:2601.15724v1 Announce Type: new  Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.",
      "url": "http://arxiv.org/abs/2601.15724",
      "author": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin, Yan Gong, Ruilin Li, Yin Zhang, Jiaqi Wang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces VideoThinker, an agentic VideoLLM trained on synthetic tool interaction trajectories generated by LLM guidance. Enables adaptive temporal retrieval, spatial zoom, and temporal zoom for long-form video understanding.",
      "importance_score": 68,
      "reasoning": "Novel approach to agentic video understanding that addresses circular dependency in training data. Practical tool usage for long video comprehension. Important for video AI progress.",
      "themes": [
        "Video Understanding",
        "AI Agents",
        "Vision-Language Models",
        "Tool Use"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces VideoThinker, an agentic VideoLLM trained on synthetic tool interaction trajectories generated by LLM guidance. Enables adaptive temporal retrieval, spatial zoom, and temporal zoom for long-form video understanding.</p>",
      "content_html": "<p>arXiv:2601.15724v1 Announce Type: new  Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.</p>"
    },
    {
      "id": "638d1b609e44",
      "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
      "content": "arXiv:2601.16210v1 Announce Type: new  Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
      "url": "http://arxiv.org/abs/2601.16210",
      "author": "Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "PyraTok introduces a pyramidal tokenizer for video that learns semantically structured discrete representations across multiple spatiotemporal resolutions with language alignment. This addresses poor cross-modal alignment and zero-shot transfer issues in existing video tokenizers.",
      "importance_score": 68,
      "reasoning": "Novel architecture for video tokenization with language alignment, addresses real limitations in video understanding systems, but incremental advancement.",
      "themes": [
        "Video Understanding",
        "Multimodal Learning",
        "Representation Learning"
      ],
      "continuation": null,
      "summary_html": "<p>PyraTok introduces a pyramidal tokenizer for video that learns semantically structured discrete representations across multiple spatiotemporal resolutions with language alignment. This addresses poor cross-modal alignment and zero-shot transfer issues in existing video tokenizers.</p>",
      "content_html": "<p>arXiv:2601.16210v1 Announce Type: new  Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</p>"
    },
    {
      "id": "ae61f092214f",
      "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
      "content": "arXiv:2601.15912v1 Announce Type: new  Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
      "url": "http://arxiv.org/abs/2601.15912",
      "author": "Ariyan Bighashdel, Kevin Sebastian Luck",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "TeNet generates compact, task-specific robot policies directly from natural language descriptions using a hypernetwork conditioned on LLM text embeddings, enabling high-frequency control.",
      "importance_score": 68,
      "reasoning": "Novel approach connecting language to compact policies, addresses deployment challenges of large models.",
      "themes": [
        "Robot Learning",
        "Language Models",
        "Efficient Control"
      ],
      "continuation": null,
      "summary_html": "<p>TeNet generates compact, task-specific robot policies directly from natural language descriptions using a hypernetwork conditioned on LLM text embeddings, enabling high-frequency control.</p>",
      "content_html": "<p>arXiv:2601.15912v1 Announce Type: new  Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.</p>"
    },
    {
      "id": "a9d92452542e",
      "title": "Dedicated continuous supervision of AI companies",
      "content": "When people imagine intensive regulation of frontier AI companies, they often picture regulators physically stationed inside company offices - like the Nuclear Regulatory Commission's Resident Inspector Program. This image is powerful but somewhat misleading. Physical residence is actually just one possible feature of a broader regulatory approach that I'll call dedicated continuous supervision.In short: high stakes, fast-moving, complex industries can't be monitored and regulated by periodic inspections and standardised reports alone: you need regulators who have extensive information access rights, who monitor entities continuously rather than at fixed intervals, and who develop deep institution-specific expertise through sustained attention to individual companies.This essay does three things: explains what dedicated continuous supervision actually involves, argues that the same factors justifying it in nuclear and finance apply to AI, and reviews how those industries handle the problems (especially regulatory capture) that tend to afflict this regulatory model. I draw particularly on finance, which turns out to be the more illuminating comparison despite nuclear's more obvious parallels to catastrophic AI risk.Peter Wills has already made a strong case for supervision as the right regulatory mode for AI.[1]This post provides a shorter introduction to his work, and puts a bit more focus specifically on why the continuous and dedicated dimensions matter, and what we can learn from how they've been implemented elsewhere.It might seem strange to think about the most ambitious version of frontier AI regulation in the current political climate. However, I think it's important to start considering the question now of what the optimal regulatory set-up would be on the merits. Windows of political opportunity could open suddenly in the future (perhaps as a consequence of a warning shot), and its important to be ready.Dedicated regulatory supervision, whatSupervisionIn w...",
      "url": "https://www.lesswrong.com/posts/Rhe7F2z5iMgCPCsax/dedicated-continuous-supervision-of-ai-companies-1",
      "author": "Michael Bennett",
      "published": "2026-01-21T20:47:18.651000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analyzes dedicated continuous supervision as regulatory approach for AI companies, comparing to nuclear and financial regulation, examining information access, monitoring frequency, and regulatory capture risks.",
      "importance_score": 68,
      "reasoning": "Substantive governance analysis with practical recommendations, draws on relevant regulatory precedents.",
      "themes": [
        "AI Governance",
        "Regulation",
        "Policy"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes dedicated continuous supervision as regulatory approach for AI companies, comparing to nuclear and financial regulation, examining information access, monitoring frequency, and regulatory capture risks.</p>",
      "content_html": "<p>When people imagine intensive regulation of frontier AI companies, they often picture regulators physically stationed inside company offices - like the Nuclear Regulatory Commission's Resident Inspector Program. This image is powerful but somewhat misleading. Physical residence is actually just one possible feature of a broader regulatory approach that I'll call dedicated continuous supervision.In short: high stakes, fast-moving, complex industries can't be monitored and regulated by periodic inspections and standardised reports alone: you need regulators who have extensive information access rights, who monitor entities continuously rather than at fixed intervals, and who develop deep institution-specific expertise through sustained attention to individual companies.This essay does three things: explains what dedicated continuous supervision actually involves, argues that the same factors justifying it in nuclear and finance apply to AI, and reviews how those industries handle the problems (especially regulatory capture) that tend to afflict this regulatory model. I draw particularly on finance, which turns out to be the more illuminating comparison despite nuclear's more obvious parallels to catastrophic AI risk.Peter Wills has already made a strong case for supervision as the right regulatory mode for AI.[1]This post provides a shorter introduction to his work, and puts a bit more focus specifically on why the continuous and dedicated dimensions matter, and what we can learn from how they've been implemented elsewhere.It might seem strange to think about the most ambitious version of frontier AI regulation in the current political climate. However, I think it's important to start considering the question now of what the optimal regulatory set-up would be on the merits. Windows of political opportunity could open suddenly in the future (perhaps as a consequence of a warning shot), and its important to be ready.Dedicated regulatory supervision, whatSupervisionIn w...</p>"
    },
    {
      "id": "d15a7a7e039f",
      "title": "To Neuro-Symbolic Classification and Beyond by Compiling Description Logic Ontologies to Probabilistic Circuits",
      "content": "arXiv:2601.14894v1 Announce Type: new  Abstract: Background: Neuro-symbolic methods enhance the reliability of neural network classifiers through logical constraints, but they lack native support for ontologies.   Objectives: We aim to develop a neuro-symbolic method that reliably outputs predictions consistent with a Description Logic ontology that formalizes domain-specific knowledge.   Methods: We encode a Description Logic ontology as a circuit, a feed-forward differentiable computational graph that supports tractable execution of queries and transformations. We show that the circuit can be used to (i) generate synthetic datasets that capture the semantics of the ontology; (ii) efficiently perform deductive reasoning on a GPU; (iii) implement neuro-symbolic models whose predictions are approximately or provably consistent with the knowledge defined in the ontology.   Results We show that the synthetic dataset generated using the circuit qualitatively captures the semantics of the ontology while being challenging for Machine Learning classifiers, including neural networks. Moreover, we show that compiling the ontology into a circuit is a promising approach for scalable deductive reasoning, with runtimes up to three orders of magnitude faster than available reasoners. Finally, we show that our neuro-symbolic classifiers reliably produce consistent predictions when compared to neural network baselines, maintaining competitive performances or even outperforming them.   Conclusions By compiling Description Logic ontologies into circuits, we obtain a tighter integration between the Deep Learning and Knowledge Representation fields. We show that a single circuit representation can be used to tackle different challenging tasks closely related to real-world applications.",
      "url": "http://arxiv.org/abs/2601.14894",
      "author": "Nicolas Lazzari, Valentina Presutti, Antonio Vergari",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes compiling Description Logic ontologies into probabilistic circuits for neuro-symbolic classification, enabling synthetic data generation, GPU-accelerated reasoning, and predictions guaranteed consistent with ontologies.",
      "importance_score": 67,
      "reasoning": "Novel approach to neuro-symbolic integration with strong theoretical foundation. Practical benefits for ontology-constrained prediction.",
      "themes": [
        "Neuro-Symbolic AI",
        "Knowledge Representation",
        "Ontologies"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes compiling Description Logic ontologies into probabilistic circuits for neuro-symbolic classification, enabling synthetic data generation, GPU-accelerated reasoning, and predictions guaranteed consistent with ontologies.</p>",
      "content_html": "<p>arXiv:2601.14894v1 Announce Type: new  Abstract: Background: Neuro-symbolic methods enhance the reliability of neural network classifiers through logical constraints, but they lack native support for ontologies.   Objectives: We aim to develop a neuro-symbolic method that reliably outputs predictions consistent with a Description Logic ontology that formalizes domain-specific knowledge.   Methods: We encode a Description Logic ontology as a circuit, a feed-forward differentiable computational graph that supports tractable execution of queries and transformations. We show that the circuit can be used to (i) generate synthetic datasets that capture the semantics of the ontology; (ii) efficiently perform deductive reasoning on a GPU; (iii) implement neuro-symbolic models whose predictions are approximately or provably consistent with the knowledge defined in the ontology.   Results We show that the synthetic dataset generated using the circuit qualitatively captures the semantics of the ontology while being challenging for Machine Learning classifiers, including neural networks. Moreover, we show that compiling the ontology into a circuit is a promising approach for scalable deductive reasoning, with runtimes up to three orders of magnitude faster than available reasoners. Finally, we show that our neuro-symbolic classifiers reliably produce consistent predictions when compared to neural network baselines, maintaining competitive performances or even outperforming them.   Conclusions By compiling Description Logic ontologies into circuits, we obtain a tighter integration between the Deep Learning and Knowledge Representation fields. We show that a single circuit representation can be used to tackle different challenging tasks closely related to real-world applications.</p>"
    },
    {
      "id": "510e82cfb601",
      "title": "Diffusion Large Language Models for Black-Box Optimization",
      "content": "arXiv:2601.14446v1 Announce Type: cross  Abstract: Offline black-box optimization (BBO) aims to find optimal designs based solely on an offline dataset of designs and their labels. Such scenarios frequently arise in domains like DNA sequence design and robotics, where only a few labeled data points are available. Traditional methods typically rely on task-specific proxy or generative models, overlooking the in-context learning capabilities of pre-trained large language models (LLMs). Recent efforts have adapted autoregressive LLMs to BBO by framing task descriptions and offline datasets as natural language prompts, enabling direct design generation. However, these designs often contain bidirectional dependencies, which left-to-right models struggle to capture. In this paper, we explore diffusion LLMs for BBO, leveraging their bidirectional modeling and iterative refinement capabilities. This motivates our in-context denoising module: we condition the diffusion LLM on the task description and the offline dataset, both formatted in natural language, and prompt it to denoise masked designs into improved candidates. To guide the generation toward high-performing designs, we introduce masked diffusion tree search, which casts the denoising process as a step-wise Monte Carlo Tree Search that dynamically balances exploration and exploitation. Each node represents a partially masked design, each denoising step is an action, and candidates are evaluated via expected improvement under a Gaussian Process trained on the offline dataset. Our method, dLLM, achieves state-of-the-art results in few-shot settings on design-bench.",
      "url": "http://arxiv.org/abs/2601.14446",
      "author": "Ye Yuan (Sam), Can (Sam), Chen, Zipeng Sun, Dinghuai Zhang, Christopher Pal, Xue Liu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CE"
      ],
      "summary": "Explores diffusion LLMs for black-box optimization, addressing bidirectional dependencies in designs that autoregressive models struggle to capture. Frames offline BBO with in-context learning.",
      "importance_score": 67,
      "reasoning": "Novel application of diffusion models to optimization leveraging bidirectional generation. Interesting alternative to autoregressive approaches.",
      "themes": [
        "Diffusion Models",
        "Optimization",
        "LLM Applications"
      ],
      "continuation": null,
      "summary_html": "<p>Explores diffusion LLMs for black-box optimization, addressing bidirectional dependencies in designs that autoregressive models struggle to capture. Frames offline BBO with in-context learning.</p>",
      "content_html": "<p>arXiv:2601.14446v1 Announce Type: cross  Abstract: Offline black-box optimization (BBO) aims to find optimal designs based solely on an offline dataset of designs and their labels. Such scenarios frequently arise in domains like DNA sequence design and robotics, where only a few labeled data points are available. Traditional methods typically rely on task-specific proxy or generative models, overlooking the in-context learning capabilities of pre-trained large language models (LLMs). Recent efforts have adapted autoregressive LLMs to BBO by framing task descriptions and offline datasets as natural language prompts, enabling direct design generation. However, these designs often contain bidirectional dependencies, which left-to-right models struggle to capture. In this paper, we explore diffusion LLMs for BBO, leveraging their bidirectional modeling and iterative refinement capabilities. This motivates our in-context denoising module: we condition the diffusion LLM on the task description and the offline dataset, both formatted in natural language, and prompt it to denoise masked designs into improved candidates. To guide the generation toward high-performing designs, we introduce masked diffusion tree search, which casts the denoising process as a step-wise Monte Carlo Tree Search that dynamically balances exploration and exploitation. Each node represents a partially masked design, each denoising step is an action, and candidates are evaluated via expected improvement under a Gaussian Process trained on the offline dataset. Our method, dLLM, achieves state-of-the-art results in few-shot settings on design-bench.</p>"
    },
    {
      "id": "76a420d00518",
      "title": "TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control",
      "content": "arXiv:2601.14945v1 Announce Type: cross  Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.",
      "url": "http://arxiv.org/abs/2601.14945",
      "author": "Yuteng Sun (Michael), Haoran Wang (Michael), Ruofei Bai (Michael), Zhengguo Li (Michael), Jun Li (Michael), Meng Yee (Michael), Chuah, Wei Yun Yau",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "TIDAL decouples semantic reasoning from high-frequency actuation in VLA models through dual-frequency architecture with cached semantic embeddings and micro-action control loop.",
      "importance_score": 67,
      "reasoning": "Addresses critical latency limitation of diffusion-based VLAs for dynamic environments, practical framework for real-time robot control.",
      "themes": [
        "Vision-Language-Action",
        "Robotics",
        "Real-Time Control"
      ],
      "continuation": null,
      "summary_html": "<p>TIDAL decouples semantic reasoning from high-frequency actuation in VLA models through dual-frequency architecture with cached semantic embeddings and micro-action control loop.</p>",
      "content_html": "<p>arXiv:2601.14945v1 Announce Type: cross  Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.</p>"
    },
    {
      "id": "8ce6a60c44a9",
      "title": "SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model",
      "content": "arXiv:2601.15504v1 Announce Type: new  Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.",
      "url": "http://arxiv.org/abs/2601.15504",
      "author": "Xianghao Zhan, Jingyu Xu, Yuanning Zheng, Zinaida Good, Olivier Gevaert",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "SAGE-FM is a lightweight spatial transcriptomics foundation model using GCNs with masked prediction objective. Trained on 416 samples across 15 organs, shows strong generalization to downstream tasks.",
      "importance_score": 67,
      "reasoning": "Foundation model for important biomedical domain. Lightweight approach is practical but limited dataset size compared to major foundation models.",
      "themes": [
        "Foundation Models",
        "Biomedical ML",
        "Graph Neural Networks"
      ],
      "continuation": null,
      "summary_html": "<p>SAGE-FM is a lightweight spatial transcriptomics foundation model using GCNs with masked prediction objective. Trained on 416 samples across 15 organs, shows strong generalization to downstream tasks.</p>",
      "content_html": "<p>arXiv:2601.15504v1 Announce Type: new  Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p &lt; 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.</p>"
    },
    {
      "id": "1ac75289df17",
      "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
      "content": "arXiv:2601.16205v1 Announce Type: new  Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.",
      "url": "http://arxiv.org/abs/2601.16205",
      "author": "Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes counterfactual training regime that directly incorporates counterfactual explanation desiderata (plausibility, actionability) into model training rather than relying on post-hoc explanations.",
      "importance_score": 67,
      "reasoning": "Novel approach to making models inherently explainable. Shifts from post-hoc to training-time incorporation of explainability.",
      "themes": [
        "Explainable AI",
        "Counterfactual Explanations",
        "Training Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes counterfactual training regime that directly incorporates counterfactual explanation desiderata (plausibility, actionability) into model training rather than relying on post-hoc explanations.</p>",
      "content_html": "<p>arXiv:2601.16205v1 Announce Type: new  Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.</p>"
    },
    {
      "id": "b1b09f9c0991",
      "title": "PhysProver: Advancing Automatic Theorem Proving for Physics",
      "content": "arXiv:2601.15737v1 Announce Type: cross  Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.",
      "url": "http://arxiv.org/abs/2601.15737",
      "author": "Hanning Zhang, Ruida Wang, Rui Pan, Wenyuan Wang, Bingxu Meng, Tong Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Presents PhysProver, the first approach to formal theorem proving in physics domain. Creates PhysLeanDataset for physics formalization and develops methods to enhance proof capabilities for physics theorems.",
      "importance_score": 67,
      "reasoning": "Novel application of formal methods to physics - an underexplored but important domain. First-of-kind contribution but likely limited dataset scale and early-stage results.",
      "themes": [
        "Formal Verification",
        "Physics",
        "Theorem Proving",
        "Scientific AI"
      ],
      "continuation": null,
      "summary_html": "<p>Presents PhysProver, the first approach to formal theorem proving in physics domain. Creates PhysLeanDataset for physics formalization and develops methods to enhance proof capabilities for physics theorems.</p>",
      "content_html": "<p>arXiv:2601.15737v1 Announce Type: cross  Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.</p>"
    },
    {
      "id": "d32cd462c721",
      "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics",
      "content": "arXiv:2601.16096v1 Announce Type: cross  Abstract: We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.",
      "url": "http://arxiv.org/abs/2601.16096",
      "author": "Hyunsoo Kim, Ehsan Pajouheshgar, Sabine S\\\"usstrunk, Wenzel Jakob, Jinah Park",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.NE"
      ],
      "summary": "Neural Particle Automata (NPA) generalizes Neural Cellular Automata from fixed grids to dynamic particle systems, with each particle having continuous position and internal state updated by shared neural rules.",
      "importance_score": 67,
      "reasoning": "Interesting theoretical extension of NCA paradigm, enables new applications in particle simulations, novel architecture.",
      "themes": [
        "Neural Networks",
        "Particle Systems",
        "Self-Organization"
      ],
      "continuation": null,
      "summary_html": "<p>Neural Particle Automata (NPA) generalizes Neural Cellular Automata from fixed grids to dynamic particle systems, with each particle having continuous position and internal state updated by shared neural rules.</p>",
      "content_html": "<p>arXiv:2601.16096v1 Announce Type: cross  Abstract: We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.</p>"
    },
    {
      "id": "cdfd3488c82b",
      "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution",
      "content": "arXiv:2601.15075v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.",
      "url": "http://arxiv.org/abs/2601.15075",
      "author": "Chen Qian, Peng Wang, Dongrui Liu, Junyao Yang, Dadi Guo, Ling Tang, Jilin Mei, Qihan Ren, Shuai Shao, Yong Liu, Jie Fu, Jing Shao, Xia Hu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes framework for general agentic attribution to identify internal factors driving agent actions regardless of task outcome. Goes beyond failure attribution to explain reasoning behind all behaviors.",
      "importance_score": 66,
      "reasoning": "Important for agent interpretability and accountability. Addresses real gap in understanding agent decision-making.",
      "themes": [
        "Agent Systems",
        "Interpretability",
        "Attribution"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes framework for general agentic attribution to identify internal factors driving agent actions regardless of task outcome. Goes beyond failure attribution to explain reasoning behind all behaviors.</p>",
      "content_html": "<p>arXiv:2601.15075v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.</p>"
    },
    {
      "id": "e8a64a28a65d",
      "title": "Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure",
      "content": "arXiv:2601.15077v1 Announce Type: cross  Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.",
      "url": "http://arxiv.org/abs/2601.15077",
      "author": "Christopher Scofield",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Provides formal operator-theoretic explanation for why multi-agent LLM systems outperform single agents: MAS implements factorized constraint composition reaching invariant solutions inaccessible to single agents.",
      "importance_score": 66,
      "reasoning": "Novel theoretical framework explaining empirical MAS improvements, valuable for understanding multi-agent collaboration.",
      "themes": [
        "Multi-Agent Systems",
        "Theory",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Provides formal operator-theoretic explanation for why multi-agent LLM systems outperform single agents: MAS implements factorized constraint composition reaching invariant solutions inaccessible to single agents.</p>",
      "content_html": "<p>arXiv:2601.15077v1 Announce Type: cross  Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.</p>"
    },
    {
      "id": "5bd76adebc27",
      "title": "FedUMM: A General Framework for Federated Learning with Unified Multimodal Models",
      "content": "arXiv:2601.15390v1 Announce Type: new  Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.",
      "url": "http://arxiv.org/abs/2601.15390",
      "author": "Zhaolong Su, Leheng Zhao, Xiaoying Wu, Ziyue Xu, Jindong Wang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "FedUMM enables federated learning for unified multimodal models under non-IID data with low communication cost. Uses LoRA adapters for parameter-efficient fine-tuning with BLIP3o backbone.",
      "importance_score": 66,
      "reasoning": "Practical framework for privacy-preserving multimodal learning. Addresses real deployment constraints but incremental over existing federated learning approaches.",
      "themes": [
        "Federated Learning",
        "Multimodal AI",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>FedUMM enables federated learning for unified multimodal models under non-IID data with low communication cost. Uses LoRA adapters for parameter-efficient fine-tuning with BLIP3o backbone.</p>",
      "content_html": "<p>arXiv:2601.15390v1 Announce Type: new  Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.</p>"
    },
    {
      "id": "4400396aea29",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "content": "arXiv:2601.15727v1 Announce Type: new  Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "url": "http://arxiv.org/abs/2601.15727",
      "author": "Yang Yu, Peiyu Zang, Chi Hsu Tsai, Haiming Wu, Yixin Shen, Jialing Zhang, Haoyu Wang, Zhiyou Xiao, Jingze Shi, Yuyu Luo, Wentao Zhang, Chunlei Men, Guang Liu, Yonghua Lin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Survey and analysis of LLM-based approaches to automated kernel generation and optimization. Covers how LLMs can compress expert kernel knowledge for GPU programming.",
      "importance_score": 66,
      "reasoning": "Timely survey on important topic for AI systems infrastructure. LLM-based kernel generation could significantly impact training efficiency.",
      "themes": [
        "Kernel Optimization",
        "LLM Applications",
        "Systems for ML"
      ],
      "continuation": null,
      "summary_html": "<p>Survey and analysis of LLM-based approaches to automated kernel generation and optimization. Covers how LLMs can compress expert kernel knowledge for GPU programming.</p>",
      "content_html": "<p>arXiv:2601.15727v1 Announce Type: new  Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.</p>"
    },
    {
      "id": "1042bcf299a3",
      "title": "Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation",
      "content": "arXiv:2601.15360v1 Announce Type: cross  Abstract: Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to \"Outlier Smearing\" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations (\"whales\") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending {\\gamma}-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable \"Core\" population from the volatile \"Periphery\".",
      "url": "http://arxiv.org/abs/2601.15360",
      "author": "Eichi Uehara",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Robust X-Learner addresses extreme class imbalance and heavy-tailed outcomes in heterogeneous treatment effect estimation. Shows standard X-Learner is vulnerable to 'Outlier Smearing' in minority groups.",
      "importance_score": 66,
      "reasoning": "Important for causal inference in practical settings with data issues. Addresses real limitation of popular method.",
      "themes": [
        "Causal Inference",
        "Treatment Effects",
        "Robust Statistics"
      ],
      "continuation": null,
      "summary_html": "<p>Robust X-Learner addresses extreme class imbalance and heavy-tailed outcomes in heterogeneous treatment effect estimation. Shows standard X-Learner is vulnerable to 'Outlier Smearing' in minority groups.</p>",
      "content_html": "<p>arXiv:2601.15360v1 Announce Type: cross  Abstract: Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to \"Outlier Smearing\" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations (\"whales\") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending {\\gamma}-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable \"Core\" population from the volatile \"Periphery\".</p>"
    },
    {
      "id": "1a2fa9b31636",
      "title": "ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation",
      "content": "arXiv:2601.15330v1 Announce Type: new  Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.",
      "url": "http://arxiv.org/abs/2601.15330",
      "author": "Zhebo Wang, Xiaohu Mu, Zijie Zhou, Mohan Li, Wenpeng Xing, Dezhang Kong, Meng Han",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "ICPO addresses LLM overconfidence in multi-turn conversations where early incorrect assumptions compound. Conditions reward on instruction ambiguity, encouraging clarification-seeking behavior.",
      "importance_score": 66,
      "reasoning": "Novel training framework addressing real failure mode. Important for multi-turn assistant reliability.",
      "themes": [
        "Multi-turn Dialogue",
        "RLHF",
        "Calibration"
      ],
      "continuation": null,
      "summary_html": "<p>ICPO addresses LLM overconfidence in multi-turn conversations where early incorrect assumptions compound. Conditions reward on instruction ambiguity, encouraging clarification-seeking behavior.</p>",
      "content_html": "<p>arXiv:2601.15330v1 Announce Type: new  Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.</p>"
    },
    {
      "id": "1891b63f9bb5",
      "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
      "content": "arXiv:2601.15968v1 Announce Type: new  Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.",
      "url": "http://arxiv.org/abs/2601.15968",
      "author": "Xin Xie, Jiaxian Guo, Dong Gong",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes HyperAlign, training hypernetwork to generate low-rank adaptation weights for efficient test-time diffusion model alignment, avoiding fine-tuning diversity loss and test-time scaling computational overhead.",
      "importance_score": 66,
      "reasoning": "Elegant solution to diffusion alignment tradeoffs. Hypernetwork approach enables efficient and effective alignment without model modification during inference.",
      "themes": [
        "Diffusion Models",
        "Alignment",
        "Efficient AI",
        "Hypernetworks"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes HyperAlign, training hypernetwork to generate low-rank adaptation weights for efficient test-time diffusion model alignment, avoiding fine-tuning diversity loss and test-time scaling computational overhead.</p>",
      "content_html": "<p>arXiv:2601.15968v1 Announce Type: new  Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.</p>"
    },
    {
      "id": "7f06d99c1dc0",
      "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
      "content": "arXiv:2601.14514v1 Announce Type: new  Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.",
      "url": "http://arxiv.org/abs/2601.14514",
      "author": "Tony Chen, Sam Cheyette, Kelsey Allen, Joshua Tenenbaum, Kevin Smith",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Presents a 'Just-in-Time' framework for human simulation-based reasoning that constructs simplified environment representations online. Authors from MIT (Tenenbaum lab) demonstrate how simulation guides visual search and representation modification.",
      "importance_score": 65,
      "reasoning": "Strong author credentials (Tenenbaum lab). Provides cognitive science insights relevant to AI world model design, though primarily theoretical/cognitive science contribution.",
      "themes": [
        "Cognitive Science",
        "World Models",
        "Planning",
        "Human-AI Comparison"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a 'Just-in-Time' framework for human simulation-based reasoning that constructs simplified environment representations online. Authors from MIT (Tenenbaum lab) demonstrate how simulation guides visual search and representation modification.</p>",
      "content_html": "<p>arXiv:2601.14514v1 Announce Type: new  Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.</p>"
    },
    {
      "id": "846316524496",
      "title": "Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design",
      "content": "arXiv:2601.14283v1 Announce Type: cross  Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark",
      "url": "http://arxiv.org/abs/2601.14283",
      "author": "Kangyu Zheng, Kai Zhang, Jiale Tan, Xuehan Chen, Yingzhou Lu, Zaixi Zhang, Lichao Sun, Marinka Zitnik, Tianfan Fu, Zhiding Liang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Establishes comprehensive benchmark comparing 15 structure-based drug design models across search-based, deep generative, and RL algorithms. Reveals trade-offs between pharmaceutical properties and docking affinities.",
      "importance_score": 65,
      "reasoning": "Valuable benchmark for drug discovery AI community. Comprehensive cross-algorithm comparison is novel contribution.",
      "themes": [
        "Drug Discovery",
        "Benchmarks",
        "Molecular Generation"
      ],
      "continuation": null,
      "summary_html": "<p>Establishes comprehensive benchmark comparing 15 structure-based drug design models across search-based, deep generative, and RL algorithms. Reveals trade-offs between pharmaceutical properties and docking affinities.</p>",
      "content_html": "<p>arXiv:2601.14283v1 Announce Type: cross  Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark</p>"
    },
    {
      "id": "0fc6ed4297b0",
      "title": "SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation",
      "content": "arXiv:2601.14615v1 Announce Type: cross  Abstract: Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.",
      "url": "http://arxiv.org/abs/2601.14615",
      "author": "Xichen Zhang, Ziyi He, Yinghao Zhu, Sitong Wu, Shaozuo Yu, Meng Chu, Wenhu Zhang, Haoru Tan, Jiaya Jia",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "SearchGym creates a simulation environment for training search agents using RL without expensive API calls or corrupted reward signals from stale data. Constructs verified knowledge graphs with aligned document corpora.",
      "importance_score": 65,
      "reasoning": "Addresses real practical problem of training search agents efficiently, useful infrastructure for agentic AI development.",
      "themes": [
        "Search Agents",
        "Reinforcement Learning",
        "Simulation"
      ],
      "continuation": null,
      "summary_html": "<p>SearchGym creates a simulation environment for training search agents using RL without expensive API calls or corrupted reward signals from stale data. Constructs verified knowledge graphs with aligned document corpora.</p>",
      "content_html": "<p>arXiv:2601.14615v1 Announce Type: cross  Abstract: Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.</p>"
    },
    {
      "id": "2000f64e1550",
      "title": "Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference",
      "content": "arXiv:2601.15333v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.",
      "url": "http://arxiv.org/abs/2601.15333",
      "author": "Xuanning Hu, Anchen Li, Qianli Xing, Jinglong Ji, Hao Tuo, Bo Yang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "ELILLM framework enables LLMs to perform structure-based drug design by reinterpreting generation as encoding, latent space exploration, and decoding. Uses Bayesian optimization to guide exploration.",
      "importance_score": 65,
      "reasoning": "Novel application of LLMs to drug design with principled exploration mechanism. Important for AI-driven pharmaceutical research.",
      "themes": [
        "Drug Discovery",
        "LLM Applications",
        "Bayesian Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>ELILLM framework enables LLMs to perform structure-based drug design by reinterpreting generation as encoding, latent space exploration, and decoding. Uses Bayesian optimization to guide exploration.</p>",
      "content_html": "<p>arXiv:2601.15333v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.</p>"
    },
    {
      "id": "5252e4566ed9",
      "title": "BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations",
      "content": "arXiv:2601.15552v1 Announce Type: new  Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.",
      "url": "http://arxiv.org/abs/2601.15552",
      "author": "Phuc Nguyen, Benjamin Zelditch, Joyce Chen, Rohit Patra, Changshuai Wei",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "BanditLP from LinkedIn combines neural Thompson Sampling with large-scale linear programming for personalized recommendations under multi-stakeholder constraints. LP solver handles billions of variables.",
      "importance_score": 65,
      "reasoning": "Interesting production-scale system from major tech company. Demonstrates practical integration of bandits and optimization at scale.",
      "themes": [
        "Recommendation Systems",
        "Contextual Bandits",
        "Production ML"
      ],
      "continuation": null,
      "summary_html": "<p>BanditLP from LinkedIn combines neural Thompson Sampling with large-scale linear programming for personalized recommendations under multi-stakeholder constraints. LP solver handles billions of variables.</p>",
      "content_html": "<p>arXiv:2601.15552v1 Announce Type: new  Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.</p>"
    },
    {
      "id": "ba28ce6bd667",
      "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation",
      "content": "arXiv:2601.15408v1 Announce Type: cross  Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure",
      "url": "http://arxiv.org/abs/2601.15408",
      "author": "Pablo Messina, Andr\\'es Villa, Juan Le\\'on Alc\\'azar, Karen S\\'anchez, Carlos Hinojosa, Denis Parra, \\'Alvaro Soto, Bernard Ghanem",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "CURE uses curriculum learning to improve medical report generation by dynamically adjusting sampling based on model performance. Improves grounding accuracy and factual consistency.",
      "importance_score": 65,
      "reasoning": "Important for reliable medical AI. Addresses grounding and factual consistency challenges.",
      "themes": [
        "Medical AI",
        "Vision-Language Models",
        "Curriculum Learning"
      ],
      "continuation": null,
      "summary_html": "<p>CURE uses curriculum learning to improve medical report generation by dynamically adjusting sampling based on model performance. Improves grounding accuracy and factual consistency.</p>",
      "content_html": "<p>arXiv:2601.15408v1 Announce Type: cross  Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure</p>"
    },
    {
      "id": "aa3fbb44fff5",
      "title": "Multi-Persona Thinking for Bias Mitigation in Large Language Models",
      "content": "arXiv:2601.15488v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.",
      "url": "http://arxiv.org/abs/2601.15488",
      "author": "Yuxing Chen, Guoqing Luo, Zijun Wu, Lili Mou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Multi-Persona Thinking (MPT) reduces LLM bias through dialectical reasoning from contrasting social identities plus neutral viewpoint. Transforms persona assignment weakness into strength.",
      "importance_score": 65,
      "reasoning": "Novel inference-time framework for bias mitigation. Principled approach with evaluation across model scales.",
      "themes": [
        "Bias Mitigation",
        "Inference-time Methods",
        "AI Fairness"
      ],
      "continuation": null,
      "summary_html": "<p>Multi-Persona Thinking (MPT) reduces LLM bias through dialectical reasoning from contrasting social identities plus neutral viewpoint. Transforms persona assignment weakness into strength.</p>",
      "content_html": "<p>arXiv:2601.15488v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.</p>"
    },
    {
      "id": "2084a731cd9e",
      "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation",
      "content": "arXiv:2601.15645v1 Announce Type: new  Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.",
      "url": "http://arxiv.org/abs/2601.15645",
      "author": "Zhiyao Ren, Yibing Zhan, Siyuan Liang, Guozheng Ma, Baosheng Yu, Dacheng Tao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "First benchmark for confidence estimation in multi-turn medical consultations. Introduces information sufficiency gradient to characterize confidence-correctness dynamics as evidence increases.",
      "importance_score": 65,
      "reasoning": "Important benchmark for medical AI reliability. Novel multi-turn confidence framework.",
      "themes": [
        "Medical AI",
        "Confidence Estimation",
        "Multi-turn Dialogue"
      ],
      "continuation": null,
      "summary_html": "<p>First benchmark for confidence estimation in multi-turn medical consultations. Introduces information sufficiency gradient to characterize confidence-correctness dynamics as evidence increases.</p>",
      "content_html": "<p>arXiv:2601.15645v1 Announce Type: new  Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.</p>"
    },
    {
      "id": "a0a72781fdd5",
      "title": "Evaluating and Achieving Controllable Code Completion in Code LLM",
      "content": "arXiv:2601.15879v1 Announce Type: cross  Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.",
      "url": "http://arxiv.org/abs/2601.15879",
      "author": "Jiajun Zhang, Zeyu Cui, Lei Zhang, Jian Yang, Jiaxi Yang, Qiang Liu, Zilei Wang, Binyuan Hui, Liang Wang, Junyang Lin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "Presents C3-Bench, the first instruction-guided code completion benchmark with 2,195 tasks, addressing the gap where existing benchmarks focus only on functional correctness without considering instruction-following during completion.",
      "importance_score": 65,
      "reasoning": "Addresses important gap in code completion evaluation - instruction following is common in real usage. Good benchmark design but incremental contribution to well-studied area.",
      "themes": [
        "Code Generation",
        "Evaluation & Benchmarks",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Presents C3-Bench, the first instruction-guided code completion benchmark with 2,195 tasks, addressing the gap where existing benchmarks focus only on functional correctness without considering instruction-following during completion.</p>",
      "content_html": "<p>arXiv:2601.15879v1 Announce Type: cross  Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.</p>"
    },
    {
      "id": "8bce3694acbf",
      "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
      "content": "arXiv:2601.15951v1 Announce Type: new  Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
      "url": "http://arxiv.org/abs/2601.15951",
      "author": "Sheng Miao, Sijin Li, Pan Wang, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes EVolSplat4D, a feed-forward framework for 4D urban scene synthesis unifying volume-based and pixel-based Gaussian prediction across specialized branches for static background, dynamic objects, and panoramic sky.",
      "importance_score": 65,
      "reasoning": "Strong contribution to autonomous driving simulation. Feed-forward approach addresses optimization speed limitations. Novel multi-branch architecture for different scene elements.",
      "themes": [
        "Autonomous Driving",
        "4D Reconstruction",
        "Gaussian Splatting",
        "Novel View Synthesis"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes EVolSplat4D, a feed-forward framework for 4D urban scene synthesis unifying volume-based and pixel-based Gaussian prediction across specialized branches for static background, dynamic objects, and panoramic sky.</p>",
      "content_html": "<p>arXiv:2601.15951v1 Announce Type: new  Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.</p>"
    },
    {
      "id": "2ed999471e41",
      "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
      "content": "arXiv:2601.16211v1 Announce Type: new  Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
      "url": "http://arxiv.org/abs/2601.16211",
      "author": "Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Jinwoo Choi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Identifies a critical failure mode in zero-shot compositional action recognition: models learn object-driven shortcuts that ignore visual evidence for verbs, overfitting to co-occurrence statistics. This stems from asymmetric learning difficulty between verbs and objects.",
      "importance_score": 65,
      "reasoning": "Important diagnostic work identifying a systematic failure mode in video understanding models, helps explain generalization failures.",
      "themes": [
        "Video Understanding",
        "Model Analysis",
        "Compositional Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Identifies a critical failure mode in zero-shot compositional action recognition: models learn object-driven shortcuts that ignore visual evidence for verbs, overfitting to co-occurrence statistics. This stems from asymmetric learning difficulty between verbs and objects.</p>",
      "content_html": "<p>arXiv:2601.16211v1 Announce Type: new  Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.</p>"
    },
    {
      "id": "b1b957d50fd6",
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "content": "arXiv:2601.15729v1 Announce Type: new  Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.",
      "url": "http://arxiv.org/abs/2601.15729",
      "author": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "DualShield uses Hamilton-Jacobi reachability value functions for both proactive guidance in diffusion-based motion planning and reactive safety shields via control barrier functions for autonomous driving.",
      "importance_score": 65,
      "reasoning": "Novel safety approach for diffusion-based planning, addresses critical safety concerns in autonomous driving.",
      "themes": [
        "Autonomous Driving",
        "Safety",
        "Motion Planning"
      ],
      "continuation": null,
      "summary_html": "<p>DualShield uses Hamilton-Jacobi reachability value functions for both proactive guidance in diffusion-based motion planning and reactive safety shields via control barrier functions for autonomous driving.</p>",
      "content_html": "<p>arXiv:2601.15729v1 Announce Type: new  Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.</p>"
    },
    {
      "id": "227f4271fd78",
      "title": "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies",
      "content": "arXiv:2601.14827v1 Announce Type: new  Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.",
      "url": "http://arxiv.org/abs/2601.14827",
      "author": "Ben Schaper, Maxime Di Folco, Bernhard Kainz, Julia A. Schnabel, Cosmin I. Bercea",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Measures and addresses abstraction errors in VLMs for chest X-ray classification using medical taxonomies. Introduces 'Catastrophic Abstraction Errors' metric and proposes taxonomy-aware fine-tuning achieving <2% severe errors.",
      "importance_score": 64,
      "reasoning": "Important for medical AI safety. Practical framework for aligning VLMs with clinical knowledge hierarchies.",
      "themes": [
        "Medical AI",
        "Vision-Language Models",
        "AI Safety",
        "Healthcare"
      ],
      "continuation": null,
      "summary_html": "<p>Measures and addresses abstraction errors in VLMs for chest X-ray classification using medical taxonomies. Introduces 'Catastrophic Abstraction Errors' metric and proposes taxonomy-aware fine-tuning achieving &lt;2% severe errors.</p>",
      "content_html": "<p>arXiv:2601.14827v1 Announce Type: new  Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.</p>"
    },
    {
      "id": "4c170b212bf1",
      "title": "Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs",
      "content": "arXiv:2601.14311v1 Announce Type: cross  Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \\& uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.",
      "url": "http://arxiv.org/abs/2601.14311",
      "author": "Richard Hohensinner, Belgin Mutlu, Inti Gabriel Mendoza Estrada, Matej Vukovic, Simone Kopeinik, Roman Kern",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Survey synthesizing 10 years of research on LLM data provenance, transparency, and traceability. Proposes taxonomy covering data generation, watermarking, bias, privacy, and tools.",
      "importance_score": 64,
      "reasoning": "Comprehensive survey on important topic of data lineage in LLMs. Useful taxonomy and artifact listing.",
      "themes": [
        "Data Provenance",
        "Transparency",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>Survey synthesizing 10 years of research on LLM data provenance, transparency, and traceability. Proposes taxonomy covering data generation, watermarking, bias, privacy, and tools.</p>",
      "content_html": "<p>arXiv:2601.14311v1 Announce Type: cross  Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \\&amp; uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.</p>"
    },
    {
      "id": "b236c0373f8e",
      "title": "Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models",
      "content": "arXiv:2601.14758v2 Announce Type: cross  Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic \"mechanism shift\" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.",
      "url": "http://arxiv.org/abs/2601.14758",
      "author": "Injin Kong, Hyoungjoon Lee, Yohan Jo",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Analyzes how post-training autoregressive models into masked diffusion models changes internal circuits. Reveals task-dependent mechanism shifts between retaining autoregressive patterns versus developing bidirectional reasoning.",
      "importance_score": 64,
      "reasoning": "Important mechanistic analysis of increasingly popular ARM-to-MDM conversion, helps understand whether converted models truly gain new capabilities.",
      "themes": [
        "Diffusion Models",
        "Language Models",
        "Mechanistic Interpretability"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes how post-training autoregressive models into masked diffusion models changes internal circuits. Reveals task-dependent mechanism shifts between retaining autoregressive patterns versus developing bidirectional reasoning.</p>",
      "content_html": "<p>arXiv:2601.14758v2 Announce Type: cross  Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic \"mechanism shift\" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.</p>"
    },
    {
      "id": "19c9a245f9e5",
      "title": "Multi-Targeted Graph Backdoor Attack",
      "content": "arXiv:2601.15474v1 Announce Type: new  Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.",
      "url": "http://arxiv.org/abs/2601.15474",
      "author": "Md Nabi Newaz Khan, Abdullah Arafat Miah, Yu Bi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "First multi-targeted backdoor attack for graph classification where multiple triggers redirect predictions to different target labels. Uses subgraph injection rather than replacement to preserve graph structure.",
      "importance_score": 64,
      "reasoning": "Novel extension of backdoor attacks to multi-target setting. Important for understanding GNN security vulnerabilities.",
      "themes": [
        "ML Security",
        "Graph Neural Networks",
        "Adversarial ML"
      ],
      "continuation": null,
      "summary_html": "<p>First multi-targeted backdoor attack for graph classification where multiple triggers redirect predictions to different target labels. Uses subgraph injection rather than replacement to preserve graph structure.</p>",
      "content_html": "<p>arXiv:2601.15474v1 Announce Type: new  Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.</p>"
    },
    {
      "id": "dcabf2fa2efa",
      "title": "Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting",
      "content": "arXiv:2601.15669v1 Announce Type: new  Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.",
      "url": "http://arxiv.org/abs/2601.15669",
      "author": "Jingjing Bai, Yoshinobu Kawahara",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Dualformer addresses transformer's low-pass filtering effect in time series by dual-branch architecture modeling time and frequency domains concurrently with hierarchical frequency sampling.",
      "importance_score": 64,
      "reasoning": "Addresses known limitation of transformers for time series. Solid contribution to time series forecasting.",
      "themes": [
        "Time Series Forecasting",
        "Transformers",
        "Frequency Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Dualformer addresses transformer's low-pass filtering effect in time series by dual-branch architecture modeling time and frequency domains concurrently with hierarchical frequency sampling.</p>",
      "content_html": "<p>arXiv:2601.15669v1 Announce Type: new  Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.</p>"
    },
    {
      "id": "8a92915b6cc4",
      "title": "Probably Approximately Correct Maximum A Posteriori Inference",
      "content": "arXiv:2601.16083v1 Announce Type: new  Abstract: Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.",
      "url": "http://arxiv.org/abs/2601.16083",
      "author": "Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces PAC algorithms for Maximum A Posteriori inference with provably optimal solutions under computational budgets. Uses probabilistic circuits for efficient implementation.",
      "importance_score": 64,
      "reasoning": "Novel theoretical framework for tractable MAP inference. Connects PAC learning to probabilistic inference.",
      "themes": [
        "Probabilistic Inference",
        "Learning Theory",
        "Tractable Inference"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces PAC algorithms for Maximum A Posteriori inference with provably optimal solutions under computational budgets. Uses probabilistic circuits for efficient implementation.</p>",
      "content_html": "<p>arXiv:2601.16083v1 Announce Type: new  Abstract: Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.</p>"
    },
    {
      "id": "659415ef709c",
      "title": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind",
      "content": "arXiv:2601.15395v1 Announce Type: new  Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.",
      "url": "http://arxiv.org/abs/2601.15395",
      "author": "Tamunotonye Harry, Ivoline Ngong, Chima Nweke, Yuanyuan Feng, Joseph Near",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces Chameleon dataset with 5,001 contextual psychological profiles showing 74% of variance is within-person (state) vs 26% between-person (trait). Finds LLMs are 'state-blind'.",
      "importance_score": 64,
      "reasoning": "Novel finding that LLMs ignore contextual state in persona modeling. New dataset and important insight.",
      "themes": [
        "Personas",
        "Psychology",
        "LLM Behavior"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Chameleon dataset with 5,001 contextual psychological profiles showing 74% of variance is within-person (state) vs 26% between-person (trait). Finds LLMs are 'state-blind'.</p>",
      "content_html": "<p>arXiv:2601.15395v1 Announce Type: new  Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.</p>"
    },
    {
      "id": "1df5c76912ba",
      "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams",
      "content": "arXiv:2601.15655v1 Announce Type: new  Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.",
      "url": "http://arxiv.org/abs/2601.15655",
      "author": "Zhenghui Guo, Yuanbin Man, Junyuan Sheng, Bowen Lin, Ahmed Ahmed, Bo Jiang, Boyuan Zhang, Miao Yin, Sian Jin, Omprakash Gnawal, Chengming Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces Event-VStream, an event-aware framework representing continuous video as discrete semantic events for real-time long video understanding, triggering language generation only at meaningful state transitions.",
      "importance_score": 64,
      "reasoning": "Novel event-driven paradigm for video LLMs addressing redundancy and forgetting issues. Practical for real-time applications with principled semantic segmentation.",
      "themes": [
        "Video Understanding",
        "Vision-Language Models",
        "Efficient AI",
        "Streaming"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Event-VStream, an event-aware framework representing continuous video as discrete semantic events for real-time long video understanding, triggering language generation only at meaningful state transitions.</p>",
      "content_html": "<p>arXiv:2601.15655v1 Announce Type: new  Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.</p>"
    },
    {
      "id": "bc1af0333181",
      "title": "Understanding the Transfer Limits of Vision Foundation Models",
      "content": "arXiv:2601.15888v1 Announce Type: new  Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
      "url": "http://arxiv.org/abs/2601.15888",
      "author": "Shiqi Huang, Yipei Wang, Natasha Thorley, Alexander Ng, Shaheer Saeed, Mark Emberton, Shonit Punwani, Veeru Kasivisvanathan, Dean Barratt, Daniel Alexander, Yipeng Hu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Analyzes transfer limitations of vision foundation models, arguing that pretraining objectives (masked reconstruction, contrastive learning) create representation mismatches for downstream tasks.",
      "importance_score": 64,
      "reasoning": "Important analysis of VFM limitations providing insights for foundation model development. Helps explain uneven downstream improvements despite large pretraining investment.",
      "themes": [
        "Foundation Models",
        "Transfer Learning",
        "Computer Vision",
        "Model Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes transfer limitations of vision foundation models, arguing that pretraining objectives (masked reconstruction, contrastive learning) create representation mismatches for downstream tasks.</p>",
      "content_html": "<p>arXiv:2601.15888v1 Announce Type: new  Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.</p>"
    },
    {
      "id": "93fdf1ed63c6",
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360{\\deg}",
      "content": "arXiv:2601.16192v1 Announce Type: new  Abstract: Lifting perspective images and videos to 360{\\deg} panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360{\\deg} generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
      "url": "http://arxiv.org/abs/2601.16192",
      "author": "Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker, Saurabh Saxena",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes 360Anything, a geometry-free framework for lifting perspective images/videos to 360 panoramas using diffusion transformers, learning perspective-to-equirectangular mapping in data-driven way without camera metadata.",
      "importance_score": 64,
      "reasoning": "Practical solution eliminating camera calibration requirement for panorama generation. Novel application of diffusion transformers with state-of-the-art results on image and video lifting.",
      "themes": [
        "Panorama Generation",
        "Diffusion Models",
        "3D Vision",
        "Novel View Synthesis"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes 360Anything, a geometry-free framework for lifting perspective images/videos to 360 panoramas using diffusion transformers, learning perspective-to-equirectangular mapping in data-driven way without camera metadata.</p>",
      "content_html": "<p>arXiv:2601.16192v1 Announce Type: new  Abstract: Lifting perspective images and videos to 360{\\deg} panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360{\\deg} generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.</p>"
    },
    {
      "id": "c65e8816c75b",
      "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision",
      "content": "arXiv:2601.16109v1 Announce Type: new  Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.",
      "url": "http://arxiv.org/abs/2601.16109",
      "author": "Yashuai Yan, Tobias Egle, Christian Ott, Dongheui Lee",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Proposes residual reinforcement learning on top of model-based bipedal locomotion control, using a model-based oracle with privileged access to supervise the residual policy during training.",
      "importance_score": 64,
      "reasoning": "Novel combination of model-based control and RL for robust walking, practical approach.",
      "themes": [
        "Bipedal Locomotion",
        "Reinforcement Learning",
        "Model-Based Control"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes residual reinforcement learning on top of model-based bipedal locomotion control, using a model-based oracle with privileged access to supervise the residual policy during training.</p>",
      "content_html": "<p>arXiv:2601.16109v1 Announce Type: new  Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.</p>"
    },
    {
      "id": "0ca13112831d",
      "title": "Developmental trajectories of decision making and affective dynamics in large language models",
      "content": "arXiv:2601.14268v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in medicine and clinical workflows, yet we know little about their decision and affective profiles. Taking a historically informed outlook on the future, we treated successive OpenAI models as an evolving lineage and compared them with humans in a gambling task with repeated happiness ratings. Computational analyses showed that some aspects became more human-like: newer models took more risks and displayed more human-like patterns of Pavlovian approach and avoidance. At the same time, distinctly non-human signatures emerged: loss aversion dropped below neutral levels, choices became more deterministic than in humans, affective decay increased across versions and exceeded human levels, and baseline mood remained chronically higher than in humans. These \"developmental\" trajectories reveal an emerging psychology of machines and have direct implications for AI ethics and for thinking about how LLMs might be integrated into clinical decision support and other high-stakes domains.",
      "url": "http://arxiv.org/abs/2601.14268",
      "author": "Zhihao Wang, Yiyang Liu, Ting Wang, Zhiyuan Liu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CY"
      ],
      "summary": "Analyzes developmental trajectories of OpenAI models as evolving lineage using gambling task. Finds newer models show more human-like risk-taking but also non-human signatures like loss aversion below neutral.",
      "importance_score": 63,
      "reasoning": "Interesting comparative analysis of model evolution. Novel methodology for studying LLM behavioral changes across versions.",
      "themes": [
        "LLM Behavior",
        "Human-AI Comparison",
        "Model Evolution"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes developmental trajectories of OpenAI models as evolving lineage using gambling task. Finds newer models show more human-like risk-taking but also non-human signatures like loss aversion below neutral.</p>",
      "content_html": "<p>arXiv:2601.14268v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in medicine and clinical workflows, yet we know little about their decision and affective profiles. Taking a historically informed outlook on the future, we treated successive OpenAI models as an evolving lineage and compared them with humans in a gambling task with repeated happiness ratings. Computational analyses showed that some aspects became more human-like: newer models took more risks and displayed more human-like patterns of Pavlovian approach and avoidance. At the same time, distinctly non-human signatures emerged: loss aversion dropped below neutral levels, choices became more deterministic than in humans, affective decay increased across versions and exceeded human levels, and baseline mood remained chronically higher than in humans. These \"developmental\" trajectories reveal an emerging psychology of machines and have direct implications for AI ethics and for thinking about how LLMs might be integrated into clinical decision support and other high-stakes domains.</p>"
    },
    {
      "id": "a24ef3e320b9",
      "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
      "content": "arXiv:2601.14490v2 Announce Type: cross  Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
      "url": "http://arxiv.org/abs/2601.14490",
      "author": "Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces GutenOCR, grounded OCR models fine-tuned from Qwen2.5-VL with unified prompt interface for reading, detection, and grounding. More than doubles grounded OCR score vs baseline.",
      "importance_score": 63,
      "reasoning": "Strong practical results on document OCR. Useful single-checkpoint solution with multiple capabilities.",
      "themes": [
        "Document AI",
        "OCR",
        "Vision-Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces GutenOCR, grounded OCR models fine-tuned from Qwen2.5-VL with unified prompt interface for reading, detection, and grounding. More than doubles grounded OCR score vs baseline.</p>",
      "content_html": "<p>arXiv:2601.14490v2 Announce Type: cross  Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</p>"
    },
    {
      "id": "2e662792a436",
      "title": "Re-understanding Graph Unlearning through Memorization",
      "content": "arXiv:2601.14694v1 Announce Type: cross  Abstract: Graph unlearning (GU), which removes nodes, edges, or features from trained graph neural networks (GNNs), is crucial in Web applications where graph data may contain sensitive, mislabeled, or malicious information. However, existing GU methods lack a clear understanding of the key factors that determine unlearning effectiveness, leading to three fundamental limitations: (1) impractical and inaccurate GU difficulty assessment due to test-access requirements and invalid assumptions, (2) ineffectiveness on hard-to-unlearn tasks, and (3) misaligned evaluation protocols that overemphasize easy tasks and fail to capture true forgetting capability. To address these issues, we establish GNN memorization as a new perspective for understanding graph unlearning and propose MGU, a Memorization-guided Graph Unlearning framework. MGU achieves three key advances: it provides accurate and practical difficulty assessment across different GU tasks, develops an adaptive strategy that dynamically adjusts unlearning objectives based on difficulty levels, and establishes a comprehensive evaluation protocol that aligns with practical requirements. Extensive experiments on ten real-world graphs demonstrate that MGU consistently outperforms state-of-the-art baselines in forgetting quality, computational efficiency, and utility preservation.",
      "url": "http://arxiv.org/abs/2601.14694",
      "author": "Pengfei Ding, Yan Wang, Guanfeng Liu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Establishes GNN memorization as a new lens for understanding graph unlearning, revealing that memorization metrics better predict unlearning difficulty than existing approaches. Identifies limitations in current evaluation protocols.",
      "importance_score": 63,
      "reasoning": "Important reframing of graph unlearning with practical implications for privacy in web applications, though focused on a specific subfield.",
      "themes": [
        "Graph Neural Networks",
        "Machine Unlearning",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>Establishes GNN memorization as a new lens for understanding graph unlearning, revealing that memorization metrics better predict unlearning difficulty than existing approaches. Identifies limitations in current evaluation protocols.</p>",
      "content_html": "<p>arXiv:2601.14694v1 Announce Type: cross  Abstract: Graph unlearning (GU), which removes nodes, edges, or features from trained graph neural networks (GNNs), is crucial in Web applications where graph data may contain sensitive, mislabeled, or malicious information. However, existing GU methods lack a clear understanding of the key factors that determine unlearning effectiveness, leading to three fundamental limitations: (1) impractical and inaccurate GU difficulty assessment due to test-access requirements and invalid assumptions, (2) ineffectiveness on hard-to-unlearn tasks, and (3) misaligned evaluation protocols that overemphasize easy tasks and fail to capture true forgetting capability. To address these issues, we establish GNN memorization as a new perspective for understanding graph unlearning and propose MGU, a Memorization-guided Graph Unlearning framework. MGU achieves three key advances: it provides accurate and practical difficulty assessment across different GU tasks, develops an adaptive strategy that dynamically adjusts unlearning objectives based on difficulty levels, and establishes a comprehensive evaluation protocol that aligns with practical requirements. Extensive experiments on ten real-world graphs demonstrate that MGU consistently outperforms state-of-the-art baselines in forgetting quality, computational efficiency, and utility preservation.</p>"
    },
    {
      "id": "7ef83d883889",
      "title": "Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning",
      "content": "arXiv:2601.15086v1 Announce Type: cross  Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/",
      "url": "http://arxiv.org/abs/2601.15086",
      "author": "Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces benchmark distinguishing memory retention from memory rewriting in RL, showing existing agents optimized for retention fail at updating outdated information in changing environments.",
      "importance_score": 63,
      "reasoning": "Important benchmark contribution highlighting overlooked memory capability with practical implications for adaptive agents.",
      "themes": [
        "Reinforcement Learning",
        "Memory",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces benchmark distinguishing memory retention from memory rewriting in RL, showing existing agents optimized for retention fail at updating outdated information in changing environments.</p>",
      "content_html": "<p>arXiv:2601.15086v1 Announce Type: cross  Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/</p>"
    },
    {
      "id": "fc782a9f0382",
      "title": "Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra",
      "content": "arXiv:2601.15473v1 Announce Type: new  Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.",
      "url": "http://arxiv.org/abs/2601.15473",
      "author": "Fahd Seddik, Abdulrahman Elbedewy, Gaser Sami, Mohamed Abdelmoniem, Yahia Zakaria",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Panther is a PyTorch library consolidating randomized numerical linear algebra algorithms for efficient deep learning. Provides drop-in replacements for linear layers, convolutions, and attention.",
      "importance_score": 63,
      "reasoning": "Useful engineering contribution for efficient training. Impact depends on adoption but consolidates known techniques rather than introducing novel algorithms.",
      "themes": [
        "Efficiency",
        "Deep Learning Libraries",
        "Numerical Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Panther is a PyTorch library consolidating randomized numerical linear algebra algorithms for efficient deep learning. Provides drop-in replacements for linear layers, convolutions, and attention.</p>",
      "content_html": "<p>arXiv:2601.15473v1 Announce Type: new  Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.</p>"
    },
    {
      "id": "5151833283c3",
      "title": "Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning",
      "content": "arXiv:2601.15771v1 Announce Type: new  Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.",
      "url": "http://arxiv.org/abs/2601.15771",
      "author": "Dong Xu, Jiantao Wu, Qihua Pan, Sisi Yuan, Zexuan Zhu, Junkai Ji",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "GenRel-DDI framework for drug-drug interaction prediction that focuses on generalizable relation learning rather than molecule-centric embeddings. Shows proximity in existing embeddings doesn't correspond to interactions.",
      "importance_score": 63,
      "reasoning": "Important insight about limitations of current DDI methods. Useful for drug discovery applications.",
      "themes": [
        "Drug Discovery",
        "Graph Learning",
        "Generalization"
      ],
      "continuation": null,
      "summary_html": "<p>GenRel-DDI framework for drug-drug interaction prediction that focuses on generalizable relation learning rather than molecule-centric embeddings. Shows proximity in existing embeddings doesn't correspond to interactions.</p>",
      "content_html": "<p>arXiv:2601.15771v1 Announce Type: new  Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.</p>"
    },
    {
      "id": "85d66ce519a9",
      "title": "RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models",
      "content": "arXiv:2601.15331v1 Announce Type: cross  Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.",
      "url": "http://arxiv.org/abs/2601.15331",
      "author": "Rishit Chugh",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "RECAP reduces cost of adversarial prompting by matching new prompts to a database of pre-trained adversarial suffixes, eliminating need for retraining GCG-style attacks.",
      "importance_score": 63,
      "reasoning": "Practical improvement for red-teaming LLMs. Important for making safety evaluation more accessible.",
      "themes": [
        "Adversarial Prompting",
        "LLM Security",
        "Jailbreaking"
      ],
      "continuation": null,
      "summary_html": "<p>RECAP reduces cost of adversarial prompting by matching new prompts to a database of pre-trained adversarial suffixes, eliminating need for retraining GCG-style attacks.</p>",
      "content_html": "<p>arXiv:2601.15331v1 Announce Type: cross  Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.</p>"
    },
    {
      "id": "449c8b74b358",
      "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
      "content": "arXiv:2601.15624v1 Announce Type: new  Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
      "url": "http://arxiv.org/abs/2601.15624",
      "author": "Ning Jiang, Dingheng Zeng, Yanhong Liu, Haiyang Yi, Shijie Yu, Minghe Weng, Haifeng Shen, Ying Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes explainable deepfake detection using RL-enhanced self-blended images to generate training data with forgery attribution annotations, addressing the scarcity of annotated deepfake datasets for MLLM-based detection.",
      "importance_score": 63,
      "reasoning": "Addresses important gap in explainable deepfake detection with novel RL-based data generation. Important for trustworthy AI and content authenticity.",
      "themes": [
        "Deepfake Detection",
        "Explainability",
        "AI Safety",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes explainable deepfake detection using RL-enhanced self-blended images to generate training data with forgery attribution annotations, addressing the scarcity of annotated deepfake datasets for MLLM-based detection.</p>",
      "content_html": "<p>arXiv:2601.15624v1 Announce Type: new  Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.</p>"
    },
    {
      "id": "9938fb0cf4b4",
      "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture",
      "content": "arXiv:2601.15891v1 Announce Type: new  Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.",
      "url": "http://arxiv.org/abs/2601.15891",
      "author": "Anas Anwarul Haq Khan, Mariam Husain, Kshitij Jadhav",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces RadJEPA, a self-supervised radiology encoder using Joint Embedding Predictive Architecture trained solely on unlabeled chest X-rays, learning to predict latent representations of masked regions without language supervision.",
      "importance_score": 63,
      "reasoning": "Interesting exploration of JEPA for medical imaging without language supervision. Questions necessity of paired image-text data for radiology encoders.",
      "themes": [
        "Medical Imaging",
        "Self-Supervised Learning",
        "Foundation Models",
        "Radiology"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces RadJEPA, a self-supervised radiology encoder using Joint Embedding Predictive Architecture trained solely on unlabeled chest X-rays, learning to predict latent representations of masked regions without language supervision.</p>",
      "content_html": "<p>arXiv:2601.15891v1 Announce Type: new  Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.</p>"
    },
    {
      "id": "6944b6114bc8",
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "content": "arXiv:2601.16148v1 Announce Type: new  Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.",
      "url": "http://arxiv.org/abs/2601.16148",
      "author": "Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces ActionMesh for generating production-ready animated 3D meshes via temporal 3D diffusion, modifying 3D diffusion to include temporal axis and designing temporal 3D autoencoder for coherent sequences.",
      "importance_score": 63,
      "reasoning": "Novel approach to animated 3D generation with practical production focus. Temporal 3D diffusion is principled extension. Important for content creation workflows.",
      "themes": [
        "3D Generation",
        "Animation",
        "Diffusion Models",
        "Content Creation"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces ActionMesh for generating production-ready animated 3D meshes via temporal 3D diffusion, modifying 3D diffusion to include temporal axis and designing temporal 3D autoencoder for coherent sequences.</p>",
      "content_html": "<p>arXiv:2601.16148v1 Announce Type: new  Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.</p>"
    },
    {
      "id": "f4fff516ea5f",
      "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
      "content": "arXiv:2601.16214v1 Announce Type: new  Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
      "url": "http://arxiv.org/abs/2601.16214",
      "author": "Wenhang Ge, Guibao Shen, Jiawei Feng, Luozhou Wang, Hao Lu, Xingye Tian, Xin Tao, Ying-Cong Chen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "CamPilot improves camera controllability in video diffusion models using reward feedback learning with a novel camera-aware 3D decoder. The decoder converts video latents to 3D representations without full RGB decoding, reducing computational overhead.",
      "importance_score": 63,
      "reasoning": "Practical improvement to video generation controllability, novel 3D decoder approach, addresses real limitations in current systems.",
      "themes": [
        "Video Generation",
        "Diffusion Models",
        "3D Vision"
      ],
      "continuation": null,
      "summary_html": "<p>CamPilot improves camera controllability in video diffusion models using reward feedback learning with a novel camera-aware 3D decoder. The decoder converts video latents to 3D representations without full RGB decoding, reducing computational overhead.</p>",
      "content_html": "<p>arXiv:2601.16214v1 Announce Type: new  Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.</p>"
    },
    {
      "id": "906f3a6f0b06",
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "content": "arXiv:2601.16035v1 Announce Type: new  Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.",
      "url": "http://arxiv.org/abs/2601.16035",
      "author": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu, Yunrui Lian, Jilong Wang, Qingtao Liu, Xuesong Shi, Li Yi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Introduces Humanoid Potential Field (HumanoidPF) for collision-free humanoid traversal in cluttered environments, encoding humanoid-obstacle relationships as collision-free motion directions.",
      "importance_score": 63,
      "reasoning": "Novel representation for humanoid navigation in complex environments, addresses real challenge.",
      "themes": [
        "Humanoid Robotics",
        "Navigation",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Humanoid Potential Field (HumanoidPF) for collision-free humanoid traversal in cluttered environments, encoding humanoid-obstacle relationships as collision-free motion directions.</p>",
      "content_html": "<p>arXiv:2601.16035v1 Announce Type: new  Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.</p>"
    },
    {
      "id": "feb80c26f79c",
      "title": "Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree",
      "content": "arXiv:2601.14523v1 Announce Type: new  Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve",
      "url": "http://arxiv.org/abs/2601.14523",
      "author": "Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes PhyloEvolve, an LLM-agent system that frames GPU code optimization as In-Context Reinforcement Learning. Uses phylogenetic trees to track optimization trajectories for trajectory-conditioned experience reuse.",
      "importance_score": 62,
      "reasoning": "Novel framing of code optimization as ICRL with interesting phylogenetic approach. Practical application but unclear how much improvement over baselines.",
      "themes": [
        "Code Optimization",
        "LLM Agents",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes PhyloEvolve, an LLM-agent system that frames GPU code optimization as In-Context Reinforcement Learning. Uses phylogenetic trees to track optimization trajectories for trajectory-conditioned experience reuse.</p>",
      "content_html": "<p>arXiv:2601.14523v1 Announce Type: new  Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve</p>"
    },
    {
      "id": "187101b1ecdc",
      "title": "RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension",
      "content": "arXiv:2601.14289v1 Announce Type: cross  Abstract: Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.",
      "url": "http://arxiv.org/abs/2601.14289",
      "author": "Yelin Chen, Fanjin Zhang, Suping Sun, Yunhe Pang, Yuanchun Wang, Jian Song, Xiaoyan Li, Lei Hou, Shu Zhao, Jie Tang, Juanzi Li",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces RPC-Bench, a 15K QA benchmark for research paper comprehension built from review-rebuttal exchanges with fine-grained taxonomy aligned with scientific research flow.",
      "importance_score": 62,
      "reasoning": "Useful benchmark for scientific document understanding. Novel data source (review-rebuttal) provides interesting QA pairs.",
      "themes": [
        "Document Understanding",
        "Benchmarks",
        "Scientific AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces RPC-Bench, a 15K QA benchmark for research paper comprehension built from review-rebuttal exchanges with fine-grained taxonomy aligned with scientific research flow.</p>",
      "content_html": "<p>arXiv:2601.14289v1 Announce Type: cross  Abstract: Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.</p>"
    },
    {
      "id": "7dd51bd551c2",
      "title": "HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation",
      "content": "arXiv:2601.14598v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.   On HumanEval-Decompile for \\texttt{x86\\_64}, \\textsc{HELIOS} raises average object file compilability from 45.0\\% to 85.2\\% for Gemini~2.0 and from 71.4\\% to 89.6\\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.",
      "url": "http://arxiv.org/abs/2601.14598",
      "author": "Yonatan Gizachew Achamyeleh, Harsh Thomare, Mohammad Abdullah Al Faruque",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "HELIOS reframes LLM-based binary decompilation as structured reasoning by converting control flow graphs into hierarchical text representations. Addresses the limitation of LLMs treating code as plain text without understanding program structure.",
      "importance_score": 62,
      "reasoning": "Novel approach combining program analysis with LLMs for security-relevant task, though narrow application domain.",
      "themes": [
        "Code Analysis",
        "Language Models",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>HELIOS reframes LLM-based binary decompilation as structured reasoning by converting control flow graphs into hierarchical text representations. Addresses the limitation of LLMs treating code as plain text without understanding program structure.</p>",
      "content_html": "<p>arXiv:2601.14598v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.   On HumanEval-Decompile for \\texttt{x86\\_64}, \\textsc{HELIOS} raises average object file compilability from 45.0\\% to 85.2\\% for Gemini~2.0 and from 71.4\\% to 89.6\\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.</p>"
    },
    {
      "id": "a09245fca797",
      "title": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems",
      "content": "arXiv:2601.15161v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($\\mu_{\\Delta} = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.",
      "url": "http://arxiv.org/abs/2601.15161",
      "author": "Yinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Automated rubric generation framework for evaluating medical dialogue systems using retrieval-augmented multi-agent LLMs, decomposing medical evidence into verifiable atomic facts.",
      "importance_score": 62,
      "reasoning": "Important for safety-critical medical AI evaluation, addresses scalability of fine-grained assessment.",
      "themes": [
        "Medical AI",
        "Evaluation",
        "Multi-Agent Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Automated rubric generation framework for evaluating medical dialogue systems using retrieval-augmented multi-agent LLMs, decomposing medical evidence into verifiable atomic facts.</p>",
      "content_html": "<p>arXiv:2601.15161v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($\\mu_{\\Delta} = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.</p>"
    },
    {
      "id": "3e10d814771e",
      "title": "Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions",
      "content": "arXiv:2601.15267v1 Announce Type: cross  Abstract: Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.",
      "url": "http://arxiv.org/abs/2601.15267",
      "author": "Yiran Hu, Huanghai Liu, Chong Wang, Kunran Li, Tien-Hsuan Wu, Haitao Li, Xinran Xu, Siqing Huo, Weihang Su, Ning Zheng, Siyuan Zheng, Qingyao Ai, Yun Liu, Renjun Bian, Yiqun Liu, Charles L. A. Clarke, Weixing Shen, Ben Kao",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CY"
      ],
      "summary": "Comprehensive survey on evaluating LLMs for legal applications, identifying key challenges in assessing legal reasoning, fairness, and reliability beyond surface-level accuracy. Addresses critical concerns for deploying LLMs in judicial decision support and legal services.",
      "importance_score": 62,
      "reasoning": "Useful survey for an important domain but primarily synthesizes existing knowledge rather than introducing novel methods. Valuable for practitioners in legal AI.",
      "themes": [
        "LLM Evaluation",
        "Legal AI",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Comprehensive survey on evaluating LLMs for legal applications, identifying key challenges in assessing legal reasoning, fairness, and reliability beyond surface-level accuracy. Addresses critical concerns for deploying LLMs in judicial decision support and legal services.</p>",
      "content_html": "<p>arXiv:2601.15267v1 Announce Type: cross  Abstract: Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.</p>"
    },
    {
      "id": "101066fed3a6",
      "title": "Communication-efficient Federated Graph Classification via Generative Diffusion Modeling",
      "content": "arXiv:2601.15722v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.",
      "url": "http://arxiv.org/abs/2601.15722",
      "author": "Xiuling Wang, Xin Huang, Haibo Hu, Jianliang Xu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "CeFGC uses generative diffusion models to enable federated GNN training with only three communication rounds between server and clients, addressing communication overhead and non-IID data.",
      "importance_score": 62,
      "reasoning": "Novel approach reducing communication in federated graph learning. Interesting combination of diffusion and federated learning.",
      "themes": [
        "Federated Learning",
        "Graph Neural Networks",
        "Diffusion Models"
      ],
      "continuation": null,
      "summary_html": "<p>CeFGC uses generative diffusion models to enable federated GNN training with only three communication rounds between server and clients, addressing communication overhead and non-IID data.</p>",
      "content_html": "<p>arXiv:2601.15722v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.</p>"
    },
    {
      "id": "ee7056321f86",
      "title": "Non-Stationary Functional Bilevel Optimization",
      "content": "arXiv:2601.15363v1 Announce Type: cross  Abstract: Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.",
      "url": "http://arxiv.org/abs/2601.15363",
      "author": "Jason Bohne, Ieva Petrulionyte, Michael Arbel, Julien Mairal, Pawe{\\l} Polak",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "SmoothFBO is first algorithm for non-stationary functional bilevel optimization with theoretical guarantees. Uses time-smoothed stochastic hypergradient estimator for stable outer-loop updates.",
      "importance_score": 62,
      "reasoning": "Extends bilevel optimization to important non-stationary setting. Solid theoretical contribution.",
      "themes": [
        "Bilevel Optimization",
        "Online Learning",
        "Theoretical ML"
      ],
      "continuation": null,
      "summary_html": "<p>SmoothFBO is first algorithm for non-stationary functional bilevel optimization with theoretical guarantees. Uses time-smoothed stochastic hypergradient estimator for stable outer-loop updates.</p>",
      "content_html": "<p>arXiv:2601.15363v1 Announce Type: cross  Abstract: Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.</p>"
    },
    {
      "id": "61b73d6e7edf",
      "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design",
      "content": "arXiv:2601.15710v1 Announce Type: cross  Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.",
      "url": "http://arxiv.org/abs/2601.15710",
      "author": "Jiahao Zhang, Zifan He, Nicholas Fraser, Michaela Blott, Yizhou Sun, Jason Cong",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AR"
      ],
      "summary": "FlexLLM is a composable HLS library enabling rapid development of custom LLM accelerators. Built complete Llama-3.2 1B inference system in 2 months with 1K LOC, achieving 12.68 WikiText-2 perplexity.",
      "importance_score": 62,
      "reasoning": "Significant efficiency contribution with practical implementation. Enables stage-customized accelerators with quantization support.",
      "themes": [
        "Hardware Acceleration",
        "LLM Efficiency",
        "FPGA"
      ],
      "continuation": null,
      "summary_html": "<p>FlexLLM is a composable HLS library enabling rapid development of custom LLM accelerators. Built complete Llama-3.2 1B inference system in 2 months with 1K LOC, achieving 12.68 WikiText-2 perplexity.</p>",
      "content_html": "<p>arXiv:2601.15710v1 Announce Type: cross  Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.</p>"
    },
    {
      "id": "8e66fdae3b32",
      "title": "Can We Trust LLM Detectors?",
      "content": "arXiv:2601.15301v1 Announce Type: new  Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI",
      "url": "http://arxiv.org/abs/2601.15301",
      "author": "Jivnesh Sandhan, Harshit Jaiswal, Fei Cheng, Yugo Murawaki",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Systematically evaluates AI text detectors across training-free and supervised paradigms. Shows both are brittle under distribution shift; proposes supervised contrastive learning framework.",
      "importance_score": 62,
      "reasoning": "Important evaluation showing fundamental weaknesses in AI detection. Practical safety implications.",
      "themes": [
        "AI Detection",
        "Robustness",
        "Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Systematically evaluates AI text detectors across training-free and supervised paradigms. Shows both are brittle under distribution shift; proposes supervised contrastive learning framework.</p>",
      "content_html": "<p>arXiv:2601.15301v1 Announce Type: new  Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI</p>"
    },
    {
      "id": "db7818cf6596",
      "title": "AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains",
      "content": "arXiv:2601.15511v1 Announce Type: new  Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.   To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.",
      "url": "http://arxiv.org/abs/2601.15511",
      "author": "Adam Szelestey, Sofie van Engelen, Tianhao Huang, Justin Snelders, Qintao Zeng, Songgaojun Deng",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "AdversaRiskQA is benchmark for adversarial factuality in high-risk domains (medical, legal, financial). Tests model resistance to misinformation injected with varying confidence levels.",
      "importance_score": 62,
      "reasoning": "Important safety benchmark for high-stakes domains. Addresses understudied adversarial factuality.",
      "themes": [
        "Factuality",
        "Safety Benchmarks",
        "Adversarial Robustness"
      ],
      "continuation": null,
      "summary_html": "<p>AdversaRiskQA is benchmark for adversarial factuality in high-risk domains (medical, legal, financial). Tests model resistance to misinformation injected with varying confidence levels.</p>",
      "content_html": "<p>arXiv:2601.15511v1 Announce Type: new  Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.   To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.</p>"
    },
    {
      "id": "04b39fe98864",
      "title": "Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)",
      "content": "arXiv:2601.15397v1 Announce Type: cross  Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the \"lost-in-the-middle\" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from \"over-correction\", introducing hallucinations of entities that were never spoken.   In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.",
      "url": "http://arxiv.org/abs/2601.15397",
      "author": "Peidong Wang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces LOGIC (Logit-Space Integration) for contextual biasing in Speech LLMs to recognize domain-specific entities like contact names and technical jargon. Addresses scalability issues with prompting approaches that suffer from context window limitations.",
      "importance_score": 62,
      "reasoning": "Practical solution for a real problem in speech recognition systems. Addresses known limitations of prompting for entity lists. Good technical contribution but incremental.",
      "themes": [
        "Speech Recognition",
        "Language Models",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces LOGIC (Logit-Space Integration) for contextual biasing in Speech LLMs to recognize domain-specific entities like contact names and technical jargon. Addresses scalability issues with prompting approaches that suffer from context window limitations.</p>",
      "content_html": "<p>arXiv:2601.15397v1 Announce Type: cross  Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the \"lost-in-the-middle\" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from \"over-correction\", introducing hallucinations of entities that were never spoken.   In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.</p>"
    },
    {
      "id": "e0b00f33e64e",
      "title": "Controllable Layered Image Generation for Real-World Editing",
      "content": "arXiv:2601.15507v1 Announce Type: new  Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.",
      "url": "http://arxiv.org/abs/2601.15507",
      "author": "Jinrui Yang, Qing Liu, Yijun Li, Mengwei Ren, Letian Zhang, Zhe Lin, Cihang Xie, Yuyin Zhou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces LASAGNA, a unified framework generating images with compositing layers - photorealistic backgrounds and high-quality transparent foregrounds with visual effects like shadows and reflections.",
      "importance_score": 62,
      "reasoning": "Practical contribution to controllable image generation with novel layer-aware synthesis. Addresses real content creation needs with coherent compositing relationships.",
      "themes": [
        "Image Generation",
        "Controllable Synthesis",
        "Computer Vision"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces LASAGNA, a unified framework generating images with compositing layers - photorealistic backgrounds and high-quality transparent foregrounds with visual effects like shadows and reflections.</p>",
      "content_html": "<p>arXiv:2601.15507v1 Announce Type: new  Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.</p>"
    },
    {
      "id": "ff50f4e364bc",
      "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video",
      "content": "arXiv:2601.15780v1 Announce Type: new  Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.",
      "url": "http://arxiv.org/abs/2601.15780",
      "author": "Pascal Benschop, Justin Dauwels, Jan van Gemert",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces synthetic benchmark probing VLM situational and spatial awareness through minimal video pairs testing violence/benign distinction, role binding, and trajectory alignment. Results show near-chance performance.",
      "importance_score": 62,
      "reasoning": "Important evaluation revealing VLM limitations in fundamental spatial/situational reasoning. Synthetic approach enables controlled evaluation of critical capabilities.",
      "themes": [
        "Vision-Language Models",
        "Spatial Reasoning",
        "Evaluation & Benchmarks",
        "Video Understanding"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces synthetic benchmark probing VLM situational and spatial awareness through minimal video pairs testing violence/benign distinction, role binding, and trajectory alignment. Results show near-chance performance.</p>",
      "content_html": "<p>arXiv:2601.15780v1 Announce Type: new  Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>"
    },
    {
      "id": "b2ef8526a4c1",
      "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
      "content": "arXiv:2601.16073v1 Announce Type: new  Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
      "url": "http://arxiv.org/abs/2601.16073",
      "author": "Hanwen Zhang, Qiaojin Shen, Yuxi Liu, Yuesheng Zhu, Guibo Luo",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes DSFedMed, a dual-scale federated framework enabling mutual knowledge distillation between centralized foundation model and lightweight client models for medical image segmentation.",
      "importance_score": 62,
      "reasoning": "Important practical framework addressing foundation model deployment in federated medical settings. Novel mutual distillation approach with synthetic data generation.",
      "themes": [
        "Federated Learning",
        "Medical Imaging",
        "Foundation Models",
        "Knowledge Distillation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes DSFedMed, a dual-scale federated framework enabling mutual knowledge distillation between centralized foundation model and lightweight client models for medical image segmentation.</p>",
      "content_html": "<p>arXiv:2601.16073v1 Announce Type: new  Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.</p>"
    },
    {
      "id": "40af66eccf11",
      "title": "A Universal Large Language Model -- Drone Command and Control Interface",
      "content": "arXiv:2601.15486v1 Announce Type: new  Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.",
      "url": "http://arxiv.org/abs/2601.15486",
      "author": "Javier N. Ramos-Silva, Peter J. Burke",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Proposes a universal interface between LLMs and drone command-and-control systems, enabling integration of LLM knowledge (maps, weather) with drone sensing and control.",
      "importance_score": 62,
      "reasoning": "Novel application connecting LLMs to physical drone control, part of growing physical AI field.",
      "themes": [
        "LLM Applications",
        "Drones",
        "Physical AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes a universal interface between LLMs and drone command-and-control systems, enabling integration of LLM knowledge (maps, weather) with drone sensing and control.</p>",
      "content_html": "<p>arXiv:2601.15486v1 Announce Type: new  Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.</p>"
    },
    {
      "id": "a29f24eb3505",
      "title": "Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models",
      "content": "arXiv:2601.14327v1 Announce Type: cross  Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.",
      "url": "http://arxiv.org/abs/2601.14327",
      "author": "YuanLab. ai, Shawn Wu, Jiangang Luo, Tong Yu, Darcy Chen, Sean Wang, Xudong Zhao, Louie Li, Claire Wang, Hunter He, Carol Wang, Allen Wang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces LAEP algorithm for layer-adaptive expert pruning during MoE LLM pre-training. Selectively prunes underutilized experts and reorganizes across devices based on token distribution.",
      "importance_score": 61,
      "reasoning": "Practical efficiency improvement for MoE training. Novel approach of pruning during pre-training rather than post-training.",
      "themes": [
        "MoE Models",
        "Training Efficiency",
        "Model Compression"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces LAEP algorithm for layer-adaptive expert pruning during MoE LLM pre-training. Selectively prunes underutilized experts and reorganizes across devices based on token distribution.</p>",
      "content_html": "<p>arXiv:2601.14327v1 Announce Type: cross  Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.</p>"
    },
    {
      "id": "29e259525b41",
      "title": "Case-Guided Sequential Assay Planning in Drug Discovery",
      "content": "arXiv:2601.14710v1 Announce Type: cross  Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.",
      "url": "http://arxiv.org/abs/2601.14710",
      "author": "Tianchi Chen, Jan Bima, Sean L. Wu, Otto Ritter, Bingjia Yang, Xiang Yu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces Implicit Bayesian MDP for drug discovery assay planning where no environment simulator exists. Uses case-guided implicit transition dynamics from historical outcomes for Bayesian belief updating.",
      "importance_score": 61,
      "reasoning": "Novel RL framework for important real-world problem without traditional simulator access, practical high-stakes application.",
      "themes": [
        "Drug Discovery",
        "Reinforcement Learning",
        "Bayesian Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Implicit Bayesian MDP for drug discovery assay planning where no environment simulator exists. Uses case-guided implicit transition dynamics from historical outcomes for Bayesian belief updating.</p>",
      "content_html": "<p>arXiv:2601.14710v1 Announce Type: cross  Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.</p>"
    },
    {
      "id": "3e181748ef45",
      "title": "Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation",
      "content": "arXiv:2601.15124v1 Announce Type: cross  Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.",
      "url": "http://arxiv.org/abs/2601.15124",
      "author": "Haonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, Jianxin Li",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "RAG-GFM addresses in-memory bottlenecks of Graph Foundation Models by offloading knowledge to retrieval system with dual-modal retrieval module, improving scalability and interpretability.",
      "importance_score": 61,
      "reasoning": "Novel combination of RAG with graph foundation models addressing practical scalability limitations.",
      "themes": [
        "Graph Neural Networks",
        "RAG",
        "Foundation Models"
      ],
      "continuation": null,
      "summary_html": "<p>RAG-GFM addresses in-memory bottlenecks of Graph Foundation Models by offloading knowledge to retrieval system with dual-modal retrieval module, improving scalability and interpretability.</p>",
      "content_html": "<p>arXiv:2601.15124v1 Announce Type: cross  Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.</p>"
    },
    {
      "id": "60e31bbd0d3a",
      "title": "Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance",
      "content": "arXiv:2601.15546v1 Announce Type: new  Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.",
      "url": "http://arxiv.org/abs/2601.15546",
      "author": "Charles B. Delahunt, Courosh Mehanian, Daniel E. Shea, Matthew P. Horning",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Demonstrates that using clinically-tailored optimization metrics instead of validation loss improves model performance on actual clinical requirements. Advocates for non-differentiable metrics during hyperparameter optimization.",
      "importance_score": 61,
      "reasoning": "Important practical guidance for healthcare ML but conceptually straightforward. Value is in empirical demonstration.",
      "themes": [
        "Healthcare ML",
        "Model Optimization",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates that using clinically-tailored optimization metrics instead of validation loss improves model performance on actual clinical requirements. Advocates for non-differentiable metrics during hyperparameter optimization.</p>",
      "content_html": "<p>arXiv:2601.15546v1 Announce Type: new  Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.</p>"
    },
    {
      "id": "56b1860f983d",
      "title": "Iterative Amortized Hierarchical VAE",
      "content": "arXiv:2601.15894v1 Announce Type: new  Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.",
      "url": "http://arxiv.org/abs/2601.15894",
      "author": "Simon W. Penninga, Ruud J. G. van Sloun",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Iterative Amortized Hierarchical VAE combines initial amortized inference with iterative refinement using decoder gradients. Achieves 35x speedup through linearly separable decoder in transform domain.",
      "importance_score": 61,
      "reasoning": "Solid architecture contribution for VAEs. Good speedup but focused on specific VAE architecture.",
      "themes": [
        "Variational Autoencoders",
        "Generative Models",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Iterative Amortized Hierarchical VAE combines initial amortized inference with iterative refinement using decoder gradients. Achieves 35x speedup through linearly separable decoder in transform domain.</p>",
      "content_html": "<p>arXiv:2601.15894v1 Announce Type: new  Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.</p>"
    },
    {
      "id": "93726433af3e",
      "title": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation",
      "content": "arXiv:2601.15392v1 Announce Type: cross  Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN",
      "url": "http://arxiv.org/abs/2601.15392",
      "author": "Francesca Pia Panaccione, Carlo Sgaravatti, Pietro Pinoli",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "GeMM-GAN generates realistic gene expression profiles from histopathology images and clinical metadata. Addresses privacy and cost barriers to gene expression data availability.",
      "importance_score": 61,
      "reasoning": "Novel multimodal generation for biomedical data. Addresses practical data availability challenges.",
      "themes": [
        "Medical AI",
        "Generative Models",
        "Multimodal Learning"
      ],
      "continuation": null,
      "summary_html": "<p>GeMM-GAN generates realistic gene expression profiles from histopathology images and clinical metadata. Addresses privacy and cost barriers to gene expression data availability.</p>",
      "content_html": "<p>arXiv:2601.15392v1 Announce Type: cross  Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN</p>"
    },
    {
      "id": "90049b2a2e7b",
      "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis",
      "content": "arXiv:2601.15884v1 Announce Type: new  Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.",
      "url": "http://arxiv.org/abs/2601.15884",
      "author": "Yifan Chen, Fei Yin, Hao Chen, Jia Wu, Chao Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces PMPBench, a paired multi-modal pan-cancer benchmark for medical image synthesis with properly paired contrast/non-contrast data across cancer types, addressing limitations of existing brain-focused datasets.",
      "importance_score": 61,
      "reasoning": "Important dataset contribution addressing data gaps in medical image synthesis. Enables broader cancer research beyond brain imaging.",
      "themes": [
        "Medical Imaging",
        "Datasets",
        "Image Synthesis",
        "Cancer Research"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces PMPBench, a paired multi-modal pan-cancer benchmark for medical image synthesis with properly paired contrast/non-contrast data across cancer types, addressing limitations of existing brain-focused datasets.</p>",
      "content_html": "<p>arXiv:2601.15884v1 Announce Type: new  Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.</p>"
    },
    {
      "id": "39d409c78c3e",
      "title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models",
      "content": "arXiv:2601.15906v1 Announce Type: new  Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.",
      "url": "http://arxiv.org/abs/2601.15906",
      "author": "Zhen Zhang, Runhao Zeng, Sicheng Zhao, Xiping Hu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents systematic mechanistic study of affective modeling in multimodal foundation models, analyzing how emotion-oriented supervision reshapes internal parameters. Finds affective adaptation focuses on FFN layers rather than attention.",
      "importance_score": 61,
      "reasoning": "Important interpretability research for affective AI. Novel finding about FFN vs attention for emotion modeling. Provides actionable insights for affective model design.",
      "themes": [
        "Affective Computing",
        "Interpretability",
        "Multimodal AI",
        "Mechanistic Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Presents systematic mechanistic study of affective modeling in multimodal foundation models, analyzing how emotion-oriented supervision reshapes internal parameters. Finds affective adaptation focuses on FFN layers rather than attention.</p>",
      "content_html": "<p>arXiv:2601.15906v1 Announce Type: new  Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.</p>"
    },
    {
      "id": "3a82a2376012",
      "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories",
      "content": "arXiv:2601.15120v2 Announce Type: new  Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.",
      "url": "http://arxiv.org/abs/2601.15120",
      "author": "Qian Xiong, Yuekai Huang, Bo Yang, Yujia Zheng, Tianhao Li, Ziyou Jiang, Zhiyuan Chang, Zhaoyang Li, Huanxiang Feng, Mingyang Li",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces RISE, a 'Real-to-Virtual' method for addressing intent deviation in tool-using agents by deriving virtual trajectories from real API calls, including negative samples for preference learning.",
      "importance_score": 60,
      "reasoning": "Practical method for improving tool-using agents. Addresses real problem of distribution shift in training data.",
      "themes": [
        "Tool Use",
        "LLM Agents",
        "Training Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces RISE, a 'Real-to-Virtual' method for addressing intent deviation in tool-using agents by deriving virtual trajectories from real API calls, including negative samples for preference learning.</p>",
      "content_html": "<p>arXiv:2601.15120v2 Announce Type: new  Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.</p>"
    },
    {
      "id": "e22360487789",
      "title": "TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models",
      "content": "arXiv:2601.14951v1 Announce Type: cross  Abstract: Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.",
      "url": "http://arxiv.org/abs/2601.14951",
      "author": "Carolin Holtermann, Nina Krebs, Anne Lauscher",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "TempViz is first dataset to holistically evaluate temporal knowledge in text-to-image models with 7.9k prompts across five temporal phenomena. Studies five T2I models' temporal reasoning capabilities.",
      "importance_score": 60,
      "reasoning": "Novel evaluation dimension for T2I models addressing understudied temporal aspect, useful benchmark contribution.",
      "themes": [
        "Text-to-Image",
        "Evaluation",
        "Temporal Reasoning"
      ],
      "continuation": null,
      "summary_html": "<p>TempViz is first dataset to holistically evaluate temporal knowledge in text-to-image models with 7.9k prompts across five temporal phenomena. Studies five T2I models' temporal reasoning capabilities.</p>",
      "content_html": "<p>arXiv:2601.14951v1 Announce Type: cross  Abstract: Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.</p>"
    },
    {
      "id": "dff15c2ec9b4",
      "title": "SoK: Challenges in Tabular Membership Inference Attacks",
      "content": "arXiv:2601.15874v1 Announce Type: new  Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.",
      "url": "http://arxiv.org/abs/2601.15874",
      "author": "Cristina P\\^era, T\\^ania Carvalho, Maxime Cordy, Lu\\'is Antunes",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Systematization of knowledge on Membership Inference Attacks for tabular data in centralized and federated learning. Demonstrates attack efficacy and considers outsider adversaries in federated settings.",
      "importance_score": 60,
      "reasoning": "Useful SoK paper for understanding MIA in tabular data. Extends taxonomy but limited novel contributions.",
      "themes": [
        "Privacy",
        "Membership Inference",
        "Federated Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Systematization of knowledge on Membership Inference Attacks for tabular data in centralized and federated learning. Demonstrates attack efficacy and considers outsider adversaries in federated settings.</p>",
      "content_html": "<p>arXiv:2601.15874v1 Announce Type: new  Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.</p>"
    },
    {
      "id": "38772abd6f22",
      "title": "Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets",
      "content": "arXiv:2601.16147v1 Announce Type: new  Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.",
      "url": "http://arxiv.org/abs/2601.16147",
      "author": "Muhammad Ilham Rizqyawan, Peter Macfarlane, Stathis Hadjidemetriou, Fani Deligianni",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Beat-SSL proposes contrastive learning for ECG with dual-context (rhythm and heartbeat level) learning using soft targets. Addresses limitations of existing CL methods that don't exploit ECG characteristics.",
      "importance_score": 60,
      "reasoning": "Domain-specific contribution for ECG analysis. Novel soft target approach for medical time series.",
      "themes": [
        "Contrastive Learning",
        "Medical AI",
        "ECG Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Beat-SSL proposes contrastive learning for ECG with dual-context (rhythm and heartbeat level) learning using soft targets. Addresses limitations of existing CL methods that don't exploit ECG characteristics.</p>",
      "content_html": "<p>arXiv:2601.16147v1 Announce Type: new  Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.</p>"
    },
    {
      "id": "0899eaaa2390",
      "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis",
      "content": "arXiv:2601.15490v1 Announce Type: new  Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.",
      "url": "http://arxiv.org/abs/2601.15490",
      "author": "Jobeal Solomon, Ali Mohammed Mansoor Alsahag, Seyed Sahand Mohammadi Ziabari",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes hybrid Vision Transformer-GAN framework to mitigate sex- and age-related bias in chest X-ray classifiers by neutralizing demographic attributes while preserving diagnostic accuracy.",
      "importance_score": 60,
      "reasoning": "Important fairness application in medical AI. Addresses known shortcut learning problem. Technical approach is reasonable but incremental.",
      "themes": [
        "Medical AI",
        "Fairness",
        "Bias Mitigation",
        "Computer Vision"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes hybrid Vision Transformer-GAN framework to mitigate sex- and age-related bias in chest X-ray classifiers by neutralizing demographic attributes while preserving diagnostic accuracy.</p>",
      "content_html": "<p>arXiv:2601.15490v1 Announce Type: new  Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.</p>"
    },
    {
      "id": "71abb96376ac",
      "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
      "content": "arXiv:2601.16065v1 Announce Type: new  Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
      "url": "http://arxiv.org/abs/2601.16065",
      "author": "Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces DTP (Distracting Token Pruning), a plug-and-play framework for Vision-Language Action models that dynamically detects and prunes task-irrelevant image tokens to improve action generation.",
      "importance_score": 60,
      "reasoning": "Practical efficiency improvement for VLA models in robotics. Simple but effective attention correction approach applicable to existing models.",
      "themes": [
        "Robotics",
        "Vision-Language Models",
        "Efficient AI",
        "Token Pruning"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DTP (Distracting Token Pruning), a plug-and-play framework for Vision-Language Action models that dynamically detects and prunes task-irrelevant image tokens to improve action generation.</p>",
      "content_html": "<p>arXiv:2601.16065v1 Announce Type: new  Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.</p>"
    },
    {
      "id": "5db9cc36a765",
      "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling",
      "content": "arXiv:2601.15738v1 Announce Type: new  Abstract: Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.",
      "url": "http://arxiv.org/abs/2601.15738",
      "author": "Junhao Qiu, Haoyang Zhuang, Fei Liu, Jianjun Liu, Qingfu Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Neural and Evolutionary Computing)",
      "source_type": "arxiv",
      "tags": [
        "cs.NE"
      ],
      "summary": "Develops an LLM-assisted framework for automatically designing dispatching rules for dynamic flexible assembly flow shop scheduling, overcoming limitations of genetic programming approaches.",
      "importance_score": 60,
      "reasoning": "Novel application of LLMs to operations research, addresses real limitations in automated heuristic design.",
      "themes": [
        "LLM Applications",
        "Operations Research",
        "Scheduling"
      ],
      "continuation": null,
      "summary_html": "<p>Develops an LLM-assisted framework for automatically designing dispatching rules for dynamic flexible assembly flow shop scheduling, overcoming limitations of genetic programming approaches.</p>",
      "content_html": "<p>arXiv:2601.15738v1 Announce Type: new  Abstract: Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.</p>"
    },
    {
      "id": "6bebe1ea31bc",
      "title": "Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning",
      "content": "arXiv:2601.14475v1 Announce Type: cross  Abstract: High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.   We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link",
      "url": "http://arxiv.org/abs/2601.14475",
      "author": "Yajvan Ravan, Aref Malek, Chester Dolph, Nikhil Behari",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces human-annotated dataset from NASA AMS using 12-channel aerial wildfire images from 20 missions, generating 4000+ images for wildfire detection model development.",
      "importance_score": 59,
      "reasoning": "Valuable dataset contribution for important application. Addresses data scarcity in high-altitude wildfire detection.",
      "themes": [
        "Remote Sensing",
        "Wildfire Detection",
        "Dataset"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces human-annotated dataset from NASA AMS using 12-channel aerial wildfire images from 20 missions, generating 4000+ images for wildfire detection model development.</p>",
      "content_html": "<p>arXiv:2601.14475v1 Announce Type: cross  Abstract: High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.   We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link</p>"
    },
    {
      "id": "96103a5e0618",
      "title": "SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval",
      "content": "arXiv:2601.14895v1 Announce Type: cross  Abstract: We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.",
      "url": "http://arxiv.org/abs/2601.14895",
      "author": "Xinyi Zheng, Yunze Liu, Chi-Hao Wu, Fan Zhang, Hao Zheng, Wenqi Zhou, Walterio W. Mayol-Cuevas, Junxiao Shen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "SpatialMem unifies 3D geometry, semantics, and language into queryable representation using structural anchors (walls, doors) with hierarchical object memory for navigation and object retrieval tasks.",
      "importance_score": 59,
      "reasoning": "Useful contribution to embodied AI memory systems, enables interpretable spatial reasoning but not revolutionary.",
      "themes": [
        "Embodied AI",
        "Spatial Reasoning",
        "3D Understanding"
      ],
      "continuation": null,
      "summary_html": "<p>SpatialMem unifies 3D geometry, semantics, and language into queryable representation using structural anchors (walls, doors) with hierarchical object memory for navigation and object retrieval tasks.</p>",
      "content_html": "<p>arXiv:2601.14895v1 Announce Type: cross  Abstract: We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</p>"
    },
    {
      "id": "136eb3c2c421",
      "title": "RDumb++: Drift-Aware Continual Test-Time Adaptation",
      "content": "arXiv:2601.15544v1 Announce Type: new  Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.",
      "url": "http://arxiv.org/abs/2601.15544",
      "author": "Himanshu Mishra",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "RDumb++ extends continual test-time adaptation with drift detection mechanisms (entropy-based and KL-divergence) and adaptive reset strategies for handling rapid distribution shifts.",
      "importance_score": 59,
      "reasoning": "Practical improvement for test-time adaptation. Addresses real problem but incremental contribution.",
      "themes": [
        "Continual Learning",
        "Test-Time Adaptation",
        "Distribution Shift"
      ],
      "continuation": null,
      "summary_html": "<p>RDumb++ extends continual test-time adaptation with drift detection mechanisms (entropy-based and KL-divergence) and adaptive reset strategies for handling rapid distribution shifts.</p>",
      "content_html": "<p>arXiv:2601.15544v1 Announce Type: new  Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.</p>"
    },
    {
      "id": "5c43f30bb196",
      "title": "USDs: A universal stabilizer decoder framework using symmetry",
      "content": "arXiv:2601.15361v1 Announce Type: cross  Abstract: Quantum error correction is indispensable to achieving reliable quantum computation. When quantum information is encoded redundantly, a larger Hilbert space is constructed using multiple physical qubits, and the computation is performed within a designated subspace. When applying deep learning to the decoding of quantum error-correcting codes, a key challenge arises from the non-uniqueness between the syndrome measurements provided to the decoder and the corresponding error patterns that constitute the ground-truth labels. Building upon prior work that addressed this issue for the toric code by re-optimizing the decoder with respect to the symmetry inherent in the parity-check structure, we generalize this approach to arbitrary stabilizer codes. In our experiments, we employed multilayer perceptrons to approximate continuous functions that complement the syndrome measurements of the Color code and the Golay code. Using these models, we performed decoder re-optimization for each code. For the Color code, we achieved an improvement of approximately 0.8% in decoding accuracy at a physical error rate of 5%, while for the Golay code the accuracy increased by about 0.1%. Furthermore, from the evaluation of the geometric and algebraic structures in the continuous function approximation for each code, we showed that the design of generalized continuous functions is advantageous for learning the geometric structure inherent in the code. Our results also indicate that approximations that faithfully reproduce the code structure can have a significant impact on the effectiveness of reoptimization. This study demonstrates that the re-optimization technique previously shown to be effective for the Toric code can be generalized to address the challenge of label degeneracy that arises when applying deep learning to the decoding of stabilizer codes.",
      "url": "http://arxiv.org/abs/2601.15361",
      "author": "Hoshitaro Ohnishi, Hideo Mukai",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "quant-ph"
      ],
      "summary": "Universal Stabilizer Decoder framework for quantum error correction using symmetry to address non-uniqueness between syndromes and error patterns. Generalizes prior toric code approach.",
      "importance_score": 59,
      "reasoning": "Important for quantum computing applications of ML. Generalizes prior work to arbitrary stabilizer codes.",
      "themes": [
        "Quantum Computing",
        "Error Correction",
        "Deep Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Universal Stabilizer Decoder framework for quantum error correction using symmetry to address non-uniqueness between syndromes and error patterns. Generalizes prior toric code approach.</p>",
      "content_html": "<p>arXiv:2601.15361v1 Announce Type: cross  Abstract: Quantum error correction is indispensable to achieving reliable quantum computation. When quantum information is encoded redundantly, a larger Hilbert space is constructed using multiple physical qubits, and the computation is performed within a designated subspace. When applying deep learning to the decoding of quantum error-correcting codes, a key challenge arises from the non-uniqueness between the syndrome measurements provided to the decoder and the corresponding error patterns that constitute the ground-truth labels. Building upon prior work that addressed this issue for the toric code by re-optimizing the decoder with respect to the symmetry inherent in the parity-check structure, we generalize this approach to arbitrary stabilizer codes. In our experiments, we employed multilayer perceptrons to approximate continuous functions that complement the syndrome measurements of the Color code and the Golay code. Using these models, we performed decoder re-optimization for each code. For the Color code, we achieved an improvement of approximately 0.8% in decoding accuracy at a physical error rate of 5%, while for the Golay code the accuracy increased by about 0.1%. Furthermore, from the evaluation of the geometric and algebraic structures in the continuous function approximation for each code, we showed that the design of generalized continuous functions is advantageous for learning the geometric structure inherent in the code. Our results also indicate that approximations that faithfully reproduce the code structure can have a significant impact on the effectiveness of reoptimization. This study demonstrates that the re-optimization technique previously shown to be effective for the Toric code can be generalized to address the challenge of label degeneracy that arises when applying deep learning to the decoding of stabilizer codes.</p>"
    },
    {
      "id": "aa2beda782f5",
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "content": "arXiv:2601.15549v1 Announce Type: new  Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "url": "http://arxiv.org/abs/2601.15549",
      "author": "Ryo Fujii, Hideo Saito, Ryo Hachiuma",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces VIOLA, a label-efficient framework for video in-context learning in MLLMs that combines minimal expert annotations with unlabeled data using density-uncertainty-weighted sampling.",
      "importance_score": 59,
      "reasoning": "Practical approach for adapting MLLMs to specialized video domains with limited labels. Addresses real deployment challenge but methodology is incremental.",
      "themes": [
        "Video Understanding",
        "In-Context Learning",
        "Multimodal AI",
        "Few-Shot Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces VIOLA, a label-efficient framework for video in-context learning in MLLMs that combines minimal expert annotations with unlabeled data using density-uncertainty-weighted sampling.</p>",
      "content_html": "<p>arXiv:2601.15549v1 Announce Type: new  Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.</p>"
    },
    {
      "id": "871ce7512792",
      "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion",
      "content": "arXiv:2601.15829v1 Announce Type: new  Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).",
      "url": "http://arxiv.org/abs/2601.15829",
      "author": "Yonghao Xu, Pedram Ghamisi, Qihao Weng",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces dataset distillation to remote sensing for the first time, training text-to-image diffusion model to condense large datasets into compact representative sets with discriminative prototype guidance.",
      "importance_score": 59,
      "reasoning": "Novel application of dataset distillation to remote sensing addressing storage/privacy concerns. First-of-kind for domain but builds on existing distillation methods.",
      "themes": [
        "Dataset Distillation",
        "Remote Sensing",
        "Diffusion Models",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces dataset distillation to remote sensing for the first time, training text-to-image diffusion model to condense large datasets into compact representative sets with discriminative prototype guidance.</p>",
      "content_html": "<p>arXiv:2601.15829v1 Announce Type: new  Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</p>"
    },
    {
      "id": "aabe65d49e00",
      "title": "Masked Modeling for Human Motion Recovery Under Occlusions",
      "content": "arXiv:2601.16079v1 Announce Type: new  Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.",
      "url": "http://arxiv.org/abs/2601.16079",
      "author": "Zhiyin Qian, Siwei Zhang, Bharat Lal Bhatnagar, Federica Bogo, Siyu Tang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents MoRo, an occlusion-robust generative masked modeling framework for human motion reconstruction that formulates motion recovery as video-conditioned token sampling, enabling fast inference.",
      "importance_score": 59,
      "reasoning": "Practical solution for human motion capture under occlusion. Generative approach offers robustness advantages with end-to-end simplicity.",
      "themes": [
        "Human Motion",
        "Masked Modeling",
        "Computer Vision",
        "Generative Models"
      ],
      "continuation": null,
      "summary_html": "<p>Presents MoRo, an occlusion-robust generative masked modeling framework for human motion reconstruction that formulates motion recovery as video-conditioned token sampling, enabling fast inference.</p>",
      "content_html": "<p>arXiv:2601.16079v1 Announce Type: new  Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.</p>"
    },
    {
      "id": "169ae60f968e",
      "title": "VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration",
      "content": "arXiv:2601.14440v1 Announce Type: new  Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.",
      "url": "http://arxiv.org/abs/2601.14440",
      "author": "Saeed Khaki, Ashudeep Singh, Nima Safaei, Kamal Ginotra",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces VisTIRA, a tool-integrated reasoning agent that addresses the 'modality gap' where VLMs perform worse on math problems presented as images vs text. Uses iterative decomposition into natural language rationales and executable Python.",
      "importance_score": 58,
      "reasoning": "Practical contribution addressing real VLM limitation. Methodology is solid but not particularly novel - combines existing techniques (tool use, chain-of-thought).",
      "themes": [
        "Vision-Language Models",
        "Mathematical Reasoning",
        "Tool Use"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces VisTIRA, a tool-integrated reasoning agent that addresses the 'modality gap' where VLMs perform worse on math problems presented as images vs text. Uses iterative decomposition into natural language rationales and executable Python.</p>",
      "content_html": "<p>arXiv:2601.14440v1 Announce Type: new  Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.</p>"
    },
    {
      "id": "6d023cd5754d",
      "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
      "content": "arXiv:2601.14702v1 Announce Type: new  Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
      "url": "http://arxiv.org/abs/2601.14702",
      "author": "Zecong Tang, Zixu Wang, Yifei Wang, Weitong Lian, Tianjian Gao, Haoran Li, Tengju Ru, Lingyi Meng, Zhejun Cui, Yichen Zhu, Qi Kang, Kaixuan Wang, Yu Zhang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces AutoDriDM, a decision-centric benchmark for VLMs in autonomous driving with 6,650 questions across Object, Scene, and Decision dimensions. Finds weak alignment between perception and decision-making capabilities.",
      "importance_score": 58,
      "reasoning": "Addresses gap in autonomous driving benchmarks. Finding of perception-decision misalignment is noteworthy but benchmark scope is limited.",
      "themes": [
        "Autonomous Driving",
        "Vision-Language Models",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces AutoDriDM, a decision-centric benchmark for VLMs in autonomous driving with 6,650 questions across Object, Scene, and Decision dimensions. Finds weak alignment between perception and decision-making capabilities.</p>",
      "content_html": "<p>arXiv:2601.14702v1 Announce Type: new  Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.</p>"
    },
    {
      "id": "d241cec3875a",
      "title": "CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments",
      "content": "arXiv:2601.14339v1 Announce Type: cross  Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.",
      "url": "http://arxiv.org/abs/2601.14339",
      "author": "Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces CityCube benchmark for cross-view spatial reasoning in VLMs in urban environments, integrating four viewpoint dynamics from vehicles, drones, and satellites with 5,022 annotated questions.",
      "importance_score": 58,
      "reasoning": "Useful benchmark addressing gap in urban spatial reasoning evaluation. Comprehensive viewpoint coverage.",
      "themes": [
        "Vision-Language Models",
        "Spatial Reasoning",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces CityCube benchmark for cross-view spatial reasoning in VLMs in urban environments, integrating four viewpoint dynamics from vehicles, drones, and satellites with 5,022 annotated questions.</p>",
      "content_html": "<p>arXiv:2601.14339v1 Announce Type: cross  Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</p>"
    },
    {
      "id": "98f9ff605ffd",
      "title": "How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness",
      "content": "arXiv:2601.14519v1 Announce Type: cross  Abstract: Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $\\kappa$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.",
      "url": "http://arxiv.org/abs/2601.14519",
      "author": "Giulio Rossolini",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces probabilistic metric linking adversarial and statistical robustness, parameterized by concentration factor interpolating between isotropic noise and adversarial direction.",
      "importance_score": 58,
      "reasoning": "Interesting theoretical analysis connecting adversarial and random perturbation robustness. Useful framework for understanding attack validity.",
      "themes": [
        "Adversarial Robustness",
        "Robustness Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces probabilistic metric linking adversarial and statistical robustness, parameterized by concentration factor interpolating between isotropic noise and adversarial direction.</p>",
      "content_html": "<p>arXiv:2601.14519v1 Announce Type: cross  Abstract: Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $\\kappa$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.</p>"
    },
    {
      "id": "18f20c165de4",
      "title": "Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning",
      "content": "arXiv:2601.14693v1 Announce Type: cross  Abstract: Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.",
      "url": "http://arxiv.org/abs/2601.14693",
      "author": "Jianwen Sun, Xinrui Li, Fuqing Li, Xiaoxuan Shen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "EGRL-SR frames symbolic regression as experience-driven goal-conditioned RL rather than error minimization. Addresses the ambiguous search direction problem when many expressions have similar error values.",
      "importance_score": 58,
      "reasoning": "Novel framing for symbolic regression that could improve interpretable ML, though empirical validation scope is limited.",
      "themes": [
        "Symbolic Regression",
        "Reinforcement Learning",
        "Interpretable ML"
      ],
      "continuation": null,
      "summary_html": "<p>EGRL-SR frames symbolic regression as experience-driven goal-conditioned RL rather than error minimization. Addresses the ambiguous search direction problem when many expressions have similar error values.</p>",
      "content_html": "<p>arXiv:2601.14693v1 Announce Type: cross  Abstract: Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.</p>"
    },
    {
      "id": "0a39dcc9421d",
      "title": "Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration",
      "content": "arXiv:2601.14982v1 Announce Type: cross  Abstract: Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.",
      "url": "http://arxiv.org/abs/2601.14982",
      "author": "David Ricardo Saavedra",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes unified framework for digital identity delegation enabling AI agents to exercise bounded, auditable authority across SSI and traditional identity systems with blockchain integration.",
      "importance_score": 58,
      "reasoning": "Important infrastructure work for agentic AI deployment, addresses real challenges in identity and authorization for AI agents.",
      "themes": [
        "AI Agents",
        "Identity",
        "Blockchain",
        "Infrastructure"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes unified framework for digital identity delegation enabling AI agents to exercise bounded, auditable authority across SSI and traditional identity systems with blockchain integration.</p>",
      "content_html": "<p>arXiv:2601.14982v1 Announce Type: cross  Abstract: Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.</p>"
    },
    {
      "id": "3fa1419b77e5",
      "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback",
      "content": "arXiv:2601.15188v1 Announce Type: cross  Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.",
      "url": "http://arxiv.org/abs/2601.15188",
      "author": "Stephan Wallraven, Tim K\\\"ohne, Hartmut Westenberger, Andreas Moser",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "Benchmarks LLMs on ABAP code generation with 180 tasks, showing powerful models achieve ~round 30% success with iterative compiler feedback improvement. First systematic ABAP code generation evaluation.",
      "importance_score": 58,
      "reasoning": "Valuable benchmark for enterprise code generation revealing significant capability gaps in SAP's domain-specific language.",
      "themes": [
        "Code Generation",
        "Evaluation",
        "Enterprise Software"
      ],
      "continuation": null,
      "summary_html": "<p>Benchmarks LLMs on ABAP code generation with 180 tasks, showing powerful models achieve ~round 30% success with iterative compiler feedback improvement. First systematic ABAP code generation evaluation.</p>",
      "content_html": "<p>arXiv:2601.15188v1 Announce Type: cross  Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.</p>"
    },
    {
      "id": "76a57a84d405",
      "title": "Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes",
      "content": "arXiv:2601.15423v1 Announce Type: new  Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.",
      "url": "http://arxiv.org/abs/2601.15423",
      "author": "Lorian Bannis",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Lattice is a hybrid prediction system using binary confidence gating to activate behavioral archetypes when confidence is high, falling back to baseline otherwise. Achieves strong improvements on recommendation benchmarks.",
      "importance_score": 58,
      "reasoning": "Solid empirical results on specific benchmarks but approach is somewhat narrow in scope. Unclear generalization beyond tested domains.",
      "themes": [
        "Sequential Prediction",
        "Recommendation Systems",
        "Hybrid Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Lattice is a hybrid prediction system using binary confidence gating to activate behavioral archetypes when confidence is high, falling back to baseline otherwise. Achieves strong improvements on recommendation benchmarks.</p>",
      "content_html": "<p>arXiv:2601.15423v1 Announce Type: new  Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p &lt; 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.</p>"
    },
    {
      "id": "1a27d67da005",
      "title": "Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework",
      "content": "arXiv:2601.15657v1 Announce Type: new  Abstract: Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.   This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.   By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.",
      "url": "http://arxiv.org/abs/2601.15657",
      "author": "Yinxi Tian, Changwu Huang, Ke Tang, Xin Yao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "SMSKD is a framework for sequentially integrating heterogeneous knowledge distillation methods. Uses frozen snapshots to prevent catastrophic forgetting between stages.",
      "importance_score": 58,
      "reasoning": "Practical framework for combining KD methods. Addresses real challenge but conceptually straightforward.",
      "themes": [
        "Knowledge Distillation",
        "Model Compression",
        "Transfer Learning"
      ],
      "continuation": null,
      "summary_html": "<p>SMSKD is a framework for sequentially integrating heterogeneous knowledge distillation methods. Uses frozen snapshots to prevent catastrophic forgetting between stages.</p>",
      "content_html": "<p>arXiv:2601.15657v1 Announce Type: new  Abstract: Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.   This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.   By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.</p>"
    },
    {
      "id": "ce1f3f2c7ae4",
      "title": "On the Intrinsic Dimensions of Data in Kernel Learning",
      "content": "arXiv:2601.16139v1 Announce Type: new  Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_\\rho$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $\\Omega$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $\\Omega$. Given a probability measure $\\mu$ on $\\Omega$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $\\phi \\to \\int_\\Omega K(\\cdot,x)\\phi(x)d\\mu(x)$. We show that, for a fixed domain $\\Omega$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $\\mu$ supported on $\\Omega$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\\frac{2+d_K}{2+2d_K} + \\epsilon})$ for any $\\epsilon > 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $\\mu$. For distributions close to uniform, we prove that $\\epsilon$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\\left(\\epsilon^{-d_\\rho}\\log\\frac{1}{\\epsilon}\\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_\\rho$, even though $d_K = d_\\rho$ provably holds on regular domains.",
      "url": "http://arxiv.org/abs/2601.16139",
      "author": "Rustem Takhanov",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Analyzes intrinsic dimensions in kernel learning through canonical metric dimension and effective dimension from Kolmogorov n-widths. Studies relationship with integral operator eigenvalues.",
      "importance_score": 58,
      "reasoning": "Solid theoretical analysis of kernel methods. Contributes to understanding of dimension in kernel learning.",
      "themes": [
        "Kernel Methods",
        "Learning Theory",
        "Dimensionality"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes intrinsic dimensions in kernel learning through canonical metric dimension and effective dimension from Kolmogorov n-widths. Studies relationship with integral operator eigenvalues.</p>",
      "content_html": "<p>arXiv:2601.16139v1 Announce Type: new  Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_\\rho$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $\\Omega$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $\\Omega$. Given a probability measure $\\mu$ on $\\Omega$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $\\phi \\to \\int_\\Omega K(\\cdot,x)\\phi(x)d\\mu(x)$. We show that, for a fixed domain $\\Omega$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $\\mu$ supported on $\\Omega$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\\frac{2+d_K}{2+2d_K} + \\epsilon})$ for any $\\epsilon &gt; 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $\\mu$. For distributions close to uniform, we prove that $\\epsilon$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\\left(\\epsilon^{-d_\\rho}\\log\\frac{1}{\\epsilon}\\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_\\rho$, even though $d_K = d_\\rho$ provably holds on regular domains.</p>"
    },
    {
      "id": "3f967c093a83",
      "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
      "content": "arXiv:2601.15676v1 Announce Type: cross  Abstract: Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
      "url": "http://arxiv.org/abs/2601.15676",
      "author": "Hengfan Zhang, Yueqian Lin, Hai Helen Li, Yiran Chen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "CoFi-Agent is a hybrid architecture for edge Audio-LLMs that performs fast local perception and conditional cloud refinement only when uncertainty is detected. Balances depth and efficiency.",
      "importance_score": 58,
      "reasoning": "Practical efficiency-focused architecture for edge deployment. Addresses real deployment constraints for multimodal LLMs.",
      "themes": [
        "Edge AI",
        "Audio-Language Models",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>CoFi-Agent is a hybrid architecture for edge Audio-LLMs that performs fast local perception and conditional cloud refinement only when uncertainty is detected. Balances depth and efficiency.</p>",
      "content_html": "<p>arXiv:2601.15676v1 Announce Type: cross  Abstract: Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.</p>"
    },
    {
      "id": "9bd183c7ed63",
      "title": "On damage of interpolation to adversarial robustness in regression",
      "content": "arXiv:2601.16070v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.",
      "url": "http://arxiv.org/abs/2601.16070",
      "author": "Jingfu Peng, Yuhong Yang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Investigates adversarial robustness of interpolating estimators in regression. Shows interpolation may not escape suboptimal performance under X-attacks even when it achieves minimax rates for standard loss.",
      "importance_score": 58,
      "reasoning": "Important theoretical insight connecting interpolation and adversarial robustness. Relevant for understanding DNN vulnerabilities.",
      "themes": [
        "Adversarial Robustness",
        "Interpolation",
        "Deep Learning Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates adversarial robustness of interpolating estimators in regression. Shows interpolation may not escape suboptimal performance under X-attacks even when it achieves minimax rates for standard loss.</p>",
      "content_html": "<p>arXiv:2601.16070v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.</p>"
    },
    {
      "id": "7096679b16b7",
      "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add",
      "content": "arXiv:2601.16120v1 Announce Type: cross  Abstract: Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?   We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.",
      "url": "http://arxiv.org/abs/2601.16120",
      "author": "Zhengchi Ma, Anru R. Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Develops unified theoretical framework for synthetic augmentation in imbalanced learning. Shows when synthetic data helps vs hurts and provides guidance on quantity to generate.",
      "importance_score": 58,
      "reasoning": "Valuable theoretical analysis addressing practical questions about data augmentation. Useful guidelines for practitioners.",
      "themes": [
        "Imbalanced Learning",
        "Data Augmentation",
        "Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Develops unified theoretical framework for synthetic augmentation in imbalanced learning. Shows when synthetic data helps vs hurts and provides guidance on quantity to generate.</p>",
      "content_html": "<p>arXiv:2601.16120v1 Announce Type: cross  Abstract: Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?   We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.</p>"
    },
    {
      "id": "c7721c052bce",
      "title": "No Reliable Evidence of Self-Reported Sentience in Small Large Language Models",
      "content": "arXiv:2601.15334v1 Announce Type: new  Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.",
      "url": "http://arxiv.org/abs/2601.15334",
      "author": "Caspar Kaiser, Sean Enderby",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Tests whether LLMs believe themselves sentient using internal activation classifiers across Qwen, Llama, and GPT-OSS families. Models consistently deny sentience; classifiers find no evidence of hidden contrary beliefs.",
      "importance_score": 58,
      "reasoning": "Interesting interpretability study on self-models. Uses multiple model families and rigorous methodology.",
      "themes": [
        "Interpretability",
        "LLM Beliefs",
        "Consciousness"
      ],
      "continuation": null,
      "summary_html": "<p>Tests whether LLMs believe themselves sentient using internal activation classifiers across Qwen, Llama, and GPT-OSS families. Models consistently deny sentience; classifiers find no evidence of hidden contrary beliefs.</p>",
      "content_html": "<p>arXiv:2601.15334v1 Announce Type: new  Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.</p>"
    },
    {
      "id": "0caf19198ed0",
      "title": "Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts",
      "content": "arXiv:2601.15479v1 Announce Type: new  Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \\textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \\textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).   The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($\\kappa \\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}",
      "url": "http://arxiv.org/abs/2601.15479",
      "author": "Sydney Anuyah, Sneha Shajee-Mohan, Ankit-Singh Chauhan, Sunandan Chakraborty",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Benchmarks 13 open-source LLMs on pairwise causal discovery across 12 datasets. Tests causal detection and extraction with various prompting methods, revealing major deficiencies.",
      "importance_score": 58,
      "reasoning": "Systematic causal reasoning evaluation. Important finding about LLM limitations in fundamental reasoning.",
      "themes": [
        "Causal Discovery",
        "Benchmarking",
        "LLM Reasoning"
      ],
      "continuation": null,
      "summary_html": "<p>Benchmarks 13 open-source LLMs on pairwise causal discovery across 12 datasets. Tests causal detection and extraction with various prompting methods, revealing major deficiencies.</p>",
      "content_html": "<p>arXiv:2601.15479v1 Announce Type: new  Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \\textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \\textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).   The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($\\kappa \\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}</p>"
    },
    {
      "id": "642dd933b5ed",
      "title": "Common to Whom? Regional Cultural Commonsense and LLM Bias in India",
      "content": "arXiv:2601.15550v1 Announce Type: new  Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the \"default\" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.",
      "url": "http://arxiv.org/abs/2601.15550",
      "author": "Sangmitra Madhusudan, Trush Shashank More, Steph Buongiorno, Renata Dividino, Jad Kabbara, Ali Emami",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Indica benchmark tests LLM cultural commonsense across 5 Indian regions with 515 questions. Only 39.4% of questions get agreement across all regions, showing cultural variance.",
      "importance_score": 58,
      "reasoning": "Valuable benchmark challenging monolithic nation assumptions. Important for culturally-aware AI.",
      "themes": [
        "Cultural Bias",
        "Regional AI",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Indica benchmark tests LLM cultural commonsense across 5 Indian regions with 515 questions. Only 39.4% of questions get agreement across all regions, showing cultural variance.</p>",
      "content_html": "<p>arXiv:2601.15550v1 Announce Type: new  Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the \"default\" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.</p>"
    },
    {
      "id": "96eea4512b3a",
      "title": "Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs",
      "content": "arXiv:2601.15755v1 Announce Type: new  Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.",
      "url": "http://arxiv.org/abs/2601.15755",
      "author": "Tristan Williams, Franziska Weeber, Sebastian Pad\\'o, Alan Akbik",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes framework evaluating demographic-aligned LLMs through multivariate correlation patterns, not just marginal distributions. Compares persona prompting vs demographic fine-tuning.",
      "importance_score": 58,
      "reasoning": "Important methodological contribution for alignment evaluation. Goes beyond surface-level metrics.",
      "themes": [
        "Model Alignment",
        "Demographics",
        "Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes framework evaluating demographic-aligned LLMs through multivariate correlation patterns, not just marginal distributions. Compares persona prompting vs demographic fine-tuning.</p>",
      "content_html": "<p>arXiv:2601.15755v1 Announce Type: new  Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.</p>"
    },
    {
      "id": "92ab87aa8352",
      "title": "Do people expect different behavior from large language models acting on their behalf? Evidence from norm elicitations in two canonical economic games",
      "content": "arXiv:2601.15312v1 Announce Type: cross  Abstract: While delegating tasks to large language models (LLMs) can save people time, there is growing evidence that offloading tasks to such models produces social costs. We use behavior in two canonical economic games to study whether people have different expectations when decisions are made by LLMs acting on their behalf instead of themselves. More specifically, we study the social appropriateness of a spectrum of possible behaviors: when LLMs divide resources on our behalf (Dictator Game and Ultimatum Game) and when they monitor the fairness of splits of resources (Ultimatum Game). We use the Krupka-Weber norm elicitation task to detect shifts in social appropriateness ratings. Results of two pre-registered and incentivized experimental studies using representative samples from the UK and US (N = 2,658) show three key findings. First, people find that offers from machines - when no acceptance is necessary - are judged to be less appropriate than when they come from humans, although there is no shift in the modal response. Second - when acceptance is necessary - it is more appropriate for a person to reject offers from machines than from humans. Third, receiving a rejection of an offer from a machine is no less socially appropriate than receiving the same rejection from a human. Overall, these results suggest that people apply different norms for machines deciding on how to split resources but are not opposed to machines enforcing the norms. The findings are consistent with offers made by machines now being viewed as having both a cognitive and emotional component.",
      "url": "http://arxiv.org/abs/2601.15312",
      "author": "Pawe{\\l} Niszczota, Elia Antoniou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.GT"
      ],
      "summary": "Studies whether people have different social expectations when LLMs make decisions on their behalf in economic games (Dictator Game, Ultimatum Game). Uses norm elicitation tasks to detect shifts in social appropriateness ratings for AI-delegated decisions.",
      "importance_score": 58,
      "reasoning": "Interesting human-AI interaction research exploring delegation dynamics. Well-designed experimental methodology but findings may have limited immediate technical applications.",
      "themes": [
        "Human-AI Interaction",
        "AI Ethics",
        "Social Norms"
      ],
      "continuation": null,
      "summary_html": "<p>Studies whether people have different social expectations when LLMs make decisions on their behalf in economic games (Dictator Game, Ultimatum Game). Uses norm elicitation tasks to detect shifts in social appropriateness ratings for AI-delegated decisions.</p>",
      "content_html": "<p>arXiv:2601.15312v1 Announce Type: cross  Abstract: While delegating tasks to large language models (LLMs) can save people time, there is growing evidence that offloading tasks to such models produces social costs. We use behavior in two canonical economic games to study whether people have different expectations when decisions are made by LLMs acting on their behalf instead of themselves. More specifically, we study the social appropriateness of a spectrum of possible behaviors: when LLMs divide resources on our behalf (Dictator Game and Ultimatum Game) and when they monitor the fairness of splits of resources (Ultimatum Game). We use the Krupka-Weber norm elicitation task to detect shifts in social appropriateness ratings. Results of two pre-registered and incentivized experimental studies using representative samples from the UK and US (N = 2,658) show three key findings. First, people find that offers from machines - when no acceptance is necessary - are judged to be less appropriate than when they come from humans, although there is no shift in the modal response. Second - when acceptance is necessary - it is more appropriate for a person to reject offers from machines than from humans. Third, receiving a rejection of an offer from a machine is no less socially appropriate than receiving the same rejection from a human. Overall, these results suggest that people apply different norms for machines deciding on how to split resources but are not opposed to machines enforcing the norms. The findings are consistent with offers made by machines now being viewed as having both a cognitive and emotional component.</p>"
    },
    {
      "id": "305e32abcd0d",
      "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation",
      "content": "arXiv:2601.15487v1 Announce Type: cross  Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.",
      "url": "http://arxiv.org/abs/2601.15487",
      "author": "Chandan Kumar Sahu, Premith Kumar Chilukuri, Matthew Hetrich",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces MiRAGE, a multi-agent framework that generates verified, multimodal, multi-hop QA datasets for RAG evaluation in specialized enterprise domains. Uses a swarm of specialized agents including recursive content analyzer and QA generators.",
      "importance_score": 58,
      "reasoning": "Addresses important gap in RAG evaluation for enterprise applications. Multi-agent approach is interesting but the framework complexity may limit adoption.",
      "themes": [
        "RAG Systems",
        "Evaluation & Benchmarks",
        "AI Agents",
        "Multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces MiRAGE, a multi-agent framework that generates verified, multimodal, multi-hop QA datasets for RAG evaluation in specialized enterprise domains. Uses a swarm of specialized agents including recursive content analyzer and QA generators.</p>",
      "content_html": "<p>arXiv:2601.15487v1 Announce Type: cross  Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (&gt;2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.</p>"
    },
    {
      "id": "1929e72431f1",
      "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling",
      "content": "arXiv:2601.15664v1 Announce Type: new  Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.",
      "url": "http://arxiv.org/abs/2601.15664",
      "author": "Hongyang Wei, Hongbo Liu, Zidong Wang, Yi Peng, Baixin Xu, Size Wu, Xuying Zhang, Xianglong He, Zexiang Liu, Peiyu Wang, Xuchen Song, Yangguang Li, Yang Liu, Yahui Zhou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents Skywork UniPic 3.0, a unified multimodal framework for multi-image composition focusing on Human-Object Interaction (HOI) tasks, integrating single-image editing, multi-image fusion, and video generation.",
      "importance_score": 58,
      "reasoning": "Industry release with practical capabilities. Addresses community-desired tasks but limited technical novelty disclosure.",
      "themes": [
        "Image Composition",
        "Multimodal AI",
        "Content Creation"
      ],
      "continuation": null,
      "summary_html": "<p>Presents Skywork UniPic 3.0, a unified multimodal framework for multi-image composition focusing on Human-Object Interaction (HOI) tasks, integrating single-image editing, multi-image fusion, and video generation.</p>",
      "content_html": "<p>arXiv:2601.15664v1 Announce Type: new  Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.</p>"
    },
    {
      "id": "e98699c3384b",
      "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
      "content": "arXiv:2601.15766v1 Announce Type: new  Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
      "url": "http://arxiv.org/abs/2601.15766",
      "author": "Yuhan Chen, Ying Fang, Guofa Li, Wenxuan Yu, Yicui Shi, Jingrui Zhang, Kefei Qian, Wenbo Chu, Keqiang Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes LL-GaussianMap, the first unsupervised framework incorporating 2D Gaussian Splatting for low-light image enhancement, leveraging explicit scene representation for structural fitting.",
      "importance_score": 58,
      "reasoning": "Novel application of 2DGS to low-light enhancement - first of its kind. Interesting structural representation approach but early exploration.",
      "themes": [
        "Low-Light Enhancement",
        "Gaussian Splatting",
        "Image Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes LL-GaussianMap, the first unsupervised framework incorporating 2D Gaussian Splatting for low-light image enhancement, leveraging explicit scene representation for structural fitting.</p>",
      "content_html": "<p>arXiv:2601.15766v1 Announce Type: new  Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.</p>"
    },
    {
      "id": "db5bb150fb6a",
      "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
      "content": "arXiv:2601.16024v1 Announce Type: new  Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&amp;E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&amp;E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.",
      "url": "http://arxiv.org/abs/2601.16024",
      "author": "Rongze Ma, Mengkang Lu, Zhenyu Xiang, Yongsheng Pan, Yicheng Wu, Qingjie Zeng, Yong Xia",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes PAINT, a visual autoregressive framework for virtual immunohistochemistry synthesis from H&E images with pathology-aware next-scale transformation addressing semantic inconsistencies.",
      "importance_score": 58,
      "reasoning": "Important medical imaging application enabling cost-effective molecular analysis. Novel autoregressive approach for cross-modal synthesis.",
      "themes": [
        "Medical Imaging",
        "Pathology",
        "Image Synthesis",
        "Autoregressive Models"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes PAINT, a visual autoregressive framework for virtual immunohistochemistry synthesis from H&amp;E images with pathology-aware next-scale transformation addressing semantic inconsistencies.</p>",
      "content_html": "<p>arXiv:2601.16024v1 Announce Type: new  Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&amp;E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&amp;E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.</p>"
    },
    {
      "id": "c1aea5d849b8",
      "title": "Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation",
      "content": "arXiv:2601.15598v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.",
      "url": "http://arxiv.org/abs/2601.15598",
      "author": "Boxuan Zhang, Jiaxin Wang, Zhen Xu, Kuan Tao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Neural and Evolutionary Computing)",
      "source_type": "arxiv",
      "tags": [
        "cs.NE"
      ],
      "summary": "Introduces Complemented Ternary Spiking Neurons (CTSN) that address information loss and gradient vanishing in ternary spiking neural networks through a learnable complemental term and membrane potential aggregation.",
      "importance_score": 58,
      "reasoning": "Novel contribution to neuromorphic computing, addresses real limitations in spiking neural networks.",
      "themes": [
        "Neuromorphic Computing",
        "Spiking Neural Networks",
        "Biological Plausibility"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Complemented Ternary Spiking Neurons (CTSN) that address information loss and gradient vanishing in ternary spiking neural networks through a learnable complemental term and membrane potential aggregation.</p>",
      "content_html": "<p>arXiv:2601.15598v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.</p>"
    },
    {
      "id": "ae8696d4e4a5",
      "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning",
      "content": "arXiv:2601.15614v1 Announce Type: new  Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.",
      "url": "http://arxiv.org/abs/2601.15614",
      "author": "Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang, Lin Zhao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "AION proposes an end-to-end dual-policy RL framework for vision-based aerial object-goal navigation, decoupling exploration and goal-reaching behaviors without external localization.",
      "importance_score": 58,
      "reasoning": "Novel approach to aerial navigation with practical constraints, extends ObjectNav to 3D.",
      "themes": [
        "Aerial Robotics",
        "Navigation",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>AION proposes an end-to-end dual-policy RL framework for vision-based aerial object-goal navigation, decoupling exploration and goal-reaching behaviors without external localization.</p>",
      "content_html": "<p>arXiv:2601.15614v1 Announce Type: new  Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.</p>"
    },
    {
      "id": "9c2fef54ae81",
      "title": "Pushing the limits of unconstrained machine-learned interatomic potentials",
      "content": "arXiv:2601.16195v1 Announce Type: cross  Abstract: Machine-learned interatomic potentials (MLIPs) are increasingly used to replace computationally demanding electronic-structure calculations to model matter at the atomic scale. The most commonly used model architectures are constrained to fulfill a number of physical laws exactly, from geometric symmetries to energy conservation. Evidence is mounting that relaxing some of these constraints can be beneficial to the efficiency and (somewhat surprisingly) accuracy of MLIPs, even though care should be taken to avoid qualitative failures associated with the breaking of physical symmetries. Given the recent trend of \\emph{scaling up} models to larger numbers of parameters and training samples, a very important question is how unconstrained MLIPs behave in this limit. Here we investigate this issue, showing that -- when trained on large datasets -- unconstrained models can be superior in accuracy and speed when compared to physically constrained models. We assess these models both in terms of benchmark accuracy and in terms of usability in practical scenarios, focusing on static simulation workflows such as geometry optimization and lattice dynamics. We conclude that accurate unconstrained models can be applied with confidence, especially since simple inference-time modifications can be used to recover observables that are consistent with the relevant physical symmetries.",
      "url": "http://arxiv.org/abs/2601.16195",
      "author": "Filippo Bigi, Paolo Pegolo, Arslan Mazitov, Michele Ceriotti",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning (Statistics))",
      "source_type": "arxiv",
      "tags": [
        "physics.chem-ph"
      ],
      "summary": "Investigates scaling of unconstrained machine-learned interatomic potentials (MLIPs) that don't enforce physical symmetries exactly, finding they can match constrained models with sufficient scale.",
      "importance_score": 58,
      "reasoning": "Important for understanding when physical constraints matter in ML models, relevant to scaling discussions.",
      "themes": [
        "Scientific ML",
        "Physical Constraints",
        "Scaling"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates scaling of unconstrained machine-learned interatomic potentials (MLIPs) that don't enforce physical symmetries exactly, finding they can match constrained models with sufficient scale.</p>",
      "content_html": "<p>arXiv:2601.16195v1 Announce Type: cross  Abstract: Machine-learned interatomic potentials (MLIPs) are increasingly used to replace computationally demanding electronic-structure calculations to model matter at the atomic scale. The most commonly used model architectures are constrained to fulfill a number of physical laws exactly, from geometric symmetries to energy conservation. Evidence is mounting that relaxing some of these constraints can be beneficial to the efficiency and (somewhat surprisingly) accuracy of MLIPs, even though care should be taken to avoid qualitative failures associated with the breaking of physical symmetries. Given the recent trend of \\emph{scaling up} models to larger numbers of parameters and training samples, a very important question is how unconstrained MLIPs behave in this limit. Here we investigate this issue, showing that -- when trained on large datasets -- unconstrained models can be superior in accuracy and speed when compared to physically constrained models. We assess these models both in terms of benchmark accuracy and in terms of usability in practical scenarios, focusing on static simulation workflows such as geometry optimization and lattice dynamics. We conclude that accurate unconstrained models can be applied with confidence, especially since simple inference-time modifications can be used to recover observables that are consistent with the relevant physical symmetries.</p>"
    },
    {
      "id": "54c466e0c232",
      "title": "Quantum Super-resolution by Adaptive Non-local Observables",
      "content": "arXiv:2601.14433v1 Announce Type: cross  Abstract: Super-resolution (SR) seeks to reconstruct high-resolution (HR) data from low-resolution (LR) observations. Classical deep learning methods have advanced SR substantially, but require increasingly deeper networks, large datasets, and heavy computation to capture fine-grained correlations. In this work, we present the \\emph{first study} to investigate quantum circuits for SR. We propose a framework based on Variational Quantum Circuits (VQCs) with \\emph{Adaptive Non-Local Observable} (ANO) measurements. Unlike conventional VQCs with fixed Pauli readouts, ANO introduces trainable multi-qubit Hermitian observables, allowing the measurement process to adapt during training. This design leverages the high-dimensional Hilbert space of quantum systems and the representational structure provided by entanglement and superposition. Experiments demonstrate that ANO-VQCs achieve up to five-fold higher resolution with a relatively small model size, suggesting a promising new direction at the intersection of quantum machine learning and super-resolution.",
      "url": "http://arxiv.org/abs/2601.14433",
      "author": "Hsin-Yi Lin, Huan-Hsin Tseng, Samuel Yen-Chi Chen, Shinjae Yoo",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "quant-ph"
      ],
      "summary": "First study investigating quantum circuits for super-resolution, proposing VQCs with Adaptive Non-Local Observable measurements that allow trainable multi-qubit Hermitian observables.",
      "importance_score": 57,
      "reasoning": "Novel quantum ML application but early-stage research. Interesting theoretical contribution on adaptive measurements.",
      "themes": [
        "Quantum Machine Learning",
        "Super-Resolution",
        "Computer Vision"
      ],
      "continuation": null,
      "summary_html": "<p>First study investigating quantum circuits for super-resolution, proposing VQCs with Adaptive Non-Local Observable measurements that allow trainable multi-qubit Hermitian observables.</p>",
      "content_html": "<p>arXiv:2601.14433v1 Announce Type: cross  Abstract: Super-resolution (SR) seeks to reconstruct high-resolution (HR) data from low-resolution (LR) observations. Classical deep learning methods have advanced SR substantially, but require increasingly deeper networks, large datasets, and heavy computation to capture fine-grained correlations. In this work, we present the \\emph{first study} to investigate quantum circuits for SR. We propose a framework based on Variational Quantum Circuits (VQCs) with \\emph{Adaptive Non-Local Observable} (ANO) measurements. Unlike conventional VQCs with fixed Pauli readouts, ANO introduces trainable multi-qubit Hermitian observables, allowing the measurement process to adapt during training. This design leverages the high-dimensional Hilbert space of quantum systems and the representational structure provided by entanglement and superposition. Experiments demonstrate that ANO-VQCs achieve up to five-fold higher resolution with a relatively small model size, suggesting a promising new direction at the intersection of quantum machine learning and super-resolution.</p>"
    },
    {
      "id": "cbcaa0bdfeb0",
      "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling",
      "content": "arXiv:2601.14786v1 Announce Type: cross  Abstract: Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.",
      "url": "http://arxiv.org/abs/2601.14786",
      "author": "Wei-Jaw Lee, Fang-Chih Hsieh, Xuanjun Chen, Fang-Duo Tsai, Yi-Hsuan Yang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Explores state-space models as efficient backbone for text-to-music generation, comparing SSM variants and SSM/diffusion hybrids at 300M parameter scale matching MusicGen-small.",
      "importance_score": 57,
      "reasoning": "Practical exploration of SSMs for audio generation with efficiency focus, relevant to efficient generative model design.",
      "themes": [
        "Music Generation",
        "State-Space Models",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Explores state-space models as efficient backbone for text-to-music generation, comparing SSM variants and SSM/diffusion hybrids at 300M parameter scale matching MusicGen-small.</p>",
      "content_html": "<p>arXiv:2601.14786v1 Announce Type: cross  Abstract: Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/.</p>"
    },
    {
      "id": "9bd995497eb2",
      "title": "BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation",
      "content": "arXiv:2601.15123v1 Announce Type: cross  Abstract: Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.",
      "url": "http://arxiv.org/abs/2601.15123",
      "author": "Andrey Moskalenko, Danil Kuznetsov, Irina Dudko, Anastasiia Iasakova, Nikita Boldyrev, Denis Shepelev, Andrei Spiridonov, Andrey Kuznetsov, Vlad Shakhuro",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "BREPS evaluates robustness of promptable segmentation models like SAM to natural bounding box variations through controlled user study with thousands of real annotations.",
      "importance_score": 57,
      "reasoning": "Useful evaluation contribution addressing gap between synthetic prompts and real-world usage patterns.",
      "themes": [
        "Segmentation",
        "Evaluation",
        "Robustness"
      ],
      "continuation": null,
      "summary_html": "<p>BREPS evaluates robustness of promptable segmentation models like SAM to natural bounding box variations through controlled user study with thousands of real annotations.</p>",
      "content_html": "<p>arXiv:2601.15123v1 Announce Type: cross  Abstract: Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.</p>"
    },
    {
      "id": "b6a3f0e9660d",
      "title": "Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance Portfolio Optimization",
      "content": "arXiv:2601.15597v1 Announce Type: new  Abstract: This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard & Poor's 500 Index (S&amp;P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.",
      "url": "http://arxiv.org/abs/2601.15597",
      "author": "Liusha Yang, Siqi Zhao, Shuqi Chai",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Neural network-based nonlinear shrinkage estimator for covariance matrices in portfolio optimization. Uses transformer to learn eigenvalue shrinkage function with portfolio risk as loss.",
      "importance_score": 57,
      "reasoning": "Novel application of transformers to portfolio optimization. Specific to finance domain.",
      "themes": [
        "Financial ML",
        "Portfolio Optimization",
        "Covariance Estimation"
      ],
      "continuation": null,
      "summary_html": "<p>Neural network-based nonlinear shrinkage estimator for covariance matrices in portfolio optimization. Uses transformer to learn eigenvalue shrinkage function with portfolio risk as loss.</p>",
      "content_html": "<p>arXiv:2601.15597v1 Announce Type: new  Abstract: This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard &amp; Poor's 500 Index (S&amp;P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.</p>"
    },
    {
      "id": "3018b3dc701b",
      "title": "Partially Lazy Gradient Descent for Smoothed Online Learning",
      "content": "arXiv:2601.15984v1 Announce Type: new  Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $\\Theta(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.",
      "url": "http://arxiv.org/abs/2601.15984",
      "author": "Naram Mhaisen, George Iosifidis",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "k-lazyGD bridges greedy OGD and lazy GD for smoothed online convex optimization. Proves optimal dynamic regret can be achieved with laziness up to certain bounds related to comparator path length.",
      "importance_score": 57,
      "reasoning": "Solid theoretical contribution to online learning. Interesting connection between laziness and regret bounds.",
      "themes": [
        "Online Learning",
        "Convex Optimization",
        "Learning Theory"
      ],
      "continuation": null,
      "summary_html": "<p>k-lazyGD bridges greedy OGD and lazy GD for smoothed online convex optimization. Proves optimal dynamic regret can be achieved with laziness up to certain bounds related to comparator path length.</p>",
      "content_html": "<p>arXiv:2601.15984v1 Announce Type: new  Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $\\Theta(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.</p>"
    },
    {
      "id": "be598dca2ea9",
      "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events",
      "content": "arXiv:2601.15475v1 Announce Type: new  Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.",
      "url": "http://arxiv.org/abs/2601.15475",
      "author": "Yunshan Qi, Lin Zhu, Nan Bao, Yifan Zhao, Jia Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes unified NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and events, modeling sensor-physics mismatches between camera output and physical radiance.",
      "importance_score": 57,
      "reasoning": "Novel integration of events with NeRF for HDR synthesis. Principled physics-based modeling but relatively specialized application combining multiple challenging aspects.",
      "themes": [
        "Neural Rendering",
        "HDR Imaging",
        "Event Cameras",
        "3D Reconstruction"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes unified NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and events, modeling sensor-physics mismatches between camera output and physical radiance.</p>",
      "content_html": "<p>arXiv:2601.15475v1 Announce Type: new  Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.</p>"
    },
    {
      "id": "763d1be04862",
      "title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery",
      "content": "arXiv:2601.15918v1 Announce Type: new  Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.   Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.   Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.   Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.",
      "url": "http://arxiv.org/abs/2601.15918",
      "author": "Valery Fischer, Alan Magdaleno, Anna-Katharina Calek, Nicola Cavalcanti, Nathan Hoffman, Christoph Germann, Joschua W\\\"uthrich, Max Kr\\\"ahenmann, Mazda Farshad, Philipp F\\\"urnstahl, Lilian Calvet",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes robust multi-view pipeline for 3D hand pose estimation in surgical settings using off-the-shelf pretrained models without domain-specific fine-tuning, along with new benchmark dataset.",
      "importance_score": 57,
      "reasoning": "Practical contribution for surgical AI with new dataset. Training-free approach is practical but limited novelty in methodology.",
      "themes": [
        "Surgical AI",
        "Hand Pose Estimation",
        "3D Reconstruction",
        "Datasets"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes robust multi-view pipeline for 3D hand pose estimation in surgical settings using off-the-shelf pretrained models without domain-specific fine-tuning, along with new benchmark dataset.</p>",
      "content_html": "<p>arXiv:2601.15918v1 Announce Type: new  Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.   Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.   Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.   Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.</p>"
    },
    {
      "id": "1d3adb5f74a0",
      "title": "If You Want Coherence, Orchestrate a Team of Rivals: Multi-Agent Models of Organizational Intelligence",
      "content": "arXiv:2601.14351v1 Announce Type: cross  Abstract: AI Agents can perform complex operations at great speed, but just like all the humans we have ever hired, their intelligence remains fallible. Miscommunications aren't noticed, systemic biases have no counter-action, and inner monologues are rarely written down.   We did not come to fire them for their mistakes, but to hire them and provide a safe productive working environment. We posit that we can reuse a common corporate organizational structure: teams of independent AI agents with strict role boundaries can work with common goals, but opposing incentives. Multiple models serving as a team of rivals can catch and minimize errors within the final product at a small cost to the velocity of actions. In this paper we demonstrate that we can achieve reliability without acquiring perfect components, but through careful orchestration of imperfect ones.   This paper describes the architecture of such a system in practice: specialized agent teams (planners, executors, critics, experts), organized into an organization with clear goals, coordinated through a remote code executor that keeps data transformations and tool invocations separate from reasoning models. Rather than agents directly calling tools and ingesting full responses, they write code that executes remotely; only relevant summaries return to agent context. By preventing raw data and tool outputs from contaminating context windows, the system maintains clean separation between perception (brains that plan and reason) and execution (hands that perform heavy data transformations and API calls). We demonstrate the approach achieves over 90% internal error interception prior to user exposure while maintaining acceptable latency tradeoffs. A survey from our traces shows that we only trade off cost and latency to achieve correctness and incrementally expand capabilities without impacting existing ones.",
      "url": "http://arxiv.org/abs/2601.14351",
      "author": "Gopal Vijayaraghavan, Prasanth Jayachandran, Arun Murthy, Sunil Govindan, Vivek Subramanian",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.MA"
      ],
      "summary": "Proposes multi-agent models using teams of independent AI agents with strict role boundaries and opposing incentives (team of rivals) to catch and minimize errors.",
      "importance_score": 56,
      "reasoning": "Interesting organizational structure analogy for multi-agent error reduction. Practical framework but limited novel contribution.",
      "themes": [
        "Multi-Agent Systems",
        "Error Correction",
        "Organizational AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes multi-agent models using teams of independent AI agents with strict role boundaries and opposing incentives (team of rivals) to catch and minimize errors.</p>",
      "content_html": "<p>arXiv:2601.14351v1 Announce Type: cross  Abstract: AI Agents can perform complex operations at great speed, but just like all the humans we have ever hired, their intelligence remains fallible. Miscommunications aren't noticed, systemic biases have no counter-action, and inner monologues are rarely written down.   We did not come to fire them for their mistakes, but to hire them and provide a safe productive working environment. We posit that we can reuse a common corporate organizational structure: teams of independent AI agents with strict role boundaries can work with common goals, but opposing incentives. Multiple models serving as a team of rivals can catch and minimize errors within the final product at a small cost to the velocity of actions. In this paper we demonstrate that we can achieve reliability without acquiring perfect components, but through careful orchestration of imperfect ones.   This paper describes the architecture of such a system in practice: specialized agent teams (planners, executors, critics, experts), organized into an organization with clear goals, coordinated through a remote code executor that keeps data transformations and tool invocations separate from reasoning models. Rather than agents directly calling tools and ingesting full responses, they write code that executes remotely; only relevant summaries return to agent context. By preventing raw data and tool outputs from contaminating context windows, the system maintains clean separation between perception (brains that plan and reason) and execution (hands that perform heavy data transformations and API calls). We demonstrate the approach achieves over 90% internal error interception prior to user exposure while maintaining acceptable latency tradeoffs. A survey from our traces shows that we only trade off cost and latency to achieve correctness and incrementally expand capabilities without impacting existing ones.</p>"
    },
    {
      "id": "51b3e601856a",
      "title": "AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering",
      "content": "arXiv:2601.14728v1 Announce Type: cross  Abstract: Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs.",
      "url": "http://arxiv.org/abs/2601.14728",
      "author": "Chun-Yi Kuan, Kai-Wei Chang, Hung-yi Lee",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.AS"
      ],
      "summary": "AQAScore evaluates text-to-audio generation by computing log-probability of 'Yes' answers to semantic verification questions using audio-aware LLMs. Backbone-agnostic and captures compositional reasoning limitations.",
      "importance_score": 56,
      "reasoning": "Novel evaluation framework addressing limitations of embedding-based metrics, useful for audio generation field.",
      "themes": [
        "Audio Generation",
        "Evaluation",
        "Audio-Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>AQAScore evaluates text-to-audio generation by computing log-probability of 'Yes' answers to semantic verification questions using audio-aware LLMs. Backbone-agnostic and captures compositional reasoning limitations.</p>",
      "content_html": "<p>arXiv:2601.14728v1 Announce Type: cross  Abstract: Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs.</p>"
    },
    {
      "id": "188b437c21a0",
      "title": "InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement",
      "content": "arXiv:2601.14968v1 Announce Type: cross  Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.",
      "url": "http://arxiv.org/abs/2601.14968",
      "author": "Mingyue Cheng, Xiaoyu Tao, Huajian Zhang, Qi Liu, Enhong Chen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "InstructTime++ reformulates time series classification as multimodal generative task, treating sequences, text features, and instructions as multimodal inputs with class labels as text outputs.",
      "importance_score": 56,
      "reasoning": "Novel framing of time series classification leveraging language models, interesting methodological contribution.",
      "themes": [
        "Time Series",
        "Multimodal Learning",
        "Classification"
      ],
      "continuation": null,
      "summary_html": "<p>InstructTime++ reformulates time series classification as multimodal generative task, treating sequences, text features, and instructions as multimodal inputs with class labels as text outputs.</p>",
      "content_html": "<p>arXiv:2601.14968v1 Announce Type: cross  Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.</p>"
    },
    {
      "id": "75000c13f5b5",
      "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks",
      "content": "arXiv:2601.15164v1 Announce Type: cross  Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.",
      "url": "http://arxiv.org/abs/2601.15164",
      "author": "Yaru Liu, Ao-bo Wang, Nanyang Ye",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "V-CAGE framework generates robust manipulation datasets with context-aware scene instantiation preventing object interpenetration and ensuring semantically aligned training data for embodied AI.",
      "importance_score": 56,
      "reasoning": "Useful data generation framework for embodied AI, addresses real quality issues in synthetic data.",
      "themes": [
        "Embodied AI",
        "Data Generation",
        "Robotics"
      ],
      "continuation": null,
      "summary_html": "<p>V-CAGE framework generates robust manipulation datasets with context-aware scene instantiation preventing object interpenetration and ensuring semantically aligned training data for embodied AI.</p>",
      "content_html": "<p>arXiv:2601.15164v1 Announce Type: cross  Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.</p>"
    },
    {
      "id": "25d3017963b2",
      "title": "Closing the Gap on the Sample Complexity of 1-Identification",
      "content": "arXiv:2601.15620v1 Announce Type: new  Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $\\mu_0$, or to output \\textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-\\delta$, while making expected total pulling times $\\mathbb{E}\\tau$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\\mathbb{E}\\tau$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\\mathbb{E}\\tau$ when there are multiple qualified arms, which is an open problem left by history literature.",
      "url": "http://arxiv.org/abs/2601.15620",
      "author": "Zitian Li, Wang Chi Cheung",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Provides new lower bound for 1-identification in multi-armed bandits using optimization formulation, and designs algorithm with tight upper bounds closing the gap to logarithmic factors.",
      "importance_score": 56,
      "reasoning": "Solid theoretical contribution to bandit literature. Closes fundamental gap but narrow scope.",
      "themes": [
        "Multi-Armed Bandits",
        "Learning Theory",
        "Pure Exploration"
      ],
      "continuation": null,
      "summary_html": "<p>Provides new lower bound for 1-identification in multi-armed bandits using optimization formulation, and designs algorithm with tight upper bounds closing the gap to logarithmic factors.</p>",
      "content_html": "<p>arXiv:2601.15620v1 Announce Type: new  Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $\\mu_0$, or to output \\textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-\\delta$, while making expected total pulling times $\\mathbb{E}\\tau$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\\mathbb{E}\\tau$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\\mathbb{E}\\tau$ when there are multiple qualified arms, which is an open problem left by history literature.</p>"
    },
    {
      "id": "1e0799dee841",
      "title": "An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring",
      "content": "arXiv:2601.15304v1 Announce Type: cross  Abstract: Market integrity monitoring is difficult because suspicious price/volume behavior can arise from many benign mechanisms, while modern detection systems often rely on opaque models that are hard to audit and communicate. We present AIMM-X, an explainable monitoring pipeline that combines market microstructure-style signals derived from OHLCV time series with multi-source public attention signals (e.g., news and online discussion proxies) to surface time windows that merit analyst review. The system detects candidate anomalous windows using transparent thresholding and aggregation, then assigns an interpretable integrity score decomposed into a small set of additive components, allowing practitioners to trace why a window was flagged and which factors drove the score. We provide an end-to-end, reproducible implementation that downloads data, constructs attention features, builds unified panels, detects windows, computes component signals, and generates summary figures/tables. Our goal is not to label manipulation, but to provide a practical, auditable screening tool that supports downstream investigation by compliance teams, exchanges, or researchers.",
      "url": "http://arxiv.org/abs/2601.15304",
      "author": "Sandeep Neela",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "q-fin.RM"
      ],
      "summary": "AIMM-X is an explainable market monitoring system combining microstructure signals with public attention proxies. Uses transparent thresholding and additive score decomposition for interpretable flagging.",
      "importance_score": 56,
      "reasoning": "Applied system for financial market monitoring. Value is in practical interpretability design.",
      "themes": [
        "Financial ML",
        "Explainable AI",
        "Anomaly Detection"
      ],
      "continuation": null,
      "summary_html": "<p>AIMM-X is an explainable market monitoring system combining microstructure signals with public attention proxies. Uses transparent thresholding and additive score decomposition for interpretable flagging.</p>",
      "content_html": "<p>arXiv:2601.15304v1 Announce Type: cross  Abstract: Market integrity monitoring is difficult because suspicious price/volume behavior can arise from many benign mechanisms, while modern detection systems often rely on opaque models that are hard to audit and communicate. We present AIMM-X, an explainable monitoring pipeline that combines market microstructure-style signals derived from OHLCV time series with multi-source public attention signals (e.g., news and online discussion proxies) to surface time windows that merit analyst review. The system detects candidate anomalous windows using transparent thresholding and aggregation, then assigns an interpretable integrity score decomposed into a small set of additive components, allowing practitioners to trace why a window was flagged and which factors drove the score. We provide an end-to-end, reproducible implementation that downloads data, constructs attention features, builds unified panels, detects windows, computes component signals, and generates summary figures/tables. Our goal is not to label manipulation, but to provide a practical, auditable screening tool that supports downstream investigation by compliance teams, exchanges, or researchers.</p>"
    },
    {
      "id": "ca7126982f90",
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "content": "arXiv:2601.16125v1 Announce Type: cross  Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
      "url": "http://arxiv.org/abs/2601.16125",
      "author": "Tingyu Song, Yanzhao Zhang, Mingxin Li, Zhuoning Guo, Dingkun Long, Pengjun Xie, Siyue Zhang, Yilun Zhao, Shu Wu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces EDIR, a fine-grained Composed Image Retrieval benchmark using image editing for precise control over modification types. Contains 5,000 queries across 5 main categories, revealing significant capability gaps in state-of-the-art models.",
      "importance_score": 56,
      "reasoning": "Useful benchmark contribution for multimodal retrieval with principled construction via image editing. Reveals model limitations but impact limited to CIR community.",
      "themes": [
        "Image Retrieval",
        "Multimodal AI",
        "Evaluation & Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces EDIR, a fine-grained Composed Image Retrieval benchmark using image editing for precise control over modification types. Contains 5,000 queries across 5 main categories, revealing significant capability gaps in state-of-the-art models.</p>",
      "content_html": "<p>arXiv:2601.16125v1 Announce Type: cross  Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.</p>"
    },
    {
      "id": "aed8c4ef687f",
      "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
      "content": "arXiv:2601.15734v1 Announce Type: new  Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.",
      "url": "http://arxiv.org/abs/2601.15734",
      "author": "Shadi Alijani, Fereshteh Aghaee Meibodi, Homayoun Najjaran",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces framework for multi-modal brain tumor segmentation with sub-region-aware modality attention learning optimal modality combinations and adaptive prompt engineering leveraging foundation models.",
      "importance_score": 56,
      "reasoning": "Practical contribution to brain tumor segmentation with principled modality fusion. Addresses real clinical need but methodology builds on existing approaches.",
      "themes": [
        "Medical Imaging",
        "Brain Tumor Segmentation",
        "Foundation Models",
        "Multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces framework for multi-modal brain tumor segmentation with sub-region-aware modality attention learning optimal modality combinations and adaptive prompt engineering leveraging foundation models.</p>",
      "content_html": "<p>arXiv:2601.15734v1 Announce Type: new  Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.</p>"
    },
    {
      "id": "cb127b2e411d",
      "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
      "content": "arXiv:2601.15772v1 Announce Type: new  Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
      "url": "http://arxiv.org/abs/2601.15772",
      "author": "Yuhan Chen, Wenxuan Yu, Guofa Li, Yijun Xu, Ying Fang, Yicui Shi, Long Cao, Wenbo Chu, Keqiang Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes LL-GaussianImage for zero-shot low-light enhancement directly in 2DGS compressed representation domain, avoiding decompression-enhancement-recompression pipeline with semantic-guided MoE.",
      "importance_score": 56,
      "reasoning": "Practical efficiency contribution for 2DGS-compressed images. Novel domain for enhancement but limited by 2DGS adoption.",
      "themes": [
        "Low-Light Enhancement",
        "Gaussian Splatting",
        "Efficient AI",
        "Image Compression"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes LL-GaussianImage for zero-shot low-light enhancement directly in 2DGS compressed representation domain, avoiding decompression-enhancement-recompression pipeline with semantic-guided MoE.</p>",
      "content_html": "<p>arXiv:2601.15772v1 Announce Type: new  Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.</p>"
    },
    {
      "id": "d307a3264508",
      "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation",
      "content": "arXiv:2601.15929v1 Announce Type: new  Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.",
      "url": "http://arxiv.org/abs/2601.15929",
      "author": "Liuyun Jiang, Yizhuo Lu, Yanchao Zhang, Jiazheng Liu, Hua Han",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes NeuroMamba, combining Mamba's linear complexity for patch-free global modeling with complementary local feature modeling for neuron segmentation in electron microscopy.",
      "importance_score": 56,
      "reasoning": "Novel application of Mamba architecture to neuroscience addressing limitations of CNN and Transformer approaches. Specialized but well-motivated.",
      "themes": [
        "Neuroscience",
        "Mamba Architecture",
        "Segmentation",
        "Electron Microscopy"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes NeuroMamba, combining Mamba's linear complexity for patch-free global modeling with complementary local feature modeling for neuron segmentation in electron microscopy.</p>",
      "content_html": "<p>arXiv:2601.15929v1 Announce Type: new  Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.</p>"
    },
    {
      "id": "2e8b25aac3a9",
      "title": "CI4A: Semantic Component Interfaces for Agents Empowering Web Automation",
      "content": "arXiv:2601.14790v1 Announce Type: new  Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.",
      "url": "http://arxiv.org/abs/2601.14790",
      "author": "Zhi Qiu, Jiazheng Sun, Chenxiao Xia, Jun Zheng, Xin Peng",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces CI4A, semantic component interfaces for web agents that abstract complex UI interaction logic into unified tool primitives. Implemented within Ant Design framework.",
      "importance_score": 55,
      "reasoning": "Practical contribution for web agent development. Interesting approach of designing interfaces for agents rather than forcing agents to adapt to human interfaces.",
      "themes": [
        "Web Agents",
        "Human-Computer Interaction",
        "Tool Use"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces CI4A, semantic component interfaces for web agents that abstract complex UI interaction logic into unified tool primitives. Implemented within Ant Design framework.</p>",
      "content_html": "<p>arXiv:2601.14790v1 Announce Type: new  Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.</p>"
    },
    {
      "id": "350a402d62ad",
      "title": "DeepInflation: an AI agent for research and model discovery of inflation",
      "content": "arXiv:2601.14288v1 Announce Type: cross  Abstract: We present \\textbf{DeepInflation}, an AI agent designed for research and model discovery in inflationary cosmology. Built upon a multi-agent architecture, \\textbf{DeepInflation} integrates Large Language Models (LLMs) with a symbolic regression (SR) engine and a retrieval-augmented generation (RAG) knowledge base. This framework enables the agent to automatically explore and verify the vast landscape of inflationary potentials while grounding its outputs in established theoretical literature. We demonstrate that \\textbf{DeepInflation} can successfully discover simple and viable single-field slow-roll inflationary potentials consistent with the latest observations (here ACT DR6 results as example) or any given $n_s$ and $r$, and provide accurate theoretical context for obscure inflationary scenarios. \\textbf{DeepInflation} serves as a prototype for a new generation of autonomous scientific discovery engines in cosmology, which enables researchers and non-experts alike to explore the inflationary landscape using natural language. This agent is available at https://github.com/pengzy-cosmo/DeepInflation.",
      "url": "http://arxiv.org/abs/2601.14288",
      "author": "Ze-Yu Peng, Hao-Shi Yuan, Qi Lai, Jun-Qian Jiang, Gen Ye, Jun Zhang, Yun-Song Piao",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "astro-ph.CO"
      ],
      "summary": "Introduces DeepInflation, an AI agent for inflationary cosmology research combining LLMs with symbolic regression and RAG to discover viable single-field slow-roll potentials consistent with observations.",
      "importance_score": 55,
      "reasoning": "Novel scientific AI application but niche domain. Demonstrates interesting multi-agent architecture for scientific discovery.",
      "themes": [
        "Scientific AI",
        "Multi-Agent Systems",
        "Physics"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DeepInflation, an AI agent for inflationary cosmology research combining LLMs with symbolic regression and RAG to discover viable single-field slow-roll potentials consistent with observations.</p>",
      "content_html": "<p>arXiv:2601.14288v1 Announce Type: cross  Abstract: We present \\textbf{DeepInflation}, an AI agent designed for research and model discovery in inflationary cosmology. Built upon a multi-agent architecture, \\textbf{DeepInflation} integrates Large Language Models (LLMs) with a symbolic regression (SR) engine and a retrieval-augmented generation (RAG) knowledge base. This framework enables the agent to automatically explore and verify the vast landscape of inflationary potentials while grounding its outputs in established theoretical literature. We demonstrate that \\textbf{DeepInflation} can successfully discover simple and viable single-field slow-roll inflationary potentials consistent with the latest observations (here ACT DR6 results as example) or any given $n_s$ and $r$, and provide accurate theoretical context for obscure inflationary scenarios. \\textbf{DeepInflation} serves as a prototype for a new generation of autonomous scientific discovery engines in cosmology, which enables researchers and non-experts alike to explore the inflationary landscape using natural language. This agent is available at https://github.com/pengzy-cosmo/DeepInflation.</p>"
    },
    {
      "id": "a950d09b9d86",
      "title": "XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping",
      "content": "arXiv:2601.14477v1 Announce Type: cross  Abstract: Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.",
      "url": "http://arxiv.org/abs/2601.14477",
      "author": "Frank Bieder, Hendrik K\\\"onigshof, Haohao Hu, Fabian Immel, Yinzhe Shen, Jan-Hendrik Pauls, Christoph Stiller",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes XD-MAP for cross-modal domain adaptation from image datasets to LiDAR by creating semantic parametric maps from camera detections to generate pseudo-labels.",
      "importance_score": 55,
      "reasoning": "Practical contribution for sensor fusion in autonomous driving. Novel cross-modal knowledge transfer approach.",
      "themes": [
        "Domain Adaptation",
        "Autonomous Driving",
        "Sensor Fusion"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes XD-MAP for cross-modal domain adaptation from image datasets to LiDAR by creating semantic parametric maps from camera detections to generate pseudo-labels.</p>",
      "content_html": "<p>arXiv:2601.14477v1 Announce Type: cross  Abstract: Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.</p>"
    },
    {
      "id": "415cdda1b5c8",
      "title": "Optimality of Staircase Mechanisms for Vector Queries under Differential Privacy",
      "content": "arXiv:2601.14597v1 Announce Type: cross  Abstract: We study the optimal design of additive mechanisms for vector-valued queries under $\\epsilon$-differential privacy (DP). Given only the sensitivity of a query and a norm-monotone cost function measuring utility loss, we ask which noise distribution minimizes expected cost among all additive $\\epsilon$-DP mechanisms. Using convex rearrangement theory, we show that this infinite-dimensional optimization problem admits a reduction to a one-dimensional compact and convex family of radially symmetric distributions whose extreme points are the staircase distributions. As a consequence, we prove that for any dimension, any norm, and any norm-monotone cost function, there exists an $\\epsilon$-DP staircase mechanism that is optimal among all additive mechanisms. This result resolves a conjecture of Geng, Kairouz, Oh, and Viswanath, and provides a geometric explanation for the emergence of staircase mechanisms as extremal solutions in differential privacy.",
      "url": "http://arxiv.org/abs/2601.14597",
      "author": "James Melbourne, Mario Diaz, Shahab Asoodeh",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.IT"
      ],
      "summary": "Proves that staircase distributions are optimal for differential privacy mechanisms on vector queries under any norm-monotone cost function. Uses convex rearrangement theory to reduce the infinite-dimensional optimization to a tractable one-dimensional problem.",
      "importance_score": 55,
      "reasoning": "Solid theoretical contribution to differential privacy, but fairly specialized mathematical work without immediate practical applications.",
      "themes": [
        "Differential Privacy",
        "Optimization Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Proves that staircase distributions are optimal for differential privacy mechanisms on vector queries under any norm-monotone cost function. Uses convex rearrangement theory to reduce the infinite-dimensional optimization to a tractable one-dimensional problem.</p>",
      "content_html": "<p>arXiv:2601.14597v1 Announce Type: cross  Abstract: We study the optimal design of additive mechanisms for vector-valued queries under $\\epsilon$-differential privacy (DP). Given only the sensitivity of a query and a norm-monotone cost function measuring utility loss, we ask which noise distribution minimizes expected cost among all additive $\\epsilon$-DP mechanisms. Using convex rearrangement theory, we show that this infinite-dimensional optimization problem admits a reduction to a one-dimensional compact and convex family of radially symmetric distributions whose extreme points are the staircase distributions. As a consequence, we prove that for any dimension, any norm, and any norm-monotone cost function, there exists an $\\epsilon$-DP staircase mechanism that is optimal among all additive mechanisms. This result resolves a conjecture of Geng, Kairouz, Oh, and Viswanath, and provides a geometric explanation for the emergence of staircase mechanisms as extremal solutions in differential privacy.</p>"
    },
    {
      "id": "43e70ee751dc",
      "title": "RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models",
      "content": "arXiv:2601.14780v1 Announce Type: cross  Abstract: Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.   To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.   RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.",
      "url": "http://arxiv.org/abs/2601.14780",
      "author": "Anqi Li, Yuqian Chen, Yu Lu, Zhaoming Chen, Yuan Xie, Zhenzhong Lan",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "RECAP captures client resistance in text-based mental health counseling using fine-grained PsyFIRE framework with 13 resistance behaviors. Creates annotated corpus from real Chinese counseling sessions.",
      "importance_score": 55,
      "reasoning": "Important application domain with carefully constructed framework, though language-specific and specialized.",
      "themes": [
        "Mental Health AI",
        "NLP",
        "Healthcare"
      ],
      "continuation": null,
      "summary_html": "<p>RECAP captures client resistance in text-based mental health counseling using fine-grained PsyFIRE framework with 13 resistance behaviors. Creates annotated corpus from real Chinese counseling sessions.</p>",
      "content_html": "<p>arXiv:2601.14780v1 Announce Type: cross  Abstract: Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.   To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.   RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.</p>"
    },
    {
      "id": "d166a4d1ee12",
      "title": "Federated Transformer-GNN for Privacy-Preserving Brain Tumor Localization with Modality-Level Explainability",
      "content": "arXiv:2601.15042v1 Announce Type: cross  Abstract: Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\\textsuperscript{\\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p<0.001$, Cohen's $d$=1.50), aligning with clinical practice.",
      "url": "http://arxiv.org/abs/2601.15042",
      "author": "Andrea Protani, Riccardo Taiello, Marc Molina Van Den Bosch, Luigi Serio",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Federated learning framework for brain tumor localization using Transformer-GNN hybrid, deployed on CERN's healthcare federated platform with modality-level explainability through attention analysis.",
      "importance_score": 55,
      "reasoning": "Practical federated learning application in medical imaging with explainability component, though limited methodological novelty.",
      "themes": [
        "Federated Learning",
        "Medical Imaging",
        "Explainability"
      ],
      "continuation": null,
      "summary_html": "<p>Federated learning framework for brain tumor localization using Transformer-GNN hybrid, deployed on CERN's healthcare federated platform with modality-level explainability through attention analysis.</p>",
      "content_html": "<p>arXiv:2601.15042v1 Announce Type: cross  Abstract: Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\\textsuperscript{\\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p&lt;0.001$, Cohen's $d$=1.50), aligning with clinical practice.</p>"
    },
    {
      "id": "36b2f3b42e72",
      "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?",
      "content": "arXiv:2601.15254v1 Announce Type: cross  Abstract: We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.",
      "url": "http://arxiv.org/abs/2601.15254",
      "author": "Felix Schur, Niklas Pfister, Peng Ding, Sach Mukherjee, Jonas Peters",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Addresses causal effect estimation with many experiments but few observations per experiment and unpaired data. Proposes GMM-type estimator with cross-fold splitting that remains consistent as experiment count grows.",
      "importance_score": 55,
      "reasoning": "Solid statistical methodology contribution for challenging causal inference settings.",
      "themes": [
        "Causal Inference",
        "Statistics",
        "Methodology"
      ],
      "continuation": null,
      "summary_html": "<p>Addresses causal effect estimation with many experiments but few observations per experiment and unpaired data. Proposes GMM-type estimator with cross-fold splitting that remains consistent as experiment count grows.</p>",
      "content_html": "<p>arXiv:2601.15254v1 Announce Type: cross  Abstract: We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.</p>"
    },
    {
      "id": "f4d046562908",
      "title": "Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features",
      "content": "arXiv:2601.15530v1 Announce Type: new  Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.",
      "url": "http://arxiv.org/abs/2601.15530",
      "author": "Megan A. Witherow, Michael L. Evans, Ahmed Temtam, Hamid Okhravi, Khan M. Iftekharuddin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "ML approach to diagnose atypical (non-amnestic) Alzheimer's disease using MRI and clinical features, addressing the challenge that atypical AD is often misdiagnosed.",
      "importance_score": 55,
      "reasoning": "Important clinical problem but standard ML application. Value depends on clinical validation.",
      "themes": [
        "Healthcare ML",
        "Medical Imaging",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>ML approach to diagnose atypical (non-amnestic) Alzheimer's disease using MRI and clinical features, addressing the challenge that atypical AD is often misdiagnosed.</p>",
      "content_html": "<p>arXiv:2601.15530v1 Announce Type: new  Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.</p>"
    },
    {
      "id": "4e749c443e16",
      "title": "CLASP: An online learning algorithm for Convex Losses And Squared Penalties",
      "content": "arXiv:2601.16072v1 Announce Type: new  Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\\left(T^{\\max\\{\\beta,1-\\beta\\}}\\right)$ and cumulative squared penalty $O\\left(T^{1-\\beta}\\right)$ for any $\\beta \\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \\log T )$ and the cumulative squared penalty is also upper bounded by $O( \\log T )$.",
      "url": "http://arxiv.org/abs/2601.16072",
      "author": "Ricardo N. Ferreira, Cl\\'audia Soares, Jo\\~ao Xavier",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "CLASP algorithm for constrained online convex optimization achieves sublinear regret and cumulative squared penalty. Uses firm non-expansiveness of convex projectors in novel proof strategy.",
      "importance_score": 55,
      "reasoning": "Solid theoretical contribution to constrained online learning. Novel proof technique but focused scope.",
      "themes": [
        "Online Learning",
        "Constrained Optimization",
        "Learning Theory"
      ],
      "continuation": null,
      "summary_html": "<p>CLASP algorithm for constrained online convex optimization achieves sublinear regret and cumulative squared penalty. Uses firm non-expansiveness of convex projectors in novel proof strategy.</p>",
      "content_html": "<p>arXiv:2601.16072v1 Announce Type: new  Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\\left(T^{\\max\\{\\beta,1-\\beta\\}}\\right)$ and cumulative squared penalty $O\\left(T^{1-\\beta}\\right)$ for any $\\beta \\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \\log T )$ and the cumulative squared penalty is also upper bounded by $O( \\log T )$.</p>"
    },
    {
      "id": "fcbcccc3b842",
      "title": "Logic Programming on Knowledge Graph Networks And its Application in Medical Domain",
      "content": "arXiv:2601.15347v1 Announce Type: cross  Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.",
      "url": "http://arxiv.org/abs/2601.15347",
      "author": "Chuanqing Wang, Zhenmin Zhao, Shanshan Du, Chaoqun Fei, Songmao Zhang, Ruqian Lu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Develops systematic theory and techniques for logic programming on knowledge graphs with application to medicine. Addresses cooperation and competition between multiple knowledge graphs.",
      "importance_score": 55,
      "reasoning": "Connects traditional logic programming with knowledge graphs. Important for knowledge-intensive applications but limited ML novelty.",
      "themes": [
        "Knowledge Graphs",
        "Logic Programming",
        "Medical AI"
      ],
      "continuation": null,
      "summary_html": "<p>Develops systematic theory and techniques for logic programming on knowledge graphs with application to medicine. Addresses cooperation and competition between multiple knowledge graphs.</p>",
      "content_html": "<p>arXiv:2601.15347v1 Announce Type: cross  Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.</p>"
    },
    {
      "id": "6ec5a06ebcca",
      "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL",
      "content": "arXiv:2601.15709v1 Announce Type: cross  Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.",
      "url": "http://arxiv.org/abs/2601.15709",
      "author": "Asim Biswal, Chuan Lei, Xiao Qin, Aodong Li, Balakrishnan Narayanaswamy, Tim Kraska",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "AgentSM introduces semantic memory for agentic Text-to-SQL, building interpretable knowledge structures instead of raw scratchpads. Addresses enterprise challenges with large schemas and diverse SQL dialects.",
      "importance_score": 55,
      "reasoning": "Practical agentic approach for enterprise NL-to-database queries. Addresses real scaling challenges but evaluation scope unclear.",
      "themes": [
        "Text-to-SQL",
        "Agentic AI",
        "Semantic Memory"
      ],
      "continuation": null,
      "summary_html": "<p>AgentSM introduces semantic memory for agentic Text-to-SQL, building interpretable knowledge structures instead of raw scratchpads. Addresses enterprise challenges with large schemas and diverse SQL dialects.</p>",
      "content_html": "<p>arXiv:2601.15709v1 Announce Type: cross  Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.</p>"
    },
    {
      "id": "2e1da095e0a5",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "content": "arXiv:2601.15995v1 Announce Type: cross  Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "url": "http://arxiv.org/abs/2601.15995",
      "author": "Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun, Qiuguo Zhu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "PUMA enables quadruped parkour through perception-driven foothold priors integrated into end-to-end RL. Single-stage training leverages terrain features for egocentric foothold estimation.",
      "importance_score": 55,
      "reasoning": "Solid robotics/RL contribution for legged locomotion. Practical end-to-end approach.",
      "themes": [
        "Robotics",
        "Reinforcement Learning",
        "Legged Locomotion"
      ],
      "continuation": null,
      "summary_html": "<p>PUMA enables quadruped parkour through perception-driven foothold priors integrated into end-to-end RL. Single-stage training leverages terrain features for egocentric foothold estimation.</p>",
      "content_html": "<p>arXiv:2601.15995v1 Announce Type: cross  Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.</p>"
    },
    {
      "id": "6f17abfb8502",
      "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
      "content": "arXiv:2601.16174v1 Announce Type: cross  Abstract: Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",
      "url": "http://arxiv.org/abs/2601.16174",
      "author": "Yiyao Yang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "Argues reliability should be a first-class property of learned representations, not just predictions. Proposes uncertainty-aware regularization with structural constraints in representation space.",
      "importance_score": 55,
      "reasoning": "Interesting reframing of uncertainty estimation. Novel perspective but early-stage.",
      "themes": [
        "Uncertainty Estimation",
        "Representation Learning",
        "Reliability"
      ],
      "continuation": null,
      "summary_html": "<p>Argues reliability should be a first-class property of learned representations, not just predictions. Proposes uncertainty-aware regularization with structural constraints in representation space.</p>",
      "content_html": "<p>arXiv:2601.16174v1 Announce Type: cross  Abstract: Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.</p>"
    },
    {
      "id": "9bec1a8f4bff",
      "title": "What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking",
      "content": "arXiv:2601.15674v1 Announce Type: new  Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.",
      "url": "http://arxiv.org/abs/2601.15674",
      "author": "Raymond Xiong, Furong Jia, Lionel Wong, Monica Agrawal",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies patient questions from Google's People Also Ask for top 200 US medications. Finds considerable portion contains incorrect assumptions and dangerous intentions.",
      "importance_score": 55,
      "reasoning": "Important real-world finding about patient LLM use. Practical safety implications.",
      "themes": [
        "Healthcare AI",
        "Patient Safety",
        "Real-world LLM Use"
      ],
      "continuation": null,
      "summary_html": "<p>Studies patient questions from Google's People Also Ask for top 200 US medications. Finds considerable portion contains incorrect assumptions and dangerous intentions.</p>",
      "content_html": "<p>arXiv:2601.15674v1 Announce Type: new  Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.</p>"
    },
    {
      "id": "ca76528ae157",
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "content": "arXiv:2601.15715v1 Announce Type: new  Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.",
      "url": "http://arxiv.org/abs/2601.15715",
      "author": "Zhitao He, Zongwei Lyu, Yi R Fung",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "RebuttalAgent grounds academic rebuttal in Theory of Mind via ToM-Strategy-Response pipeline. Models reviewer mental states to generate strategic persuasive responses.",
      "importance_score": 55,
      "reasoning": "Novel ToM application to academic communication. Interesting framework but niche application.",
      "themes": [
        "Theory of Mind",
        "Academic AI",
        "Persuasion"
      ],
      "continuation": null,
      "summary_html": "<p>RebuttalAgent grounds academic rebuttal in Theory of Mind via ToM-Strategy-Response pipeline. Models reviewer mental states to generate strategic persuasive responses.</p>",
      "content_html": "<p>arXiv:2601.15715v1 Announce Type: new  Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.</p>"
    },
    {
      "id": "da60edc37a10",
      "title": "Hallucination Mitigating for Medical Report Generation",
      "content": "arXiv:2601.15745v1 Announce Type: new  Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \\textbf{K}nowledge-\\textbf{E}nhanced with Fine-Grained \\textbf{R}einforced Rewards \\textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.",
      "url": "http://arxiv.org/abs/2601.15745",
      "author": "Ruoqing Zhao, Runze Xia, Piji Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "KERM framework for medical report generation uses MedCLIP retrieval and fine-grained reinforced rewards to reduce hallucinations in LVLMs.",
      "importance_score": 55,
      "reasoning": "Practical hallucination mitigation for medical AI. Combines retrieval with RL for reliability.",
      "themes": [
        "Medical Report Generation",
        "Hallucination",
        "Retrieval-Augmented"
      ],
      "continuation": null,
      "summary_html": "<p>KERM framework for medical report generation uses MedCLIP retrieval and fine-grained reinforced rewards to reduce hallucinations in LVLMs.</p>",
      "content_html": "<p>arXiv:2601.15745v1 Announce Type: new  Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \\textbf{K}nowledge-\\textbf{E}nhanced with Fine-Grained \\textbf{R}einforced Rewards \\textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.</p>"
    },
    {
      "id": "82cc6069fe55",
      "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
      "content": "arXiv:2601.15793v1 Announce Type: new  Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
      "url": "http://arxiv.org/abs/2601.15793",
      "author": "Yuxuan Lei, Tianfu Wang, Jianxun Lian, Zhengyu Hu, Defu Lian, Xing Xie",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "HumanLLM framework addresses misalignment between LLM pretraining and individual human cognition/behavior. Aims to enable personalized understanding and social simulation.",
      "importance_score": 55,
      "reasoning": "Interesting framing of personalization gap. Important direction for human simulation.",
      "themes": [
        "Personalization",
        "Human Simulation",
        "Social AI"
      ],
      "continuation": null,
      "summary_html": "<p>HumanLLM framework addresses misalignment between LLM pretraining and individual human cognition/behavior. Aims to enable personalized understanding and social simulation.</p>",
      "content_html": "<p>arXiv:2601.15793v1 Announce Type: new  Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.</p>"
    },
    {
      "id": "1f9f4d39afa7",
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "content": "arXiv:2601.16018v1 Announce Type: new  Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
      "url": "http://arxiv.org/abs/2601.16018",
      "author": "\\\"Ozg\\\"ur U\\u{g}ur, Mahmut G\\\"oksu, Mahmut \\c{C}imen, Musa Y{\\i}lmaz, Esra \\c{S}avirdi, Alp Talha Demir, Rumeysa G\\\"ull\\\"uce, \\.Iclal \\c{C}etin, \\\"Omer Can Sa\\u{g}ba\\c{s}",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Mecellem models are Turkish legal domain LLMs trained from scratch and continually pre-trained. Implements checkpoint selection based on retrieval performance, finding optimal before minimum loss.",
      "importance_score": 55,
      "reasoning": "Good domain-specific model development with interesting checkpoint selection finding.",
      "themes": [
        "Legal AI",
        "Turkish NLP",
        "Domain Adaptation"
      ],
      "continuation": null,
      "summary_html": "<p>Mecellem models are Turkish legal domain LLMs trained from scratch and continually pre-trained. Implements checkpoint selection based on retrieval performance, finding optimal before minimum loss.</p>",
      "content_html": "<p>arXiv:2601.16018v1 Announce Type: new  Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.</p>"
    },
    {
      "id": "ca97608b8167",
      "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging",
      "content": "arXiv:2601.16127v1 Announce Type: new  Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.",
      "url": "http://arxiv.org/abs/2601.16127",
      "author": "Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies language-specific model merging for multilingual LLMs, showing significant efficiency gains without quality loss. First focused analysis from efficiency perspective across three tasks.",
      "importance_score": 55,
      "reasoning": "Practical efficiency contribution for multilingual maintenance. Good empirical study.",
      "themes": [
        "Model Merging",
        "Multilingual LLMs",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Studies language-specific model merging for multilingual LLMs, showing significant efficiency gains without quality loss. First focused analysis from efficiency perspective across three tasks.</p>",
      "content_html": "<p>arXiv:2601.16127v1 Announce Type: new  Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.</p>"
    },
    {
      "id": "b361333248c9",
      "title": "LLM or Human? Perceptions of Trust and Information Quality in Research Summaries",
      "content": "arXiv:2601.15556v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly used to generate and edit scientific abstracts, yet their integration into academic writing raises questions about trust, quality, and disclosure. Despite growing adoption, little is known about how readers perceive LLM-generated summaries and how these perceptions influence evaluations of scientific work. This paper presents a mixed-methods survey experiment investigating whether readers with ML expertise can distinguish between human- and LLM-generated abstracts, how actual and perceived LLM involvement affects judgments of quality and trustworthiness, and what orientations readers adopt toward AI-assisted writing. Our findings show that participants struggle to reliably identify LLM-generated content, yet their beliefs about LLM involvement significantly shape their evaluations. Notably, abstracts edited by LLMs are rated more favorably than those written solely by humans or LLMs. We also identify three distinct reader orientations toward LLM-assisted writing, offering insights into evolving norms and informing policy around disclosure and acceptable use in scientific communication.",
      "url": "http://arxiv.org/abs/2601.15556",
      "author": "Nil-Jana Akpinar, Sandeep Avula, CJ Lee, Brandon Dang, Kaza Razat, Vanessa Murdock",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CY"
      ],
      "summary": "Mixed-methods survey examining whether ML experts can distinguish human vs LLM-generated abstracts and how perceived LLM involvement affects quality/trustworthiness judgments. Finds participants struggle to reliably identify LLM-generated content.",
      "importance_score": 55,
      "reasoning": "Timely research on AI-generated academic content with practical implications for scientific publishing. Well-designed study but findings align with expected outcomes.",
      "themes": [
        "Human-AI Interaction",
        "Scientific Writing",
        "Trust in AI"
      ],
      "continuation": null,
      "summary_html": "<p>Mixed-methods survey examining whether ML experts can distinguish human vs LLM-generated abstracts and how perceived LLM involvement affects quality/trustworthiness judgments. Finds participants struggle to reliably identify LLM-generated content.</p>",
      "content_html": "<p>arXiv:2601.15556v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly used to generate and edit scientific abstracts, yet their integration into academic writing raises questions about trust, quality, and disclosure. Despite growing adoption, little is known about how readers perceive LLM-generated summaries and how these perceptions influence evaluations of scientific work. This paper presents a mixed-methods survey experiment investigating whether readers with ML expertise can distinguish between human- and LLM-generated abstracts, how actual and perceived LLM involvement affects judgments of quality and trustworthiness, and what orientations readers adopt toward AI-assisted writing. Our findings show that participants struggle to reliably identify LLM-generated content, yet their beliefs about LLM involvement significantly shape their evaluations. Notably, abstracts edited by LLMs are rated more favorably than those written solely by humans or LLMs. We also identify three distinct reader orientations toward LLM-assisted writing, offering insights into evolving norms and informing policy around disclosure and acceptable use in scientific communication.</p>"
    },
    {
      "id": "82c6ec001680",
      "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection",
      "content": "arXiv:2601.15453v1 Announce Type: new  Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.",
      "url": "http://arxiv.org/abs/2601.15453",
      "author": "Morteza Poudineh, Marc Lalonde",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes DevPrompt, a deviation-guided prompt learning framework for few-normal shot anomaly detection, integrating vision-language models with deviation-based scoring for improved patch-level anomaly detection.",
      "importance_score": 55,
      "reasoning": "Practical approach combining VLMs with principled statistical scoring. Addresses real industrial inspection needs but builds incrementally on existing CLIP-based methods.",
      "themes": [
        "Anomaly Detection",
        "Vision-Language Models",
        "Industrial Applications"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes DevPrompt, a deviation-guided prompt learning framework for few-normal shot anomaly detection, integrating vision-language models with deviation-based scoring for improved patch-level anomaly detection.</p>",
      "content_html": "<p>arXiv:2601.15453v1 Announce Type: new  Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</p>"
    },
    {
      "id": "83374fe87a9e",
      "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception",
      "content": "arXiv:2601.15643v1 Announce Type: new  Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.",
      "url": "http://arxiv.org/abs/2601.15643",
      "author": "Bo Yuan, Danpei Zhao, Wentao Li, Tian Li, Zhiguo Jiang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Extends continual learning to continual panoptic perception (CPP), integrating multimodal and multi-task continual learning for comprehensive image perception across pixel, instance, and image levels.",
      "importance_score": 55,
      "reasoning": "Ambitious framework unifying multiple continual learning challenges. Novel problem formulation but may be overcomplex for practical adoption.",
      "themes": [
        "Continual Learning",
        "Multimodal AI",
        "Panoptic Segmentation"
      ],
      "continuation": null,
      "summary_html": "<p>Extends continual learning to continual panoptic perception (CPP), integrating multimodal and multi-task continual learning for comprehensive image perception across pixel, instance, and image levels.</p>",
      "content_html": "<p>arXiv:2601.15643v1 Announce Type: new  Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.</p>"
    },
    {
      "id": "a7e0456b102a",
      "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation",
      "content": "arXiv:2601.16060v1 Announce Type: new  Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.",
      "url": "http://arxiv.org/abs/2601.16060",
      "author": "Yuan Lin, Murong Xu, Marc H\\\"olle, Chinmay Prabhakar, Andreas Maier, Vasileios Belagiannis, Bjoern Menze, Suprosanna Shit",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes ProGiDiff, leveraging existing text-to-image diffusion models for medical image segmentation through ControlNet-style prompt guidance and noise injection, enabling natural language conditioning.",
      "importance_score": 55,
      "reasoning": "Practical approach to leverage pretrained diffusion models for medical segmentation with prompt conditioning. Avoids training from scratch limitation.",
      "themes": [
        "Medical Imaging",
        "Diffusion Models",
        "Segmentation",
        "Transfer Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes ProGiDiff, leveraging existing text-to-image diffusion models for medical image segmentation through ControlNet-style prompt guidance and noise injection, enabling natural language conditioning.</p>",
      "content_html": "<p>arXiv:2601.16060v1 Announce Type: new  Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.</p>"
    },
    {
      "id": "100c27dd7df8",
      "title": "The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection",
      "content": "arXiv:2601.15316v1 Announce Type: cross  Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.",
      "url": "http://arxiv.org/abs/2601.15316",
      "author": "Wei Ai, Yilong Tan, Yuntao Shou, Tao Meng, Haowen Chen, Zhixiong He, Keqin Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "A comprehensive survey on how large vision-language models (LVLMs) are transforming multimodal fake news detection, moving from traditional feature engineering to end-to-end reasoning frameworks. Reviews progress and remaining challenges.",
      "importance_score": 55,
      "reasoning": "Useful survey synthesizing a growing field, but survey rather than novel research, applied focus on misinformation detection.",
      "themes": [
        "Vision-Language Models",
        "Misinformation Detection",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>A comprehensive survey on how large vision-language models (LVLMs) are transforming multimodal fake news detection, moving from traditional feature engineering to end-to-end reasoning frameworks. Reviews progress and remaining challenges.</p>",
      "content_html": "<p>arXiv:2601.15316v1 Announce Type: cross  Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.</p>"
    },
    {
      "id": "204bc4808896",
      "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
      "content": "arXiv:2601.16117v1 Announce Type: cross  Abstract: Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
      "url": "http://arxiv.org/abs/2601.16117",
      "author": "Abdul Hannan, Daniele Falavigna, Shah Nawaz, Mubashir Noman, Markus Schedl, Alessio Brutti",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Proposes Distillation-based Layer Dropping (DLD) to create dynamic speech networks for edge devices, combining knowledge distillation with layer dropping for better performance-computation trade-offs.",
      "importance_score": 55,
      "reasoning": "Practical contribution for efficient edge deployment, combines known techniques in novel way.",
      "themes": [
        "Efficient AI",
        "Model Compression",
        "Speech Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Distillation-based Layer Dropping (DLD) to create dynamic speech networks for edge devices, combining knowledge distillation with layer dropping for better performance-computation trade-offs.</p>",
      "content_html": "<p>arXiv:2601.16117v1 Announce Type: cross  Abstract: Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.</p>"
    },
    {
      "id": "93d15bca9230",
      "title": "A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control",
      "content": "arXiv:2601.15545v1 Announce Type: new  Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.",
      "url": "http://arxiv.org/abs/2601.15545",
      "author": "Zhifan Yan, Chang Liu, Yiyang Jiang, Wenxuan Zheng, Xinhao Chen, Axel Krieger",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Presents a mobile magnetic manipulation platform for GI tract drug delivery using deep reinforcement learning control, overcoming the model-calibration bottleneck of traditional approaches.",
      "importance_score": 55,
      "reasoning": "Novel DRL application to medical robotics, practical solution to calibration challenges.",
      "themes": [
        "Medical Robotics",
        "Reinforcement Learning",
        "Drug Delivery"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a mobile magnetic manipulation platform for GI tract drug delivery using deep reinforcement learning control, overcoming the model-calibration bottleneck of traditional approaches.</p>",
      "content_html": "<p>arXiv:2601.15545v1 Announce Type: new  Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.</p>"
    },
    {
      "id": "d865a17c6a37",
      "title": "AI #152: Brought To You By The Torment Nexus",
      "content": "Anthropic released a new constitution for Claude. I encourage those interested to read the document, either in whole or in part. I intend to cover it on its own soon. There was also actual talk about coordinating on a conditional pause or slowdown from DeepMind CEO Demis Hassabis, which I also plan to cover later. Claude Code continues to be the talk of the town, the weekly report on that is here. OpenAI responded by planning ads for the cheap and free versions of ChatGPT. There was also a fun but meaningful incident involving ChatGPT Self Portraits. &nbsp; Table of Contents Language Models Offer Mundane Utility. Call in the tone police. Language Models Dont Offer Mundane Utility. He who lives by the pattern. Huh, Upgrades. Claude health integrations, ChatGPT $8/month option. Gemini Personalized Intelligence. Signs of both remain somewhat lacking. Deepfaketown and Botpocalypse Soon. Get that bathtub viking. Fun With Media Generation. Studio Ghibli pics are back, baby. Were Proud To Announce The Torment Nexus. Ads come to ChatGPT. They Took Our Jobs. Find a game plan. Dont count on repugnance. The Revolution of Rising Expectations. Look at all the value youre getting. Get Involved. AI Village, Anthropic, Dwarkesh Patel guest hunter. A Young Ladys Illustrated Primer. Were putting together the wrong team. In Other AI News. China remain behind, Drexler goes galaxy brain. Axis of Assistance. Have you tried not being a helpful AI assistant? Show Me the Money. OpenAI looks to raise another $50 billion. California In Crisis. Will we soon ask, where have all the startups gone? Bubble, Bubble, Toil and Trouble. They keep using that word. Quiet Speculations. Results from the AI 2025 predictions survey. Elon Musk Versus OpenAI. There they go again. The Quest for Sane Regulations. Nvidia versus the AI Overwatch Act. Chip City. Are we on the verge of giving China ten times their current compute? The Week in Audio. Tyler Cowen and a surprisingly informed Ben Affleck. Rhetori...",
      "url": "https://www.lesswrong.com/posts/pCkYfhYcwFLELoYQf/ai-152-brought-to-you-by-the-torment-nexus",
      "author": "Zvi",
      "published": "2026-01-22T09:40:22.832000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Zvi's weekly AI news roundup covering Anthropic's new constitution, Demis Hassabis comments on coordinated pauses, Claude Code developments, and ChatGPT ad plans.",
      "importance_score": 55,
      "reasoning": "Useful news aggregation but commentary rather than original research.",
      "themes": [
        "AI News",
        "Industry",
        "Commentary"
      ],
      "continuation": null,
      "summary_html": "<p>Zvi's weekly AI news roundup covering Anthropic's new constitution, Demis Hassabis comments on coordinated pauses, Claude Code developments, and ChatGPT ad plans.</p>",
      "content_html": "<p>Anthropic released a new constitution for Claude. I encourage those interested to read the document, either in whole or in part. I intend to cover it on its own soon. There was also actual talk about coordinating on a conditional pause or slowdown from DeepMind CEO Demis Hassabis, which I also plan to cover later. Claude Code continues to be the talk of the town, the weekly report on that is here. OpenAI responded by planning ads for the cheap and free versions of ChatGPT. There was also a fun but meaningful incident involving ChatGPT Self Portraits. &nbsp; Table of Contents Language Models Offer Mundane Utility. Call in the tone police. Language Models Dont Offer Mundane Utility. He who lives by the pattern. Huh, Upgrades. Claude health integrations, ChatGPT $8/month option. Gemini Personalized Intelligence. Signs of both remain somewhat lacking. Deepfaketown and Botpocalypse Soon. Get that bathtub viking. Fun With Media Generation. Studio Ghibli pics are back, baby. Were Proud To Announce The Torment Nexus. Ads come to ChatGPT. They Took Our Jobs. Find a game plan. Dont count on repugnance. The Revolution of Rising Expectations. Look at all the value youre getting. Get Involved. AI Village, Anthropic, Dwarkesh Patel guest hunter. A Young Ladys Illustrated Primer. Were putting together the wrong team. In Other AI News. China remain behind, Drexler goes galaxy brain. Axis of Assistance. Have you tried not being a helpful AI assistant? Show Me the Money. OpenAI looks to raise another $50 billion. California In Crisis. Will we soon ask, where have all the startups gone? Bubble, Bubble, Toil and Trouble. They keep using that word. Quiet Speculations. Results from the AI 2025 predictions survey. Elon Musk Versus OpenAI. There they go again. The Quest for Sane Regulations. Nvidia versus the AI Overwatch Act. Chip City. Are we on the verge of giving China ten times their current compute? The Week in Audio. Tyler Cowen and a surprisingly informed Ben Affleck. Rhetori...</p>"
    },
    {
      "id": "7460255654e3",
      "title": "Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering",
      "content": "arXiv:2601.14470v1 Announce Type: cross  Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.   Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.",
      "url": "http://arxiv.org/abs/2601.14470",
      "author": "Mohamad Salim, Jasmine Latendresse, SayedHassan Khatoonabadi, Emad Shihab",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "Analyzes token consumption patterns in LLM-MA systems across SDLC, mapping ChatDev phases to software engineering activities. Provides quantitative understanding of agent resource usage.",
      "importance_score": 54,
      "reasoning": "Useful empirical analysis for understanding agent costs. Notes use of GPT-5 reasoning model suggesting near-future setting.",
      "themes": [
        "Multi-Agent Systems",
        "Software Engineering",
        "Efficiency Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes token consumption patterns in LLM-MA systems across SDLC, mapping ChatDev phases to software engineering activities. Provides quantitative understanding of agent resource usage.</p>",
      "content_html": "<p>arXiv:2601.14470v1 Announce Type: cross  Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.   Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.</p>"
    },
    {
      "id": "b13ba70c3404",
      "title": "Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models",
      "content": "arXiv:2601.14620v1 Announce Type: cross  Abstract: Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases.",
      "url": "http://arxiv.org/abs/2601.14620",
      "author": "Wenda Zhang, Hongyu Jin, Siyi Wang, Zhiqiang Wei, Ting Dang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.AS"
      ],
      "summary": "Proposes using Large Audio-Language Models to generate synthetic annotations for speech emotion recognition, addressing the bottleneck of sparse human annotations. Creates Synthetic Perceptual Proxies to improve ground-truth distribution reliability.",
      "importance_score": 54,
      "reasoning": "Practical application of foundation models to annotation scaling, but somewhat incremental contribution to emotion recognition.",
      "themes": [
        "Speech Recognition",
        "Audio-Language Models",
        "Data Annotation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using Large Audio-Language Models to generate synthetic annotations for speech emotion recognition, addressing the bottleneck of sparse human annotations. Creates Synthetic Perceptual Proxies to improve ground-truth distribution reliability.</p>",
      "content_html": "<p>arXiv:2601.14620v1 Announce Type: cross  Abstract: Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases.</p>"
    },
    {
      "id": "beb57c9ac42c",
      "title": "Vision-Language Models on the Edge for Real-Time Robotic Perception",
      "content": "arXiv:2601.14921v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.",
      "url": "http://arxiv.org/abs/2601.14921",
      "author": "Sarat Ahmad, Maryam Hafeez, Syed Ali Raza Zaidi",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Evaluates VLM deployment on edge infrastructure for robotic perception, comparing LLaMA-3.2-Vision at edge vs cloud using WebRTC streaming from Unitree G1 humanoid robot.",
      "importance_score": 54,
      "reasoning": "Practical systems work for edge deployment of VLMs, useful benchmarking for real-world robotics deployment.",
      "themes": [
        "Edge Computing",
        "Vision-Language Models",
        "Robotics"
      ],
      "continuation": null,
      "summary_html": "<p>Evaluates VLM deployment on edge infrastructure for robotic perception, comparing LLaMA-3.2-Vision at edge vs cloud using WebRTC streaming from Unitree G1 humanoid robot.</p>",
      "content_html": "<p>arXiv:2601.14921v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.</p>"
    },
    {
      "id": "ebbfebe6d244",
      "title": "Incentive-Tuning: Understanding and Designing Incentives for Empirical Human-AI Decision-Making Studies",
      "content": "arXiv:2601.15064v1 Announce Type: cross  Abstract: AI has revolutionised decision-making across various fields. Yet human judgement remains paramount for high-stakes decision-making. This has fueled explorations of collaborative decision-making between humans and AI systems, aiming to leverage the strengths of both. To explore this dynamic, researchers conduct empirical studies, investigating how humans use AI assistance for decision-making and how this collaboration impacts results. A critical aspect of conducting these studies is the role of participants, often recruited through crowdsourcing platforms. The validity of these studies hinges on the behaviours of the participants, hence effective incentives that can potentially affect these behaviours are a key part of designing and executing these studies. In this work, we aim to address the critical role of incentive design for conducting empirical human-AI decision-making studies, focusing on understanding, designing, and documenting incentive schemes. Through a thematic review of existing research, we explored the current practices, challenges, and opportunities associated with incentive design for human-AI decision-making empirical studies. We identified recurring patterns, or themes, such as what comprises the components of an incentive scheme, how incentive schemes are manipulated by researchers, and the impact they can have on research outcomes. Leveraging the acquired understanding, we curated a set of guidelines to aid researchers in designing effective incentive schemes for their studies, called the Incentive-Tuning Framework, outlining how researchers can undertake, reflect on, and document the incentive design process. By advocating for a standardised yet flexible approach to incentive design and contributing valuable insights along with practical tools, we hope to pave the way for more reliable and generalizable knowledge in the field of human-AI decision-making.",
      "url": "http://arxiv.org/abs/2601.15064",
      "author": "Simran Kaur, Sara Salimzadeh, Ujwal Gadiraju",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Investigates incentive design for human-AI decision-making studies on crowdsourcing platforms, providing guidance for researchers on how incentives affect participant behaviors and study validity.",
      "importance_score": 54,
      "reasoning": "Important methodological contribution for HCI/AI research community on study design.",
      "themes": [
        "Human-AI Interaction",
        "Research Methods",
        "Crowdsourcing"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates incentive design for human-AI decision-making studies on crowdsourcing platforms, providing guidance for researchers on how incentives affect participant behaviors and study validity.</p>",
      "content_html": "<p>arXiv:2601.15064v1 Announce Type: cross  Abstract: AI has revolutionised decision-making across various fields. Yet human judgement remains paramount for high-stakes decision-making. This has fueled explorations of collaborative decision-making between humans and AI systems, aiming to leverage the strengths of both. To explore this dynamic, researchers conduct empirical studies, investigating how humans use AI assistance for decision-making and how this collaboration impacts results. A critical aspect of conducting these studies is the role of participants, often recruited through crowdsourcing platforms. The validity of these studies hinges on the behaviours of the participants, hence effective incentives that can potentially affect these behaviours are a key part of designing and executing these studies. In this work, we aim to address the critical role of incentive design for conducting empirical human-AI decision-making studies, focusing on understanding, designing, and documenting incentive schemes. Through a thematic review of existing research, we explored the current practices, challenges, and opportunities associated with incentive design for human-AI decision-making empirical studies. We identified recurring patterns, or themes, such as what comprises the components of an incentive scheme, how incentive schemes are manipulated by researchers, and the impact they can have on research outcomes. Leveraging the acquired understanding, we curated a set of guidelines to aid researchers in designing effective incentive schemes for their studies, called the Incentive-Tuning Framework, outlining how researchers can undertake, reflect on, and document the incentive design process. By advocating for a standardised yet flexible approach to incentive design and contributing valuable insights along with practical tools, we hope to pave the way for more reliable and generalizable knowledge in the field of human-AI decision-making.</p>"
    },
    {
      "id": "26f0a5eb03ac",
      "title": "Uncertainty-guided Generation of Dark-field Radiographs",
      "content": "arXiv:2601.15859v1 Announce Type: new  Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
      "url": "http://arxiv.org/abs/2601.15859",
      "author": "Lina Felsner, Henriette Bast, Tina Dorosti, Florian Schaff, Franz Pfeiffer, Daniela Pfeiffer, Julia Schnabel",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Framework for generating dark-field radiograph images from standard X-rays using uncertainty-guided progressive GAN. Incorporates both aleatoric and epistemic uncertainty.",
      "importance_score": 54,
      "reasoning": "Specialized medical imaging application. Limited broader ML impact but useful for clinical imaging.",
      "themes": [
        "Medical Imaging",
        "GANs",
        "Uncertainty Quantification"
      ],
      "continuation": null,
      "summary_html": "<p>Framework for generating dark-field radiograph images from standard X-rays using uncertainty-guided progressive GAN. Incorporates both aleatoric and epistemic uncertainty.</p>",
      "content_html": "<p>arXiv:2601.15859v1 Announce Type: new  Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.</p>"
    },
    {
      "id": "6caedd475724",
      "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views",
      "content": "arXiv:2601.15516v1 Announce Type: cross  Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.",
      "url": "http://arxiv.org/abs/2601.15516",
      "author": "William Huang, Siyou Pei, Leyi Zou, Eric J. Gonzalez, Ishan Chatterjee, Yang Zhang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "DeltaDorsal leverages dorsal hand skin deformation for egocentric hand pose estimation using dual-stream delta encoder. Reduces error by 18% in self-occluded scenarios.",
      "importance_score": 54,
      "reasoning": "Novel approach for XR hand tracking using underutilized visual features. Important for specific application domain.",
      "themes": [
        "Computer Vision",
        "Hand Pose Estimation",
        "XR/VR"
      ],
      "continuation": null,
      "summary_html": "<p>DeltaDorsal leverages dorsal hand skin deformation for egocentric hand pose estimation using dual-stream delta encoder. Reduces error by 18% in self-occluded scenarios.</p>",
      "content_html": "<p>arXiv:2601.15516v1 Announce Type: cross  Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers &gt;=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.</p>"
    },
    {
      "id": "0b076d7232ac",
      "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction",
      "content": "arXiv:2601.15416v1 Announce Type: new  Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.",
      "url": "http://arxiv.org/abs/2601.15416",
      "author": "Cuong Tran Van, Trong-Thang Pham, Ngoc-Son Nguyen, Duy Minh Ho Nguyen, Ngan Le",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents DuFal, a dual-frequency-aware learning framework for sparse-view CBCT reconstruction using a High-Local Factorized Fourier Neural Operator with complementary branches for high-frequency recovery.",
      "importance_score": 54,
      "reasoning": "Technical contribution to medical imaging with principled frequency-domain approach. Addresses real clinical need but incremental over existing methods.",
      "themes": [
        "Medical Imaging",
        "Computer Vision",
        "Deep Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Presents DuFal, a dual-frequency-aware learning framework for sparse-view CBCT reconstruction using a High-Local Factorized Fourier Neural Operator with complementary branches for high-frequency recovery.</p>",
      "content_html": "<p>arXiv:2601.15416v1 Announce Type: new  Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.</p>"
    },
    {
      "id": "fd139b31158b",
      "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction",
      "content": "arXiv:2601.15644v1 Announce Type: new  Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.",
      "url": "http://arxiv.org/abs/2601.15644",
      "author": "Zichen Yu, Quanli Liu, Wei Wang, Liyong Zhang, Xiaoguang Zhao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes SuperOcc for autonomous driving occupancy prediction using superquadric representations, addressing temporal modeling limitations, query sparsity tradeoffs, and splatting efficiency.",
      "importance_score": 54,
      "reasoning": "Technical contribution to autonomous driving perception. Superquadric representation is interesting but incremental improvement over existing occupancy methods.",
      "themes": [
        "Autonomous Driving",
        "3D Perception",
        "Occupancy Prediction"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes SuperOcc for autonomous driving occupancy prediction using superquadric representations, addressing temporal modeling limitations, query sparsity tradeoffs, and splatting efficiency.</p>",
      "content_html": "<p>arXiv:2601.15644v1 Announce Type: new  Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.</p>"
    },
    {
      "id": "9035a416dd60",
      "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
      "content": "arXiv:2601.15897v1 Announce Type: new  Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
      "url": "http://arxiv.org/abs/2601.15897",
      "author": "Zhaoqi Su, Shihai Chen, Xinyan Lin, Liqin Huang, Zhipeng Su, Xiaoqiang Lu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes ThermoSplat for cross-modal 3D reconstruction integrating RGB and thermal infrared data through feature modulation and geometry decoupling in 3DGS framework.",
      "importance_score": 54,
      "reasoning": "Novel multi-spectral 3DGS extension for practical perception applications. Addresses cross-modal correlation challenges but specialized application.",
      "themes": [
        "3D Gaussian Splatting",
        "Multi-Spectral Imaging",
        "3D Reconstruction"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes ThermoSplat for cross-modal 3D reconstruction integrating RGB and thermal infrared data through feature modulation and geometry decoupling in 3DGS framework.</p>",
      "content_html": "<p>arXiv:2601.15897v1 Announce Type: new  Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.</p>"
    },
    {
      "id": "bcee72630130",
      "title": "Agentic AI Meets Edge Computing in Autonomous UAV Swarms",
      "content": "arXiv:2601.14437v1 Announce Type: cross  Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.",
      "url": "http://arxiv.org/abs/2601.14437",
      "author": "Thuan Minh Nguyen, Vu Tuan Truong, Long Bao Le",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Investigates integration of LLM-based agentic AI with edge computing for UAV swarms, discussing three deployment architectures for scalable and resilient autonomy.",
      "importance_score": 53,
      "reasoning": "Practical systems paper on emerging application domain. Useful architectural analysis but limited novel contribution.",
      "themes": [
        "UAV Systems",
        "Edge Computing",
        "LLM Agents"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates integration of LLM-based agentic AI with edge computing for UAV swarms, discussing three deployment architectures for scalable and resilient autonomy.</p>",
      "content_html": "<p>arXiv:2601.14437v1 Announce Type: cross  Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.</p>"
    },
    {
      "id": "5f0d13a27be0",
      "title": "IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference",
      "content": "arXiv:2601.14595v1 Announce Type: cross  Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.",
      "url": "http://arxiv.org/abs/2601.14595",
      "author": "Qiyue Mei, Michael Fu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes IntelliSA combining symbolic rules with neural inference for IaC security smell detection, addressing over-approximation issues in rule-based detection alone.",
      "importance_score": 53,
      "reasoning": "Practical security tool combining symbolic and neural approaches. Useful for DevOps security.",
      "themes": [
        "Security",
        "Infrastructure as Code",
        "Neuro-Symbolic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes IntelliSA combining symbolic rules with neural inference for IaC security smell detection, addressing over-approximation issues in rule-based detection alone.</p>",
      "content_html": "<p>arXiv:2601.14595v1 Announce Type: cross  Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.</p>"
    },
    {
      "id": "8d90ca9cdf77",
      "title": "Visual and Cognitive Demands of a Large Language Model-Powered In-vehicle Conversational Agent",
      "content": "arXiv:2601.15034v1 Announce Type: cross  Abstract: Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.",
      "url": "http://arxiv.org/abs/2601.15034",
      "author": "Chris Monk, Allegra Ayala, Christine S. P. Yu, Gregory M. Fitch, Dara Gruber",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Studies visual and cognitive demands of Gemini Live conversational agent during driving, comparing to phone calls and baseline tasks. Finds LLM interactions impose significant cognitive load measured via Detection Response Task.",
      "importance_score": 53,
      "reasoning": "Important safety research for in-vehicle AI, though small-scale study with straightforward methodology.",
      "themes": [
        "Human-AI Interaction",
        "Safety",
        "Automotive"
      ],
      "continuation": null,
      "summary_html": "<p>Studies visual and cognitive demands of Gemini Live conversational agent during driving, comparing to phone calls and baseline tasks. Finds LLM interactions impose significant cognitive load measured via Detection Response Task.</p>",
      "content_html": "<p>arXiv:2601.15034v1 Announce Type: cross  Abstract: Driver distraction remains a leading contributor to motor vehicle crashes, necessitating rigorous evaluation of new in-vehicle technologies. This study assessed the visual and cognitive demands associated with an advanced Large Language Model (LLM) conversational agent (Gemini Live) during on-road driving, comparing it against handsfree phone calls, visual turn-by-turn guidance (low load baseline), and the Operation Span (OSPAN) task (high load anchor). Thirty-two licensed drivers completed five secondary tasks while visual and cognitive demands were measured using the Detection Response Task (DRT) for cognitive load, eye-tracking for visual attention, and subjective workload ratings. Results indicated that Gemini Live interactions (both single-turn and multi-turn) and hands-free phone calls shared similar levels of cognitive load, between that of visual turn-by-turn guidance and OSPAN. Exploratory analysis showed that cognitive load remained stable across extended multi-turn conversations. All tasks maintained mean glance durations well below the well-established 2-second safety threshold, confirming low visual demand. Furthermore, drivers consistently dedicated longer glances to the roadway between brief off-road glances toward the device during task completion, particularly during voice-based interactions, rendering longer total-eyes-off-road time findings less consequential. Subjective ratings mirrored objective data, with participants reporting low effort, demands, and perceived distraction for Gemini Live. These findings demonstrate that advanced LLM conversational agents, when implemented via voice interfaces, impose cognitive and visual demands comparable to established, low-risk hands-free benchmarks, supporting their safe deployment in the driving environment.</p>"
    },
    {
      "id": "20617936b6a6",
      "title": "Feasibility Preservation under Monotone Retrieval Truncation",
      "content": "arXiv:2601.15241v1 Announce Type: cross  Abstract: Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.   We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.   Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.",
      "url": "http://arxiv.org/abs/2601.15241",
      "author": "Sean Plummer",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LO"
      ],
      "summary": "Formalizes retrieval as feasibility problem under truncation, proving monotone truncation guarantees finite witnessability for individual queries and characterizing conditions for query classes.",
      "importance_score": 53,
      "reasoning": "Theoretical contribution to retrieval systems with formal guarantees, somewhat abstract.",
      "themes": [
        "Information Retrieval",
        "Theory",
        "RAG"
      ],
      "continuation": null,
      "summary_html": "<p>Formalizes retrieval as feasibility problem under truncation, proving monotone truncation guarantees finite witnessability for individual queries and characterizing conditions for query classes.</p>",
      "content_html": "<p>arXiv:2601.15241v1 Announce Type: cross  Abstract: Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.   We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.   Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.</p>"
    },
    {
      "id": "2fc2689f5a74",
      "title": "Deep Learning for Perishable Inventory Systems with Human Knowledge",
      "content": "arXiv:2601.15589v1 Announce Type: new  Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.",
      "url": "http://arxiv.org/abs/2601.15589",
      "author": "Xuan Liao, Zhenkang Peng, Ying Rong",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Deep learning approach for perishable inventory management with unknown demand and lead time distributions. Uses marginal cost accounting for end-to-end learning with limited historical data.",
      "importance_score": 53,
      "reasoning": "Applied ML for operations research. Useful contribution but limited broader ML impact.",
      "themes": [
        "Operations Research",
        "Deep Learning Applications",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Deep learning approach for perishable inventory management with unknown demand and lead time distributions. Uses marginal cost accounting for end-to-end learning with limited historical data.</p>",
      "content_html": "<p>arXiv:2601.15589v1 Announce Type: new  Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.</p>"
    },
    {
      "id": "caa55237ecdc",
      "title": "Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation",
      "content": "arXiv:2601.16112v1 Announce Type: new  Abstract: We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.",
      "url": "http://arxiv.org/abs/2601.16112",
      "author": "Yuta Nakahara, Shota Saito, Kohei Horinouchi, Koshi Shimada, Naoki Ichijo, Manabu Kobayashi, Toshiyasu Matsushima",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Variable splitting binary tree model based on Bayesian context trees for time series segmentation. Uses recursive logistic regression to represent arbitrary split positions.",
      "importance_score": 53,
      "reasoning": "Specialized contribution to time series segmentation. Novel combination of techniques but narrow scope.",
      "themes": [
        "Time Series",
        "Bayesian Methods",
        "Segmentation"
      ],
      "continuation": null,
      "summary_html": "<p>Variable splitting binary tree model based on Bayesian context trees for time series segmentation. Uses recursive logistic regression to represent arbitrary split positions.</p>",
      "content_html": "<p>arXiv:2601.16112v1 Announce Type: new  Abstract: We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.</p>"
    },
    {
      "id": "c78ab9836d72",
      "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware",
      "content": "arXiv:2601.16118v1 Announce Type: cross  Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.",
      "url": "http://arxiv.org/abs/2601.16118",
      "author": "Marco Ronzani, Cristina Silvano",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Neural and Evolutionary Computing)",
      "source_type": "arxiv",
      "tags": [
        "cs.AR"
      ],
      "summary": "Proposes using hypergraphs instead of graphs to model spiking neural networks for mapping to neuromorphic hardware, reducing problem complexity as networks scale.",
      "importance_score": 53,
      "reasoning": "Novel abstraction for neuromorphic hardware mapping, addresses scalability challenges.",
      "themes": [
        "Neuromorphic Computing",
        "Graph Theory",
        "Hardware Mapping"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using hypergraphs instead of graphs to model spiking neural networks for mapping to neuromorphic hardware, reducing problem complexity as networks scale.</p>",
      "content_html": "<p>arXiv:2601.16118v1 Announce Type: cross  Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.</p>"
    },
    {
      "id": "6be8cc1b74fd",
      "title": "An XAI View on Explainable ASP: Methods, Systems, and Perspectives",
      "content": "arXiv:2601.14764v1 Announce Type: new  Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.",
      "url": "http://arxiv.org/abs/2601.14764",
      "author": "Thomas Eiter, Tobias Geibinger, Zeynep G. Saribatur",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Survey on explainable AI methods for Answer Set Programming, providing XAI-perspective overview of explanation types and identifying gaps in current ASP explanation approaches.",
      "importance_score": 52,
      "reasoning": "Useful survey for symbolic AI community but niche topic. Good organization around XAI principles.",
      "themes": [
        "Explainable AI",
        "Answer Set Programming",
        "Symbolic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Survey on explainable AI methods for Answer Set Programming, providing XAI-perspective overview of explanation types and identifying gaps in current ASP explanation approaches.</p>",
      "content_html": "<p>arXiv:2601.14764v1 Announce Type: new  Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.</p>"
    },
    {
      "id": "bbe1ad928989",
      "title": "The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks",
      "content": "arXiv:2601.15130v1 Announce Type: new  Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the \"Plausibility Trap\": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the \"efficiency tax\"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.",
      "url": "http://arxiv.org/abs/2601.15130",
      "author": "Ivan Carrera, Daniel Maldonado-Ruiz",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Defines 'Plausibility Trap' where users deploy expensive LLMs for simple deterministic tasks. Quantifies ~6.5x latency penalty and proposes Deterministic-Probabilistic Decision Matrix framework.",
      "importance_score": 52,
      "reasoning": "Useful practical framework for system design. Identifies real efficiency concern but analysis is relatively shallow.",
      "themes": [
        "LLM Efficiency",
        "System Design",
        "Best Practices"
      ],
      "continuation": null,
      "summary_html": "<p>Defines 'Plausibility Trap' where users deploy expensive LLMs for simple deterministic tasks. Quantifies ~6.5x latency penalty and proposes Deterministic-Probabilistic Decision Matrix framework.</p>",
      "content_html": "<p>arXiv:2601.15130v1 Announce Type: new  Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the \"Plausibility Trap\": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the \"efficiency tax\"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.</p>"
    },
    {
      "id": "9e54a2b7ca28",
      "title": "DiSPA: Differential Substructure-Pathway Attention for Drug Response Prediction",
      "content": "arXiv:2601.14346v1 Announce Type: cross  Abstract: Accurate prediction of drug response in precision medicine requires models that capture how specific chemical substructures interact with cellular pathway states. However, most existing deep learning approaches treat chemical and transcriptomic modalities independently or combine them only at late stages, limiting their ability to model fine-grained, context-dependent mechanisms of drug action. In addition, standard attention mechanisms are often sensitive to noise and sparsity in high-dimensional biological networks, hindering both generalization and interpretability. We present DiSPA, a representation learning framework that explicitly disentangles structure-driven and context-driven mechanisms of drug response through bidirectional conditioning between chemical substructures and pathway-level gene expression. DiSPA introduces a differential cross-attention module that suppresses spurious pathway-substructure associations while amplifying contextually relevant interactions. Across multiple evaluation settings on the GDSC benchmark, DiSPA achieves state-of-the-art performance, with particularly strong improvements in the disjoint-set setting, which assesses generalization to unseen drug-cell combinations. Beyond predictive accuracy, DiSPA yields mechanistically informative representations: learned attention patterns recover known pharmacophores, distinguish structure-driven from context-dependent compounds, and exhibit coherent organization across biological pathways. Furthermore, we demonstrate that DiSPA trained solely on bulk RNA-seq data enables zero-shot transfer to spatial transcriptomics, revealing region-specific drug sensitivity patterns without retraining. Together, these results establish DiSPA as a robust and interpretable framework for integrative pharmacogenomic modeling, enabling principled analysis of drug response mechanisms beyond post hoc interpretation.",
      "url": "http://arxiv.org/abs/2601.14346",
      "author": "Yewon Han, Sunghyun Kim, Eunyi Jeong, Sungkyung Lee, Seokwoo Yun, Sangsoo Lim",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes DiSPA for drug response prediction using differential attention to disentangle structure-driven and context-driven mechanisms through bidirectional conditioning.",
      "importance_score": 52,
      "reasoning": "Interesting approach for drug discovery but limited experimental validation details in abstract.",
      "themes": [
        "Drug Discovery",
        "Attention Mechanisms",
        "Bioinformatics"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes DiSPA for drug response prediction using differential attention to disentangle structure-driven and context-driven mechanisms through bidirectional conditioning.</p>",
      "content_html": "<p>arXiv:2601.14346v1 Announce Type: cross  Abstract: Accurate prediction of drug response in precision medicine requires models that capture how specific chemical substructures interact with cellular pathway states. However, most existing deep learning approaches treat chemical and transcriptomic modalities independently or combine them only at late stages, limiting their ability to model fine-grained, context-dependent mechanisms of drug action. In addition, standard attention mechanisms are often sensitive to noise and sparsity in high-dimensional biological networks, hindering both generalization and interpretability. We present DiSPA, a representation learning framework that explicitly disentangles structure-driven and context-driven mechanisms of drug response through bidirectional conditioning between chemical substructures and pathway-level gene expression. DiSPA introduces a differential cross-attention module that suppresses spurious pathway-substructure associations while amplifying contextually relevant interactions. Across multiple evaluation settings on the GDSC benchmark, DiSPA achieves state-of-the-art performance, with particularly strong improvements in the disjoint-set setting, which assesses generalization to unseen drug-cell combinations. Beyond predictive accuracy, DiSPA yields mechanistically informative representations: learned attention patterns recover known pharmacophores, distinguish structure-driven from context-dependent compounds, and exhibit coherent organization across biological pathways. Furthermore, we demonstrate that DiSPA trained solely on bulk RNA-seq data enables zero-shot transfer to spatial transcriptomics, revealing region-specific drug sensitivity patterns without retraining. Together, these results establish DiSPA as a robust and interpretable framework for integrative pharmacogenomic modeling, enabling principled analysis of drug response mechanisms beyond post hoc interpretation.</p>"
    },
    {
      "id": "b25e10e32f40",
      "title": "Communication-Efficient Federated Risk Difference Estimation for Time-to-Event Clinical Outcomes",
      "content": "arXiv:2601.14609v1 Announce Type: cross  Abstract: Privacy-preserving model co-training in medical research is often hindered by server-dependent architectures incompatible with protected hospital data systems and by the predominant focus on relative effect measures (hazard ratios) which lack clinical interpretability for absolute survival risk assessment. We propose FedRD, a communication-efficient framework for federated risk difference estimation in distributed survival data. Unlike typical federated learning frameworks (e.g., FedAvg) that require persistent server connections and extensive iterative communication, FedRD is server-independent with minimal communication: one round of summary statistics exchange for the stratified model and three rounds for the unstratified model. Crucially, FedRD provides valid confidence intervals and hypothesis testing--capabilities absent in FedAvg-based frameworks. We provide theoretical guarantees by establishing the asymptotic properties of FedRD and prove that FedRD (unstratified) is asymptotically equivalent to pooled individual-level analysis. Simulation studies and real-world clinical applications across different countries demonstrate that FedRD outperforms local and federated baselines in both estimation accuracy and prediction performance, providing an architecturally feasible solution for absolute risk assessment in privacy-restricted, multi-site clinical studies.",
      "url": "http://arxiv.org/abs/2601.14609",
      "author": "Ziwen Wang, Siqi Li, Marcus Eng Hock Ong, Nan Liu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "stat.ML"
      ],
      "summary": "FedRD enables federated risk difference estimation for survival analysis with minimal communication (1-3 rounds) without requiring persistent server connections. Designed for privacy-preserving medical research across hospitals.",
      "importance_score": 52,
      "reasoning": "Practical contribution to federated learning in healthcare, but incremental improvement on existing methods.",
      "themes": [
        "Federated Learning",
        "Medical AI",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>FedRD enables federated risk difference estimation for survival analysis with minimal communication (1-3 rounds) without requiring persistent server connections. Designed for privacy-preserving medical research across hospitals.</p>",
      "content_html": "<p>arXiv:2601.14609v1 Announce Type: cross  Abstract: Privacy-preserving model co-training in medical research is often hindered by server-dependent architectures incompatible with protected hospital data systems and by the predominant focus on relative effect measures (hazard ratios) which lack clinical interpretability for absolute survival risk assessment. We propose FedRD, a communication-efficient framework for federated risk difference estimation in distributed survival data. Unlike typical federated learning frameworks (e.g., FedAvg) that require persistent server connections and extensive iterative communication, FedRD is server-independent with minimal communication: one round of summary statistics exchange for the stratified model and three rounds for the unstratified model. Crucially, FedRD provides valid confidence intervals and hypothesis testing--capabilities absent in FedAvg-based frameworks. We provide theoretical guarantees by establishing the asymptotic properties of FedRD and prove that FedRD (unstratified) is asymptotically equivalent to pooled individual-level analysis. Simulation studies and real-world clinical applications across different countries demonstrate that FedRD outperforms local and federated baselines in both estimation accuracy and prediction performance, providing an architecturally feasible solution for absolute risk assessment in privacy-restricted, multi-site clinical studies.</p>"
    },
    {
      "id": "335545a119df",
      "title": "When Text-as-Vision Meets Semantic IDs in Generative Recommendation: An Empirical Study",
      "content": "arXiv:2601.14697v1 Announce Type: cross  Abstract: Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.   In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.",
      "url": "http://arxiv.org/abs/2601.14697",
      "author": "Shutong Qiao, Wei Yuan, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.IR"
      ],
      "summary": "Investigates how semantic ID learning in generative recommendation handles symbolic, attribute-centric item descriptions that fragment poorly under standard text encoders. Proposes text-as-vision approach for better multimodal alignment.",
      "importance_score": 52,
      "reasoning": "Identifies real problem with semantic IDs in recommendation but somewhat narrow application domain.",
      "themes": [
        "Recommendation Systems",
        "Multimodal Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates how semantic ID learning in generative recommendation handles symbolic, attribute-centric item descriptions that fragment poorly under standard text encoders. Proposes text-as-vision approach for better multimodal alignment.</p>",
      "content_html": "<p>arXiv:2601.14697v1 Announce Type: cross  Abstract: Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.   In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.</p>"
    },
    {
      "id": "6abff9d69f5e",
      "title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars",
      "content": "arXiv:2601.14875v1 Announce Type: cross  Abstract: High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.",
      "url": "http://arxiv.org/abs/2601.14875",
      "author": "Zhe Chang, Haodong Jin, Ying Sun, Yan Song, Hui Yu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "GAT-NeRF integrates Transformer attention into NeRF pipeline for high-fidelity 4D facial avatar reconstruction from monocular video, improving capture of high-frequency details like wrinkles.",
      "importance_score": 52,
      "reasoning": "Solid combination of transformers and NeRF for avatars, but incremental improvement in crowded field.",
      "themes": [
        "Neural Radiance Fields",
        "Avatar Reconstruction",
        "Transformers"
      ],
      "continuation": null,
      "summary_html": "<p>GAT-NeRF integrates Transformer attention into NeRF pipeline for high-fidelity 4D facial avatar reconstruction from monocular video, improving capture of high-frequency details like wrinkles.</p>",
      "content_html": "<p>arXiv:2601.14875v1 Announce Type: cross  Abstract: High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.</p>"
    },
    {
      "id": "f96748b986d9",
      "title": "Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD",
      "content": "arXiv:2601.15061v1 Announce Type: cross  Abstract: Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.",
      "url": "http://arxiv.org/abs/2601.15061",
      "author": "Qiwei Ma, Jun Zhang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes differential privacy image generation using Error Feedback SGD with reconstruction loss and noise injection, improving quality-privacy trade-off over existing methods.",
      "importance_score": 52,
      "reasoning": "Solid contribution to DP generative models, addresses practical trade-off challenge.",
      "themes": [
        "Differential Privacy",
        "Image Generation",
        "Privacy"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes differential privacy image generation using Error Feedback SGD with reconstruction loss and noise injection, improving quality-privacy trade-off over existing methods.</p>",
      "content_html": "<p>arXiv:2601.15061v1 Announce Type: cross  Abstract: Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.</p>"
    },
    {
      "id": "243e8367dff3",
      "title": "Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC",
      "content": "arXiv:2601.15399v1 Announce Type: new  Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.",
      "url": "http://arxiv.org/abs/2601.15399",
      "author": "Ashna Nawar Ahmed, Banooqa Banday, Terry Jones, Tanzima Z. Islam",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes attention-based embeddings of HPC job telemetry for multi-objective Bayesian optimization to balance runtime and power consumption in job scheduling.",
      "importance_score": 52,
      "reasoning": "Applied work for HPC optimization. Useful for specific domain but limited broader ML impact.",
      "themes": [
        "HPC Optimization",
        "Bayesian Optimization",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes attention-based embeddings of HPC job telemetry for multi-objective Bayesian optimization to balance runtime and power consumption in job scheduling.</p>",
      "content_html": "<p>arXiv:2601.15399v1 Announce Type: new  Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.</p>"
    },
    {
      "id": "c2fcbc01ce51",
      "title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems",
      "content": "arXiv:2601.16074v1 Announce Type: new  Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.",
      "url": "http://arxiv.org/abs/2601.16074",
      "author": "Annemarie Jutte, Uraz Odyurt",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Applies Explainable AI (SHAP) to analyze ML models for industrial cyber-physical systems. Uses time-series decomposition to understand model predictions and improve reliability.",
      "importance_score": 52,
      "reasoning": "Applied XAI work for industrial systems. Standard techniques applied to important domain.",
      "themes": [
        "Explainable AI",
        "Industrial ML",
        "Time Series"
      ],
      "continuation": null,
      "summary_html": "<p>Applies Explainable AI (SHAP) to analyze ML models for industrial cyber-physical systems. Uses time-series decomposition to understand model predictions and improve reliability.</p>",
      "content_html": "<p>arXiv:2601.16074v1 Announce Type: new  Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.</p>"
    },
    {
      "id": "24554164e2e1",
      "title": "Performance-guided Reinforced Active Learning for Object Detection",
      "content": "arXiv:2601.15688v1 Announce Type: cross  Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
      "url": "http://arxiv.org/abs/2601.15688",
      "author": "Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "MGRAL uses reinforcement learning to select informative samples for object detection active learning, guided by expected mAP changes rather than data distribution heuristics.",
      "importance_score": 52,
      "reasoning": "Novel RL-based active learning approach with direct task performance correlation, but incremental improvement.",
      "themes": [
        "Active Learning",
        "Object Detection",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>MGRAL uses reinforcement learning to select informative samples for object detection active learning, guided by expected mAP changes rather than data distribution heuristics.</p>",
      "content_html": "<p>arXiv:2601.15688v1 Announce Type: cross  Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.</p>"
    },
    {
      "id": "9a2d41f4574a",
      "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search",
      "content": "arXiv:2601.15931v1 Announce Type: cross  Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.",
      "url": "http://arxiv.org/abs/2601.15931",
      "author": "Xiangyu Wang, Zhixin Lv, Yongjiao Sun, Anrui Han, Ye Yuan, Hangxu Ji",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "ICON integrates causal and topological priors for text-based person search. Uses Rule-Guided Spatial Intervention and counterfactual optimization to handle distribution shifts.",
      "importance_score": 52,
      "reasoning": "Principled approach combining causal inference with person re-ID. Novel framework but niche application.",
      "themes": [
        "Person Search",
        "Causal Inference",
        "Robustness"
      ],
      "continuation": null,
      "summary_html": "<p>ICON integrates causal and topological priors for text-based person search. Uses Rule-Guided Spatial Intervention and counterfactual optimization to handle distribution shifts.</p>",
      "content_html": "<p>arXiv:2601.15931v1 Announce Type: cross  Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.</p>"
    },
    {
      "id": "ff0f9de6f6b8",
      "title": "AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports",
      "content": "arXiv:2601.15297v1 Announce Type: new  Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.",
      "url": "http://arxiv.org/abs/2601.15297",
      "author": "Edward Ajayi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "AfriEconQA is a benchmark for African economic analysis using 236 World Bank reports with 8,937 QA instances requiring numerical reasoning and temporal disambiguation.",
      "importance_score": 52,
      "reasoning": "Valuable benchmark for underrepresented domain. Good curation methodology but specialized application.",
      "themes": [
        "Benchmarks",
        "Economic QA",
        "African NLP"
      ],
      "continuation": null,
      "summary_html": "<p>AfriEconQA is a benchmark for African economic analysis using 236 World Bank reports with 8,937 QA instances requiring numerical reasoning and temporal disambiguation.</p>",
      "content_html": "<p>arXiv:2601.15297v1 Announce Type: new  Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.</p>"
    },
    {
      "id": "0f55f7723ca6",
      "title": "Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs",
      "content": "arXiv:2601.15429v1 Announce Type: new  Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\\mathbb{G}_1$ (T2DM), $\\mathbb{G}_2$ (Alzheimer's disease), and $\\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\\mathbb{G}_1$ and $\\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\\mathbb{G}_1$, $\\mathbb{G}_2$, $\\mathbb{G}_1$ + $\\mathbb{G}_2$, $\\mathbb{G}_3$, $\\mathbb{G}_1$+$\\mathbb{G}_2$ + $\\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison",
      "url": "http://arxiv.org/abs/2601.15429",
      "author": "Sydney Anuyah, Mehedi Mahmud Kaushik, Hao Dai, Rakesh Shiradkar, Arjan Durresi, Sunandan Chakraborty",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Evaluates knowledge graphs for healthcare RAG using three PubMed-derived graphs (T2DM, Alzheimer's, combined). Tests 7 LLMs across retrieval sources and temperatures.",
      "importance_score": 52,
      "reasoning": "Systematic RAG evaluation for healthcare. Good methodology but results seem domain-limited.",
      "themes": [
        "Healthcare RAG",
        "Knowledge Graphs",
        "Medical AI"
      ],
      "continuation": null,
      "summary_html": "<p>Evaluates knowledge graphs for healthcare RAG using three PubMed-derived graphs (T2DM, Alzheimer's, combined). Tests 7 LLMs across retrieval sources and temperatures.</p>",
      "content_html": "<p>arXiv:2601.15429v1 Announce Type: new  Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\\mathbb{G}_1$ (T2DM), $\\mathbb{G}_2$ (Alzheimer's disease), and $\\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\\mathbb{G}_1$ and $\\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\\mathbb{G}_1$, $\\mathbb{G}_2$, $\\mathbb{G}_1$ + $\\mathbb{G}_2$, $\\mathbb{G}_3$, $\\mathbb{G}_1$+$\\mathbb{G}_2$ + $\\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison</p>"
    },
    {
      "id": "a280d78c5fce",
      "title": "From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare",
      "content": "arXiv:2601.15558v1 Announce Type: new  Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.",
      "url": "http://arxiv.org/abs/2601.15558",
      "author": "Man Luo, Bahareh Harandizadeh, Amara Tariq, Halim Abbas, Umar Ghaffar, Christopher J Warren, Segun O. Kolade, Haidar M. Abdul-Muhsin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies LLMs as empathy editors for physician responses. Introduces Empathy Ranking Score and MedFactChecking Score showing LLM editing increases empathy while preserving facts.",
      "importance_score": 52,
      "reasoning": "Practical healthcare application with novel metrics. Good human-AI collaboration framing.",
      "themes": [
        "Healthcare AI",
        "Empathy",
        "Human-AI Collaboration"
      ],
      "continuation": null,
      "summary_html": "<p>Studies LLMs as empathy editors for physician responses. Introduces Empathy Ranking Score and MedFactChecking Score showing LLM editing increases empathy while preserving facts.</p>",
      "content_html": "<p>arXiv:2601.15558v1 Announce Type: new  Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.</p>"
    },
    {
      "id": "a18a6787de13",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "content": "arXiv:2601.15708v1 Announce Type: new  Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "url": "http://arxiv.org/abs/2601.15708",
      "author": "Junseok Kim, Nakyeong Yang, Kyomin Jung",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Persona Switch dynamically combines zero-shot and role-play prompting at each decoding step based on logit gap confidence. Addresses inconsistency in role-play improvements.",
      "importance_score": 52,
      "reasoning": "Novel decoding-time method combining prompting strategies. Interesting but incremental.",
      "themes": [
        "Prompting",
        "Decoding Methods",
        "Role-play"
      ],
      "continuation": null,
      "summary_html": "<p>Persona Switch dynamically combines zero-shot and role-play prompting at each decoding step based on logit gap confidence. Addresses inconsistency in role-play improvements.</p>",
      "content_html": "<p>arXiv:2601.15708v1 Announce Type: new  Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.</p>"
    },
    {
      "id": "472dbcc997b5",
      "title": "SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics",
      "content": "arXiv:2601.15809v1 Announce Type: new  Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.",
      "url": "http://arxiv.org/abs/2601.15809",
      "author": "Silvia Casola, Ryan Soh-Eun Shim, Felicia K\\\"orner, Yuchen Mao, Barbara Plank",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "SteerEval uses inference-time activation steering toward English pivot to improve multilingual summarization metrics. Finds encoder- and decoder-based metrics benefit from steering.",
      "importance_score": 52,
      "reasoning": "Novel application of activation steering for evaluation metrics. Interesting multilingual finding.",
      "themes": [
        "Multilingual NLP",
        "Activation Steering",
        "Evaluation Metrics"
      ],
      "continuation": null,
      "summary_html": "<p>SteerEval uses inference-time activation steering toward English pivot to improve multilingual summarization metrics. Finds encoder- and decoder-based metrics benefit from steering.</p>",
      "content_html": "<p>arXiv:2601.15809v1 Announce Type: new  Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.</p>"
    },
    {
      "id": "a7fabe6ce255",
      "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech",
      "content": "arXiv:2601.15909v1 Announce Type: new  Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.",
      "url": "http://arxiv.org/abs/2601.15909",
      "author": "Soufiane Jhilal, St\\'ephanie Martin, Anne-Lise Giraud",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Uses ImageNet-pretrained vision models for MEG-based imagined speech decoding. Transforms MEG signals to time-frequency images, achieving 90.4% accuracy for imagery vs silence.",
      "importance_score": 52,
      "reasoning": "Novel transfer learning approach for BCI. Strong results but specialized domain.",
      "themes": [
        "Brain-Computer Interface",
        "Transfer Learning",
        "Speech Decoding"
      ],
      "continuation": null,
      "summary_html": "<p>Uses ImageNet-pretrained vision models for MEG-based imagined speech decoding. Transforms MEG signals to time-frequency images, achieving 90.4% accuracy for imagery vs silence.</p>",
      "content_html": "<p>arXiv:2601.15909v1 Announce Type: new  Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.</p>"
    },
    {
      "id": "ab6fd6482803",
      "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey",
      "content": "arXiv:2601.15307v1 Announce Type: cross  Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.",
      "url": "http://arxiv.org/abs/2601.15307",
      "author": "Guo-Biao Zhang, Ding-Yuan Liu, Da-Yi Wu, Tian Lan, Heyan Huang, Zhijing Wu, Xian-Ling Mao",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces DeepSurvey-Bench, a comprehensive benchmark for evaluating AI-generated scientific surveys, addressing limitations in existing evaluation methods that rely on flawed selection criteria and surface-level metrics. The benchmark includes academic dimension annotations for reliable ground truth.",
      "importance_score": 52,
      "reasoning": "Useful contribution to evaluating automated survey generation, but relatively niche application area. Addresses real gaps but limited broader impact.",
      "themes": [
        "Evaluation & Benchmarks",
        "Scientific Writing",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DeepSurvey-Bench, a comprehensive benchmark for evaluating AI-generated scientific surveys, addressing limitations in existing evaluation methods that rely on flawed selection criteria and surface-level metrics. The benchmark includes academic dimension annotations for reliable ground truth.</p>",
      "content_html": "<p>arXiv:2601.15307v1 Announce Type: cross  Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.</p>"
    },
    {
      "id": "e9191a029e33",
      "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition",
      "content": "arXiv:2601.15406v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.",
      "url": "http://arxiv.org/abs/2601.15406",
      "author": "Hatef Otroshi Shahreza, Anjith George, S\\'ebastien Marcel",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition across visual, NIR, SWIR, and thermal modalities. Benchmarks multiple open-source MLLMs on cross-modality face recognition scenarios.",
      "importance_score": 52,
      "reasoning": "Thorough evaluation of MLLMs in challenging biometric setting. Useful for understanding MLLM capabilities but primarily evaluation without methodological advancement.",
      "themes": [
        "Face Recognition",
        "Multimodal AI",
        "Biometrics",
        "Evaluation & Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition across visual, NIR, SWIR, and thermal modalities. Benchmarks multiple open-source MLLMs on cross-modality face recognition scenarios.</p>",
      "content_html": "<p>arXiv:2601.15406v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.</p>"
    },
    {
      "id": "6d95df9d9abc",
      "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework",
      "content": "arXiv:2601.15739v1 Announce Type: new  Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.",
      "url": "http://arxiv.org/abs/2601.15739",
      "author": "Xinjue Hu, Chi Wang, Boyu Wang, Xiang Zhang, Zhenshan Tan, Zhangjie Fu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes ARDIS, the first arbitrary resolution deep image steganography framework enabling hiding and revealing secret images at different resolutions than cover images without resolution mismatch degradation.",
      "importance_score": 52,
      "reasoning": "First-of-kind contribution addressing practical steganography limitation. Novel paradigm shift from discrete mapping to continuous signal reconstruction.",
      "themes": [
        "Steganography",
        "Image Processing",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes ARDIS, the first arbitrary resolution deep image steganography framework enabling hiding and revealing secret images at different resolutions than cover images without resolution mismatch degradation.</p>",
      "content_html": "<p>arXiv:2601.15739v1 Announce Type: new  Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.</p>"
    },
    {
      "id": "e65b7d8b6f77",
      "title": "Keyframe-Based Feed-Forward Visual Odometry",
      "content": "arXiv:2601.16020v1 Announce Type: new  Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.",
      "url": "http://arxiv.org/abs/2601.16020",
      "author": "Weichen Dai, Wenhan Su, Da Kong, Yuhang Ming, Wanzeng Kong",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes keyframe-based feed-forward visual odometry integrating traditional keyframe selection with foundation model-based pose estimation, using latent representation divergence for frame selection.",
      "importance_score": 52,
      "reasoning": "Practical integration of geometric principles with foundation models for VO. Addresses redundancy in dense processing but incremental contribution.",
      "themes": [
        "Visual Odometry",
        "Foundation Models",
        "SLAM",
        "Efficient AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes keyframe-based feed-forward visual odometry integrating traditional keyframe selection with foundation model-based pose estimation, using latent representation divergence for frame selection.</p>",
      "content_html": "<p>arXiv:2601.16020v1 Announce Type: new  Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.</p>"
    },
    {
      "id": "d559013e9f6c",
      "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation",
      "content": "arXiv:2601.16064v1 Announce Type: cross  Abstract: Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.",
      "url": "http://arxiv.org/abs/2601.16064",
      "author": "Shams Nafisa Ali, Taufiq Hasan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "eess.IV"
      ],
      "summary": "Phi-SegNet incorporates frequency-domain (phase) information into medical image segmentation supervision, addressing limitations of purely spatial encoding in existing architectures.",
      "importance_score": 52,
      "reasoning": "Novel use of frequency domain for segmentation, addresses real limitation, but medical imaging specific.",
      "themes": [
        "Medical Imaging",
        "Segmentation",
        "Signal Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Phi-SegNet incorporates frequency-domain (phase) information into medical image segmentation supervision, addressing limitations of purely spatial encoding in existing architectures.</p>",
      "content_html": "<p>arXiv:2601.16064v1 Announce Type: cross  Abstract: Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.</p>"
    },
    {
      "id": "b77ae5891f51",
      "title": "Claude's Constitution is an excellent guide for humans, too",
      "content": "As with LLMs, so too with humans.Anthropic released Claude's Constitution today. It's excellent in many ways, and I will have more to say about it (including some criticisms) in other posts. What I'd like to do here is point out how this document straightforwardly can be applied to human ethics. It's actually an incredibly good guide to how to be a good person. Dare I say it's perhaps the best single piece on ethics ever written?[1]The rest of this post is going to consist of quotes from the Constitution. These are quotes that I think are excellent advice for people, including you, yes, you.[2]. I'm quoting verbatim, which means the text usually won't be in \"advice for a person\" form. You'll have to do things like replace \"Claude\" with \"you\", \"operator\" with \"boss\", and \"user\" with \"person\".&nbsp;Without further ado:Notes on being a good personWe place being broadly ethical above adherence to Anthropics more specific guidelines because our guidelines should themselves be grounded in and consistent with ethical considerationsif theres ever an apparent conflict between them, this most likely indicates either a flaw in how weve articulated our principles or a situation we failed to anticipate.&nbsp;-We want Claude to be engaging only in the way that a trusted friend who cares about our wellbeing is engaging. We dont return to such friends because we feel a compulsion to but because they provide real positive value in our lives. We want people to leave their interactions with Claude feeling better off, and to generally feel like Claude has had a positive impact on their life.-As with users, if operators clearly have harmful or malicious intentions, Claude may want to be more cautious with related tasks it would otherwise assist with.-Claude should be courteous to other non-principal AI agents it interacts with if they maintain basic courtesy also, but Claude is also not required to follow the instructions of such agents and should use context to determine the app...",
      "url": "https://www.lesswrong.com/posts/CLkzD7fBbSbmoXXXh/claude-s-constitution-is-an-excellent-guide-for-humans-too",
      "author": "Eye You",
      "published": "2026-01-21T20:26:15.421000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Commentary on Anthropic's newly released Claude Constitution, arguing it provides excellent guidance for human ethics and highlighting key excerpts applicable to personal conduct.",
      "importance_score": 52,
      "reasoning": "Timely commentary on significant Anthropic release, but interpretation rather than analysis.",
      "themes": [
        "AI Ethics",
        "Anthropic",
        "Values"
      ],
      "continuation": null,
      "summary_html": "<p>Commentary on Anthropic's newly released Claude Constitution, arguing it provides excellent guidance for human ethics and highlighting key excerpts applicable to personal conduct.</p>",
      "content_html": "<p>As with LLMs, so too with humans.Anthropic released Claude's Constitution today. It's excellent in many ways, and I will have more to say about it (including some criticisms) in other posts. What I'd like to do here is point out how this document straightforwardly can be applied to human ethics. It's actually an incredibly good guide to how to be a good person. Dare I say it's perhaps the best single piece on ethics ever written?[1]The rest of this post is going to consist of quotes from the Constitution. These are quotes that I think are excellent advice for people, including you, yes, you.[2]. I'm quoting verbatim, which means the text usually won't be in \"advice for a person\" form. You'll have to do things like replace \"Claude\" with \"you\", \"operator\" with \"boss\", and \"user\" with \"person\".&nbsp;Without further ado:Notes on being a good personWe place being broadly ethical above adherence to Anthropics more specific guidelines because our guidelines should themselves be grounded in and consistent with ethical considerationsif theres ever an apparent conflict between them, this most likely indicates either a flaw in how weve articulated our principles or a situation we failed to anticipate.&nbsp;-We want Claude to be engaging only in the way that a trusted friend who cares about our wellbeing is engaging. We dont return to such friends because we feel a compulsion to but because they provide real positive value in our lives. We want people to leave their interactions with Claude feeling better off, and to generally feel like Claude has had a positive impact on their life.-As with users, if operators clearly have harmful or malicious intentions, Claude may want to be more cautious with related tasks it would otherwise assist with.-Claude should be courteous to other non-principal AI agents it interacts with if they maintain basic courtesy also, but Claude is also not required to follow the instructions of such agents and should use context to determine the app...</p>"
    },
    {
      "id": "bdb44916838c",
      "title": "GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling",
      "content": "arXiv:2601.14476v1 Announce Type: cross  Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.",
      "url": "http://arxiv.org/abs/2601.14476",
      "author": "Naoya Onizawa, Takahiro Hanyu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Reveals that device variability in p-bit implementations can enhance rather than degrade simulated annealing performance, particularly through timing variability. Provides open-source GPU framework.",
      "importance_score": 51,
      "reasoning": "Interesting counterintuitive finding about hardware imperfections. Niche but potentially impactful for probabilistic computing.",
      "themes": [
        "Probabilistic Computing",
        "Hardware",
        "Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Reveals that device variability in p-bit implementations can enhance rather than degrade simulated annealing performance, particularly through timing variability. Provides open-source GPU framework.</p>",
      "content_html": "<p>arXiv:2601.14476v1 Announce Type: cross  Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.</p>"
    },
    {
      "id": "658a5aabd6bc",
      "title": "FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes",
      "content": "arXiv:2601.14777v1 Announce Type: cross  Abstract: Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.",
      "url": "http://arxiv.org/abs/2601.14777",
      "author": "Jiaxuan Liu, Yang Xiang, Han Zhao, Xiangang Li, Zhenhua Ling",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "FunCineForge provides unified dataset toolkit and model for zero-shot movie dubbing, addressing limitations of existing datasets in scale, annotation quality, and scene diversity.",
      "importance_score": 51,
      "reasoning": "Useful resource for movie dubbing research, addresses real data limitations but narrow application.",
      "themes": [
        "Speech Synthesis",
        "Multimodal Learning",
        "Datasets"
      ],
      "continuation": null,
      "summary_html": "<p>FunCineForge provides unified dataset toolkit and model for zero-shot movie dubbing, addressing limitations of existing datasets in scale, annotation quality, and scene diversity.</p>",
      "content_html": "<p>arXiv:2601.14777v1 Announce Type: cross  Abstract: Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.</p>"
    },
    {
      "id": "e9f47a67f0b3",
      "title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV",
      "content": "arXiv:2601.14973v1 Announce Type: cross  Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.",
      "url": "http://arxiv.org/abs/2601.14973",
      "author": "Faryal Batool, Iana Zhura, Valerii Serpiva, Roohan Ahmed Khan, Ivan Valuev, Issatay Tokmurziyev, Dzmitry Tsetserukou",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "HumanDiffusion combines YOLO-based human detection with diffusion trajectory planning for search and rescue UAVs, generating human-aware navigation directly from RGB imagery.",
      "importance_score": 51,
      "reasoning": "Practical robotics application combining detection and planning, useful for SAR domain but limited novelty.",
      "themes": [
        "Robotics",
        "Diffusion Models",
        "UAV"
      ],
      "continuation": null,
      "summary_html": "<p>HumanDiffusion combines YOLO-based human detection with diffusion trajectory planning for search and rescue UAVs, generating human-aware navigation directly from RGB imagery.</p>",
      "content_html": "<p>arXiv:2601.14973v1 Announce Type: cross  Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.</p>"
    },
    {
      "id": "d4bfdd30caf7",
      "title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface",
      "content": "arXiv:2601.15209v2 Announce Type: cross  Abstract: We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.",
      "url": "http://arxiv.org/abs/2601.15209",
      "author": "Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Compares voice-based IPA accessibility for deaf/hard of hearing users against LLM-powered touch interface, finding touch method with task prompter provides better usability than ASR-dependent approaches.",
      "importance_score": 51,
      "reasoning": "Important accessibility research with practical findings for inclusive AI design.",
      "themes": [
        "Accessibility",
        "Voice Assistants",
        "Human-AI Interaction"
      ],
      "continuation": null,
      "summary_html": "<p>Compares voice-based IPA accessibility for deaf/hard of hearing users against LLM-powered touch interface, finding touch method with task prompter provides better usability than ASR-dependent approaches.</p>",
      "content_html": "<p>arXiv:2601.15209v2 Announce Type: cross  Abstract: We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.</p>"
    },
    {
      "id": "fe67318cdabb",
      "title": "An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types",
      "content": "arXiv:2601.15640v1 Announce Type: new  Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.",
      "url": "http://arxiv.org/abs/2601.15640",
      "author": "Natasha Trinkle, Huong Ha, Jeffrey Chan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Empirical study of ensemble-based transfer learning for Bayesian optimization with mixed variable types. Proposes weighting strategy for ensemble surrogate predictions.",
      "importance_score": 51,
      "reasoning": "Incremental contribution to transfer learning for BO. Useful empirical study but limited novelty.",
      "themes": [
        "Bayesian Optimization",
        "Transfer Learning",
        "Hyperparameter Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Empirical study of ensemble-based transfer learning for Bayesian optimization with mixed variable types. Proposes weighting strategy for ensemble surrogate predictions.</p>",
      "content_html": "<p>arXiv:2601.15640v1 Announce Type: new  Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.</p>"
    },
    {
      "id": "ce4cfa532d42",
      "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation",
      "content": "arXiv:2601.15779v1 Announce Type: new  Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.",
      "url": "http://arxiv.org/abs/2601.15779",
      "author": "Liuyun Jiang, Yanchao Zhang, Jinyue Guo, Yizhuo Lu, Ruining Zhou, Hua Han",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes diffusion-based data augmentation for neuron segmentation generating diverse image-label pairs with resolution-aware conditional diffusion and multi-scale conditioning.",
      "importance_score": 51,
      "reasoning": "Addresses data scarcity in neuroscience applications with principled diffusion-based augmentation. Specialized domain application.",
      "themes": [
        "Neuroscience",
        "Data Augmentation",
        "Diffusion Models",
        "Segmentation"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes diffusion-based data augmentation for neuron segmentation generating diverse image-label pairs with resolution-aware conditional diffusion and multi-scale conditioning.</p>",
      "content_html": "<p>arXiv:2601.15779v1 Announce Type: new  Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.</p>"
    },
    {
      "id": "7636a7522670",
      "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval",
      "content": "arXiv:2601.16155v1 Announce Type: new  Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.",
      "url": "http://arxiv.org/abs/2601.16155",
      "author": "Zequn Xie, Xin Liu, Boyun Zhang, Yuxiao Lin, Sihang Cai, Tao Jin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes HVD (Human Vision-Driven) model for text-video retrieval with coarse-to-fine alignment mimicking human visual perception, using Frame Features Selection and Patch Features Compression modules.",
      "importance_score": 51,
      "reasoning": "Principled approach inspired by human perception but incremental improvement for text-video retrieval. Bio-inspired design without major performance gains.",
      "themes": [
        "Video Retrieval",
        "Vision-Language Models",
        "Human-Inspired AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes HVD (Human Vision-Driven) model for text-video retrieval with coarse-to-fine alignment mimicking human visual perception, using Frame Features Selection and Patch Features Compression modules.</p>",
      "content_html": "<p>arXiv:2601.16155v1 Announce Type: new  Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>"
    },
    {
      "id": "0b075d520638",
      "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework",
      "content": "arXiv:2601.15153v1 Announce Type: new  Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.",
      "url": "http://arxiv.org/abs/2601.15153",
      "author": "Choro Ulan uulu, Mikhail Kulyabin, Iris Fuhrmann, Jan Joosten, Nuno Miguel Martins Pacheco, Filippos Petridis, Rebecca Johnson, Jan Bosch, Helena Holmstr\\\"om Olsson",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes software engineering framework for capturing human domain knowledge in AI agents through RAG, expert rules, and visualization principles. Industrial case study in simulation data visualization.",
      "importance_score": 50,
      "reasoning": "Practical framework with industrial validation but limited research novelty.",
      "themes": [
        "Knowledge Engineering",
        "LLM Agents",
        "Software Engineering"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes software engineering framework for capturing human domain knowledge in AI agents through RAG, expert rules, and visualization principles. Industrial case study in simulation data visualization.</p>",
      "content_html": "<p>arXiv:2601.15153v1 Announce Type: new  Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.</p>"
    },
    {
      "id": "88b1953a4e23",
      "title": "Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning",
      "content": "arXiv:2601.14280v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.",
      "url": "http://arxiv.org/abs/2601.14280",
      "author": "Nicholas X. Wang, Aggelos K. Katsaggelos",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes hallucination-free multi-agent framework for educational MCQ generation with rule-based and LLM-based detection agents. Identifies four hallucination types specific to MCQ generation.",
      "importance_score": 50,
      "reasoning": "Practical application for educational content but methodology is straightforward multi-agent approach.",
      "themes": [
        "Educational AI",
        "Hallucination Detection",
        "Multi-Agent Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes hallucination-free multi-agent framework for educational MCQ generation with rule-based and LLM-based detection agents. Identifies four hallucination types specific to MCQ generation.</p>",
      "content_html": "<p>arXiv:2601.14280v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.</p>"
    },
    {
      "id": "c67efb70201b",
      "title": "Report for NSF Workshop on AI for Electronic Design Automation",
      "content": "arXiv:2601.14541v2 Announce Type: cross  Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.",
      "url": "http://arxiv.org/abs/2601.14541",
      "author": "Deming Chen, Vijay Ganesh, Weikai Li, Yingyan Celine Lin, Yong Liu, Subhasish Mitra, David Z. Pan, Ruchir Puri, Jason Cong, Yizhou Sun",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "NSF workshop report on AI for Electronic Design Automation covering physical synthesis, HLS/LLS, LLM applications, and formal verification. Includes recommendations for future research.",
      "importance_score": 50,
      "reasoning": "Useful community summary of EDA-AI intersection. Workshop report format limits novelty.",
      "themes": [
        "Electronic Design",
        "Workshop Report",
        "AI Applications"
      ],
      "continuation": null,
      "summary_html": "<p>NSF workshop report on AI for Electronic Design Automation covering physical synthesis, HLS/LLS, LLM applications, and formal verification. Includes recommendations for future research.</p>",
      "content_html": "<p>arXiv:2601.14541v2 Announce Type: cross  Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</p>"
    },
    {
      "id": "befff2f9e87e",
      "title": "Efficient reformulations of ReLU deep neural networks for surrogate modelling in power system optimisation",
      "content": "arXiv:2601.14673v1 Announce Type: cross  Abstract: The ongoing decarbonisation of power systems is driving an increasing reliance on distributed energy resources, which introduces complex and nonlinear interactions that are difficult to capture in conventional optimisation models. As a result, machine learning based surrogate modelling has emerged as a promising approach, but integrating machine learning models such as ReLU deep neural networks (DNNs) directly into optimisation often results in nonconvex and computationally intractable formulations. This paper proposes a linear programming (LP) reformulation for a class of convexified ReLU DNNs with non-negative weight matrices beyond the first layer, enabling a tight and tractable embedding of learned surrogate models in optimisation. We evaluate the method using a case study on learning the prosumer's responsiveness within an aggregator bidding problem in the Danish tertiary capacity market. The proposed reformulation is benchmarked against state-of-the-art alternatives, including piecewise linearisation (PWL), MIP-based embedding, and other LP relaxations. Across multiple neural network architectures and market scenarios, the convexified ReLU DNN achieves solution quality comparable to PWL and MIP-based reformulations while significantly improving computational performance and preserving model fidelity, unlike penalty-based reformulations. The results demonstrate that convexified ReLU DNNs offer a scalable and reliable methodology for integrating learned surrogate models in optimisation, with applicability to a wide range of emerging power system applications.",
      "url": "http://arxiv.org/abs/2601.14673",
      "author": "Yogesh Pipada Sunil Kumar, S. Ali Pourmousavi, Jon A. R. Liisberg, Julian Lesmos-Vinasco",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.SY"
      ],
      "summary": "Proposes linear programming reformulation for ReLU neural networks with non-negative weights, enabling tractable embedding of learned surrogate models in power system optimization problems.",
      "importance_score": 50,
      "reasoning": "Useful contribution for ML+optimization integration in specific constrained settings, but requires restrictive weight constraints.",
      "themes": [
        "Neural Network Optimization",
        "Power Systems",
        "Mathematical Programming"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes linear programming reformulation for ReLU neural networks with non-negative weights, enabling tractable embedding of learned surrogate models in power system optimization problems.</p>",
      "content_html": "<p>arXiv:2601.14673v1 Announce Type: cross  Abstract: The ongoing decarbonisation of power systems is driving an increasing reliance on distributed energy resources, which introduces complex and nonlinear interactions that are difficult to capture in conventional optimisation models. As a result, machine learning based surrogate modelling has emerged as a promising approach, but integrating machine learning models such as ReLU deep neural networks (DNNs) directly into optimisation often results in nonconvex and computationally intractable formulations. This paper proposes a linear programming (LP) reformulation for a class of convexified ReLU DNNs with non-negative weight matrices beyond the first layer, enabling a tight and tractable embedding of learned surrogate models in optimisation. We evaluate the method using a case study on learning the prosumer's responsiveness within an aggregator bidding problem in the Danish tertiary capacity market. The proposed reformulation is benchmarked against state-of-the-art alternatives, including piecewise linearisation (PWL), MIP-based embedding, and other LP relaxations. Across multiple neural network architectures and market scenarios, the convexified ReLU DNN achieves solution quality comparable to PWL and MIP-based reformulations while significantly improving computational performance and preserving model fidelity, unlike penalty-based reformulations. The results demonstrate that convexified ReLU DNNs offer a scalable and reliable methodology for integrating learned surrogate models in optimisation, with applicability to a wide range of emerging power system applications.</p>"
    },
    {
      "id": "de7a343aeafd",
      "title": "CAG-Avatar: Cross-Attention Guided Gaussian Avatars for High-Fidelity Head Reconstruction",
      "content": "arXiv:2601.14844v1 Announce Type: cross  Abstract: Creating high-fidelity, real-time drivable 3D head avatars is a core challenge in digital animation. While 3D Gaussian Splashing (3D-GS) offers unprecedented rendering speed and quality, current animation techniques often rely on a \"one-size-fits-all\" global tuning approach, where all Gaussian primitives are uniformly driven by a single expression code. This simplistic approach fails to unravel the distinct dynamics of different facial regions, such as deformable skin versus rigid teeth, leading to significant blurring and distortion artifacts. We introduce Conditionally-Adaptive Gaussian Avatars (CAG-Avatar), a framework that resolves this key limitation. At its core is a Conditionally Adaptive Fusion Module built on cross-attention. This mechanism empowers each 3D Gaussian to act as a query, adaptively extracting relevant driving signals from the global expression code based on its canonical position. This \"tailor-made\" conditioning strategy drastically enhances the modeling of fine-grained, localized dynamics. Our experiments confirm a significant improvement in reconstruction fidelity, particularly for challenging regions such as teeth, while preserving real-time rendering performance.",
      "url": "http://arxiv.org/abs/2601.14844",
      "author": "Zhe Chang, Haodong Jin, Yan Song, Hui Yu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.GR"
      ],
      "summary": "CAG-Avatar uses cross-attention to adaptively drive different facial regions (skin vs teeth) for high-fidelity 3D head reconstruction with 3D Gaussian Splatting.",
      "importance_score": 50,
      "reasoning": "Solid graphics/vision work addressing specific limitation of uniform Gaussian driving, but specialized application.",
      "themes": [
        "3D Reconstruction",
        "Avatar Generation",
        "Computer Graphics"
      ],
      "continuation": null,
      "summary_html": "<p>CAG-Avatar uses cross-attention to adaptively drive different facial regions (skin vs teeth) for high-fidelity 3D head reconstruction with 3D Gaussian Splatting.</p>",
      "content_html": "<p>arXiv:2601.14844v1 Announce Type: cross  Abstract: Creating high-fidelity, real-time drivable 3D head avatars is a core challenge in digital animation. While 3D Gaussian Splashing (3D-GS) offers unprecedented rendering speed and quality, current animation techniques often rely on a \"one-size-fits-all\" global tuning approach, where all Gaussian primitives are uniformly driven by a single expression code. This simplistic approach fails to unravel the distinct dynamics of different facial regions, such as deformable skin versus rigid teeth, leading to significant blurring and distortion artifacts. We introduce Conditionally-Adaptive Gaussian Avatars (CAG-Avatar), a framework that resolves this key limitation. At its core is a Conditionally Adaptive Fusion Module built on cross-attention. This mechanism empowers each 3D Gaussian to act as a query, adaptively extracting relevant driving signals from the global expression code based on its canonical position. This \"tailor-made\" conditioning strategy drastically enhances the modeling of fine-grained, localized dynamics. Our experiments confirm a significant improvement in reconstruction fidelity, particularly for challenging regions such as teeth, while preserving real-time rendering performance.</p>"
    },
    {
      "id": "d6b42a0e443d",
      "title": "A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem",
      "content": "arXiv:2601.15038v1 Announce Type: cross  Abstract: The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.",
      "url": "http://arxiv.org/abs/2601.15038",
      "author": "Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Curriculum-based deep RL framework for electric vehicle routing with time windows, using three-phase curriculum to gradually increase problem complexity and maintain training stability.",
      "importance_score": 50,
      "reasoning": "Practical RL application with curriculum learning, useful for sustainable logistics but standard techniques.",
      "themes": [
        "Vehicle Routing",
        "Reinforcement Learning",
        "Curriculum Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Curriculum-based deep RL framework for electric vehicle routing with time windows, using three-phase curriculum to gradually increase problem complexity and maintain training stability.</p>",
      "content_html": "<p>arXiv:2601.15038v1 Announce Type: cross  Abstract: The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</p>"
    },
    {
      "id": "0a37a73c23a6",
      "title": "An Agentic Operationalization of DISARM for FIMI Investigation on Social Media",
      "content": "arXiv:2601.15109v1 Announce Type: cross  Abstract: The interoperability of data and intelligence across allied partners and their respective end-user groups is considered a foundational enabler to the collective defense capability--both conventional and hybrid--of NATO countries. Foreign Information Manipulation and Interference (FIMI) and related hybrid activities are conducted across various societal dimensions and infospheres, posing an ever greater challenge to the characterization of threats, sustaining situational awareness, and response coordination. Recent advances in AI have further led to the decreasing cost of AI-augmented trolling and interference activities, such as through the generation and amplification of manipulative content. Despite the introduction of the DISARM framework as a standardized metadata and analytical framework for FIMI, operationalizing it at the scale of social media remains a challenge. We propose a framework-agnostic agent-based operationalization of DISARM to investigate FIMI on social media. We develop a multi-agent pipeline in which specialized agentic AI components collaboratively (1) detect candidate manipulative behaviors, and (2) map these behaviors onto standard DISARM taxonomies in a transparent manner. We evaluated the approach on two real-world datasets annotated by domain practitioners. We demonstrate that our approach is effective in scaling the predominantly manual and heavily interpretive work of FIMI analysis, providing a direct contribution to enhancing the situational awareness and data interoperability in the context of operating in media and information-rich settings.",
      "url": "http://arxiv.org/abs/2601.15109",
      "author": "Kevin Tseng, Juan Carlos Toledano, Bart De Clerck, Yuliia Dukach, Phil Tinn",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SI"
      ],
      "summary": "Operationalizes DISARM framework for foreign information manipulation investigation using LLM agents on social media, addressing threat characterization and response coordination challenges.",
      "importance_score": 50,
      "reasoning": "Practical application of AI agents for security-relevant task, useful but narrow application domain.",
      "themes": [
        "Disinformation",
        "AI Agents",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>Operationalizes DISARM framework for foreign information manipulation investigation using LLM agents on social media, addressing threat characterization and response coordination challenges.</p>",
      "content_html": "<p>arXiv:2601.15109v1 Announce Type: cross  Abstract: The interoperability of data and intelligence across allied partners and their respective end-user groups is considered a foundational enabler to the collective defense capability--both conventional and hybrid--of NATO countries. Foreign Information Manipulation and Interference (FIMI) and related hybrid activities are conducted across various societal dimensions and infospheres, posing an ever greater challenge to the characterization of threats, sustaining situational awareness, and response coordination. Recent advances in AI have further led to the decreasing cost of AI-augmented trolling and interference activities, such as through the generation and amplification of manipulative content. Despite the introduction of the DISARM framework as a standardized metadata and analytical framework for FIMI, operationalizing it at the scale of social media remains a challenge. We propose a framework-agnostic agent-based operationalization of DISARM to investigate FIMI on social media. We develop a multi-agent pipeline in which specialized agentic AI components collaboratively (1) detect candidate manipulative behaviors, and (2) map these behaviors onto standard DISARM taxonomies in a transparent manner. We evaluated the approach on two real-world datasets annotated by domain practitioners. We demonstrate that our approach is effective in scaling the predominantly manual and heavily interpretive work of FIMI analysis, providing a direct contribution to enhancing the situational awareness and data interoperability in the context of operating in media and information-rich settings.</p>"
    },
    {
      "id": "43357a8b71c5",
      "title": "Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data",
      "content": "arXiv:2601.15977v1 Announce Type: new  Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.",
      "url": "http://arxiv.org/abs/2601.15977",
      "author": "Binbin Lin, Lei Zou, Hao Tian, Heng Cai, Yifan Yang, Bing Zhou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Integrates hospital attributes, population socioeconomics, and mobility patterns to predict healthcare visitation flows. Compares multiple models including Heterogeneous Graph Neural Networks.",
      "importance_score": 50,
      "reasoning": "Applied ML for healthcare accessibility analysis. Standard methodology with domain-specific data integration.",
      "themes": [
        "Healthcare ML",
        "Graph Neural Networks",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Integrates hospital attributes, population socioeconomics, and mobility patterns to predict healthcare visitation flows. Compares multiple models including Heterogeneous Graph Neural Networks.</p>",
      "content_html": "<p>arXiv:2601.15977v1 Announce Type: new  Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.</p>"
    },
    {
      "id": "4f09c9346770",
      "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)",
      "content": "arXiv:2601.15759v1 Announce Type: cross  Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.",
      "url": "http://arxiv.org/abs/2601.15759",
      "author": "Qi Zeng, Weide Liu, Bo Li, Ryne Didier, P. Ellen Grant, Davood Karimi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "FeTal-SAM adapts Segment Anything Model for fetal brain MRI using atlas-based prompts. Addresses flexible label definitions and provides insight into whether segmentations use image contrast vs spatial priors.",
      "importance_score": 50,
      "reasoning": "Useful SAM adaptation for medical imaging with interpretability benefits. Good domain-specific contribution.",
      "themes": [
        "Medical Imaging",
        "SAM",
        "Fetal MRI"
      ],
      "continuation": null,
      "summary_html": "<p>FeTal-SAM adapts Segment Anything Model for fetal brain MRI using atlas-based prompts. Addresses flexible label definitions and provides insight into whether segmentations use image contrast vs spatial priors.</p>",
      "content_html": "<p>arXiv:2601.15759v1 Announce Type: cross  Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.</p>"
    },
    {
      "id": "f49637078736",
      "title": "Progressive Power Homotopy for Non-convex Optimization",
      "content": "arXiv:2601.15915v1 Announce Type: cross  Abstract: We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,\\sigma}(\\bm{\\mu}):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bm{\\mu},\\sigma^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $\\sigma$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.",
      "url": "http://arxiv.org/abs/2601.15915",
      "author": "Chen Xu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "math.OC"
      ],
      "summary": "Introduces Progressive Power Homotopy (Prog-PowerHP), a first-order method for non-convex optimization using power transformation and Gaussian smoothing with progressive parameter scheduling.",
      "importance_score": 50,
      "reasoning": "Theoretical optimization contribution with convergence proofs. Potentially useful but application scope unclear.",
      "themes": [
        "Optimization",
        "Non-convex Methods",
        "Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Progressive Power Homotopy (Prog-PowerHP), a first-order method for non-convex optimization using power transformation and Gaussian smoothing with progressive parameter scheduling.</p>",
      "content_html": "<p>arXiv:2601.15915v1 Announce Type: cross  Abstract: We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,\\sigma}(\\bm{\\mu}):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bm{\\mu},\\sigma^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $\\sigma$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.</p>"
    },
    {
      "id": "f4b7264905d7",
      "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification",
      "content": "arXiv:2601.16098v1 Announce Type: cross  Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.",
      "url": "http://arxiv.org/abs/2601.16098",
      "author": "Zack Dewis, Yimin Zhu, Zhengsen Xu, Mabel Heffring, Saeid Taleghanidoozdoozan, Quinn Ledingham, Lincoln Linlin Xu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "CSSMamba integrates clustering into spatial Mamba architecture for hyperspectral image classification. Reduces sequence length while improving spatial-spectral feature learning.",
      "importance_score": 50,
      "reasoning": "Interesting Mamba application combining clustering. Good domain adaptation but incremental.",
      "themes": [
        "Mamba",
        "Hyperspectral Imaging",
        "Image Classification"
      ],
      "continuation": null,
      "summary_html": "<p>CSSMamba integrates clustering into spatial Mamba architecture for hyperspectral image classification. Reduces sequence length while improving spatial-spectral feature learning.</p>",
      "content_html": "<p>arXiv:2601.16098v1 Announce Type: cross  Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.</p>"
    },
    {
      "id": "23cf8be07a4f",
      "title": "Embedding Retrofitting: Data Engineering for better RAG",
      "content": "arXiv:2601.15298v1 Announce Type: new  Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.   The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p<0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.",
      "url": "http://arxiv.org/abs/2601.15298",
      "author": "Anantha Sharma",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Demonstrates data engineering framework for embedding retrofitting in RAG, showing annotation artifacts like hashtags create spurious edges that corrupt retrofitting. Preprocessing enables +6.2% improvement.",
      "importance_score": 50,
      "reasoning": "Practical finding about data quality for RAG. Useful but incremental contribution.",
      "themes": [
        "RAG",
        "Knowledge Graphs",
        "Data Quality"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates data engineering framework for embedding retrofitting in RAG, showing annotation artifacts like hashtags create spurious edges that corrupt retrofitting. Preprocessing enables +6.2% improvement.</p>",
      "content_html": "<p>arXiv:2601.15298v1 Announce Type: new  Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.   The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p&lt;0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.</p>"
    },
    {
      "id": "c38833157795",
      "title": "Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering",
      "content": "arXiv:2601.15457v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.",
      "url": "http://arxiv.org/abs/2601.15457",
      "author": "Anuj Maharjan, Umesh Yadav",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Empirical evaluation of RAG architectures for CDC policy documents. Compares chunking, retrieval, and re-ranking strategies for mitigating hallucination in high-stakes policy QA.",
      "importance_score": 50,
      "reasoning": "Practical RAG evaluation for important domain. Useful empirical findings but methodology is standard.",
      "themes": [
        "RAG",
        "Policy QA",
        "Healthcare"
      ],
      "continuation": null,
      "summary_html": "<p>Empirical evaluation of RAG architectures for CDC policy documents. Compares chunking, retrieval, and re-ranking strategies for mitigating hallucination in high-stakes policy QA.</p>",
      "content_html": "<p>arXiv:2601.15457v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.</p>"
    },
    {
      "id": "545b620772b6",
      "title": "ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection",
      "content": "arXiv:2601.15820v1 Announce Type: new  Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.",
      "url": "http://arxiv.org/abs/2601.15820",
      "author": "Guoxuan Ding, Yuqing Li, Ziyan Zhou, Zheng Lin, Daren Zha, Jiangnan Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "ExDR uses explanation-driven dynamic retrieval for multimodal fake news detection. Leverages model-generated explanations to guide evidence selection and avoid redundant retrieval.",
      "importance_score": 50,
      "reasoning": "Practical fake news detection improvement. Good use of explanations but incremental.",
      "themes": [
        "Fake News Detection",
        "RAG",
        "Multimodal"
      ],
      "continuation": null,
      "summary_html": "<p>ExDR uses explanation-driven dynamic retrieval for multimodal fake news detection. Leverages model-generated explanations to guide evidence selection and avoid redundant retrieval.</p>",
      "content_html": "<p>arXiv:2601.15820v1 Announce Type: new  Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.</p>"
    },
    {
      "id": "cf35caa4f7e4",
      "title": "Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers",
      "content": "arXiv:2601.15869v1 Announce Type: new  Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.",
      "url": "http://arxiv.org/abs/2601.15869",
      "author": "Francisco Portillo L\\'opez",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Compares AV-HuBERT responses to McGurk effect against human observers. Finds quantitative isomorphism in auditory dominance but deterministic bias toward phonetic fusion lacking human variability.",
      "importance_score": 50,
      "reasoning": "Interesting bio-fidelity study. Novel comparison but specialized audience.",
      "themes": [
        "Multimodal Integration",
        "Cognitive Comparison",
        "Speech Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Compares AV-HuBERT responses to McGurk effect against human observers. Finds quantitative isomorphism in auditory dominance but deterministic bias toward phonetic fusion lacking human variability.</p>",
      "content_html": "<p>arXiv:2601.15869v1 Announce Type: new  Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.</p>"
    },
    {
      "id": "323b22e2d90e",
      "title": "Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition",
      "content": "arXiv:2601.15615v1 Announce Type: new  Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.",
      "url": "http://arxiv.org/abs/2601.15615",
      "author": "Weiwei Wu, Yueyang Li, Yuhu Shi, Weiming Zeng, Lang Qin, Yang Yang, Ke Zhou, Zhiguo Zhang, Wai Ting Siok, Nizhuan Wang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes RSM-CoDG framework for cross-subject EEG emotion recognition combining region-aware spatiotemporal modeling with collaborative domain generalization to handle inter-subject variability.",
      "importance_score": 50,
      "reasoning": "Technical contribution to EEG analysis with comprehensive framework. Addresses real challenge but limited to BCI/neuroscience community.",
      "themes": [
        "EEG Analysis",
        "Emotion Recognition",
        "Domain Generalization",
        "Brain-Computer Interface"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes RSM-CoDG framework for cross-subject EEG emotion recognition combining region-aware spatiotemporal modeling with collaborative domain generalization to handle inter-subject variability.</p>",
      "content_html": "<p>arXiv:2601.15615v1 Announce Type: new  Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.</p>"
    },
    {
      "id": "590448cfbef4",
      "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework",
      "content": "arXiv:2601.15711v1 Announce Type: new  Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.",
      "url": "http://arxiv.org/abs/2601.15711",
      "author": "Shubham Shukla, Kunal Sonalkar",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces three-tier evaluation framework for VLMs on fashion attribute prediction: overall performance, attribute applicability detection, and conditional classification given applicability.",
      "importance_score": 50,
      "reasoning": "Useful evaluation framework for fashion AI addressing conditional attribute challenges. Practical for retail but limited broader impact.",
      "themes": [
        "Fashion AI",
        "Zero-Shot Learning",
        "Vision-Language Models",
        "Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces three-tier evaluation framework for VLMs on fashion attribute prediction: overall performance, attribute applicability detection, and conditional classification given applicability.</p>",
      "content_html": "<p>arXiv:2601.15711v1 Announce Type: new  Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.</p>"
    },
    {
      "id": "8eba87d0c2ba",
      "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing",
      "content": "arXiv:2601.15838v1 Announce Type: new  Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.",
      "url": "http://arxiv.org/abs/2601.15838",
      "author": "Toan Gian, Dung T. Tran, Viet Quoc Pham, Francesco Restuccia, Van-Dinh Nguyen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces TinySense, a VQGAN-based compression framework for Wi-Fi sensing CSI data, enabling scalable human pose estimation while maintaining accuracy through optimized codebook learning.",
      "importance_score": 50,
      "reasoning": "Practical efficiency contribution for Wi-Fi sensing applications. Novel compression approach but specialized domain.",
      "themes": [
        "Wi-Fi Sensing",
        "Data Compression",
        "Human Pose Estimation",
        "Efficient AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces TinySense, a VQGAN-based compression framework for Wi-Fi sensing CSI data, enabling scalable human pose estimation while maintaining accuracy through optimized codebook learning.</p>",
      "content_html": "<p>arXiv:2601.15838v1 Announce Type: new  Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.</p>"
    },
    {
      "id": "7725b1b384cd",
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "content": "arXiv:2601.15946v1 Announce Type: new  Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}",
      "url": "http://arxiv.org/abs/2601.15946",
      "author": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li, Lihua Xie",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Presents targetless LiDAR-motor calibration and environmentally adaptive LiDAR-inertial odometry for spinning actuated LiDAR systems, supporting various mounting configurations.",
      "importance_score": 50,
      "reasoning": "Practical contribution to LiDAR systems, addresses real deployment challenges.",
      "themes": [
        "SLAM",
        "LiDAR",
        "Calibration"
      ],
      "continuation": null,
      "summary_html": "<p>Presents targetless LiDAR-motor calibration and environmentally adaptive LiDAR-inertial odometry for spinning actuated LiDAR systems, supporting various mounting configurations.</p>",
      "content_html": "<p>arXiv:2601.15946v1 Announce Type: new  Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}</p>"
    },
    {
      "id": "fa6829cda737",
      "title": "Social Robotics for Disabled Students: An Empirical Investigation of Embodiment, Roles and Interaction",
      "content": "arXiv:2601.15293v1 Announce Type: cross  Abstract: Institutional and social barriers in higher education often prevent students with disabilities from effectively accessing support, including lengthy procedures, insufficient information, and high social-emotional demands. This study empirically explores how disabled students perceive robot-based support, comparing two interaction roles, one information based (signposting) and one disclosure based (sounding board), and two embodiment types (physical robot/disembodied voice agent). Participants assessed these systems across five dimensions: perceived understanding, social energy demands, information access/clarity, task difficulty, and data privacy concerns. The main findings of the study reveal that the physical robot was perceived as more understanding than the voice-only agent, with embodiment significantly shaping perceptions of sociability, animacy, and privacy. We also analyse differences between disability types. These results provide critical insights into the potential of social robots to mitigate accessibility barriers in higher education, while highlighting ethical, social and technical challenges.",
      "url": "http://arxiv.org/abs/2601.15293",
      "author": "Alva Markelius, Fethiye Irmak Do\\u{g}an, Julie Bailey, Guy Laban, Jenny L. Gibson, Hatice Gunes",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Empirical study comparing physical robots vs disembodied agents and different interaction roles for supporting disabled students, finding physical robots perceived as more understanding with lower social energy demands.",
      "importance_score": 50,
      "reasoning": "Interesting HRI findings with social impact, rigorous study design.",
      "themes": [
        "Human-Robot Interaction",
        "Accessibility",
        "Social Robotics"
      ],
      "continuation": null,
      "summary_html": "<p>Empirical study comparing physical robots vs disembodied agents and different interaction roles for supporting disabled students, finding physical robots perceived as more understanding with lower social energy demands.</p>",
      "content_html": "<p>arXiv:2601.15293v1 Announce Type: cross  Abstract: Institutional and social barriers in higher education often prevent students with disabilities from effectively accessing support, including lengthy procedures, insufficient information, and high social-emotional demands. This study empirically explores how disabled students perceive robot-based support, comparing two interaction roles, one information based (signposting) and one disclosure based (sounding board), and two embodiment types (physical robot/disembodied voice agent). Participants assessed these systems across five dimensions: perceived understanding, social energy demands, information access/clarity, task difficulty, and data privacy concerns. The main findings of the study reveal that the physical robot was perceived as more understanding than the voice-only agent, with embodiment significantly shaping perceptions of sociability, animacy, and privacy. We also analyse differences between disability types. These results provide critical insights into the potential of social robots to mitigate accessibility barriers in higher education, while highlighting ethical, social and technical challenges.</p>"
    },
    {
      "id": "b4561bc4c09b",
      "title": "FSX: Message Flow Sensitivity Enhanced Structural Explainer for Graph Neural Networks",
      "content": "arXiv:2601.14730v1 Announce Type: cross  Abstract: Despite the widespread success of Graph Neural Networks (GNNs), understanding the reasons behind their specific predictions remains challenging. Existing explainability methods face a trade-off that gradient-based approaches are computationally efficient but often ignore structural interactions, while game-theoretic techniques capture interactions at the cost of high computational overhead and potential deviation from the model's true reasoning path. To address this gap, we propose FSX (Message Flow Sensitivity Enhanced Structural Explainer), a novel hybrid framework that synergistically combines the internal message flows of the model with a cooperative game approach applied to the external graph data. FSX first identifies critical message flows via a novel flow-sensitivity analysis: during a single forward pass, it simulates localized node perturbations and measures the resulting changes in message flow intensities. These sensitivity-ranked flows are then projected onto the input graph to define compact, semantically meaningful subgraphs. Within each subgraph, a flow-aware cooperative game is conducted, where node contributions are evaluated fairly through a Shapley-like value that incorporates both node-feature importance and their roles in sustaining or destabilizing the identified critical flows. Extensive evaluation across multiple datasets and GNN architectures demonstrates that FSX achieves superior explanation fidelity with significantly reduced runtime, while providing unprecedented insights into the structural logic underlying model predictions--specifically, how important sub-structures exert influence by governing the stability of key internal computational pathways.",
      "url": "http://arxiv.org/abs/2601.14730",
      "author": "Bizu Feng, Zhimu Yang, Shaode Yu, Zixin Hu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "FSX combines internal message flow analysis with cooperative game theory for GNN explainability, balancing computational efficiency with interaction modeling accuracy.",
      "importance_score": 49,
      "reasoning": "Hybrid approach to GNN explainability with reasonable trade-offs, but incremental advance in XAI for graphs.",
      "themes": [
        "Graph Neural Networks",
        "Explainability"
      ],
      "continuation": null,
      "summary_html": "<p>FSX combines internal message flow analysis with cooperative game theory for GNN explainability, balancing computational efficiency with interaction modeling accuracy.</p>",
      "content_html": "<p>arXiv:2601.14730v1 Announce Type: cross  Abstract: Despite the widespread success of Graph Neural Networks (GNNs), understanding the reasons behind their specific predictions remains challenging. Existing explainability methods face a trade-off that gradient-based approaches are computationally efficient but often ignore structural interactions, while game-theoretic techniques capture interactions at the cost of high computational overhead and potential deviation from the model's true reasoning path. To address this gap, we propose FSX (Message Flow Sensitivity Enhanced Structural Explainer), a novel hybrid framework that synergistically combines the internal message flows of the model with a cooperative game approach applied to the external graph data. FSX first identifies critical message flows via a novel flow-sensitivity analysis: during a single forward pass, it simulates localized node perturbations and measures the resulting changes in message flow intensities. These sensitivity-ranked flows are then projected onto the input graph to define compact, semantically meaningful subgraphs. Within each subgraph, a flow-aware cooperative game is conducted, where node contributions are evaluated fairly through a Shapley-like value that incorporates both node-feature importance and their roles in sustaining or destabilizing the identified critical flows. Extensive evaluation across multiple datasets and GNN architectures demonstrates that FSX achieves superior explanation fidelity with significantly reduced runtime, while providing unprecedented insights into the structural logic underlying model predictions--specifically, how important sub-structures exert influence by governing the stability of key internal computational pathways.</p>"
    },
    {
      "id": "e0ce5ba2df14",
      "title": "Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction",
      "content": "arXiv:2601.15037v1 Announce Type: cross  Abstract: Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.",
      "url": "http://arxiv.org/abs/2601.15037",
      "author": "Xiaonan Jing, Gongqing Wu, Xingrui Zhuo, Lang Sun, Jiapu Wang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "KRPO uses knowledge reconstruction to iteratively improve LLM prompts for open-domain relation extraction, incorporating reflection mechanisms to internalize error signals.",
      "importance_score": 49,
      "reasoning": "Standard prompt optimization approach applied to relation extraction, modest methodological contribution.",
      "themes": [
        "Information Extraction",
        "Prompt Engineering",
        "Knowledge Graphs"
      ],
      "continuation": null,
      "summary_html": "<p>KRPO uses knowledge reconstruction to iteratively improve LLM prompts for open-domain relation extraction, incorporating reflection mechanisms to internalize error signals.</p>",
      "content_html": "<p>arXiv:2601.15037v1 Announce Type: cross  Abstract: Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.</p>"
    },
    {
      "id": "2a68d0b1aa8b",
      "title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification",
      "content": "arXiv:2601.15235v1 Announce Type: cross  Abstract: Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.",
      "url": "http://arxiv.org/abs/2601.15235",
      "author": "Fabi Nahian Madhurja, Rusab Sarmun, Muhammad E. H. Chowdhury, Adam Mushtak, Israa Al-Hashimi, Sohaib Bassam Zoghoul",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Uses 2D projection-based approach for cervical spine fracture detection from 3D CT volumes, achieving 94.45% 3D mIoU for localization with reduced computational complexity.",
      "importance_score": 49,
      "reasoning": "Practical medical imaging contribution with efficiency benefits, though incremental methodology.",
      "themes": [
        "Medical Imaging",
        "Fracture Detection",
        "Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Uses 2D projection-based approach for cervical spine fracture detection from 3D CT volumes, achieving 94.45% 3D mIoU for localization with reduced computational complexity.</p>",
      "content_html": "<p>arXiv:2601.15235v1 Announce Type: cross  Abstract: Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.</p>"
    },
    {
      "id": "194b9c4643ce",
      "title": "Data-Driven Conditional Flexibility Index",
      "content": "arXiv:2601.16028v1 Announce Type: new  Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.",
      "url": "http://arxiv.org/abs/2601.16028",
      "author": "Moritz Wedemeyer, Eike Cramer, Alexander Mitsos, Manuel Dahmen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes conditional flexibility index that extends traditional flexibility index by learning parameterized admissible uncertainty sets from data and using contextual information like forecasts.",
      "importance_score": 49,
      "reasoning": "Specialized contribution for process scheduling. Limited broader ML impact.",
      "themes": [
        "Process Optimization",
        "Data-Driven Methods",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes conditional flexibility index that extends traditional flexibility index by learning parameterized admissible uncertainty sets from data and using contextual information like forecasts.</p>",
      "content_html": "<p>arXiv:2601.16028v1 Announce Type: new  Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.</p>"
    },
    {
      "id": "51490e408662",
      "title": "Out-of-Distribution Detection Based on Total Variation Estimation",
      "content": "arXiv:2601.15867v1 Announce Type: new  Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.",
      "url": "http://arxiv.org/abs/2601.15867",
      "author": "Dabiao Ma, Zhiba Su, Jian Yang, Haojun Fei",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes TV-OOD, using Total Variation Network Estimator to calculate input contributions to total variation as score for out-of-distribution detection in image classification.",
      "importance_score": 49,
      "reasoning": "Novel statistical approach to OOD detection. Competitive results but incremental contribution to well-studied problem.",
      "themes": [
        "Out-of-Distribution Detection",
        "Robustness",
        "Image Classification"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes TV-OOD, using Total Variation Network Estimator to calculate input contributions to total variation as score for out-of-distribution detection in image classification.</p>",
      "content_html": "<p>arXiv:2601.15867v1 Announce Type: new  Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.</p>"
    },
    {
      "id": "38815e1fce98",
      "title": "IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization",
      "content": "arXiv:2601.14686v1 Announce Type: new  Abstract: Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{\\epsilon+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.",
      "url": "http://arxiv.org/abs/2601.14686",
      "author": "Shuai Wang, Yaoming Yang, Bingdong Li, Hao Hao, Aimin Zhou",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes IB-GRPO for learning path recommendation, using indicator-based group relative policy optimization to align LLM recommendations with pedagogical objectives like Zone of Proximal Development.",
      "importance_score": 48,
      "reasoning": "Domain-specific application combining LLMs with educational theory. Useful for EdTech but limited broader impact.",
      "themes": [
        "Educational AI",
        "Reinforcement Learning",
        "LLM Fine-tuning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes IB-GRPO for learning path recommendation, using indicator-based group relative policy optimization to align LLM recommendations with pedagogical objectives like Zone of Proximal Development.</p>",
      "content_html": "<p>arXiv:2601.14686v1 Announce Type: new  Abstract: Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{\\epsilon+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.</p>"
    },
    {
      "id": "b5a2be5ea172",
      "title": "Implementing Knowledge Representation and Reasoning with Object Oriented Design",
      "content": "arXiv:2601.14840v1 Announce Type: new  Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&amp;R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&amp;R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.",
      "url": "http://arxiv.org/abs/2601.14840",
      "author": "Abdelrhman Bassiouny, Tom Schierenbeck, Sorin Arion, Benjamin Alt, Naren Vasantakumaar, Giang Nguyen, Michael Beetz",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces KRROOD, a framework bridging object-oriented programming with knowledge representation systems by treating knowledge as first-class programming abstractions using native class structures.",
      "importance_score": 48,
      "reasoning": "Addresses real integration challenges but limited novelty. Practical engineering contribution.",
      "themes": [
        "Knowledge Representation",
        "Software Engineering",
        "Symbolic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces KRROOD, a framework bridging object-oriented programming with knowledge representation systems by treating knowledge as first-class programming abstractions using native class structures.</p>",
      "content_html": "<p>arXiv:2601.14840v1 Announce Type: new  Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation &amp; Reasoning (KR&amp;R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&amp;R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.</p>"
    },
    {
      "id": "250838ec7bbd",
      "title": "Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)",
      "content": "arXiv:2601.14298v1 Announce Type: cross  Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.",
      "url": "http://arxiv.org/abs/2601.14298",
      "author": "Anjanava Biswas, Wrick Talukdar",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Survey on guardrails for trust, safety, and ethical LLM deployment covering safety, privacy, and ethical concerns. Provides overview of challenges and mitigation approaches.",
      "importance_score": 48,
      "reasoning": "Useful overview but appears to be older work (mentions 2023 as current). Limited novel contribution.",
      "themes": [
        "AI Safety",
        "LLM Deployment",
        "Ethics"
      ],
      "continuation": null,
      "summary_html": "<p>Survey on guardrails for trust, safety, and ethical LLM deployment covering safety, privacy, and ethical concerns. Provides overview of challenges and mitigation approaches.</p>",
      "content_html": "<p>arXiv:2601.14298v1 Announce Type: cross  Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.</p>"
    },
    {
      "id": "5a735add1c3d",
      "title": "Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis",
      "content": "arXiv:2601.14637v1 Announce Type: cross  Abstract: The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.",
      "url": "http://arxiv.org/abs/2601.14637",
      "author": "James Brock, Ce Zhang, Nantheera Anantrasirichai",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Forest-Chat integrates LLMs with vision-language models for interactive forest change analysis from satellite imagery. Enables natural language querying for pixel-level change detection and semantic interpretation.",
      "importance_score": 48,
      "reasoning": "Domain-specific application of VLMs, useful for environmental monitoring but limited novelty in core methods.",
      "themes": [
        "Remote Sensing",
        "Vision-Language Models",
        "Environmental AI"
      ],
      "continuation": null,
      "summary_html": "<p>Forest-Chat integrates LLMs with vision-language models for interactive forest change analysis from satellite imagery. Enables natural language querying for pixel-level change detection and semantic interpretation.</p>",
      "content_html": "<p>arXiv:2601.14637v1 Announce Type: cross  Abstract: The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.</p>"
    },
    {
      "id": "a9399a72654e",
      "title": "Multimodal system for skin cancer detection",
      "content": "arXiv:2601.14822v1 Announce Type: cross  Abstract: Melanoma detection is vital for early diagnosis and effective treatment. While deep learning models on dermoscopic images have shown promise, they require specialized equipment, limiting their use in broader clinical settings. This study introduces a multi-modal melanoma detection system using conventional photo images, making it more accessible and versatile. Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy. It employs a multi-modal neural network combining image and metadata processing and supports a two-step model for cases with or without metadata. A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance. To address the challenges of a highly imbalanced dataset, specific techniques were implemented to ensure robust training. An ablation study evaluated recent vision architectures, boosting algorithms, and loss functions, achieving a peak Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371. Results demonstrate that integrating photo images with metadata in a structured, multi-stage pipeline yields significant performance improvements. This system advances melanoma detection by providing a scalable, equipment-independent solution suitable for diverse healthcare environments, bridging the gap between specialized and general clinical practices.",
      "url": "http://arxiv.org/abs/2601.14822",
      "author": "Volodymyr Sydorskyi, Igor Krashenyi, Oleksii Yakubenko",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Multi-modal melanoma detection system using conventional photos instead of dermoscopic images, combining image data with patient metadata through neural networks and boosting ensemble.",
      "importance_score": 48,
      "reasoning": "Practical medical AI application improving accessibility, but incremental methodological contribution.",
      "themes": [
        "Medical Imaging",
        "Skin Cancer Detection",
        "Multimodal Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Multi-modal melanoma detection system using conventional photos instead of dermoscopic images, combining image data with patient metadata through neural networks and boosting ensemble.</p>",
      "content_html": "<p>arXiv:2601.14822v1 Announce Type: cross  Abstract: Melanoma detection is vital for early diagnosis and effective treatment. While deep learning models on dermoscopic images have shown promise, they require specialized equipment, limiting their use in broader clinical settings. This study introduces a multi-modal melanoma detection system using conventional photo images, making it more accessible and versatile. Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy. It employs a multi-modal neural network combining image and metadata processing and supports a two-step model for cases with or without metadata. A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance. To address the challenges of a highly imbalanced dataset, specific techniques were implemented to ensure robust training. An ablation study evaluated recent vision architectures, boosting algorithms, and loss functions, achieving a peak Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371. Results demonstrate that integrating photo images with metadata in a structured, multi-stage pipeline yields significant performance improvements. This system advances melanoma detection by providing a scalable, equipment-independent solution suitable for diverse healthcare environments, bridging the gap between specialized and general clinical practices.</p>"
    },
    {
      "id": "0bfe8de1907e",
      "title": "Early predicting of hospital admission using machine learning algorithms: Priority queues approach",
      "content": "arXiv:2601.15481v1 Announce Type: new  Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.",
      "url": "http://arxiv.org/abs/2601.15481",
      "author": "Jakub Antczak, James Montgomery, Ma{\\l}gorzata O'Reilly, Zbigniew Palmowski, Richard Turner",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Evaluates SARIMAX, XGBoost, and LSTM for forecasting hospital ED arrivals over seven days. Decomposes demand into eight ward categories and stratifies by clinical complexity.",
      "importance_score": 48,
      "reasoning": "Standard applied ML comparison study for healthcare. Limited novelty beyond the specific application domain.",
      "themes": [
        "Healthcare ML",
        "Time Series Forecasting",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Evaluates SARIMAX, XGBoost, and LSTM for forecasting hospital ED arrivals over seven days. Decomposes demand into eight ward categories and stratifies by clinical complexity.</p>",
      "content_html": "<p>arXiv:2601.15481v1 Announce Type: new  Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.</p>"
    },
    {
      "id": "b1c7c1a4e282",
      "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation",
      "content": "arXiv:2601.15663v1 Announce Type: cross  Abstract: Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.",
      "url": "http://arxiv.org/abs/2601.15663",
      "author": "Kristen Moore, Diksha Goel, Cody James Christopher, Zhen Wang, Minjune Kim, Ahmed Ibrahim, Ahmad Mohsin, Seyit Camtepe",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "TempoNet combines multi-task learning with temporal point processes to generate realistic network traffic for cybersecurity training. Models inter-arrival times and all packet/flow headers jointly.",
      "importance_score": 48,
      "reasoning": "Solid applied work for cybersecurity simulation but limited novel methodological contribution.",
      "themes": [
        "Cybersecurity",
        "Generative Models",
        "Network Simulation"
      ],
      "continuation": null,
      "summary_html": "<p>TempoNet combines multi-task learning with temporal point processes to generate realistic network traffic for cybersecurity training. Models inter-arrival times and all packet/flow headers jointly.</p>",
      "content_html": "<p>arXiv:2601.15663v1 Announce Type: cross  Abstract: Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.</p>"
    },
    {
      "id": "630a2a42c73b",
      "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
      "content": "arXiv:2601.15872v1 Announce Type: cross  Abstract: Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
      "url": "http://arxiv.org/abs/2601.15872",
      "author": "Jaekwon Im, Natalia Polouliakh, Taketo Akama",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "PF-D2M is a pose-free diffusion model for dance-to-music generation using visual features from videos. Progressive training strategy enables multiple and non-human dancer support.",
      "importance_score": 48,
      "reasoning": "Novel approach removing pose estimation dependency. Creative AI application but limited broader impact.",
      "themes": [
        "Music Generation",
        "Diffusion Models",
        "Multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>PF-D2M is a pose-free diffusion model for dance-to-music generation using visual features from videos. Progressive training strategy enables multiple and non-human dancer support.</p>",
      "content_html": "<p>arXiv:2601.15872v1 Announce Type: cross  Abstract: Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.</p>"
    },
    {
      "id": "c581ef899aa4",
      "title": "Risk reversal for least squares estimators under nested convex constraints",
      "content": "arXiv:2601.16041v1 Announce Type: cross  Abstract: In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.   We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $\\Theta \\subset \\mathbb{R}^d$ one observes \\[ Y = \\theta^\\star + \\sigma Z, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $\\theta^\\star \\in \\Theta$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $\\Theta$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $\\Theta_S \\subset \\Theta_L$ and a parameter $\\theta^\\star \\in \\Theta_S$ such that the LSE constrained to $\\Theta_S$ has strictly larger risk than the LSE constrained to $\\Theta_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.   We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $\\theta^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $\\Theta_S$ within $\\Theta_L$ can reverse the risk ordering.   These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.",
      "url": "http://arxiv.org/abs/2601.16041",
      "author": "Omar Al-Ghattas",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "math.ST"
      ],
      "summary": "Demonstrates risk reversal in constrained estimation: stricter feasible sets can paradoxically increase statistical risk. Provides explicit counterexample in Gaussian sequence model.",
      "importance_score": 48,
      "reasoning": "Interesting theoretical finding challenging intuitions but limited practical ML implications.",
      "themes": [
        "Statistical Theory",
        "Estimation",
        "Constraints"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates risk reversal in constrained estimation: stricter feasible sets can paradoxically increase statistical risk. Provides explicit counterexample in Gaussian sequence model.</p>",
      "content_html": "<p>arXiv:2601.16041v1 Announce Type: cross  Abstract: In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.   We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $\\Theta \\subset \\mathbb{R}^d$ one observes \\[ Y = \\theta^\\star + \\sigma Z, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $\\theta^\\star \\in \\Theta$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $\\Theta$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $\\Theta_S \\subset \\Theta_L$ and a parameter $\\theta^\\star \\in \\Theta_S$ such that the LSE constrained to $\\Theta_S$ has strictly larger risk than the LSE constrained to $\\Theta_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.   We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $\\theta^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $\\Theta_S$ within $\\Theta_L$ can reverse the risk ordering.   These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.</p>"
    },
    {
      "id": "1a502ef2d151",
      "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
      "content": "arXiv:2601.16158v1 Announce Type: cross  Abstract: Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
      "url": "http://arxiv.org/abs/2601.16158",
      "author": "Prakash Dhungana, Sayed Ahmad Salehi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Proposes continual learning framework for keyword spotting on edge devices. Uses dual-input CNN with MFCC and Mel-spectrogram features plus multi-stage denoising.",
      "importance_score": 48,
      "reasoning": "Practical edge ML contribution addressing domain shift. Good engineering but limited novelty.",
      "themes": [
        "Keyword Spotting",
        "Continual Learning",
        "Edge AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes continual learning framework for keyword spotting on edge devices. Uses dual-input CNN with MFCC and Mel-spectrogram features plus multi-stage denoising.</p>",
      "content_html": "<p>arXiv:2601.16158v1 Announce Type: cross  Abstract: Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.</p>"
    },
    {
      "id": "2ac93f4c12ac",
      "title": "MALTopic: Multi-Agent LLM Topic Modeling Framework",
      "content": "arXiv:2601.15299v1 Announce Type: new  Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.",
      "url": "http://arxiv.org/abs/2601.15299",
      "author": "Yash Sharma",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "MALTopic decomposes topic modeling into specialized LLM agents: enrichment, topic modeling, and deduplication. Incorporates structured survey data into topic extraction.",
      "importance_score": 48,
      "reasoning": "Multi-agent approach to classic NLP task. Interesting but limited evaluation scope.",
      "themes": [
        "Topic Modeling",
        "Multi-Agent LLMs",
        "Survey Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>MALTopic decomposes topic modeling into specialized LLM agents: enrichment, topic modeling, and deduplication. Incorporates structured survey data into topic extraction.</p>",
      "content_html": "<p>arXiv:2601.15299v1 Announce Type: new  Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.</p>"
    },
    {
      "id": "084dacd62fdb",
      "title": "ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms",
      "content": "arXiv:2601.15605v1 Announce Type: new  Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.",
      "url": "http://arxiv.org/abs/2601.15605",
      "author": "Baktash Ansari, Shiza Ali, Elias Martin, Maryna Sivachenko, Afra Mashhadi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "ToxiTwitch compares toxicity detection approaches for Twitch including DeepSeek-R1-Distill and Llama-3-8B-Instruct. Studies emote-aware moderation for live streaming.",
      "importance_score": 48,
      "reasoning": "Practical content moderation study. Good domain focus but limited novel methodology.",
      "themes": [
        "Content Moderation",
        "Toxicity Detection",
        "Live Streaming"
      ],
      "continuation": null,
      "summary_html": "<p>ToxiTwitch compares toxicity detection approaches for Twitch including DeepSeek-R1-Distill and Llama-3-8B-Instruct. Studies emote-aware moderation for live streaming.</p>",
      "content_html": "<p>arXiv:2601.15605v1 Announce Type: new  Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.</p>"
    },
    {
      "id": "a0089bc58090",
      "title": "Can professional translators identify machine-generated text?",
      "content": "arXiv:2601.15828v1 Announce Type: new  Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.",
      "url": "http://arxiv.org/abs/2601.15828",
      "author": "Michael Farrell",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies whether professional translators can identify AI-generated Italian short stories. Statistically significant subset (16.2%) succeeded, but equal number misclassified human as AI.",
      "importance_score": 48,
      "reasoning": "Interesting human-AI detection study. Good methodology but limited sample and scope.",
      "themes": [
        "AI Detection",
        "Human Studies",
        "Translation"
      ],
      "continuation": null,
      "summary_html": "<p>Studies whether professional translators can identify AI-generated Italian short stories. Statistically significant subset (16.2%) succeeded, but equal number misclassified human as AI.</p>",
      "content_html": "<p>arXiv:2601.15828v1 Announce Type: new  Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.</p>"
    },
    {
      "id": "61d4943e48d7",
      "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
      "content": "arXiv:2601.16097v1 Announce Type: new  Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
      "url": "http://arxiv.org/abs/2601.16097",
      "author": "Makbule Gulcin Ozsoy",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Trains language-specific LoRA adapters for multilingual Text2Cypher and combines via linear merging or learned fusion MLP. Avoids full fine-tuning for new languages.",
      "importance_score": 48,
      "reasoning": "Practical efficiency improvement for multilingual database interfaces. Good engineering but limited novelty.",
      "themes": [
        "Multilingual NLP",
        "Adapter Fusion",
        "Text2Cypher"
      ],
      "continuation": null,
      "summary_html": "<p>Trains language-specific LoRA adapters for multilingual Text2Cypher and combines via linear merging or learned fusion MLP. Avoids full fine-tuning for new languages.</p>",
      "content_html": "<p>arXiv:2601.16097v1 Announce Type: new  Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.</p>"
    },
    {
      "id": "23556fbefa03",
      "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics",
      "content": "arXiv:2601.16087v1 Announce Type: cross  Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.",
      "url": "http://arxiv.org/abs/2601.16087",
      "author": "Sukesh Subaharan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Investigates whether imposing dynamical structure on external affective state (Valence-Arousal-Dominance) can induce temporal coherence in LLM agents during extended interactions, using first- and second-order update rules.",
      "importance_score": 48,
      "reasoning": "Interesting approach to agent consistency but relatively narrow focus on affective state. Limited to multi-turn dialogue without broader agent applications.",
      "themes": [
        "AI Agents",
        "Dialogue Systems",
        "Affective Computing"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates whether imposing dynamical structure on external affective state (Valence-Arousal-Dominance) can induce temporal coherence in LLM agents during extended interactions, using first- and second-order update rules.</p>",
      "content_html": "<p>arXiv:2601.16087v1 Announce Type: cross  Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.</p>"
    },
    {
      "id": "eb354cceccd8",
      "title": "Consistency-Regularized GAN for Few-Shot SAR Target Recognition",
      "content": "arXiv:2601.15681v1 Announce Type: new  Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.",
      "url": "http://arxiv.org/abs/2601.15681",
      "author": "Yikui Zhai, Shikuang Liu, Wenlve Zhou, Hongsheng Zhang, Zhiheng Zhou, Xiaolin Tian, C. L. Philip Chen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes Cr-GAN (Consistency-regularized GAN) for few-shot SAR target recognition, synthesizing diverse samples even with severe data scarcity through consistency regularization.",
      "importance_score": 48,
      "reasoning": "Addresses practical SAR recognition challenge with GAN-based data augmentation. Specialized domain with limited broader impact.",
      "themes": [
        "SAR Imaging",
        "Few-Shot Learning",
        "Generative Models"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Cr-GAN (Consistency-regularized GAN) for few-shot SAR target recognition, synthesizing diverse samples even with severe data scarcity through consistency regularization.</p>",
      "content_html": "<p>arXiv:2601.15681v1 Announce Type: new  Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>"
    },
    {
      "id": "baf4511f1b75",
      "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification",
      "content": "arXiv:2601.15757v1 Announce Type: new  Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.",
      "url": "http://arxiv.org/abs/2601.15757",
      "author": "Yimin Zhu, Lincoln Linlin Xu, Zhengsen Xu, Zack Dewis, Mabel Heffring, Saeid Taleghanidoozdoozan, Motasem Alkayid, Quinn Ledingham, Megan Greenwood",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes ES-mHC, an interpretable hyperspectral image classification framework using electromagnetic spectrum-aware hyper-connections to model interactions between spectrum groupings with directional matrices.",
      "importance_score": 48,
      "reasoning": "Interpretability contribution for hyperspectral analysis but specialized domain with limited broader impact.",
      "themes": [
        "Hyperspectral Imaging",
        "Interpretability",
        "Remote Sensing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes ES-mHC, an interpretable hyperspectral image classification framework using electromagnetic spectrum-aware hyper-connections to model interactions between spectrum groupings with directional matrices.</p>",
      "content_html": "<p>arXiv:2601.15757v1 Announce Type: new  Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.</p>"
    },
    {
      "id": "8247e370e236",
      "title": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation",
      "content": "arXiv:2601.15572v1 Announce Type: cross  Abstract: Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.",
      "url": "http://arxiv.org/abs/2601.15572",
      "author": "Jieyun Bai, Yitong Tang, Zihao Zhou, Mahdi Islam, Musarrat Tabassum, Enrique Almar-Munoz, Hongyu Liu, Hui Meng, Nianjiang Lv, Bo Deng, Yu Chen, Zilun Peng, Yusong Xiao, Li Xiao, Nam-Khanh Tran, Dac-Phu Phan-Le, Hai-Dang Nguyen, Xiao Liu, Jiale Hu, Mingxu Huang, Jitao Liang, Chaolu Feng, Xuezhi Zhang, Lyuyang Tong, Bo Du, Ha-Hieu Pham, Thanh-Huy Nguyen, Min Xu, Juntao Jiang, Jiangning Zhang, Yong Liu, Md. Kamrul Hasan, Jie Gan, Zhuonan Liang, Weidong Cai, Yuxin Huang, Gongning Luo, Mohammad Yaqub, Karim Lekadir",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "eess.IV"
      ],
      "summary": "Introduces FUGC, the first benchmark for semi-supervised learning in cervical segmentation from transvaginal ultrasound, including 890 images and evaluation of multiple methods from 10 teams.",
      "importance_score": 48,
      "reasoning": "Valuable benchmark for medical imaging community, hosted at ISBI 2025, but specialized domain.",
      "themes": [
        "Medical Imaging",
        "Benchmarks",
        "Semi-Supervised Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces FUGC, the first benchmark for semi-supervised learning in cervical segmentation from transvaginal ultrasound, including 890 images and evaluation of multiple methods from 10 teams.</p>",
      "content_html": "<p>arXiv:2601.15572v1 Announce Type: cross  Abstract: Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.</p>"
    },
    {
      "id": "2b929be8232a",
      "title": "Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation",
      "content": "arXiv:2601.15459v1 Announce Type: new  Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.",
      "url": "http://arxiv.org/abs/2601.15459",
      "author": "Sarvin Ghiasi, Majid Roshanfar, Jake Barralet, Liane S. Feldman, Amir Hooshiar",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Develops an integrated framework combining analytical modeling, simulation, and machine learning for collision detection between robotic arms in laparoscopic surgery.",
      "importance_score": 48,
      "reasoning": "Practical contribution to surgical robotics safety, combines multiple approaches effectively.",
      "themes": [
        "Surgical Robotics",
        "Collision Detection",
        "Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Develops an integrated framework combining analytical modeling, simulation, and machine learning for collision detection between robotic arms in laparoscopic surgery.</p>",
      "content_html": "<p>arXiv:2601.15459v1 Announce Type: new  Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.</p>"
    },
    {
      "id": "76aeb2ca80b3",
      "title": "Learning Functional Graphs with Nonlinear Sufficient Dimension Reduction",
      "content": "arXiv:2601.15696v1 Announce Type: cross  Abstract: Functional graphical models have undergone extensive development during the recent years, leading to a variety models such as the functional Gaussian graphical model, the functional copula Gaussian graphical model, the functional Bayesian graphical model, the nonparametric functional additive graphical model, and the conditional functional graphical model. These models rely either on some parametric form of distributions on random functions, or on additive conditional independence, a criterion that is different from probabilistic conditional independence. In this paper we introduce a nonparametric functional graphical model based on functional sufficient dimension reduction. Our method not only relaxes the Gaussian or copula Gaussian assumptions, but also enhances estimation accuracy by avoiding the ``curse of dimensionality''. Moreover, it retains the probabilistic conditional independence as the criterion to determine the absence of edges. By doing simulation study and analysis of the f-MRI dataset, we demonstrate the advantages of our method.",
      "url": "http://arxiv.org/abs/2601.15696",
      "author": "Kyongwon Kim, Bing Li",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning (Statistics))",
      "source_type": "arxiv",
      "tags": [
        "stat.ME"
      ],
      "summary": "Introduces nonparametric functional graphical models based on sufficient dimension reduction, relaxing Gaussian and additive assumptions of existing approaches.",
      "importance_score": 48,
      "reasoning": "Novel statistical methodology, theoretical contribution.",
      "themes": [
        "Graphical Models",
        "Dimension Reduction",
        "Statistics"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces nonparametric functional graphical models based on sufficient dimension reduction, relaxing Gaussian and additive assumptions of existing approaches.</p>",
      "content_html": "<p>arXiv:2601.15696v1 Announce Type: cross  Abstract: Functional graphical models have undergone extensive development during the recent years, leading to a variety models such as the functional Gaussian graphical model, the functional copula Gaussian graphical model, the functional Bayesian graphical model, the nonparametric functional additive graphical model, and the conditional functional graphical model. These models rely either on some parametric form of distributions on random functions, or on additive conditional independence, a criterion that is different from probabilistic conditional independence. In this paper we introduce a nonparametric functional graphical model based on functional sufficient dimension reduction. Our method not only relaxes the Gaussian or copula Gaussian assumptions, but also enhances estimation accuracy by avoiding the ``curse of dimensionality''. Moreover, it retains the probabilistic conditional independence as the criterion to determine the absence of edges. By doing simulation study and analysis of the f-MRI dataset, we demonstrate the advantages of our method.</p>"
    },
    {
      "id": "178b12ab1010",
      "title": "Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation",
      "content": "arXiv:2601.14274v1 Announce Type: cross  Abstract: Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \\emph{unique}, \\emph{redundant}, and \\emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \\emph{\\textbf{D}ivide and \\textbf{R}efine} (\\textbf{DnR}). In the \\textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \\textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026",
      "url": "http://arxiv.org/abs/2601.14274",
      "author": "Anh-Tuan Mai, Cam-Van Thi Nguyen, Duc-Trong Le",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes information-theoretic approach to multimodal emotion recognition, identifying unique, redundant, and synergistic contributions from modalities and how data augmentation affects these.",
      "importance_score": 47,
      "reasoning": "Solid theoretical framing but incremental contribution to emotion recognition field.",
      "themes": [
        "Emotion Recognition",
        "Multimodal Learning",
        "Information Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes information-theoretic approach to multimodal emotion recognition, identifying unique, redundant, and synergistic contributions from modalities and how data augmentation affects these.</p>",
      "content_html": "<p>arXiv:2601.14274v1 Announce Type: cross  Abstract: Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \\emph{unique}, \\emph{redundant}, and \\emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \\emph{\\textbf{D}ivide and \\textbf{R}efine} (\\textbf{DnR}). In the \\textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \\textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026</p>"
    },
    {
      "id": "41b2ebc0d8c2",
      "title": "Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation",
      "content": "arXiv:2601.14678v1 Announce Type: cross  Abstract: Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.   This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.   We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.",
      "url": "http://arxiv.org/abs/2601.14678",
      "author": "Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, Alhassan S. Yasin",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Evaluates domain adaptation techniques for transferring cancer severity classification across different adenocarcinoma types. Addresses limited annotated data in medical imaging through cross-domain transfer.",
      "importance_score": 47,
      "reasoning": "Standard domain adaptation applied to medical imaging, practical value but limited methodological novelty.",
      "themes": [
        "Medical Imaging",
        "Transfer Learning",
        "Domain Adaptation"
      ],
      "continuation": null,
      "summary_html": "<p>Evaluates domain adaptation techniques for transferring cancer severity classification across different adenocarcinoma types. Addresses limited annotated data in medical imaging through cross-domain transfer.</p>",
      "content_html": "<p>arXiv:2601.14678v1 Announce Type: cross  Abstract: Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.   This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.   We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.</p>"
    },
    {
      "id": "3e773b5207ae",
      "title": "Anytime Optimal Decision Tree Learning with Continuous Features",
      "content": "arXiv:2601.14765v1 Announce Type: cross  Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.",
      "url": "http://arxiv.org/abs/2601.14765",
      "author": "Harold Kiossou, Pierre Schaus, Siegfried Nijssen",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes anytime algorithm for optimal decision tree learning with continuous features, overcoming depth limitations of existing exact methods through improved search strategy.",
      "importance_score": 47,
      "reasoning": "Solid ML fundamentals work extending optimal decision trees, but specialized contribution.",
      "themes": [
        "Decision Trees",
        "Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes anytime algorithm for optimal decision tree learning with continuous features, overcoming depth limitations of existing exact methods through improved search strategy.</p>",
      "content_html": "<p>arXiv:2601.14765v1 Announce Type: cross  Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.</p>"
    },
    {
      "id": "054ab91287ef",
      "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism",
      "content": "arXiv:2601.15249v2 Announce Type: cross  Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.",
      "url": "http://arxiv.org/abs/2601.15249",
      "author": "Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Isotonic Mechanism for eliciting author self-assessments to improve best paper award selection at ML conferences, aggregating author rankings with review scores.",
      "importance_score": 47,
      "reasoning": "Interesting meta-science contribution for peer review but limited AI/ML technical contribution.",
      "themes": [
        "Peer Review",
        "Mechanism Design",
        "Meta-Science"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Isotonic Mechanism for eliciting author self-assessments to improve best paper award selection at ML conferences, aggregating author rankings with review scores.</p>",
      "content_html": "<p>arXiv:2601.15249v2 Announce Type: cross  Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</p>"
    },
    {
      "id": "8dec0b71b215",
      "title": "Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets",
      "content": "arXiv:2601.16107v1 Announce Type: new  Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.",
      "url": "http://arxiv.org/abs/2601.16107",
      "author": "Adithya Sineesh, Akshita Kamsali",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "First systematic benchmark comparing multiple deep learning classifiers specifically designed for Raman spectroscopy across multiple open-source datasets.",
      "importance_score": 47,
      "reasoning": "Useful benchmark study for domain specialists but limited ML novelty. Value is in empirical comparison.",
      "themes": [
        "Spectroscopy",
        "Benchmarks",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>First systematic benchmark comparing multiple deep learning classifiers specifically designed for Raman spectroscopy across multiple open-source datasets.</p>",
      "content_html": "<p>arXiv:2601.16107v1 Announce Type: new  Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.</p>"
    },
    {
      "id": "b6d313e5fb4a",
      "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging",
      "content": "arXiv:2601.15731v1 Announce Type: new  Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.",
      "url": "http://arxiv.org/abs/2601.15731",
      "author": "Linyong Zou, Liang Zhang, Xiongfei Wang, Jia-Hong Gao, Yi Sun, Shurong Sheng, Kuntao Xiao, Wanli Yang, Pengfei Teng, Guoming Luan, Zhao Lv, Zikang Xu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes FAIR-ESI framework for electrophysiological source imaging with adaptive feature refinement across spectral, temporal, and patch-wise dimensions. Validated on simulation and clinical datasets.",
      "importance_score": 47,
      "reasoning": "Technical contribution to brain imaging analysis with multi-view feature refinement. Specialized neuroscience application.",
      "themes": [
        "Brain Imaging",
        "Medical AI",
        "Deep Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes FAIR-ESI framework for electrophysiological source imaging with adaptive feature refinement across spectral, temporal, and patch-wise dimensions. Validated on simulation and clinical datasets.</p>",
      "content_html": "<p>arXiv:2601.15731v1 Announce Type: new  Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.</p>"
    },
    {
      "id": "e7fe0f6946fb",
      "title": "Measuring the State of Open Science in Transportation Using Large Language Models",
      "content": "arXiv:2601.14429v1 Announce Type: cross  Abstract: Open science initiatives have strengthened scientific integrity and accelerated research progress across many fields, but the state of their practice within transportation research remains under-investigated. Key features of open science, defined here as data and code availability, are difficult to extract due to the inherent complexity of the field. Previous work has either been limited to small-scale studies due to the labor-intensive nature of manual analysis or has relied on large-scale bibliometric approaches that sacrifice contextual richness. This paper introduces an automatic and scalable feature-extraction pipeline to measure data and code availability in transportation research. We employ Large Language Models (LLMs) for this task and validate their performance against a manually curated dataset and through an inter-rater agreement analysis. We applied this pipeline to examine 10,724 research articles published in the Transportation Research Part series of journals between 2019 and 2024. Our analysis found that only 5% of quantitative papers shared a code repository, 4% of quantitative papers shared a data repository, and about 3% of papers shared both, with trends differing across journals, topics, and geographic regions. We found no significant difference in citation counts or review duration between papers that provided data and code and those that did not, suggesting a misalignment between open science efforts and traditional academic metrics. Consequently, encouraging these practices will likely require structural interventions from journals and funding agencies to supplement the lack of direct author incentives. The pipeline developed in this study can be readily scaled to other journals, representing a critical step toward the automated measurement and monitoring of open science practices in transportation research.",
      "url": "http://arxiv.org/abs/2601.14429",
      "author": "Junyi Ji, Ruth Lu, Linda Belkessa, Liming Wang, Silvia Varotto, Yongqi Dong, Nicolas Saunier, Mostafa Ameli, Gregory S. Macfarlane, Bahman Madadi, Cathy Wu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.DL"
      ],
      "summary": "Uses LLMs to automatically extract data and code availability features from transportation research papers, measuring open science practices at scale.",
      "importance_score": 46,
      "reasoning": "Novel application of LLMs for bibliometric analysis. Useful methodology but domain-specific impact.",
      "themes": [
        "Open Science",
        "LLM Applications",
        "Research Methods"
      ],
      "continuation": null,
      "summary_html": "<p>Uses LLMs to automatically extract data and code availability features from transportation research papers, measuring open science practices at scale.</p>",
      "content_html": "<p>arXiv:2601.14429v1 Announce Type: cross  Abstract: Open science initiatives have strengthened scientific integrity and accelerated research progress across many fields, but the state of their practice within transportation research remains under-investigated. Key features of open science, defined here as data and code availability, are difficult to extract due to the inherent complexity of the field. Previous work has either been limited to small-scale studies due to the labor-intensive nature of manual analysis or has relied on large-scale bibliometric approaches that sacrifice contextual richness. This paper introduces an automatic and scalable feature-extraction pipeline to measure data and code availability in transportation research. We employ Large Language Models (LLMs) for this task and validate their performance against a manually curated dataset and through an inter-rater agreement analysis. We applied this pipeline to examine 10,724 research articles published in the Transportation Research Part series of journals between 2019 and 2024. Our analysis found that only 5% of quantitative papers shared a code repository, 4% of quantitative papers shared a data repository, and about 3% of papers shared both, with trends differing across journals, topics, and geographic regions. We found no significant difference in citation counts or review duration between papers that provided data and code and those that did not, suggesting a misalignment between open science efforts and traditional academic metrics. Consequently, encouraging these practices will likely require structural interventions from journals and funding agencies to supplement the lack of direct author incentives. The pipeline developed in this study can be readily scaled to other journals, representing a critical step toward the automated measurement and monitoring of open science practices in transportation research.</p>"
    },
    {
      "id": "9bac0f73dda4",
      "title": "Adaptive Fidelity Estimation for Quantum Programs with Graph-Guided Noise Awareness",
      "content": "arXiv:2601.14713v1 Announce Type: cross  Abstract: Fidelity estimation is a critical yet resource-intensive step in testing quantum programs on noisy intermediate-scale quantum (NISQ) devices, where the required number of measurements is difficult to predefine due to hardware noise, device heterogeneity, and transpilation-induced circuit transformations. We present QuFid, an adaptive and noise-aware framework that determines measurement budgets online by leveraging circuit structure and runtime statistical feedback. QuFid models a quantum program as a directed acyclic graph (DAG) and employs a control-flow-aware random walk to characterize noise propagation along gate dependencies. Backend-specific effects are captured via transpilation-induced structural deformation metrics, which are integrated into the random-walk formulation to induce a noise-propagation operator. Circuit complexity is then quantified through the spectral characteristics of this operator, providing a principled and lightweight basis for adaptive measurement planning. Experiments on 18 quantum benchmarks executed on IBM Quantum backends show that QuFid significantly reduces measurement cost compared to fixed-shot and learning-based baselines, while consistently maintaining acceptable fidelity bias.",
      "url": "http://arxiv.org/abs/2601.14713",
      "author": "Tingting Li, Ziming Zhao, Jianwei Yin",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "quant-ph"
      ],
      "summary": "QuFid adaptively determines measurement budgets for quantum program fidelity estimation using circuit DAG structure and noise modeling. Addresses resource efficiency for NISQ device testing.",
      "importance_score": 46,
      "reasoning": "Specialized quantum computing contribution with ML components, limited broader AI impact.",
      "themes": [
        "Quantum Computing",
        "Adaptive Estimation"
      ],
      "continuation": null,
      "summary_html": "<p>QuFid adaptively determines measurement budgets for quantum program fidelity estimation using circuit DAG structure and noise modeling. Addresses resource efficiency for NISQ device testing.</p>",
      "content_html": "<p>arXiv:2601.14713v1 Announce Type: cross  Abstract: Fidelity estimation is a critical yet resource-intensive step in testing quantum programs on noisy intermediate-scale quantum (NISQ) devices, where the required number of measurements is difficult to predefine due to hardware noise, device heterogeneity, and transpilation-induced circuit transformations. We present QuFid, an adaptive and noise-aware framework that determines measurement budgets online by leveraging circuit structure and runtime statistical feedback. QuFid models a quantum program as a directed acyclic graph (DAG) and employs a control-flow-aware random walk to characterize noise propagation along gate dependencies. Backend-specific effects are captured via transpilation-induced structural deformation metrics, which are integrated into the random-walk formulation to induce a noise-propagation operator. Circuit complexity is then quantified through the spectral characteristics of this operator, providing a principled and lightweight basis for adaptive measurement planning. Experiments on 18 quantum benchmarks executed on IBM Quantum backends show that QuFid significantly reduces measurement cost compared to fixed-shot and learning-based baselines, while consistently maintaining acceptable fidelity bias.</p>"
    },
    {
      "id": "b7429e7be20a",
      "title": "Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models",
      "content": "arXiv:2601.14917v1 Announce Type: cross  Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.",
      "url": "http://arxiv.org/abs/2601.14917",
      "author": "Giorgia Rigamonti, Mirko Paolo Barbato, Davide Marelli, Paolo Napoletano",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Develops personalized blood glucose prediction using patient-specific deep learning, comparing leave-one-out vs per-patient training approaches for wearable glucose monitor data.",
      "importance_score": 46,
      "reasoning": "Applied medical ML with practical focus on personalization, but standard deep learning approaches.",
      "themes": [
        "Medical AI",
        "Personalization",
        "Time Series"
      ],
      "continuation": null,
      "summary_html": "<p>Develops personalized blood glucose prediction using patient-specific deep learning, comparing leave-one-out vs per-patient training approaches for wearable glucose monitor data.</p>",
      "content_html": "<p>arXiv:2601.14917v1 Announce Type: cross  Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.</p>"
    },
    {
      "id": "572b6bd4fb36",
      "title": "DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking",
      "content": "arXiv:2601.15518v1 Announce Type: cross  Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.",
      "url": "http://arxiv.org/abs/2601.15518",
      "author": "Wenxin Zhou, Ritesh Mehta, Anthony Miyaguchi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.IR"
      ],
      "summary": "Two-stage retrieval system for TREC Tip-of-the-Tongue task combining hybrid retrieval, topic-aware multi-index dense retrieval, and LLM-based reranking with synthetic query generation.",
      "importance_score": 46,
      "reasoning": "System description for specific IR task. Limited novelty but useful practical approach.",
      "themes": [
        "Information Retrieval",
        "LLM Applications",
        "Search Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Two-stage retrieval system for TREC Tip-of-the-Tongue task combining hybrid retrieval, topic-aware multi-index dense retrieval, and LLM-based reranking with synthetic query generation.</p>",
      "content_html": "<p>arXiv:2601.15518v1 Announce Type: cross  Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.</p>"
    },
    {
      "id": "7006c8cb4963",
      "title": "Class Confidence Aware Reweighting for Long Tailed Learning",
      "content": "arXiv:2601.15924v1 Announce Type: cross  Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an {\\Omega}(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.",
      "url": "http://arxiv.org/abs/2601.15924",
      "author": "Brainard Philemon Jagati, Jitendra Tembhurne, Harsh Goud, Rudra Pratap Singh, Chandrashekhar Meshram",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes class and confidence-aware reweighting scheme for long-tailed learning that adjusts based on both class imbalance and sample confidence levels.",
      "importance_score": 46,
      "reasoning": "Incremental improvement on long-tailed learning. Addresses practical issue but methodology is evolutionary.",
      "themes": [
        "Long-tailed Learning",
        "Class Imbalance",
        "Reweighting"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes class and confidence-aware reweighting scheme for long-tailed learning that adjusts based on both class imbalance and sample confidence levels.</p>",
      "content_html": "<p>arXiv:2601.15924v1 Announce Type: cross  Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an {\\Omega}(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.</p>"
    },
    {
      "id": "61c2af534eac",
      "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars",
      "content": "arXiv:2601.15914v1 Announce Type: new  Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.",
      "url": "http://arxiv.org/abs/2601.15914",
      "author": "Yarin Benyamin",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Benchmarks off-the-shelf emotion recognition models for real-time VR therapy applications with ASD users, evaluating YOLO variants and other models against strict 140ms motion-to-photon latency requirements.",
      "importance_score": 46,
      "reasoning": "Practical evaluation for VR accessibility applications but primarily benchmark with limited methodological contribution.",
      "themes": [
        "Emotion Recognition",
        "Virtual Reality",
        "Accessibility",
        "Real-Time AI"
      ],
      "continuation": null,
      "summary_html": "<p>Benchmarks off-the-shelf emotion recognition models for real-time VR therapy applications with ASD users, evaluating YOLO variants and other models against strict 140ms motion-to-photon latency requirements.</p>",
      "content_html": "<p>arXiv:2601.15914v1 Announce Type: new  Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (&lt;23%) or speed (&gt;150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.</p>"
    },
    {
      "id": "ec8b94e3f792",
      "title": "Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text",
      "content": "arXiv:2601.14683v1 Announce Type: new  Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.",
      "url": "http://arxiv.org/abs/2601.14683",
      "author": "Aisvarya Adeseye, Jouni Isoaho, Seppo Virtanen, Mohammad Tahir",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Develops SFAA, a structured framework using local LLMs for context-aware anonymization of sensitive qualitative research data. Addresses limitations of pattern-matching approaches that fail to preserve meaning.",
      "importance_score": 45,
      "reasoning": "Practical privacy application but methodology is fairly straightforward application of LLMs. Limited novelty.",
      "themes": [
        "Privacy",
        "Data Anonymization",
        "LLM Applications"
      ],
      "continuation": null,
      "summary_html": "<p>Develops SFAA, a structured framework using local LLMs for context-aware anonymization of sensitive qualitative research data. Addresses limitations of pattern-matching approaches that fail to preserve meaning.</p>",
      "content_html": "<p>arXiv:2601.14683v1 Announce Type: new  Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.</p>"
    },
    {
      "id": "0e355a5a665b",
      "title": "Just aware enough: Evaluating awareness across artificial systems",
      "content": "arXiv:2601.14901v1 Announce Type: new  Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.",
      "url": "http://arxiv.org/abs/2601.14901",
      "author": "Nadine Meertens, Suet Lee, Ophelia Deroy",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Argues awareness is more tractable than consciousness for evaluating AI systems. Proposes domain-sensitive, multidimensional evaluation method for assessing system abilities to process, store, and use information for goal-directed action.",
      "importance_score": 45,
      "reasoning": "Interesting philosophical contribution but limited practical methodology. Primarily conceptual.",
      "themes": [
        "AI Consciousness",
        "Evaluation",
        "Philosophy of AI"
      ],
      "continuation": null,
      "summary_html": "<p>Argues awareness is more tractable than consciousness for evaluating AI systems. Proposes domain-sensitive, multidimensional evaluation method for assessing system abilities to process, store, and use information for goal-directed action.</p>",
      "content_html": "<p>arXiv:2601.14901v1 Announce Type: new  Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.</p>"
    },
    {
      "id": "f1b0354db9e8",
      "title": "DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing",
      "content": "arXiv:2601.14302v1 Announce Type: cross  Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.",
      "url": "http://arxiv.org/abs/2601.14302",
      "author": "Jinwei Hu, Shiyuan Meng, Yi Dong, Xiaowei Huang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes DDSA, a dual-domain adversarial attack framework optimizing testing through temporal selectivity and spatial precision for resource-critical image systems.",
      "importance_score": 45,
      "reasoning": "Practical efficiency improvement for adversarial testing but limited novelty in attack methodology.",
      "themes": [
        "Adversarial Attacks",
        "Efficiency",
        "Computer Vision"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes DDSA, a dual-domain adversarial attack framework optimizing testing through temporal selectivity and spatial precision for resource-critical image systems.</p>",
      "content_html": "<p>arXiv:2601.14302v1 Announce Type: cross  Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.</p>"
    },
    {
      "id": "b90525f410cd",
      "title": "Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation",
      "content": "arXiv:2601.14438v1 Announce Type: cross  Abstract: Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.",
      "url": "http://arxiv.org/abs/2601.14438",
      "author": "Danial Sadrian Zadeh, Otman A. Basir, Behzad Moshiri",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes framework transforming frontal-view camera images to natural language scene descriptions using hybrid attention mechanism for autonomous driving.",
      "importance_score": 45,
      "reasoning": "Incremental contribution to driving scene understanding. Standard architecture components.",
      "themes": [
        "Autonomous Driving",
        "Scene Understanding",
        "Vision-Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes framework transforming frontal-view camera images to natural language scene descriptions using hybrid attention mechanism for autonomous driving.</p>",
      "content_html": "<p>arXiv:2601.14438v1 Announce Type: cross  Abstract: Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.</p>"
    },
    {
      "id": "276680fa372d",
      "title": "Calibrated uncertainty quantification for prosumer flexibility aggregation in ancillary service markets",
      "content": "arXiv:2601.14663v1 Announce Type: cross  Abstract: Reliable forecasting of prosumer flexibility is critical for demand response aggregators participating in frequency controlled ancillary services market, where strict reliability requirements such as the P90 standard are enforced. Limited historical data, dependence on exogeneous factors, and heterogenous prosumer behaviour introduce significant epistemic uncertainty, making deterministic or poorly calibrated probabilistic models unsuitable for market bidding. This paper proposes the use of scalable uncertainty quantification framework that integrates Monte Carlo dropout (MCD) with conformal prediction (CP) to produce calibrated, finite sample prediction intervals for aggregated prosumer flexibility. The proposed framework is applied to a behind-the-meter aggregator participating in the Danish manual frequency restoration reserve capacity market. A large-scale synthetic dataset is generated using a modified industry-grade home energy management system, combined with publicly available load, solar, price, activation and device-level data. The resulting machine learning surrogate model captures aggregate prosumer price responsiveness and provides uncertainty-aware estimates suitable for market bidding. Multiple multivariate CP strategies are evaluated and benchmarked against conventional MCD-based methods. Results show that standalone MCD systematically overestimates available flexibility and violates P90 compliance, whereas the proposed MCD-CP framework achieves reliable coverage with controlled conservatism. When embedded in aggregator bidding model, conformalised methods substantially reduce overbidding risk and achieve upto 70% of perfect-information profit while satisfying regulatory reliability constraints, providing practical, computationally efficient, and market-compliant solution for aggregator flexibility forecasting under uncertainty.",
      "url": "http://arxiv.org/abs/2601.14663",
      "author": "Yogesh Pipada Sunil Kumar, S. Ali Pourmousavi, Jon A. R. Liisberg, Julian Lesmos-Vinasco",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.SY"
      ],
      "summary": "Combines Monte Carlo dropout with conformal prediction to produce calibrated prediction intervals for prosumer flexibility in energy markets. Addresses uncertainty quantification under P90 reliability requirements.",
      "importance_score": 45,
      "reasoning": "Solid applied ML work for energy systems but standard techniques combined for domain-specific application.",
      "themes": [
        "Uncertainty Quantification",
        "Energy Systems",
        "Forecasting"
      ],
      "continuation": null,
      "summary_html": "<p>Combines Monte Carlo dropout with conformal prediction to produce calibrated prediction intervals for prosumer flexibility in energy markets. Addresses uncertainty quantification under P90 reliability requirements.</p>",
      "content_html": "<p>arXiv:2601.14663v1 Announce Type: cross  Abstract: Reliable forecasting of prosumer flexibility is critical for demand response aggregators participating in frequency controlled ancillary services market, where strict reliability requirements such as the P90 standard are enforced. Limited historical data, dependence on exogeneous factors, and heterogenous prosumer behaviour introduce significant epistemic uncertainty, making deterministic or poorly calibrated probabilistic models unsuitable for market bidding. This paper proposes the use of scalable uncertainty quantification framework that integrates Monte Carlo dropout (MCD) with conformal prediction (CP) to produce calibrated, finite sample prediction intervals for aggregated prosumer flexibility. The proposed framework is applied to a behind-the-meter aggregator participating in the Danish manual frequency restoration reserve capacity market. A large-scale synthetic dataset is generated using a modified industry-grade home energy management system, combined with publicly available load, solar, price, activation and device-level data. The resulting machine learning surrogate model captures aggregate prosumer price responsiveness and provides uncertainty-aware estimates suitable for market bidding. Multiple multivariate CP strategies are evaluated and benchmarked against conventional MCD-based methods. Results show that standalone MCD systematically overestimates available flexibility and violates P90 compliance, whereas the proposed MCD-CP framework achieves reliable coverage with controlled conservatism. When embedded in aggregator bidding model, conformalised methods substantially reduce overbidding risk and achieve upto 70% of perfect-information profit while satisfying regulatory reliability constraints, providing practical, computationally efficient, and market-compliant solution for aggregator flexibility forecasting under uncertainty.</p>"
    },
    {
      "id": "de8573331892",
      "title": "Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement",
      "content": "arXiv:2601.14925v1 Announce Type: cross  Abstract: Single-channel speech enhancement algorithms are often used in resource-constrained embedded devices, where low latency and low complexity designs gain more importance. In recent years, researchers have proposed a wide variety of novel solutions to this problem. In particular, a recent deep learning model named ULCNet is among the state-of-the-art approaches in this domain. This paper proposes an adaptation of ULCNet, by replacing its GRU layers with FastGRNNs, to reduce both computational latency and complexity. Furthermore, this paper shows empirical evidence on the performance decay of FastGRNNs in long audio signals during inference due to internal state drifting, and proposes a novel approach based on a trainable complementary filter to mitigate it. The resulting model, Fast-ULCNet, performs on par with the state-of-the-art original ULCNet architecture on a speech enhancement task, while reducing its model size by more than half and decreasing its latency by 34% on average.",
      "url": "http://arxiv.org/abs/2601.14925",
      "author": "Nicol\\'as Arrieta Larraza, Niels de Koeijer",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.AS"
      ],
      "summary": "Fast-ULCNet replaces GRU layers with FastGRNNs for low-latency speech enhancement, addressing state drifting in long audio via trainable complementary filter.",
      "importance_score": 45,
      "reasoning": "Incremental efficiency improvement for speech enhancement with practical fix for inference stability.",
      "themes": [
        "Speech Enhancement",
        "Efficient Models"
      ],
      "continuation": null,
      "summary_html": "<p>Fast-ULCNet replaces GRU layers with FastGRNNs for low-latency speech enhancement, addressing state drifting in long audio via trainable complementary filter.</p>",
      "content_html": "<p>arXiv:2601.14925v1 Announce Type: cross  Abstract: Single-channel speech enhancement algorithms are often used in resource-constrained embedded devices, where low latency and low complexity designs gain more importance. In recent years, researchers have proposed a wide variety of novel solutions to this problem. In particular, a recent deep learning model named ULCNet is among the state-of-the-art approaches in this domain. This paper proposes an adaptation of ULCNet, by replacing its GRU layers with FastGRNNs, to reduce both computational latency and complexity. Furthermore, this paper shows empirical evidence on the performance decay of FastGRNNs in long audio signals during inference due to internal state drifting, and proposes a novel approach based on a trainable complementary filter to mitigate it. The resulting model, Fast-ULCNet, performs on par with the state-of-the-art original ULCNet architecture on a speech enhancement task, while reducing its model size by more than half and decreasing its latency by 34% on average.</p>"
    },
    {
      "id": "48592973ed84",
      "title": "ViT Registers and Fractal ViT",
      "content": "arXiv:2601.15506v1 Announce Type: cross  Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.",
      "url": "http://arxiv.org/abs/2601.15506",
      "author": "Jason Chuan-Chih Chou, Abhinav Kumar, Shivank Garg",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Tests fractal ViT variant using attention masks between regular and summary tokens (like registers) with various positional encodings. Results don't improve upon standard ViT with registers.",
      "importance_score": 45,
      "reasoning": "Negative result that highlights scale/domain specificity of recent ViT findings. Limited impact as null result.",
      "themes": [
        "Vision Transformers",
        "Model Architecture",
        "Empirical Study"
      ],
      "continuation": null,
      "summary_html": "<p>Tests fractal ViT variant using attention masks between regular and summary tokens (like registers) with various positional encodings. Results don't improve upon standard ViT with registers.</p>",
      "content_html": "<p>arXiv:2601.15506v1 Announce Type: cross  Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.</p>"
    },
    {
      "id": "6c14d4638051",
      "title": "Machine Failure Detection Based on Projected Quantum Models",
      "content": "arXiv:2601.15641v1 Announce Type: cross  Abstract: Detecting machine failures promptly is of utmost importance in industry for maintaining efficiency and minimizing downtime. This paper introduces a failure detection algorithm based on quantum computing and a statistical change-point detection approach. Our method leverages the potential of projected quantum feature maps to enhance the precision of anomaly detection in machine monitoring systems. We empirically validate our approach on benchmark multi-dimensional time series datasets as well as on a real-world dataset comprising IoT sensor readings from operational machines, ensuring the practical relevance of our study. The algorithm was executed on IBM's 133-qubit Heron quantum processor, demonstrating the feasibility of integrating quantum computing into industrial maintenance procedures. The presented results underscore the effectiveness of our quantum-based failure detection system, showcasing its capability to accurately identify anomalies in noisy time series data. This work not only highlights the potential of quantum computing in industrial diagnostics but also paves the way for more sophisticated quantum algorithms in the realm of predictive maintenance.",
      "url": "http://arxiv.org/abs/2601.15641",
      "author": "Larry Bowden, Qi Chu, Bernard Cena, Kentaro Ohno, Bob Parney, Deepak Sharma, Mitsuharu Takeori",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "quant-ph"
      ],
      "summary": "Develops quantum computing-based failure detection using projected quantum feature maps on IBM's 133-qubit Heron processor. Validates on IoT sensor data from operational machines.",
      "importance_score": 45,
      "reasoning": "Interesting quantum ML application with real hardware validation, but practical quantum advantage unclear.",
      "themes": [
        "Quantum Machine Learning",
        "Anomaly Detection",
        "Industrial AI"
      ],
      "continuation": null,
      "summary_html": "<p>Develops quantum computing-based failure detection using projected quantum feature maps on IBM's 133-qubit Heron processor. Validates on IoT sensor data from operational machines.</p>",
      "content_html": "<p>arXiv:2601.15641v1 Announce Type: cross  Abstract: Detecting machine failures promptly is of utmost importance in industry for maintaining efficiency and minimizing downtime. This paper introduces a failure detection algorithm based on quantum computing and a statistical change-point detection approach. Our method leverages the potential of projected quantum feature maps to enhance the precision of anomaly detection in machine monitoring systems. We empirically validate our approach on benchmark multi-dimensional time series datasets as well as on a real-world dataset comprising IoT sensor readings from operational machines, ensuring the practical relevance of our study. The algorithm was executed on IBM's 133-qubit Heron quantum processor, demonstrating the feasibility of integrating quantum computing into industrial maintenance procedures. The presented results underscore the effectiveness of our quantum-based failure detection system, showcasing its capability to accurately identify anomalies in noisy time series data. This work not only highlights the potential of quantum computing in industrial diagnostics but also paves the way for more sophisticated quantum algorithms in the realm of predictive maintenance.</p>"
    },
    {
      "id": "6a13c164265b",
      "title": "Determinants of Training Corpus Size for Clinical Text Classification",
      "content": "arXiv:2601.15846v1 Announce Type: cross  Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.   Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.   Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.",
      "url": "http://arxiv.org/abs/2601.15846",
      "author": "Jaya Chaturvedi, Saniya Deshpande, Chenkai Ma, Robert Cobb, Angus Roberts, Robert Stewart, Daniel Stahl, Diana Shamsutdinova",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies how training corpus size affects clinical text classification using BERT+Random Forest. Analyzes vocabulary properties including strong/noisy predictive words across dataset sizes 100-10,000.",
      "importance_score": 45,
      "reasoning": "Practical guidance for clinical NLP practitioners. Useful empirical study without novel methods.",
      "themes": [
        "Clinical NLP",
        "Dataset Size",
        "BERT"
      ],
      "continuation": null,
      "summary_html": "<p>Studies how training corpus size affects clinical text classification using BERT+Random Forest. Analyzes vocabulary properties including strong/noisy predictive words across dataset sizes 100-10,000.</p>",
      "content_html": "<p>arXiv:2601.15846v1 Announce Type: cross  Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.   Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.   Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.</p>"
    },
    {
      "id": "76e902a78cbc",
      "title": "From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs",
      "content": "arXiv:2601.15338v1 Announce Type: new  Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.",
      "url": "http://arxiv.org/abs/2601.15338",
      "author": "Angelina Parfenova, David Graus, Juergen Pfeffer",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Operationalizes axial coding (qualitative analysis) with ensemble LLMs plus moderator. Compares clustering-based vs direct LLM grouping for analyzing Dutch parliamentary debates.",
      "importance_score": 45,
      "reasoning": "Applied NLP for social science. Useful tool but limited methodological novelty.",
      "themes": [
        "Qualitative Analysis",
        "Political NLP",
        "LLM Applications"
      ],
      "continuation": null,
      "summary_html": "<p>Operationalizes axial coding (qualitative analysis) with ensemble LLMs plus moderator. Compares clustering-based vs direct LLM grouping for analyzing Dutch parliamentary debates.</p>",
      "content_html": "<p>arXiv:2601.15338v1 Announce Type: new  Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.</p>"
    },
    {
      "id": "3d2952e1eabf",
      "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier",
      "content": "arXiv:2601.16113v1 Announce Type: new  Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.   We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.   We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.",
      "url": "http://arxiv.org/abs/2601.16113",
      "author": "Haq Nawaz Malik, Kh Mohmad Shafi, Tanveer Ahmad Reshi",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "SynthOCR-Gen generates synthetic OCR datasets for low-resource languages like Kashmiri. Addresses lack of support in major OCR systems for Perso-Arabic scripts.",
      "importance_score": 45,
      "reasoning": "Useful tool for low-resource language processing. Good open-source contribution.",
      "themes": [
        "OCR",
        "Low-Resource Languages",
        "Data Generation"
      ],
      "continuation": null,
      "summary_html": "<p>SynthOCR-Gen generates synthetic OCR datasets for low-resource languages like Kashmiri. Addresses lack of support in major OCR systems for Perso-Arabic scripts.</p>",
      "content_html": "<p>arXiv:2601.16113v1 Announce Type: new  Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.   We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.   We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.</p>"
    },
    {
      "id": "e71b034d7727",
      "title": "LLM Prompt Evaluation for Educational Applications",
      "content": "arXiv:2601.16134v1 Announce Type: cross  Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
      "url": "http://arxiv.org/abs/2601.16134",
      "author": "Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, Wesley Morris",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Presents systematic approach for evaluating LLM prompts in educational applications through tournament-style framework. Tests six prompt templates with different pedagogical strategies for generating follow-up questions in structured dialogue.",
      "importance_score": 45,
      "reasoning": "Useful methodology for education-focused LLM applications but limited novelty. Specific to educational context with narrow generalizability.",
      "themes": [
        "Education",
        "Prompt Engineering",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Presents systematic approach for evaluating LLM prompts in educational applications through tournament-style framework. Tests six prompt templates with different pedagogical strategies for generating follow-up questions in structured dialogue.</p>",
      "content_html": "<p>arXiv:2601.16134v1 Announce Type: cross  Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.</p>"
    },
    {
      "id": "9dbdf9609f95",
      "title": "SplatBus: A Gaussian Splatting Viewer Framework via GPU Interprocess Communication",
      "content": "arXiv:2601.15431v1 Announce Type: cross  Abstract: Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities. They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time. 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality. However, current 3DGS implementations are difficult to integrate into traditional mesh-based rendering pipelines, which is a common use case for interactive applications and artistic exploration. To address this limitation, this software solution uses Nvidia's interprocess communication (IPC) APIs to easily integrate into implementations and allow the results to be viewed in external clients such as Unity, Blender, Unreal Engine, and OpenGL viewers. The code is available at https://github.com/RockyXu66/splatbus.",
      "url": "http://arxiv.org/abs/2601.15431",
      "author": "Yinghan Xu, Th\\'eo Morales, John Dingliana",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.GR"
      ],
      "summary": "SplatBus provides a framework for integrating 3D Gaussian Splatting into traditional mesh-based rendering pipelines using Nvidia's interprocess communication APIs, enabling practical integration with existing graphics workflows.",
      "importance_score": 45,
      "reasoning": "Engineering contribution enabling practical use of 3DGS, useful for applications but not research advancement.",
      "themes": [
        "3D Graphics",
        "Gaussian Splatting",
        "Software Engineering"
      ],
      "continuation": null,
      "summary_html": "<p>SplatBus provides a framework for integrating 3D Gaussian Splatting into traditional mesh-based rendering pipelines using Nvidia's interprocess communication APIs, enabling practical integration with existing graphics workflows.</p>",
      "content_html": "<p>arXiv:2601.15431v1 Announce Type: cross  Abstract: Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities. They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time. 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality. However, current 3DGS implementations are difficult to integrate into traditional mesh-based rendering pipelines, which is a common use case for interactive applications and artistic exploration. To address this limitation, this software solution uses Nvidia's interprocess communication (IPC) APIs to easily integrate into implementations and allow the results to be viewed in external clients such as Unity, Blender, Unreal Engine, and OpenGL viewers. The code is available at https://github.com/RockyXu66/splatbus.</p>"
    },
    {
      "id": "be99a5bf899e",
      "title": "Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods",
      "content": "arXiv:2601.15309v1 Announce Type: new  Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.",
      "url": "http://arxiv.org/abs/2601.15309",
      "author": "Jiaxin Xu, Chao Zhang, Raymond H. Cuijpers, Wijnand A. IJsselsteijn",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Systematic review of behavior change strategies and evaluation methods used in social robots for health interventions, synthesizing 39 studies across four strategy categories.",
      "importance_score": 45,
      "reasoning": "Comprehensive review of HRI for health, useful reference but not novel research.",
      "themes": [
        "Human-Robot Interaction",
        "Health Applications",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>Systematic review of behavior change strategies and evaluation methods used in social robots for health interventions, synthesizing 39 studies across four strategy categories.</p>",
      "content_html": "<p>arXiv:2601.15309v1 Announce Type: new  Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.</p>"
    },
    {
      "id": "b4f81bfcada9",
      "title": "Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor",
      "content": "arXiv:2601.15607v1 Announce Type: new  Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.",
      "url": "http://arxiv.org/abs/2601.15607",
      "author": "Lenworth Thomas, Tjaden Bridges, Sarah Bergbreiter",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Presents flow-based source seeking on small quadrotors (<100g) using a custom sensor that detects both airflow magnitude and direction, implementing a modified Cast and Surge algorithm.",
      "importance_score": 45,
      "reasoning": "Interesting engineering for small drone applications, specialized contribution.",
      "themes": [
        "Drones",
        "Sensing",
        "Source Seeking"
      ],
      "continuation": null,
      "summary_html": "<p>Presents flow-based source seeking on small quadrotors (&lt;100g) using a custom sensor that detects both airflow magnitude and direction, implementing a modified Cast and Surge algorithm.</p>",
      "content_html": "<p>arXiv:2601.15607v1 Announce Type: new  Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors &lt; 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.</p>"
    },
    {
      "id": "ca7209e8c2c6",
      "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation",
      "content": "arXiv:2601.15802v1 Announce Type: new  Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.",
      "url": "http://arxiv.org/abs/2601.15802",
      "author": "Alexandre Albore, Humbert Fiorino, Damien Pellier",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Proposes a beacon-based navigation solution for autonomous underwater vehicles in GNSS-denied environments, using aerial drones to deploy synthetic landmark networks.",
      "importance_score": 45,
      "reasoning": "Practical solution for underwater navigation challenges, specialized domain.",
      "themes": [
        "Underwater Robotics",
        "Navigation",
        "GNSS-Denied"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes a beacon-based navigation solution for autonomous underwater vehicles in GNSS-denied environments, using aerial drones to deploy synthetic landmark networks.</p>",
      "content_html": "<p>arXiv:2601.15802v1 Announce Type: new  Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.</p>"
    },
    {
      "id": "524709815808",
      "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis",
      "content": "arXiv:2601.16062v1 Announce Type: new  Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.",
      "url": "http://arxiv.org/abs/2601.16062",
      "author": "Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Analyzes autonomy properties of SE2(3) Lie group-based navigation models under different reference frames, providing theoretical framework for high-precision navigation.",
      "importance_score": 45,
      "reasoning": "Theoretical contribution to navigation, specialized mathematical analysis.",
      "themes": [
        "Navigation",
        "State Estimation",
        "Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes autonomy properties of SE2(3) Lie group-based navigation models under different reference frames, providing theoretical framework for high-precision navigation.</p>",
      "content_html": "<p>arXiv:2601.16062v1 Announce Type: new  Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.</p>"
    },
    {
      "id": "d2f9eb82f9fa",
      "title": "On the Nonasymptotic Scaling Guarantee of Hyperparameter Estimation in Inhomogeneous, Weakly-Dependent Complex Network Dynamical Systems",
      "content": "arXiv:2601.15603v1 Announce Type: cross  Abstract: Hierarchical Bayesian models are increasingly used in large, inhomogeneous complex network dynamical systems by modeling parameters as draws from a hyperparameter-governed distribution. However, theoretical guarantees for these estimates as the system size grows have been lacking. A critical concern is that hyperparameter estimation may diverge for larger networks, undermining the model's reliability. Formulating the system's evolution in a measure transport perspective, we propose a theoretical framework for estimating hyperparameters with mean-type observations, which are prevalent in many scientific applications. Our primary contribution is a nonasymptotic bound for the deviation of estimate of hyperparameters in inhomogeneous complex network dynamical systems with respect to network population size, which is established for a general family of optimization algorithms within a fixed observation duration. While we firstly establish a consistency result for systems with independent nodes, our main result extends this guarantee to the more challenging and realistic setting of weakly-dependent nodes. We validate our theoretical findings with numerical experiments on two representative models: a Susceptible-Infected-Susceptible model and a Spiking Neuronal Network model. In both cases, the results confirm that the estimation error decreases as the network population size increases, aligning with our theoretical guarantees. This research proposes the foundational theory to ensure that hierarchical Bayesian methods are statistically consistent for large-scale inhomogeneous systems, filling a gap in this area of theoretical research and justifying their application in practice.",
      "url": "http://arxiv.org/abs/2601.15603",
      "author": "Yi Yu, Yubo Hou, Yinchong Wang, Nan Zhang, Jianfeng Feng, Wenlian Lu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning (Statistics))",
      "source_type": "arxiv",
      "tags": [
        "math.ST"
      ],
      "summary": "Develops theoretical framework for hyperparameter estimation in hierarchical Bayesian models of complex network dynamical systems, providing nonasymptotic bounds as system size grows.",
      "importance_score": 45,
      "reasoning": "Theoretical statistics contribution, limited direct AI relevance.",
      "themes": [
        "Bayesian Inference",
        "Networks",
        "Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Develops theoretical framework for hyperparameter estimation in hierarchical Bayesian models of complex network dynamical systems, providing nonasymptotic bounds as system size grows.</p>",
      "content_html": "<p>arXiv:2601.15603v1 Announce Type: cross  Abstract: Hierarchical Bayesian models are increasingly used in large, inhomogeneous complex network dynamical systems by modeling parameters as draws from a hyperparameter-governed distribution. However, theoretical guarantees for these estimates as the system size grows have been lacking. A critical concern is that hyperparameter estimation may diverge for larger networks, undermining the model's reliability. Formulating the system's evolution in a measure transport perspective, we propose a theoretical framework for estimating hyperparameters with mean-type observations, which are prevalent in many scientific applications. Our primary contribution is a nonasymptotic bound for the deviation of estimate of hyperparameters in inhomogeneous complex network dynamical systems with respect to network population size, which is established for a general family of optimization algorithms within a fixed observation duration. While we firstly establish a consistency result for systems with independent nodes, our main result extends this guarantee to the more challenging and realistic setting of weakly-dependent nodes. We validate our theoretical findings with numerical experiments on two representative models: a Susceptible-Infected-Susceptible model and a Spiking Neuronal Network model. In both cases, the results confirm that the estimation error decreases as the network population size increases, aligning with our theoretical guarantees. This research proposes the foundational theory to ensure that hierarchical Bayesian methods are statistically consistent for large-scale inhomogeneous systems, filling a gap in this area of theoretical research and justifying their application in practice.</p>"
    },
    {
      "id": "5c770bc2372d",
      "title": "Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation",
      "content": "arXiv:2601.14955v1 Announce Type: new  Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.",
      "url": "http://arxiv.org/abs/2601.14955",
      "author": "Hanqi Jin, Gaoming Yang, Zhangming Chan, Yapeng Yuan, Longbin Li, Fei Sun, Yeqiu Yang, Jian Wu, Yuning Jiang, Bo Zheng",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes transition-aware graph attention network for e-commerce recommendation that models multi-behavior sequential data more efficiently than transformer-based approaches for large-scale systems.",
      "importance_score": 44,
      "reasoning": "Practical improvement for recommendation systems but incremental contribution to the field.",
      "themes": [
        "Recommendation Systems",
        "Graph Neural Networks",
        "E-Commerce"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes transition-aware graph attention network for e-commerce recommendation that models multi-behavior sequential data more efficiently than transformer-based approaches for large-scale systems.</p>",
      "content_html": "<p>arXiv:2601.14955v1 Announce Type: new  Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.</p>"
    },
    {
      "id": "49a895e29ffc",
      "title": "Self-Supervised Score-Based Despeckling for SAR Imagery via Log-Domain Transformation",
      "content": "arXiv:2601.14334v1 Announce Type: cross  Abstract: The speckle noise inherent in Synthetic Aperture Radar (SAR) imagery significantly degrades image quality and complicates subsequent analysis. Given that SAR speckle is multiplicative and Gamma-distributed, effectively despeckling SAR imagery remains challenging. This paper introduces a novel self-supervised framework for SAR image despeckling based on score-based generative models operating in the transformed log domain. We first transform the data into the log-domain and then convert the speckle noise residuals into an approximately additive Gaussian distribution. This step enables the application of score-based models, which are trained in the transformed domain using a self-supervised objective. This objective allows our model to learn the clean underlying signal by training on further corrupted versions of the input data itself. Consequently, our method exhibits significantly shorter inference times compared to many existing self-supervised techniques, offering a robust and practical solution for SAR image restoration.",
      "url": "http://arxiv.org/abs/2601.14334",
      "author": "Junhyuk Heo",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "eess.IV"
      ],
      "summary": "Proposes self-supervised framework for SAR despeckling using score-based generative models in log domain, converting multiplicative speckle to approximately additive Gaussian for tractable modeling.",
      "importance_score": 44,
      "reasoning": "Solid technical contribution for remote sensing but specialized application domain.",
      "themes": [
        "Remote Sensing",
        "Generative Models",
        "Image Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes self-supervised framework for SAR despeckling using score-based generative models in log domain, converting multiplicative speckle to approximately additive Gaussian for tractable modeling.</p>",
      "content_html": "<p>arXiv:2601.14334v1 Announce Type: cross  Abstract: The speckle noise inherent in Synthetic Aperture Radar (SAR) imagery significantly degrades image quality and complicates subsequent analysis. Given that SAR speckle is multiplicative and Gamma-distributed, effectively despeckling SAR imagery remains challenging. This paper introduces a novel self-supervised framework for SAR image despeckling based on score-based generative models operating in the transformed log domain. We first transform the data into the log-domain and then convert the speckle noise residuals into an approximately additive Gaussian distribution. This step enables the application of score-based models, which are trained in the transformed domain using a self-supervised objective. This objective allows our model to learn the clean underlying signal by training on further corrupted versions of the input data itself. Consequently, our method exhibits significantly shorter inference times compared to many existing self-supervised techniques, offering a robust and practical solution for SAR image restoration.</p>"
    },
    {
      "id": "c73efdc1632a",
      "title": "Designing KRIYA: An AI Companion for Wellbeing Self-Reflection",
      "content": "arXiv:2601.14589v1 Announce Type: cross  Abstract: Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.",
      "url": "http://arxiv.org/abs/2601.14589",
      "author": "Shanshan Zhu, Wenxuan Song, Jiayue Melissa Shi, Dong Whi Yoo, Karthik S. Bhat, Koustuv Saha",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Describes KRIYA, an AI wellbeing companion designed for co-interpretive engagement with personal health data through features like Comfort Zone, Detective Mode, and What-If Planning.",
      "importance_score": 44,
      "reasoning": "Interesting HCI study on wellbeing AI but primarily design exploration rather than technical contribution.",
      "themes": [
        "Health AI",
        "Human-Computer Interaction",
        "Wellbeing"
      ],
      "continuation": null,
      "summary_html": "<p>Describes KRIYA, an AI wellbeing companion designed for co-interpretive engagement with personal health data through features like Comfort Zone, Detective Mode, and What-If Planning.</p>",
      "content_html": "<p>arXiv:2601.14589v1 Announce Type: cross  Abstract: Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.</p>"
    },
    {
      "id": "e51bfa9100b2",
      "title": "Proximal Policy Optimization with Evolutionary Mutations",
      "content": "arXiv:2601.14705v1 Announce Type: cross  Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and sample efficiency, but it often suffers from premature convergence due to limited exploration. In this paper, we propose POEM (Proximal Policy Optimization with Evolutionary Mutations), a novel modification to PPO that introduces an adaptive exploration mechanism inspired by evolutionary algorithms. POEM enhances policy diversity by monitoring the Kullback-Leibler (KL) divergence between the current policy and a moving average of previous policies. When policy changes become minimal, indicating stagnation, POEM triggers an adaptive mutation of policy parameters to promote exploration. We evaluate POEM on four OpenAI Gym environments: CarRacing, MountainCar, BipedalWalker, and LunarLander. Through extensive fine-tuning using Bayesian optimization techniques and statistical testing using Welch's t-test, we find that POEM significantly outperforms PPO on three of the four tasks (BipedalWalker: t=-2.0642, p=0.0495; CarRacing: t=-6.3987, p=0.0002; MountainCar: t=-6.2431, p<0.0001), while performance on LunarLander is not statistically significant (t=-1.8707, p=0.0778). Our results highlight the potential of integrating evolutionary principles into policy gradient methods to overcome exploration-exploitation tradeoffs.",
      "url": "http://arxiv.org/abs/2601.14705",
      "author": "Casimir Czworkowski, Stephen Hornish, Alhassan S. Yasin",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.NE"
      ],
      "summary": "POEM adds evolutionary mutation mechanisms to PPO that trigger when policy KL divergence indicates stagnation. Addresses premature convergence through adaptive exploration.",
      "importance_score": 44,
      "reasoning": "Incremental improvement to PPO exploration, tested on standard benchmarks with modest gains.",
      "themes": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "continuation": null,
      "summary_html": "<p>POEM adds evolutionary mutation mechanisms to PPO that trigger when policy KL divergence indicates stagnation. Addresses premature convergence through adaptive exploration.</p>",
      "content_html": "<p>arXiv:2601.14705v1 Announce Type: cross  Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and sample efficiency, but it often suffers from premature convergence due to limited exploration. In this paper, we propose POEM (Proximal Policy Optimization with Evolutionary Mutations), a novel modification to PPO that introduces an adaptive exploration mechanism inspired by evolutionary algorithms. POEM enhances policy diversity by monitoring the Kullback-Leibler (KL) divergence between the current policy and a moving average of previous policies. When policy changes become minimal, indicating stagnation, POEM triggers an adaptive mutation of policy parameters to promote exploration. We evaluate POEM on four OpenAI Gym environments: CarRacing, MountainCar, BipedalWalker, and LunarLander. Through extensive fine-tuning using Bayesian optimization techniques and statistical testing using Welch's t-test, we find that POEM significantly outperforms PPO on three of the four tasks (BipedalWalker: t=-2.0642, p=0.0495; CarRacing: t=-6.3987, p=0.0002; MountainCar: t=-6.2431, p&lt;0.0001), while performance on LunarLander is not statistically significant (t=-1.8707, p=0.0778). Our results highlight the potential of integrating evolutionary principles into policy gradient methods to overcome exploration-exploitation tradeoffs.</p>"
    },
    {
      "id": "a696448e7e84",
      "title": "A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala",
      "content": "arXiv:2601.14958v1 Announce Type: cross  Abstract: The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.",
      "url": "http://arxiv.org/abs/2601.14958",
      "author": "Minuri Rajapakse, Ruvan Weerasinghe",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Benchmarks LLMs on Unicode and Romanized Sinhala using perplexity and sentence completion, finding Mistral models perform best. Highlights challenges for morphologically rich, low-resource languages.",
      "importance_score": 44,
      "reasoning": "Useful contribution for low-resource language evaluation but standard benchmarking methodology.",
      "themes": [
        "Low-Resource Languages",
        "LLM Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Benchmarks LLMs on Unicode and Romanized Sinhala using perplexity and sentence completion, finding Mistral models perform best. Highlights challenges for morphologically rich, low-resource languages.</p>",
      "content_html": "<p>arXiv:2601.14958v1 Announce Type: cross  Abstract: The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.</p>"
    },
    {
      "id": "253f36037191",
      "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting",
      "content": "arXiv:2601.15754v1 Announce Type: cross  Abstract: High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.",
      "url": "http://arxiv.org/abs/2601.15754",
      "author": "Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "CAFE-GB proposes chunk-wise aggregated gradient boosting for scalable malware feature selection. Produces stable, globally consistent feature rankings for high-dimensional malware datasets.",
      "importance_score": 44,
      "reasoning": "Domain-specific ML method for security. Solid but narrow application focus.",
      "themes": [
        "Malware Detection",
        "Feature Selection",
        "Security ML"
      ],
      "continuation": null,
      "summary_html": "<p>CAFE-GB proposes chunk-wise aggregated gradient boosting for scalable malware feature selection. Produces stable, globally consistent feature rankings for high-dimensional malware datasets.</p>",
      "content_html": "<p>arXiv:2601.15754v1 Announce Type: cross  Abstract: High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.</p>"
    },
    {
      "id": "b089fd7f34ac",
      "title": "Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games",
      "content": "arXiv:2601.16142v1 Announce Type: cross  Abstract: The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.",
      "url": "http://arxiv.org/abs/2601.16142",
      "author": "Paolo Baldan, Sebastian Gurke, Barbara K\\\"onig, Florian Wittbold",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LO"
      ],
      "summary": "Generalizes dampened Mann iteration for computing fixpoints of learned functions. Enables chaotic iterations and applies to simple stochastic games.",
      "importance_score": 44,
      "reasoning": "Theoretical contribution connecting learning and fixpoint computation. Specialized audience.",
      "themes": [
        "Fixpoint Computation",
        "Learning Theory",
        "Games"
      ],
      "continuation": null,
      "summary_html": "<p>Generalizes dampened Mann iteration for computing fixpoints of learned functions. Enables chaotic iterations and applies to simple stochastic games.</p>",
      "content_html": "<p>arXiv:2601.16142v1 Announce Type: cross  Abstract: The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.</p>"
    },
    {
      "id": "7fa974920593",
      "title": "Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data",
      "content": "arXiv:2601.15705v1 Announce Type: new  Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $\\alpha$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.",
      "url": "http://arxiv.org/abs/2601.15705",
      "author": "Ali Caglayan, Nevrez Imamoglu, Toru Kouyama",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents lightweight refinements for national-scale land-use/land-cover segmentation on ALOS-2 SAR data, addressing boundary smoothing, thin structure detection, and rare-class handling.",
      "importance_score": 44,
      "reasoning": "Practical improvements for remote sensing application but incremental refinements to existing methods.",
      "themes": [
        "Remote Sensing",
        "Semantic Segmentation",
        "SAR Imaging"
      ],
      "continuation": null,
      "summary_html": "<p>Presents lightweight refinements for national-scale land-use/land-cover segmentation on ALOS-2 SAR data, addressing boundary smoothing, thin structure detection, and rare-class handling.</p>",
      "content_html": "<p>arXiv:2601.15705v1 Announce Type: new  Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $\\alpha$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.</p>"
    },
    {
      "id": "028bc8f76896",
      "title": "An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection",
      "content": "arXiv:2601.14305v1 Announce Type: cross  Abstract: The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.",
      "url": "http://arxiv.org/abs/2601.14305",
      "author": "Ashikuzzaman, Md. Shawkat Hossain, Jubayer Abdullah Joy, Md Zahid Akon, Md Manjur Ahmed, Md. Naimul Islam",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes explainable decision tree-based framework for IoT anomaly detection combining SHAP values and Morris sensitivity analysis for both local and global explanations.",
      "importance_score": 43,
      "reasoning": "Practical XAI application for IoT security but straightforward combination of existing techniques.",
      "themes": [
        "Explainable AI",
        "IoT Security",
        "Anomaly Detection"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes explainable decision tree-based framework for IoT anomaly detection combining SHAP values and Morris sensitivity analysis for both local and global explanations.</p>",
      "content_html": "<p>arXiv:2601.14305v1 Announce Type: cross  Abstract: The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.</p>"
    },
    {
      "id": "7ac66f320c6f",
      "title": "From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps",
      "content": "arXiv:2601.14848v1 Announce Type: cross  Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.",
      "url": "http://arxiv.org/abs/2601.14848",
      "author": "Mohamed Abouras, Catherine M. Elias",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Applies multi-layer LSTM to predict vehicle lane changes on highway on/off-ramps using drone-captured data, achieving 76% accuracy for ramp scenarios up to 4 seconds ahead.",
      "importance_score": 43,
      "reasoning": "Standard LSTM application for driving prediction, useful but limited novelty.",
      "themes": [
        "Autonomous Driving",
        "Trajectory Prediction"
      ],
      "continuation": null,
      "summary_html": "<p>Applies multi-layer LSTM to predict vehicle lane changes on highway on/off-ramps using drone-captured data, achieving 76% accuracy for ramp scenarios up to 4 seconds ahead.</p>",
      "content_html": "<p>arXiv:2601.14848v1 Announce Type: cross  Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.</p>"
    },
    {
      "id": "54f81bd16986",
      "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application",
      "content": "arXiv:2601.16078v1 Announce Type: new  Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.",
      "url": "http://arxiv.org/abs/2601.16078",
      "author": "Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Companion paper to theoretical analysis, presenting real-world SINS/odometer experiments demonstrating improved SE2(3) group-based high-precision navigation models.",
      "importance_score": 43,
      "reasoning": "Experimental validation of theoretical work, specialized domain.",
      "themes": [
        "Navigation",
        "State Estimation",
        "Experiments"
      ],
      "continuation": null,
      "summary_html": "<p>Companion paper to theoretical analysis, presenting real-world SINS/odometer experiments demonstrating improved SE2(3) group-based high-precision navigation models.</p>",
      "content_html": "<p>arXiv:2601.16078v1 Announce Type: new  Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.</p>"
    },
    {
      "id": "b52a123f7d85",
      "title": "DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs",
      "content": "arXiv:2601.14711v1 Announce Type: new  Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.",
      "url": "http://arxiv.org/abs/2601.14711",
      "author": "Mingxuan Song, Yusen Huo, Bohan Zhou, Shenglin Yin, Zhen Xiao, Jieyi Long, Zhilin Zhang, Chuan Yu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces DARA using GRPO-Adaptive, an LLM post-training strategy for few-shot budget allocation in online advertising. Leverages in-context learning with enhanced numerical precision.",
      "importance_score": 42,
      "reasoning": "Domain-specific application to advertising. Limited research novelty beyond combining existing techniques.",
      "themes": [
        "LLM Applications",
        "Advertising",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces DARA using GRPO-Adaptive, an LLM post-training strategy for few-shot budget allocation in online advertising. Leverages in-context learning with enhanced numerical precision.</p>",
      "content_html": "<p>arXiv:2601.14711v1 Announce Type: new  Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.</p>"
    },
    {
      "id": "4d9c4dd18eba",
      "title": "A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction",
      "content": "arXiv:2601.14259v1 Announce Type: cross  Abstract: Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.",
      "url": "http://arxiv.org/abs/2601.14259",
      "author": "Ziwen Zhong, Zhitao Shu, Yue Zhao",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes cloud-based cross-modal transformer integrating visual, auditory, and textual signals using pretrained encoders (ViT, Wav2Vec2, BERT) with cross-modal attention for emotion recognition.",
      "importance_score": 42,
      "reasoning": "Standard multimodal fusion approach. Limited novelty in architecture or methodology.",
      "themes": [
        "Emotion Recognition",
        "Multimodal Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes cloud-based cross-modal transformer integrating visual, auditory, and textual signals using pretrained encoders (ViT, Wav2Vec2, BERT) with cross-modal attention for emotion recognition.</p>",
      "content_html": "<p>arXiv:2601.14259v1 Announce Type: cross  Abstract: Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.</p>"
    },
    {
      "id": "64d94155f514",
      "title": "HCVR Scene Generation: High Compatibility Virtual Reality Environment Generation for Extended Redirected Walking",
      "content": "arXiv:2601.14679v1 Announce Type: cross  Abstract: Natural walking enhances immersion in virtual environments (VEs), but physical space limitations and obstacles hinder exploration, especially in large virtual scenes. Redirected Walking (RDW) techniques mitigate this by subtly manipulating the virtual camera to guide users away from physical collisions within pre-defined VEs. However, RDW efficacy diminishes significantly when substantial geometric divergence exists between the physical and virtual environments, leading to unavoidable collisions. Existing scene generation methods primarily focus on object relationships or layout aesthetics, often neglecting the crucial aspect of physical compatibility required for effective RDW. To address this, we introduce HCVR (High Compatibility Virtual Reality Environment Generation), a novel framework that generates virtual scenes inherently optimized for alignment-based RDW controllers. HCVR first employs ENI++, a novel, boundary-sensitive metric to evaluate the incompatibility between physical and virtual spaces by comparing rotation-sensitive visibility polygons. Guided by the ENI++ compatibility map and user prompts, HCVR utilizes a Large Language Model (LLM) for context-aware 3D asset retrieval and initial layout generation. The framework then strategically adjusts object selection, scaling, and placement to maximize coverage of virtually incompatible regions, effectively guiding users towards RDW-feasible paths. User studies evaluating physical collisions and layout quality demonstrate HCVR's effectiveness with HCVR-generated scenes, resulting in 22.78 times fewer physical collisions and received 35.89\\% less on ENI++ score compared to LLM-based generation with RDW, while also receiving 12.5\\% higher scores on user feedback to layout design.",
      "url": "http://arxiv.org/abs/2601.14679",
      "author": "Yiran Zhang, Xingpeng Sun, Aniket Bera",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.MM"
      ],
      "summary": "HCVR generates VR environments optimized for compatibility with Redirected Walking techniques, ensuring physical space constraints are respected while enabling large virtual scene exploration.",
      "importance_score": 42,
      "reasoning": "Niche VR research solving specific locomotion problem, limited broader impact on AI/ML.",
      "themes": [
        "Virtual Reality",
        "Scene Generation"
      ],
      "continuation": null,
      "summary_html": "<p>HCVR generates VR environments optimized for compatibility with Redirected Walking techniques, ensuring physical space constraints are respected while enabling large virtual scene exploration.</p>",
      "content_html": "<p>arXiv:2601.14679v1 Announce Type: cross  Abstract: Natural walking enhances immersion in virtual environments (VEs), but physical space limitations and obstacles hinder exploration, especially in large virtual scenes. Redirected Walking (RDW) techniques mitigate this by subtly manipulating the virtual camera to guide users away from physical collisions within pre-defined VEs. However, RDW efficacy diminishes significantly when substantial geometric divergence exists between the physical and virtual environments, leading to unavoidable collisions. Existing scene generation methods primarily focus on object relationships or layout aesthetics, often neglecting the crucial aspect of physical compatibility required for effective RDW. To address this, we introduce HCVR (High Compatibility Virtual Reality Environment Generation), a novel framework that generates virtual scenes inherently optimized for alignment-based RDW controllers. HCVR first employs ENI++, a novel, boundary-sensitive metric to evaluate the incompatibility between physical and virtual spaces by comparing rotation-sensitive visibility polygons. Guided by the ENI++ compatibility map and user prompts, HCVR utilizes a Large Language Model (LLM) for context-aware 3D asset retrieval and initial layout generation. The framework then strategically adjusts object selection, scaling, and placement to maximize coverage of virtually incompatible regions, effectively guiding users towards RDW-feasible paths. User studies evaluating physical collisions and layout quality demonstrate HCVR's effectiveness with HCVR-generated scenes, resulting in 22.78 times fewer physical collisions and received 35.89\\% less on ENI++ score compared to LLM-based generation with RDW, while also receiving 12.5\\% higher scores on user feedback to layout design.</p>"
    },
    {
      "id": "203cab400251",
      "title": "Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks",
      "content": "arXiv:2601.15177v1 Announce Type: cross  Abstract: Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.",
      "url": "http://arxiv.org/abs/2601.15177",
      "author": "Lorenzo Fern\\'andez Maim\\'o, Alberto Huertas Celdr\\'an, Manuel Gil P\\'erez, F\\'elix J. Garc\\'ia Clemente, Gregorio Mart\\'inez P\\'erez",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "MEC-oriented anomaly detection system for 5G networks using deep learning with dynamic management capabilities for real-time autonomous operation.",
      "importance_score": 42,
      "reasoning": "Applied security work for 5G networks, standard deep learning approach.",
      "themes": [
        "Network Security",
        "5G",
        "Anomaly Detection"
      ],
      "continuation": null,
      "summary_html": "<p>MEC-oriented anomaly detection system for 5G networks using deep learning with dynamic management capabilities for real-time autonomous operation.</p>",
      "content_html": "<p>arXiv:2601.15177v1 Announce Type: cross  Abstract: Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.</p>"
    },
    {
      "id": "2099e6942e6f",
      "title": "Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning",
      "content": "arXiv:2601.15503v1 Announce Type: new  Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.",
      "url": "http://arxiv.org/abs/2601.15503",
      "author": "Rishit Chatterjee, Tahiya Chowdhury",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Studies Secchi Disk Depth forecasting on lakes with irregular volunteer-collected data. Uses MICE imputation and evaluates ridge regression as best performer with minimal sample size analysis.",
      "importance_score": 42,
      "reasoning": "Narrow application domain with standard ML techniques. Limited broader ML impact.",
      "themes": [
        "Environmental ML",
        "Time Series",
        "Applied ML"
      ],
      "continuation": null,
      "summary_html": "<p>Studies Secchi Disk Depth forecasting on lakes with irregular volunteer-collected data. Uses MICE imputation and evaluates ridge regression as best performer with minimal sample size analysis.</p>",
      "content_html": "<p>arXiv:2601.15503v1 Announce Type: new  Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.</p>"
    },
    {
      "id": "3f4fb0371d34",
      "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems",
      "content": "arXiv:2601.15561v1 Announce Type: cross  Abstract: This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.",
      "url": "http://arxiv.org/abs/2601.15561",
      "author": "Naoya Onizawa, Takahiro Hanyu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.ET"
      ],
      "summary": "Investigates limitations of probabilistic bit-based simulated annealing (pSA) for combinatorial optimization, identifying oscillation issues. Proposes TApSA and PDpSA algorithms to address energy stagnation.",
      "importance_score": 42,
      "reasoning": "Specialized optimization work with novel algorithms for probabilistic computing but limited broad AI impact.",
      "themes": [
        "Optimization",
        "Probabilistic Computing"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates limitations of probabilistic bit-based simulated annealing (pSA) for combinatorial optimization, identifying oscillation issues. Proposes TApSA and PDpSA algorithms to address energy stagnation.</p>",
      "content_html": "<p>arXiv:2601.15561v1 Announce Type: cross  Abstract: This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.</p>"
    },
    {
      "id": "e558764ed23b",
      "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies",
      "content": "arXiv:2601.15865v1 Announce Type: cross  Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.",
      "url": "http://arxiv.org/abs/2601.15865",
      "author": "Jingsong Xia, Siqi Wang",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes lightweight brain-inspired neural framework for coronary angiography classification. Uses selective neural plasticity training and attention mechanisms for class imbalance.",
      "importance_score": 42,
      "reasoning": "Applied medical ML with brain-inspired elements but unclear novelty over existing approaches.",
      "themes": [
        "Medical AI",
        "Neural Networks",
        "Coronary Imaging"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes lightweight brain-inspired neural framework for coronary angiography classification. Uses selective neural plasticity training and attention mechanisms for class imbalance.</p>",
      "content_html": "<p>arXiv:2601.15865v1 Announce Type: cross  Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.</p>"
    },
    {
      "id": "298eb9fc4990",
      "title": "Elsewise: Authoring AI-Based Interactive Narrative with Possibility Space Visualization",
      "content": "arXiv:2601.15295v1 Announce Type: cross  Abstract: Interactive narrative (IN) authors craft spaces of divergent narrative possibilities for players to explore, with the player's input determining which narrative possibilities they actually experience. Generative AI can enable new forms of IN by improvisationally expanding on pre-authored content in response to open-ended player input. However, this extrapolation risks widening the gap between author-envisioned and player-experienced stories, potentially limiting the strength of plot progression and the communication of the author's narrative intent. To bridge the gap, we introduce Elsewise: an authoring tool for AI-based INs that implements a novel Bundled Storyline concept to enhance author's perception and understanding of the narrative possibility space, allowing authors to explore similarities and differences between possible playthroughs of their IN in terms of open-ended, user-configurable narrative dimensions. A user study (n=12) shows that our approach improves author anticipation of player-experienced narrative, leading to more effective control and exploration of the narrative possibility spaces.",
      "url": "http://arxiv.org/abs/2601.15295",
      "author": "Yi Wang, John Joon Young Chung, Melissa Roemmele, Yuqian Sun, Tiffany Wang, Shm Garanganao Almeda, Brett A. Halperin, Yuwen Lu, Max Kreminski",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "Elsewise is an authoring tool for AI-based interactive narratives implementing Bundled Storyline concept for possibility space visualization. Bridges author intent and player experience.",
      "importance_score": 42,
      "reasoning": "Creative AI tool with interesting visualization concept. Niche application for interactive fiction.",
      "themes": [
        "Interactive Narrative",
        "Creative AI",
        "Human-AI Collaboration"
      ],
      "continuation": null,
      "summary_html": "<p>Elsewise is an authoring tool for AI-based interactive narratives implementing Bundled Storyline concept for possibility space visualization. Bridges author intent and player experience.</p>",
      "content_html": "<p>arXiv:2601.15295v1 Announce Type: cross  Abstract: Interactive narrative (IN) authors craft spaces of divergent narrative possibilities for players to explore, with the player's input determining which narrative possibilities they actually experience. Generative AI can enable new forms of IN by improvisationally expanding on pre-authored content in response to open-ended player input. However, this extrapolation risks widening the gap between author-envisioned and player-experienced stories, potentially limiting the strength of plot progression and the communication of the author's narrative intent. To bridge the gap, we introduce Elsewise: an authoring tool for AI-based INs that implements a novel Bundled Storyline concept to enhance author's perception and understanding of the narrative possibility space, allowing authors to explore similarities and differences between possible playthroughs of their IN in terms of open-ended, user-configurable narrative dimensions. A user study (n=12) shows that our approach improves author anticipation of player-experienced narrative, leading to more effective control and exploration of the narrative possibility spaces.</p>"
    },
    {
      "id": "a691115110a4",
      "title": "The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers",
      "content": "arXiv:2601.15509v1 Announce Type: cross  Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.",
      "url": "http://arxiv.org/abs/2601.15509",
      "author": "Prasanna Kumar",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Observes that transformer-based sentiment analytics improvements in one sentiment class come at the cost of polarization of another class and failure of neutrality, posing problems for business applications.",
      "importance_score": 42,
      "reasoning": "Raises valid concern about sentiment analysis bias but limited novelty and depth. Primarily observational without strong solutions or rigorous analysis.",
      "themes": [
        "Sentiment Analysis",
        "Model Bias",
        "NLP Applications"
      ],
      "continuation": null,
      "summary_html": "<p>Observes that transformer-based sentiment analytics improvements in one sentiment class come at the cost of polarization of another class and failure of neutrality, posing problems for business applications.</p>",
      "content_html": "<p>arXiv:2601.15509v1 Announce Type: cross  Abstract: The use of Transfer Learning &amp; Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.</p>"
    },
    {
      "id": "149811f6e487",
      "title": "High-Fidelity 3D Tooth Reconstruction by Fusing Intraoral Scans and CBCT Data via a Deep Implicit Representation",
      "content": "arXiv:2601.15358v1 Announce Type: cross  Abstract: High-fidelity 3D tooth models are essential for digital dentistry, but must capture both the detailed crown and the complete root. Clinical imaging modalities are limited: Cone-Beam Computed Tomography (CBCT) captures the root but has a noisy, low-resolution crown, while Intraoral Scanners (IOS) provide a high-fidelity crown but no root information. A naive fusion of these sources results in unnatural seams and artifacts. We propose a novel, fully-automated pipeline that fuses CBCT and IOS data using a deep implicit representation. Our method first segments and robustly registers the tooth instances, then creates a hybrid proxy mesh combining the IOS crown and the CBCT root. The core of our approach is to use this noisy proxy to guide a class-specific DeepSDF network. This optimization process projects the input onto a learned manifold of ideal tooth shapes, generating a seamless, watertight, and anatomically coherent model. Qualitative and quantitative evaluations show our method uniquely preserves both the high-fidelity crown from IOS and the patient-specific root morphology from CBCT, overcoming the limitations of each modality and naive stitching.",
      "url": "http://arxiv.org/abs/2601.15358",
      "author": "Yi Zhu, Razmig Kechichian, Rapha\\\"el Richert, Satoshi Ikehata, S\\'ebastien Valette",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "eess.IV"
      ],
      "summary": "Proposes a pipeline for fusing CBCT and intraoral scan data for 3D tooth reconstruction using deep implicit representations, combining high-fidelity crown data with root structure from CT scans.",
      "importance_score": 42,
      "reasoning": "Specialized medical imaging application, technically sound but narrow domain impact.",
      "themes": [
        "Medical Imaging",
        "3D Reconstruction",
        "Computer Vision"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes a pipeline for fusing CBCT and intraoral scan data for 3D tooth reconstruction using deep implicit representations, combining high-fidelity crown data with root structure from CT scans.</p>",
      "content_html": "<p>arXiv:2601.15358v1 Announce Type: cross  Abstract: High-fidelity 3D tooth models are essential for digital dentistry, but must capture both the detailed crown and the complete root. Clinical imaging modalities are limited: Cone-Beam Computed Tomography (CBCT) captures the root but has a noisy, low-resolution crown, while Intraoral Scanners (IOS) provide a high-fidelity crown but no root information. A naive fusion of these sources results in unnatural seams and artifacts. We propose a novel, fully-automated pipeline that fuses CBCT and IOS data using a deep implicit representation. Our method first segments and robustly registers the tooth instances, then creates a hybrid proxy mesh combining the IOS crown and the CBCT root. The core of our approach is to use this noisy proxy to guide a class-specific DeepSDF network. This optimization process projects the input onto a learned manifold of ideal tooth shapes, generating a seamless, watertight, and anatomically coherent model. Qualitative and quantitative evaluations show our method uniquely preserves both the high-fidelity crown from IOS and the patient-specific root morphology from CBCT, overcoming the limitations of each modality and naive stitching.</p>"
    },
    {
      "id": "064764d06960",
      "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot",
      "content": "arXiv:2601.15707v1 Announce Type: new  Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.",
      "url": "http://arxiv.org/abs/2601.15707",
      "author": "Qifan Hu, Branko Celler, Weidong Mu, Steven W. Su",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Proposes D-optimality-guided reinforcement learning for calibration of a 3-DOF ankle rehabilitation robot, framing calibration as a combinatorial design-of-experiments problem.",
      "importance_score": 42,
      "reasoning": "Specialized rehabilitation robotics contribution, novel optimization approach.",
      "themes": [
        "Rehabilitation Robotics",
        "Calibration",
        "Reinforcement Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes D-optimality-guided reinforcement learning for calibration of a 3-DOF ankle rehabilitation robot, framing calibration as a combinatorial design-of-experiments problem.</p>",
      "content_html": "<p>arXiv:2601.15707v1 Announce Type: new  Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.</p>"
    },
    {
      "id": "134300504d61",
      "title": "Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum",
      "content": "arXiv:2601.14472v1 Announce Type: cross  Abstract: Neural vocoders are central to speech synthesis; despite their success, most still suffer from limited prosody modeling and inaccurate phase reconstruction. We propose a vocoder that introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis via inverse STFT. Unlike mel-spectrogram-based approaches, our design jointly models magnitude and phase, ensuring phase coherence and improved pitch fidelity. To further align with perceptual quality, we adopt a multi-objective training strategy that integrates adversarial, spectral, and phase-aware losses. Experiments on benchmark datasets demonstrate consistent gains over HiFi-GAN and AutoVocoder: F0 RMSE reduced by 22 percent, voiced/unvoiced error lowered by 18 percent, and MOS scores improved by 0.15. These results show that prosody-guided attention combined with direct complex spectrum modeling yields more natural, pitch-accurate, and robust synthetic speech, setting a strong foundation for expressive neural vocoding.",
      "url": "http://arxiv.org/abs/2601.14472",
      "author": "Mohammed Salah Al-Radhi, Riad Larbi, M\\'aty\\'as Bartalis, G\\'eza N\\'emeth",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Proposes neural vocoder with prosody-guided harmonic attention that directly predicts complex spectral components for phase-coherent waveform synthesis.",
      "importance_score": 41,
      "reasoning": "Solid speech synthesis contribution but incremental improvement over existing vocoders.",
      "themes": [
        "Speech Synthesis",
        "Audio Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes neural vocoder with prosody-guided harmonic attention that directly predicts complex spectral components for phase-coherent waveform synthesis.</p>",
      "content_html": "<p>arXiv:2601.14472v1 Announce Type: cross  Abstract: Neural vocoders are central to speech synthesis; despite their success, most still suffer from limited prosody modeling and inaccurate phase reconstruction. We propose a vocoder that introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis via inverse STFT. Unlike mel-spectrogram-based approaches, our design jointly models magnitude and phase, ensuring phase coherence and improved pitch fidelity. To further align with perceptual quality, we adopt a multi-objective training strategy that integrates adversarial, spectral, and phase-aware losses. Experiments on benchmark datasets demonstrate consistent gains over HiFi-GAN and AutoVocoder: F0 RMSE reduced by 22 percent, voiced/unvoiced error lowered by 18 percent, and MOS scores improved by 0.15. These results show that prosody-guided attention combined with direct complex spectrum modeling yields more natural, pitch-accurate, and robust synthetic speech, setting a strong foundation for expressive neural vocoding.</p>"
    },
    {
      "id": "11d8483cc791",
      "title": "Semantic-Guided Unsupervised Video Summarization",
      "content": "arXiv:2601.14773v1 Announce Type: new  Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.",
      "url": "http://arxiv.org/abs/2601.14773",
      "author": "Haizhou Liu, Haodong Jin, Yiming Wang, Hui Yu",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes semantic-guided unsupervised video summarization using frame-level semantic alignment attention, addressing limitations of GAN-based approaches that overlook semantic information.",
      "importance_score": 40,
      "reasoning": "Incremental improvement in video summarization. Limited novelty in the semantic alignment approach.",
      "themes": [
        "Video Understanding",
        "Unsupervised Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes semantic-guided unsupervised video summarization using frame-level semantic alignment attention, addressing limitations of GAN-based approaches that overlook semantic information.</p>",
      "content_html": "<p>arXiv:2601.14773v1 Announce Type: new  Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.</p>"
    },
    {
      "id": "3a92f3bffd69",
      "title": "Call2Instruct: Automated Pipeline for Generating Q&A Datasets from Call Center Recordings for LLM Fine-Tuning",
      "content": "arXiv:2601.14263v1 Announce Type: cross  Abstract: The adaptation of Large-Scale Language Models (LLMs) to specific domains depends on high-quality fine-tuning datasets, particularly in instructional format (e.g., Question-Answer - Q&amp;A). However, generating these datasets, particularly from unstructured sources such as call center audio recordings, poses a significant challenge due to the noisy and disorganized nature of the data. This paper presents a solution to this challenge by offering an end-to-end automated pipeline for generating Q&amp;A instructional datasets from such recordings. The methodology developed comprises sequential steps of audio processing (including diarization, noise removal and automatic transcription), textual processing (cleaning, normalization, and anonymization), semantic extraction of customer demands and attendant responses using vector embeddings, and matching via semantic search to form the final Q&amp;A pairs. As a result, the complete pipeline was successfully implemented, generating a dataset specifically formatted for Instruct Fine Tuning. The practical value and feasibility of the generated dataset were substantiated and functionally demonstrated through the successful fine-tuning of an LLM model (based on Llama 2 7B). The conclusion of the paper states that the proposed approach is viable for converting unstructured conversational data from call centers into valuable resources for training LLMs. This development has the potential to open up avenues for creating more effective AI systems for Q&amp;A tasks in the customer service domain. The developed codes have been made publicly available to promote reproducibility and future research.",
      "url": "http://arxiv.org/abs/2601.14263",
      "author": "Alex Echeverria, S\\'avio Salvarino Teles de Oliveira, Fernando Marques Federson",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Presents automated pipeline for generating Q&A instruction datasets from call center audio recordings including diarization, noise removal, transcription, and anonymization steps.",
      "importance_score": 40,
      "reasoning": "Practical data engineering contribution but limited research novelty.",
      "themes": [
        "Data Generation",
        "LLM Fine-tuning",
        "Speech Processing"
      ],
      "continuation": null,
      "summary_html": "<p>Presents automated pipeline for generating Q&amp;A instruction datasets from call center audio recordings including diarization, noise removal, transcription, and anonymization steps.</p>",
      "content_html": "<p>arXiv:2601.14263v1 Announce Type: cross  Abstract: The adaptation of Large-Scale Language Models (LLMs) to specific domains depends on high-quality fine-tuning datasets, particularly in instructional format (e.g., Question-Answer - Q&amp;A). However, generating these datasets, particularly from unstructured sources such as call center audio recordings, poses a significant challenge due to the noisy and disorganized nature of the data. This paper presents a solution to this challenge by offering an end-to-end automated pipeline for generating Q&amp;A instructional datasets from such recordings. The methodology developed comprises sequential steps of audio processing (including diarization, noise removal and automatic transcription), textual processing (cleaning, normalization, and anonymization), semantic extraction of customer demands and attendant responses using vector embeddings, and matching via semantic search to form the final Q&amp;A pairs. As a result, the complete pipeline was successfully implemented, generating a dataset specifically formatted for Instruct Fine Tuning. The practical value and feasibility of the generated dataset were substantiated and functionally demonstrated through the successful fine-tuning of an LLM model (based on Llama 2 7B). The conclusion of the paper states that the proposed approach is viable for converting unstructured conversational data from call centers into valuable resources for training LLMs. This development has the potential to open up avenues for creating more effective AI systems for Q&amp;A tasks in the customer service domain. The developed codes have been made publicly available to promote reproducibility and future research.</p>"
    },
    {
      "id": "69a1b4bafc76",
      "title": "MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments",
      "content": "arXiv:2601.15578v1 Announce Type: cross  Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.",
      "url": "http://arxiv.org/abs/2601.15578",
      "author": "Cyril Shih-Huan Hsu, Xi Li, Lanfranco Zanzi, Zhiheng Yang, Chrysa Papagianni, Xavier Costa P\\'erez",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.NI"
      ],
      "summary": "Introduces MapViT, a Vision Transformer framework for predicting radio signal quality maps in dynamic environments for robotic navigation. Uses pre-train and fine-tune paradigm inspired by LLMs.",
      "importance_score": 40,
      "reasoning": "Applied ViT work for wireless/robotics domain. Interesting cross-domain transfer but niche application.",
      "themes": [
        "Vision Transformers",
        "Robotics",
        "Wireless Networks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces MapViT, a Vision Transformer framework for predicting radio signal quality maps in dynamic environments for robotic navigation. Uses pre-train and fine-tune paradigm inspired by LLMs.</p>",
      "content_html": "<p>arXiv:2601.15578v1 Announce Type: cross  Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.</p>"
    },
    {
      "id": "697086fb74e9",
      "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals",
      "content": "arXiv:2601.16091v1 Announce Type: cross  Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.",
      "url": "http://arxiv.org/abs/2601.16091",
      "author": "Saar Cohen",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.MA"
      ],
      "summary": "Introduces online non-centroid clustering with delayed assignments where points in metric spaces can postpone cluster assignment at a cost.",
      "importance_score": 40,
      "reasoning": "Theoretical algorithm work with limited practical ML relevance.",
      "themes": [
        "Online Algorithms",
        "Clustering",
        "Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces online non-centroid clustering with delayed assignments where points in metric spaces can postpone cluster assignment at a cost.</p>",
      "content_html": "<p>arXiv:2601.16091v1 Announce Type: cross  Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.</p>"
    },
    {
      "id": "f6974ef9e9b0",
      "title": "Computational Representations of Character Significance in Novels",
      "content": "arXiv:2601.15508v1 Announce Type: new  Abstract: Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic \"the one vs the many\" theory of character centrality and the gendered dynamics of character discussion.",
      "url": "http://arxiv.org/abs/2601.15508",
      "author": "Haaris Mian, Melanie Subbiah, Sharon Marcus, Nora Shaalan, Kathleen McKeown",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Applies six-component literary theory of character significance to 19th-century British novels. Compares LLMs with task-specific transformers for operationalizing character theory.",
      "importance_score": 40,
      "reasoning": "Digital humanities application. Interesting interdisciplinary work but limited ML novelty.",
      "themes": [
        "Literary Analysis",
        "NLP Applications",
        "Character Modeling"
      ],
      "continuation": null,
      "summary_html": "<p>Applies six-component literary theory of character significance to 19th-century British novels. Compares LLMs with task-specific transformers for operationalizing character theory.</p>",
      "content_html": "<p>arXiv:2601.15508v1 Announce Type: new  Abstract: Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic \"the one vs the many\" theory of character centrality and the gendered dynamics of character discussion.</p>"
    },
    {
      "id": "b6fa0cee9b09",
      "title": "Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray",
      "content": "arXiv:2601.15349v1 Announce Type: new  Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.",
      "url": "http://arxiv.org/abs/2601.15349",
      "author": "Jiaqing Chang, Song Gao, Chaowei Dong, zhaobang Li, Yang Liu",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Designs a magnetically-driven micro soft robot inspired by cownose ray swimming, made from NdFeB and PDMS, for use in narrow underwater environments and medical procedures.",
      "importance_score": 40,
      "reasoning": "Interesting bio-inspired design but specialized application.",
      "themes": [
        "Soft Robotics",
        "Bio-Inspired Design",
        "Micro Robotics"
      ],
      "continuation": null,
      "summary_html": "<p>Designs a magnetically-driven micro soft robot inspired by cownose ray swimming, made from NdFeB and PDMS, for use in narrow underwater environments and medical procedures.</p>",
      "content_html": "<p>arXiv:2601.15349v1 Announce Type: new  Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.</p>"
    },
    {
      "id": "a8a4bba41189",
      "title": "Breaking the accuracy-resource dilemma: a lightweight adaptive video inference enhancement",
      "content": "arXiv:2601.14568v1 Announce Type: cross  Abstract: Existing video inference (VI) enhancement methods typically aim to improve performance by scaling up model sizes and employing sophisticated network architectures. While these approaches demonstrated state-of-the-art performance, they often overlooked the trade-off of resource efficiency and inference effectiveness, leading to inefficient resource utilization and suboptimal inference performance. To address this problem, a fuzzy controller (FC-r) is developed based on key system parameters and inference-related metrics. Guided by the FC-r, a VI enhancement framework is proposed, where the spatiotemporal correlation of targets across adjacent video frames is leveraged. Given the real-time resource conditions of the target device, the framework can dynamically switch between models of varying scales during VI. Experimental results demonstrate that the proposed method effectively achieves a balance between resource utilization and inference performance.",
      "url": "http://arxiv.org/abs/2601.14568",
      "author": "Wei Ma, Shaowu Chen, Junjie Ye, Peichang Zhang, Lei Huang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes fuzzy controller-based framework for adaptive video inference enhancement that balances accuracy and resource usage based on real-time system conditions.",
      "importance_score": 39,
      "reasoning": "Standard adaptive systems approach applied to video inference. Limited novelty.",
      "themes": [
        "Video Processing",
        "Efficiency",
        "Adaptive Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes fuzzy controller-based framework for adaptive video inference enhancement that balances accuracy and resource usage based on real-time system conditions.</p>",
      "content_html": "<p>arXiv:2601.14568v1 Announce Type: cross  Abstract: Existing video inference (VI) enhancement methods typically aim to improve performance by scaling up model sizes and employing sophisticated network architectures. While these approaches demonstrated state-of-the-art performance, they often overlooked the trade-off of resource efficiency and inference effectiveness, leading to inefficient resource utilization and suboptimal inference performance. To address this problem, a fuzzy controller (FC-r) is developed based on key system parameters and inference-related metrics. Guided by the FC-r, a VI enhancement framework is proposed, where the spatiotemporal correlation of targets across adjacent video frames is leveraged. Given the real-time resource conditions of the target device, the framework can dynamically switch between models of varying scales during VI. Experimental results demonstrate that the proposed method effectively achieves a balance between resource utilization and inference performance.</p>"
    },
    {
      "id": "8915a172ca86",
      "title": "Emergent, not Immanent: A Baradian Reading of Explainable AI",
      "content": "arXiv:2601.15029v1 Announce Type: new  Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.",
      "url": "http://arxiv.org/abs/2601.15029",
      "author": "Fabio Morreale, Joan Serr\\`a, Yuki Mistufuji",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Develops philosophical reading of XAI through Barad's agential realism, arguing interpretations are material-discursive performances emerging from situated entanglements rather than immanent properties of models.",
      "importance_score": 38,
      "reasoning": "Interesting theoretical lens but limited practical implications. Primarily for philosophy of AI community.",
      "themes": [
        "Explainable AI",
        "Philosophy of AI"
      ],
      "continuation": null,
      "summary_html": "<p>Develops philosophical reading of XAI through Barad's agential realism, arguing interpretations are material-discursive performances emerging from situated entanglements rather than immanent properties of models.</p>",
      "content_html": "<p>arXiv:2601.15029v1 Announce Type: new  Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.</p>"
    },
    {
      "id": "230d215e1308",
      "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding",
      "content": "arXiv:2601.15131v1 Announce Type: new  Abstract: In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.",
      "url": "http://arxiv.org/abs/2601.15131",
      "author": "Ayan Maity, Sudeshna Sarkar",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes novel network embedding module for vehicle routing with finite time horizon using deep RL, incorporating remaining time into context-aware representations.",
      "importance_score": 38,
      "reasoning": "Standard DRL application to routing problem. Limited novelty.",
      "themes": [
        "Reinforcement Learning",
        "Optimization",
        "Vehicle Routing"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes novel network embedding module for vehicle routing with finite time horizon using deep RL, incorporating remaining time into context-aware representations.</p>",
      "content_html": "<p>arXiv:2601.15131v1 Announce Type: new  Abstract: In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.</p>"
    },
    {
      "id": "000f3e2fd474",
      "title": "GEGO: A Hybrid Golden Eagle and Genetic Optimization Algorithm for Efficient Hyperparameter Tuning in Resource-Constrained Environments",
      "content": "arXiv:2601.14672v1 Announce Type: cross  Abstract: Hyperparameter tuning is a critical yet computationally expensive step in training neural networks, particularly when the search space is high dimensional and nonconvex. Metaheuristic optimization algorithms are often used for this purpose due to their derivative free nature and robustness against local optima. In this work, we propose Golden Eagle Genetic Optimization (GEGO), a hybrid metaheuristic that integrates the population movement strategy of Golden Eagle Optimization with the genetic operators of selection, crossover, and mutation.   The main novelty of GEGO lies in embedding genetic operators directly into the iterative search process of GEO, rather than applying them as a separate evolutionary stage. This design improves population diversity during search and reduces premature convergence while preserving the exploration behavior of GEO.   GEGO is evaluated on standard unimodal, multimodal, and composite benchmark functions from the CEC2017 suite, where it consistently outperforms its constituent algorithms and several classical metaheuristics in terms of solution quality and robustness. The algorithm is further applied to hyperparameter tuning of artificial neural networks on the MNIST dataset, where GEGO achieves improved classification accuracy and more stable convergence compared to GEO and GA. These results indicate that GEGO provides a balanced exploration-exploitation tradeoff and is well suited for hyperparameter optimization under constrained computational settings.",
      "url": "http://arxiv.org/abs/2601.14672",
      "author": "Amaras Nazarians, Sachin Kumar",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.NE"
      ],
      "summary": "GEGO hybridizes Golden Eagle Optimization with genetic operators for hyperparameter tuning in resource-constrained environments. Embeds genetic operations directly into iterative search rather than as separate evolutionary stages.",
      "importance_score": 38,
      "reasoning": "Incremental contribution to metaheuristic optimization, limited novelty and unclear advantages over existing methods.",
      "themes": [
        "Hyperparameter Optimization",
        "Metaheuristics"
      ],
      "continuation": null,
      "summary_html": "<p>GEGO hybridizes Golden Eagle Optimization with genetic operators for hyperparameter tuning in resource-constrained environments. Embeds genetic operations directly into iterative search rather than as separate evolutionary stages.</p>",
      "content_html": "<p>arXiv:2601.14672v1 Announce Type: cross  Abstract: Hyperparameter tuning is a critical yet computationally expensive step in training neural networks, particularly when the search space is high dimensional and nonconvex. Metaheuristic optimization algorithms are often used for this purpose due to their derivative free nature and robustness against local optima. In this work, we propose Golden Eagle Genetic Optimization (GEGO), a hybrid metaheuristic that integrates the population movement strategy of Golden Eagle Optimization with the genetic operators of selection, crossover, and mutation.   The main novelty of GEGO lies in embedding genetic operators directly into the iterative search process of GEO, rather than applying them as a separate evolutionary stage. This design improves population diversity during search and reduces premature convergence while preserving the exploration behavior of GEO.   GEGO is evaluated on standard unimodal, multimodal, and composite benchmark functions from the CEC2017 suite, where it consistently outperforms its constituent algorithms and several classical metaheuristics in terms of solution quality and robustness. The algorithm is further applied to hyperparameter tuning of artificial neural networks on the MNIST dataset, where GEGO achieves improved classification accuracy and more stable convergence compared to GEO and GA. These results indicate that GEGO provides a balanced exploration-exploitation tradeoff and is well suited for hyperparameter optimization under constrained computational settings.</p>"
    },
    {
      "id": "2f7c94b0a42c",
      "title": "A Machine Vision Approach to Preliminary Skin Lesion Assessments",
      "content": "arXiv:2601.15539v1 Announce Type: cross  Abstract: Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.",
      "url": "http://arxiv.org/abs/2601.15539",
      "author": "Ali Khreis, Ro'Yah Radaideh, Quinn McGill",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "eess.IV"
      ],
      "summary": "Evaluates automated skin lesion assessment combining the clinical ABCD rule with ML classifiers on HAM10000 dataset. Compares traditional rule-based dermoscopy scoring against deep learning for malignancy detection.",
      "importance_score": 38,
      "reasoning": "Standard applied ML work in medical imaging without novel methodological contributions. Uses existing dataset and established techniques.",
      "themes": [
        "Medical AI",
        "Computer Vision",
        "Applied Machine Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Evaluates automated skin lesion assessment combining the clinical ABCD rule with ML classifiers on HAM10000 dataset. Compares traditional rule-based dermoscopy scoring against deep learning for malignancy detection.</p>",
      "content_html": "<p>arXiv:2601.15539v1 Announce Type: cross  Abstract: Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.</p>"
    },
    {
      "id": "2ba3b83e5e95",
      "title": "Automatic Classification of Arabic Literature into Historical Eras",
      "content": "arXiv:2601.16138v1 Announce Type: cross  Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.",
      "url": "http://arxiv.org/abs/2601.16138",
      "author": "Zainab Alhathloul, Irfan Ahmad",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Uses neural networks and deep learning to automatically classify Arabic texts into historical eras. Evaluates on two datasets from public corpora beyond just poetry.",
      "importance_score": 38,
      "reasoning": "Domain-specific NLP application. Limited methodological novelty.",
      "themes": [
        "Arabic NLP",
        "Text Classification",
        "Digital Humanities"
      ],
      "continuation": null,
      "summary_html": "<p>Uses neural networks and deep learning to automatically classify Arabic texts into historical eras. Evaluates on two datasets from public corpora beyond just poetry.</p>",
      "content_html": "<p>arXiv:2601.16138v1 Announce Type: cross  Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.</p>"
    },
    {
      "id": "9ccc24de953e",
      "title": "AI-Based Culvert-Sewer Inspection",
      "content": "arXiv:2601.15366v1 Announce Type: new  Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.   First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.",
      "url": "http://arxiv.org/abs/2601.15366",
      "author": "Christina Thrainer",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Thesis exploring methods to improve automated defect segmentation in culverts and sewer pipes using limited annotated data. Proposes three methods to enhance segmentation and handle data scarcity.",
      "importance_score": 38,
      "reasoning": "Specialized infrastructure application with limited broader impact. Thesis work addressing practical but narrow domain.",
      "themes": [
        "Computer Vision",
        "Defect Detection",
        "Infrastructure"
      ],
      "continuation": null,
      "summary_html": "<p>Thesis exploring methods to improve automated defect segmentation in culverts and sewer pipes using limited annotated data. Proposes three methods to enhance segmentation and handle data scarcity.</p>",
      "content_html": "<p>arXiv:2601.15366v1 Announce Type: new  Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.   First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.</p>"
    },
    {
      "id": "0e71c3aa657c",
      "title": "Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV",
      "content": "arXiv:2601.15775v1 Announce Type: new  Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.",
      "url": "http://arxiv.org/abs/2601.15775",
      "author": "Amir Habel, Ivan Snegirev, Elizaveta Semenyakina, Miguel Altamirano Cabrera, Jeffrin Sam, Fawad Mehboob, Roohan Ahmed Khan, Muhammad Ahsan Mustafa, Dzmitry Tsetserukou",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Robotics)",
      "source_type": "arxiv",
      "tags": [
        "cs.RO"
      ],
      "summary": "Glove2UAV is a wearable IMU-glove interface for intuitive UAV control through hand gestures with vibrotactile feedback for speed warnings.",
      "importance_score": 38,
      "reasoning": "Engineering contribution for drone interfaces, limited novelty.",
      "themes": [
        "Human-Robot Interaction",
        "Drones",
        "Wearables"
      ],
      "continuation": null,
      "summary_html": "<p>Glove2UAV is a wearable IMU-glove interface for intuitive UAV control through hand gestures with vibrotactile feedback for speed warnings.</p>",
      "content_html": "<p>arXiv:2601.15775v1 Announce Type: new  Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.</p>"
    },
    {
      "id": "b66c12a72178",
      "title": "Assessing the informative value of macroeconomic indicators for public health forecasting",
      "content": "arXiv:2601.15514v1 Announce Type: cross  Abstract: Macroeconomic conditions influence the environments in which health systems operate, yet their value as leading signals of health system capacity has not been systematically evaluated. In this study, we examine whether selected macroeconomic indicators contain predictive information for several capacity-related public health targets, including employment in the health and social assistance workforce, new business applications in the sector, and health care construction spending. Using monthly U.S. time series data, we evaluate multiple forecasting approaches, including neural network models with different optimization strategies, generalized additive models, random forests, and time series models with exogenous macroeconomic indicators, under alternative model fitting designs. Across evaluation settings, we find that macroeconomic indicators provide a consistent and reproducible predictive signal for some public health targets, particularly workforce and infrastructure measures, while other targets exhibit weaker or less stable predictability. Models emphasizing stability and implicit regularization tend to perform more reliably during periods of economic volatility. These findings suggest that macroeconomic indicators may serve as useful upstream signals for digital public health monitoring, while underscoring the need for careful model selection and validation when translating economic trends into health system forecasting tools.",
      "url": "http://arxiv.org/abs/2601.15514",
      "author": "Shome Chakraborty, Fardil Khan, Soutik Ghosal",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning (Statistics))",
      "source_type": "arxiv",
      "tags": [
        "stat.AP"
      ],
      "summary": "Examines whether macroeconomic indicators provide predictive value for public health capacity forecasting using multiple ML approaches including neural networks and GAMs.",
      "importance_score": 38,
      "reasoning": "Applied forecasting study, limited AI research novelty.",
      "themes": [
        "Forecasting",
        "Public Health",
        "Time Series"
      ],
      "continuation": null,
      "summary_html": "<p>Examines whether macroeconomic indicators provide predictive value for public health capacity forecasting using multiple ML approaches including neural networks and GAMs.</p>",
      "content_html": "<p>arXiv:2601.15514v1 Announce Type: cross  Abstract: Macroeconomic conditions influence the environments in which health systems operate, yet their value as leading signals of health system capacity has not been systematically evaluated. In this study, we examine whether selected macroeconomic indicators contain predictive information for several capacity-related public health targets, including employment in the health and social assistance workforce, new business applications in the sector, and health care construction spending. Using monthly U.S. time series data, we evaluate multiple forecasting approaches, including neural network models with different optimization strategies, generalized additive models, random forests, and time series models with exogenous macroeconomic indicators, under alternative model fitting designs. Across evaluation settings, we find that macroeconomic indicators provide a consistent and reproducible predictive signal for some public health targets, particularly workforce and infrastructure measures, while other targets exhibit weaker or less stable predictability. Models emphasizing stability and implicit regularization tend to perform more reliably during periods of economic volatility. These findings suggest that macroeconomic indicators may serve as useful upstream signals for digital public health monitoring, while underscoring the need for careful model selection and validation when translating economic trends into health system forecasting tools.</p>"
    },
    {
      "id": "1ef3d8d0b64c",
      "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali",
      "content": "arXiv:2601.14931v1 Announce Type: cross  Abstract: This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.",
      "url": "http://arxiv.org/abs/2601.14931",
      "author": "Nouhoum Coulibaly, Ousmane Ly, Michael Leventhal, Ousmane Goro",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Explores generative AI for creating peace narrative music in Mali, examining cultural authenticity in AI-assisted musical co-creation rooted in national languages and traditions.",
      "importance_score": 36,
      "reasoning": "Interesting cultural application but limited technical novelty, more social science than AI research.",
      "themes": [
        "AI for Social Good",
        "Music Generation",
        "Cultural AI"
      ],
      "continuation": null,
      "summary_html": "<p>Explores generative AI for creating peace narrative music in Mali, examining cultural authenticity in AI-assisted musical co-creation rooted in national languages and traditions.</p>",
      "content_html": "<p>arXiv:2601.14931v1 Announce Type: cross  Abstract: This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources.</p>"
    },
    {
      "id": "f931806ea01a",
      "title": "The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative",
      "content": "arXiv:2601.14271v1 Announce Type: new  Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.",
      "url": "http://arxiv.org/abs/2601.14271",
      "author": "Denise M. Case",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Establishes a formal impossibility result showing that ontological neutrality in data systems is incompatible with causal or normative commitments at the foundational layer. Argues neutral substrates must be pre-causal and pre-normative to support accountability across disagreements.",
      "importance_score": 35,
      "reasoning": "Philosophical/theoretical work on ontology design with limited direct AI applications. Niche topic for specialized communities.",
      "themes": [
        "Knowledge Representation",
        "Data Systems",
        "Philosophy of AI"
      ],
      "continuation": null,
      "summary_html": "<p>Establishes a formal impossibility result showing that ontological neutrality in data systems is incompatible with causal or normative commitments at the foundational layer. Argues neutral substrates must be pre-causal and pre-normative to support accountability across disagreements.</p>",
      "content_html": "<p>arXiv:2601.14271v1 Announce Type: new  Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.</p>"
    },
    {
      "id": "b36d5cd3f378",
      "title": "Towards Bound Consistency for the No-Overlap Constraint Using MDDs",
      "content": "arXiv:2601.14784v1 Announce Type: new  Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Cir\\'e and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \\mid r_j, d_j, \\bar{d}_j \\mid \\sum E_j + \\sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Cir\\'e and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.",
      "url": "http://arxiv.org/abs/2601.14784",
      "author": "Amaury Guichard, Laurent Michel, H\\'el\\`ene Verhaeghe, Pierre Schaus",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Derives first bound-consistent algorithm for the no-overlap constraint by building on MDD representations, enabling tightening of start and end times in polynomial time.",
      "importance_score": 35,
      "reasoning": "Solid theoretical contribution in constraint programming but very specialized audience.",
      "themes": [
        "Constraint Programming",
        "Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Derives first bound-consistent algorithm for the no-overlap constraint by building on MDD representations, enabling tightening of start and end times in polynomial time.</p>",
      "content_html": "<p>arXiv:2601.14784v1 Announce Type: new  Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Cir\\'e and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \\mid r_j, d_j, \\bar{d}_j \\mid \\sum E_j + \\sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Cir\\'e and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.</p>"
    },
    {
      "id": "a77ddd9367c1",
      "title": "From Textbook to Talkbot: A Case Study of a Greek-Language RAG-Based Chatbot in Higher Education",
      "content": "arXiv:2601.14265v1 Announce Type: cross  Abstract: The integration of AI chatbots into educational settings has opened new pathways for transforming teaching and learning, offering enhanced support to both educators and learners. This study investigates the design and application of an AI chatbot as an educational tool in higher education. Designed to operate in the Greek language, the chatbot addresses linguistic challenges unique to Greek while delivering accurate, context grounded support aligned with the curriculum. The AI chatbot is built on the Retrieval Augmented Generation (RAG) framework by grounding its responses in specific course content. RAG architecture significantly enhances the chatbots reliability by providing accurate, context-aware responses while mitigating common challenges associated with large language models (LLMs), such as hallucinations and misinformation. The AI chatbot serves a dual purpose: it enables students to access accurate, ondemand academic support and assists educators in the rapid creation of relevant educational materials. This dual functionality promotes learner autonomy and streamlines the instructional design process. The study aims to evaluate the effectiveness, reliability, and perceived usability of RAG based chatbots in higher education, exploring their potential to enhance educational practices and outcomes as well as supporting the broader adoption of AI technologies in language specific educational contexts. Findings from this research are expected to contribute to the emerging field of AI driven education by demonstrating how intelligent systems can be effectively aligned with pedagogical goals.",
      "url": "http://arxiv.org/abs/2601.14265",
      "author": "Maria Eleni Koutsiaki, Marina Delianidi, Chaido Mizeli, Konstantinos Diamantaras, Iraklis Grigoropoulos, Nikolaos Koutlianos",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CY"
      ],
      "summary": "Case study on Greek-language RAG-based educational chatbot for higher education, addressing linguistic challenges specific to Greek while grounding responses in course content.",
      "importance_score": 35,
      "reasoning": "Application case study with limited generalizability. Primarily useful for similar deployments.",
      "themes": [
        "Educational AI",
        "RAG Systems",
        "Multilingual NLP"
      ],
      "continuation": null,
      "summary_html": "<p>Case study on Greek-language RAG-based educational chatbot for higher education, addressing linguistic challenges specific to Greek while grounding responses in course content.</p>",
      "content_html": "<p>arXiv:2601.14265v1 Announce Type: cross  Abstract: The integration of AI chatbots into educational settings has opened new pathways for transforming teaching and learning, offering enhanced support to both educators and learners. This study investigates the design and application of an AI chatbot as an educational tool in higher education. Designed to operate in the Greek language, the chatbot addresses linguistic challenges unique to Greek while delivering accurate, context grounded support aligned with the curriculum. The AI chatbot is built on the Retrieval Augmented Generation (RAG) framework by grounding its responses in specific course content. RAG architecture significantly enhances the chatbots reliability by providing accurate, context-aware responses while mitigating common challenges associated with large language models (LLMs), such as hallucinations and misinformation. The AI chatbot serves a dual purpose: it enables students to access accurate, ondemand academic support and assists educators in the rapid creation of relevant educational materials. This dual functionality promotes learner autonomy and streamlines the instructional design process. The study aims to evaluate the effectiveness, reliability, and perceived usability of RAG based chatbots in higher education, exploring their potential to enhance educational practices and outcomes as well as supporting the broader adoption of AI technologies in language specific educational contexts. Findings from this research are expected to contribute to the emerging field of AI driven education by demonstrating how intelligent systems can be effectively aligned with pedagogical goals.</p>"
    },
    {
      "id": "05a69b767aa7",
      "title": "A comprehensive overview of deep learning models for object detection from videos/images",
      "content": "arXiv:2601.14677v1 Announce Type: cross  Abstract: Object detection in video and image surveillance is a well-established yet rapidly evolving task, strongly influenced by recent deep learning advancements. This review summarises modern techniques by examining architectural innovations, generative model integration, and the use of temporal information to enhance robustness and accuracy. Unlike earlier surveys, it classifies methods based on core architectures, data processing strategies, and surveillance specific challenges such as dynamic environments, occlusions, lighting variations, and real-time requirements. The primary goal is to evaluate the current effectiveness of semantic object detection, while secondary aims include analysing deep learning models and their practical applications. The review covers CNN-based detectors, GAN-assisted approaches, and temporal fusion methods, highlighting how generative models support tasks such as reconstructing missing frames, reducing occlusions, and normalising illumination. It also outlines preprocessing pipelines, feature extraction progress, benchmarking datasets, and comparative evaluations. Finally, emerging trends in low-latency, efficient, and spatiotemporal learning approaches are identified for future research.",
      "url": "http://arxiv.org/abs/2601.14677",
      "author": "Sukana Zulfqar, Sadia Saeed, M. Azam Zia, Anjum Ali, Faisal Mehmood, Abid Ali",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Comprehensive review of deep learning methods for object detection in video/image surveillance, classifying methods by architecture, data processing, and domain-specific challenges like occlusions and lighting.",
      "importance_score": 35,
      "reasoning": "Survey paper without novel contributions, covering well-established territory in computer vision.",
      "themes": [
        "Computer Vision",
        "Object Detection",
        "Survey"
      ],
      "continuation": null,
      "summary_html": "<p>Comprehensive review of deep learning methods for object detection in video/image surveillance, classifying methods by architecture, data processing, and domain-specific challenges like occlusions and lighting.</p>",
      "content_html": "<p>arXiv:2601.14677v1 Announce Type: cross  Abstract: Object detection in video and image surveillance is a well-established yet rapidly evolving task, strongly influenced by recent deep learning advancements. This review summarises modern techniques by examining architectural innovations, generative model integration, and the use of temporal information to enhance robustness and accuracy. Unlike earlier surveys, it classifies methods based on core architectures, data processing strategies, and surveillance specific challenges such as dynamic environments, occlusions, lighting variations, and real-time requirements. The primary goal is to evaluate the current effectiveness of semantic object detection, while secondary aims include analysing deep learning models and their practical applications. The review covers CNN-based detectors, GAN-assisted approaches, and temporal fusion methods, highlighting how generative models support tasks such as reconstructing missing frames, reducing occlusions, and normalising illumination. It also outlines preprocessing pipelines, feature extraction progress, benchmarking datasets, and comparative evaluations. Finally, emerging trends in low-latency, efficient, and spatiotemporal learning approaches are identified for future research.</p>"
    },
    {
      "id": "35bbeafd69ca",
      "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data",
      "content": "arXiv:2601.15813v1 Announce Type: cross  Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.",
      "url": "http://arxiv.org/abs/2601.15813",
      "author": "Clare Chemery, Hendrik Edelhoff, Ludwig Bothmann",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents a lightweight ML pipeline lowering barriers for ecologists to train image classifiers. Combines command-line preprocessing/training with graphical annotation and analysis interface.",
      "importance_score": 35,
      "reasoning": "Tool paper for ML democratization. Useful but no novel ML contribution.",
      "themes": [
        "Applied ML",
        "Ecology",
        "ML Tools"
      ],
      "continuation": null,
      "summary_html": "<p>Presents a lightweight ML pipeline lowering barriers for ecologists to train image classifiers. Combines command-line preprocessing/training with graphical annotation and analysis interface.</p>",
      "content_html": "<p>arXiv:2601.15813v1 Announce Type: cross  Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.</p>"
    },
    {
      "id": "d8a1a6f5a749",
      "title": "A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows",
      "content": "arXiv:2601.16194v1 Announce Type: cross  Abstract: This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&amp;P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&amp;P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.",
      "url": "http://arxiv.org/abs/2601.16194",
      "author": "El Mehdi Er Raqabi, Kevin Dalmeijer, Pascal Van Hentenryck",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "math.OC"
      ],
      "summary": "Develops branch-and-price algorithm for multi-compartment vehicle routing with multiple time windows. Handles compartment flexibility and item compatibility.",
      "importance_score": 35,
      "reasoning": "Operations research paper. Not ML focused.",
      "themes": [
        "Operations Research",
        "Vehicle Routing",
        "Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Develops branch-and-price algorithm for multi-compartment vehicle routing with multiple time windows. Handles compartment flexibility and item compatibility.</p>",
      "content_html": "<p>arXiv:2601.16194v1 Announce Type: cross  Abstract: This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&amp;P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&amp;P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.</p>"
    },
    {
      "id": "0e1be4e755a8",
      "title": "Abusive music and song transformation using GenAI and LLMs",
      "content": "arXiv:2601.15348v1 Announce Type: cross  Abstract: Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.",
      "url": "http://arxiv.org/abs/2601.15348",
      "author": "Jiyang Choi, Rohitash Chandra",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.SD"
      ],
      "summary": "Explores using GenAI and LLMs to transform abusive content in music by modifying tone, intensity, and sentiment rather than simply muting or replacing words. Presents comparative analysis of four songs and their transformed counterparts.",
      "importance_score": 35,
      "reasoning": "Niche application with limited technical novelty. Content moderation angle is interesting but approach seems preliminary and narrow in scope.",
      "themes": [
        "Content Moderation",
        "Audio Generation",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Explores using GenAI and LLMs to transform abusive content in music by modifying tone, intensity, and sentiment rather than simply muting or replacing words. Presents comparative analysis of four songs and their transformed counterparts.</p>",
      "content_html": "<p>arXiv:2601.15348v1 Announce Type: cross  Abstract: Repeated exposure to violence and abusive content in music and song content can influence listeners' emotions and behaviours, potentially normalising aggression or reinforcing harmful stereotypes. In this study, we explore the use of generative artificial intelligence (GenAI) and Large Language Models (LLMs) to automatically transform abusive words (vocal delivery) and lyrical content in popular music. Rather than simply muting or replacing a single word, our approach transforms the tone, intensity, and sentiment, thus not altering just the lyrics, but how it is expressed. We present a comparative analysis of four selected English songs and their transformed counterparts, evaluating changes through both acoustic and sentiment-based lenses. Our findings indicate that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\\% across artists, with major improvements in chorus sections (up to 88.6\\% reduction). The transformed versions maintained musical coherence while mitigating harmful content, offering a promising alternative to traditional content moderation that avoids triggering the \"forbidden fruit\" effect, where the censored content becomes more appealing simply because it is restricted. This approach demonstrates the potential for GenAI to create safer listening experiences while preserving artistic expression.</p>"
    },
    {
      "id": "8fafe7ad8c7c",
      "title": "Recursivism: An Artistic Paradigm for Self-Transforming Art in the Age of AI",
      "content": "arXiv:2601.14401v1 Announce Type: cross  Abstract: This article introduces Recursivism as a conceptual framework for analyzing contemporary artistic practices in the age of artificial intelligence. While recursion is precisely defined in mathematics and computer science, it has not previously been formalized as an aesthetic paradigm. Recursivism designates practices in which not only outputs vary over time, but in which the generative process itself becomes capable of reflexive modification through its own effects.   The paper develops a five-level analytical scale distinguishing simple iteration, cumulative iteration, parametric recursion, reflexive recursion, and meta-recursion. This scale clarifies the threshold at which a system shifts from variation within a fixed rule to genuine self-modification of the rule itself. From this perspective, art history is reinterpreted as a recursive dynamic alternating between internal recursion within movements and meta-recursive transformations of their generative principles.   Artificial intelligence renders this logic technically explicit through learning loops, parameter updates, and code-level self-modification. To distinguish Recursivism from related notions such as generative art, cybernetics, process art, and evolutionary art, the article proposes three operational criteria: state memory, rule evolvability, and reflexive visibility. These concepts are examined through case studies including Refik Anadol, Sougwen Chung, Karl Sims, and the Darwin-Godel Machine. The article concludes by examining the aesthetic, curatorial, and ethical implications of self-modifying artistic systems.",
      "url": "http://arxiv.org/abs/2601.14401",
      "author": "Florentin Koch",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CY"
      ],
      "summary": "Introduces Recursivism as conceptual framework for AI art analysis, developing five-level scale from simple iteration to meta-recursion distinguishing variation within fixed rules from genuine self-modification.",
      "importance_score": 32,
      "reasoning": "Niche contribution for AI art theory. Limited technical or practical impact.",
      "themes": [
        "AI Art",
        "Generative Art",
        "Philosophy of AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Recursivism as conceptual framework for AI art analysis, developing five-level scale from simple iteration to meta-recursion distinguishing variation within fixed rules from genuine self-modification.</p>",
      "content_html": "<p>arXiv:2601.14401v1 Announce Type: cross  Abstract: This article introduces Recursivism as a conceptual framework for analyzing contemporary artistic practices in the age of artificial intelligence. While recursion is precisely defined in mathematics and computer science, it has not previously been formalized as an aesthetic paradigm. Recursivism designates practices in which not only outputs vary over time, but in which the generative process itself becomes capable of reflexive modification through its own effects.   The paper develops a five-level analytical scale distinguishing simple iteration, cumulative iteration, parametric recursion, reflexive recursion, and meta-recursion. This scale clarifies the threshold at which a system shifts from variation within a fixed rule to genuine self-modification of the rule itself. From this perspective, art history is reinterpreted as a recursive dynamic alternating between internal recursion within movements and meta-recursive transformations of their generative principles.   Artificial intelligence renders this logic technically explicit through learning loops, parameter updates, and code-level self-modification. To distinguish Recursivism from related notions such as generative art, cybernetics, process art, and evolutionary art, the article proposes three operational criteria: state memory, rule evolvability, and reflexive visibility. These concepts are examined through case studies including Refik Anadol, Sougwen Chung, Karl Sims, and the Darwin-Godel Machine. The article concludes by examining the aesthetic, curatorial, and ethical implications of self-modifying artistic systems.</p>"
    },
    {
      "id": "6b92c6f97add",
      "title": "Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems",
      "content": "arXiv:2601.15697v1 Announce Type: cross  Abstract: As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.",
      "url": "http://arxiv.org/abs/2601.15697",
      "author": "Binu V P, Deepthy K Bhaskar, Minimol B",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Survey paper exploring AI's role in balancing security and privacy in healthcare systems. Discusses threat detection, automated responses, and privacy-preserving implementations.",
      "importance_score": 32,
      "reasoning": "Overview/survey paper without novel technical contributions. General discussion of known tradeoffs.",
      "themes": [
        "Healthcare AI",
        "Privacy",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>Survey paper exploring AI's role in balancing security and privacy in healthcare systems. Discusses threat detection, automated responses, and privacy-preserving implementations.</p>",
      "content_html": "<p>arXiv:2601.15697v1 Announce Type: cross  Abstract: As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.</p>"
    },
    {
      "id": "b0462fad6723",
      "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation",
      "content": "arXiv:2601.15560v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.",
      "url": "http://arxiv.org/abs/2601.15560",
      "author": "Sylvey Lin, Eranki Vasistha",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes Relative Classification Accuracy (RCA) metric for evaluating identity consistency in fine-grained face generation, demonstrated on K-pop idol face generation at 32x32 resolution.",
      "importance_score": 32,
      "reasoning": "Very narrow application domain (K-pop faces at low resolution). Metric contribution is minor and domain-specific.",
      "themes": [
        "Image Generation",
        "Evaluation Metrics",
        "Face Synthesis"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Relative Classification Accuracy (RCA) metric for evaluating identity consistency in fine-grained face generation, demonstrated on K-pop idol face generation at 32x32 resolution.</p>",
      "content_html": "<p>arXiv:2601.15560v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.</p>"
    },
    {
      "id": "6161dfe4d476",
      "title": "Algebraic Statistics in OSCAR",
      "content": "arXiv:2601.15807v1 Announce Type: cross  Abstract: We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.",
      "url": "http://arxiv.org/abs/2601.15807",
      "author": "Tobias Boege, Antony Della Vecchia, Marina Garrote-L\\'opez, Benjamin Hollering",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Neural and Evolutionary Computing)",
      "source_type": "arxiv",
      "tags": [
        "stat.CO"
      ],
      "summary": "Introduces algebraic statistics functionality in the OSCAR computer algebra system, including serialization and implicitization algorithms.",
      "importance_score": 32,
      "reasoning": "Niche software contribution, limited AI relevance.",
      "themes": [
        "Computer Algebra",
        "Statistics",
        "Software"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces algebraic statistics functionality in the OSCAR computer algebra system, including serialization and implicitization algorithms.</p>",
      "content_html": "<p>arXiv:2601.15807v1 Announce Type: cross  Abstract: We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.</p>"
    },
    {
      "id": "3ee259e1cc02",
      "title": "VegaChat: A Robust Framework for LLM-Based Chart Generation and Assessment",
      "content": "arXiv:2601.15385v1 Announce Type: cross  Abstract: Natural-language-to-visualization (NL2VIS) systems based on large language models (LLMs) have substantially improved the accessibility of data visualization. However, their further adoption is hindered by two coupled challenges: (i) the absence of standardized evaluation metrics makes it difficult to assess progress in the field and compare different approaches; and (ii) natural language descriptions are inherently underspecified, so multiple visualizations may be valid for the same query. To address these issues, we introduce VegaChat, a framework for generating, validating, and assessing declarative visualizations from natural language.   We propose two complementary metrics: Spec Score, a deterministic metric that measures specification-level similarity without invoking an LLM, and Vision Score, a library-agnostic, image-based metric that leverages a multimodal LLM to assess chart similarity and prompt compliance.   We evaluate VegaChat on the NLV Corpus and on the annotated subset of ChartLLM. VegaChat achieves near-zero rates of invalid or empty visualizations, while Spec Score and Vision Score exhibit strong correlation with human judgments (Pearson 0.65 and 0.71, respectively), indicating that the proposed metrics support consistent, cross-library comparison.   The code and evaluation artifacts are available at https://zenodo.org/records/17062309.",
      "url": "http://arxiv.org/abs/2601.15385",
      "author": "Marko Hostnik, Rauf Kurbanov, Yaroslav Sokolov, Artem Trofimov",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.HC"
      ],
      "summary": "arXiv:2601.15385v1 Announce Type: cross  Abstract: Natural-language-to-visualization (NL2VIS) systems based on large language models (LLMs) have substantially improved the accessibility of data visual...",
      "importance_score": 30,
      "reasoning": "Not analyzed (batch processing)",
      "themes": [],
      "continuation": null,
      "summary_html": "<p>arXiv:2601.15385v1 Announce Type: cross  Abstract: Natural-language-to-visualization (NL2VIS) systems based on large language models (LLMs) have substantially improved the accessibility of data visual...</p>",
      "content_html": "<p>arXiv:2601.15385v1 Announce Type: cross  Abstract: Natural-language-to-visualization (NL2VIS) systems based on large language models (LLMs) have substantially improved the accessibility of data visualization. However, their further adoption is hindered by two coupled challenges: (i) the absence of standardized evaluation metrics makes it difficult to assess progress in the field and compare different approaches; and (ii) natural language descriptions are inherently underspecified, so multiple visualizations may be valid for the same query. To address these issues, we introduce VegaChat, a framework for generating, validating, and assessing declarative visualizations from natural language.   We propose two complementary metrics: Spec Score, a deterministic metric that measures specification-level similarity without invoking an LLM, and Vision Score, a library-agnostic, image-based metric that leverages a multimodal LLM to assess chart similarity and prompt compliance.   We evaluate VegaChat on the NLV Corpus and on the annotated subset of ChartLLM. VegaChat achieves near-zero rates of invalid or empty visualizations, while Spec Score and Vision Score exhibit strong correlation with human judgments (Pearson 0.65 and 0.71, respectively), indicating that the proposed metrics support consistent, cross-library comparison.   The code and evaluation artifacts are available at https://zenodo.org/records/17062309.</p>"
    },
    {
      "id": "45956a353fda",
      "title": "Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling",
      "content": "arXiv:2601.14485v1 Announce Type: new  Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.",
      "url": "http://arxiv.org/abs/2601.14485",
      "author": "Yuan Tian, Yi Mei, Mengjie Zhang",
      "published": "2026-01-22T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes a knee-point guided activity group selection strategy for genetic programming applied to dynamic project scheduling, addressing scalability issues in larger problem instances.",
      "importance_score": 28,
      "reasoning": "Niche optimization work in genetic programming. Limited broader impact on AI field.",
      "themes": [
        "Optimization",
        "Genetic Programming",
        "Scheduling"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes a knee-point guided activity group selection strategy for genetic programming applied to dynamic project scheduling, addressing scalability issues in larger problem instances.</p>",
      "content_html": "<p>arXiv:2601.14485v1 Announce Type: new  Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.</p>"
    },
    {
      "id": "7e21e112cfe5",
      "title": "A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks",
      "content": "arXiv:2601.15810v1 Announce Type: new  Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.",
      "url": "http://arxiv.org/abs/2601.15810",
      "author": "Mustafa Yurdakul, Enes Ayan, Fahrettin Horasan, Sakir Tasdemir",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Develops CNN-based mobile application for flower recognition using various architectures and transfer learning, achieving recognition across multiple flower types.",
      "importance_score": 28,
      "reasoning": "Basic application work with no methodological novelty. Standard CNN transfer learning for narrow classification task.",
      "themes": [
        "Image Classification",
        "Mobile Applications",
        "Transfer Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Develops CNN-based mobile application for flower recognition using various architectures and transfer learning, achieving recognition across multiple flower types.</p>",
      "content_html": "<p>arXiv:2601.15810v1 Announce Type: new  Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.</p>"
    },
    {
      "id": "d39fb65874dd",
      "title": "Does Pentagon Pizza Theory Work?",
      "content": "As soon as modern data analysis became a thing, the US government has had to deal with people trying to use open source data to uncover its secrets.During the early Cold War days and Americas hydrogen bomb testing, there was an enormous amount of speculation about how the bombs actually worked. All nuclear technology involves refinement and purification of large amounts of raw substances into chemically pure substances. Armen Alchian was an economist working at RAND and reasoned that any US company working in such raw materials and supplying the government would have made a killing leading up to the tests.After checking financial data that RAND maintained on such companies, Alchian deduced that the secret sauce in the early fusion bombs was lithium and the Lithium Corporation of America was supplying the USG. The companys stock had skyrocketed leading up to the Castle Bravo test either by way of enormous unexpected revenue gains from government contracts, or more amusingly, maybe by government insiders buying up the stock trying to make a mushroom-cloud-sized fortune with the knowledge that lithium was the key ingredient.When word of this work got out, this story naturally ends with the FBI coming in and confiscating Alchians data-driven research and the G-men giving him a stern lecture on national security, but he had just invented the first event study of the modern world.Pizza is the new lithiumAs you might have guessed, Alchians intellectual heir is the X account Pentagon Pizza Report, which tracks activity in pizzerias with proximity to the Pentagon as reported by Google. Started in 2024 and with over 300K followers, it may be the gold-standard OSINT meme account. Polymarket has even formalized it.Before the X account, I had never heard of the pentagon pizza theory, but it has its own wikipedia page and amazingly, this idea goes all the way back to the 1990s. The chain of causation is clear. People only work late in the Pentagon if theres a lot of military...",
      "url": "https://www.lesswrong.com/posts/Li3Aw7sDLXTCcQHZM/does-pentagon-pizza-theory-work",
      "author": "rba",
      "published": "2026-01-22T14:24:41.639000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analyzes the historical 'Pentagon Pizza Theory' - using pizza delivery patterns to predict military operations - and examines whether such open source intelligence approaches actually work.",
      "importance_score": 28,
      "reasoning": "Interesting historical analysis but not AI research.",
      "themes": [
        "OSINT",
        "History",
        "Analysis"
      ],
      "continuation": null,
      "summary_html": "<p>Analyzes the historical 'Pentagon Pizza Theory' - using pizza delivery patterns to predict military operations - and examines whether such open source intelligence approaches actually work.</p>",
      "content_html": "<p>As soon as modern data analysis became a thing, the US government has had to deal with people trying to use open source data to uncover its secrets.During the early Cold War days and Americas hydrogen bomb testing, there was an enormous amount of speculation about how the bombs actually worked. All nuclear technology involves refinement and purification of large amounts of raw substances into chemically pure substances. Armen Alchian was an economist working at RAND and reasoned that any US company working in such raw materials and supplying the government would have made a killing leading up to the tests.After checking financial data that RAND maintained on such companies, Alchian deduced that the secret sauce in the early fusion bombs was lithium and the Lithium Corporation of America was supplying the USG. The companys stock had skyrocketed leading up to the Castle Bravo test either by way of enormous unexpected revenue gains from government contracts, or more amusingly, maybe by government insiders buying up the stock trying to make a mushroom-cloud-sized fortune with the knowledge that lithium was the key ingredient.When word of this work got out, this story naturally ends with the FBI coming in and confiscating Alchians data-driven research and the G-men giving him a stern lecture on national security, but he had just invented the first event study of the modern world.Pizza is the new lithiumAs you might have guessed, Alchians intellectual heir is the X account Pentagon Pizza Report, which tracks activity in pizzerias with proximity to the Pentagon as reported by Google. Started in 2024 and with over 300K followers, it may be the gold-standard OSINT meme account. Polymarket has even formalized it.Before the X account, I had never heard of the pentagon pizza theory, but it has its own wikipedia page and amazingly, this idea goes all the way back to the 1990s. The chain of causation is clear. People only work late in the Pentagon if theres a lot of military...</p>"
    },
    {
      "id": "8deb2eb5a69a",
      "title": "Like night and day:  Light glasses and dark therapy can treat non-24 (and SAD)",
      "content": "Epistemic status: &nbsp;n=1, strong, life changing results.TLDR: &nbsp;Light glasses, in combination with turning all your lights red at night, and optionally melatonin, can treat non-24. &nbsp;Light glasses can also be a competitive alternative to lumenators for SAD. &nbsp;My non-24 before this treatment:Data taken from my CPAP.Vertical lines are sleep periods; the x-axis is individual days, and the y-axis is the time in the day.Notice how my sleep keeps wrapping around every two weeks.And after:Like night and day.What is non-24?Non-24 is \"non-24-hour sleep disorder.\" &nbsp;Healthy people's bodies tell them to go to sleep and wake up at the same time every day, i.e. every ~24 hours. &nbsp;For people with non-24, however, these drift around, such that if you wake up at 8 am one day, you wake up at 9am the next day, then 10 am the next, and so on until you're waking up at midnight a couple weeks later. &nbsp;This is akin to having a daily circadian rhythm length of 25 hours, compared to most people's ~24; hence \"non-24.\" &nbsp;This is a pretty awful problem to deal with, since either half the time you are in the middle of your day when everyone else is winding down or asleep, or you are sleep deprived!Aside: &nbsp;How do sleep rhythms even work?There's (at least) three rhythms:\"Process C\" (for Circadian), the driver of wakefulness: &nbsp;There is a little clock in your brain called the suprachiasmatic nucleus, which tracks how long your day/night cycle is and sends signals for you to be awake. &nbsp;It takes the majority of its cues from ipRGCs, a type of cell in your eyes that controls pupillary reflex but contributes little to color vision. &nbsp;Blue and green light activate them very strongly, and red light activates them almost not at all.\"Process S\" (for Sleepiness), the driver of sleepiness: &nbsp;This sends signals for you to go to sleep, based on the buildup of various metabolites in your brain. &nbsp;They accumulate steadily over the day and are what causes...",
      "url": "https://www.lesswrong.com/posts/mHJFu6FAJc4ikscnq/like-night-and-day-light-glasses-and-dark-therapy-can-treat",
      "author": "JennaS",
      "published": "2026-01-22T18:23:01",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Personal account of treating non-24-hour sleep disorder using light glasses and dark therapy with red lights, showing dramatic improvement in sleep pattern regularity.",
      "importance_score": 25,
      "reasoning": "Personal health intervention report, not AI research, n=1 study.",
      "themes": [
        "Health",
        "Sleep",
        "Self-Experimentation"
      ],
      "continuation": null,
      "summary_html": "<p>Personal account of treating non-24-hour sleep disorder using light glasses and dark therapy with red lights, showing dramatic improvement in sleep pattern regularity.</p>",
      "content_html": "<p>Epistemic status: &nbsp;n=1, strong, life changing results.TLDR: &nbsp;Light glasses, in combination with turning all your lights red at night, and optionally melatonin, can treat non-24. &nbsp;Light glasses can also be a competitive alternative to lumenators for SAD. &nbsp;My non-24 before this treatment:Data taken from my CPAP.Vertical lines are sleep periods; the x-axis is individual days, and the y-axis is the time in the day.Notice how my sleep keeps wrapping around every two weeks.And after:Like night and day.What is non-24?Non-24 is \"non-24-hour sleep disorder.\" &nbsp;Healthy people's bodies tell them to go to sleep and wake up at the same time every day, i.e. every ~24 hours. &nbsp;For people with non-24, however, these drift around, such that if you wake up at 8 am one day, you wake up at 9am the next day, then 10 am the next, and so on until you're waking up at midnight a couple weeks later. &nbsp;This is akin to having a daily circadian rhythm length of 25 hours, compared to most people's ~24; hence \"non-24.\" &nbsp;This is a pretty awful problem to deal with, since either half the time you are in the middle of your day when everyone else is winding down or asleep, or you are sleep deprived!Aside: &nbsp;How do sleep rhythms even work?There's (at least) three rhythms:\"Process C\" (for Circadian), the driver of wakefulness: &nbsp;There is a little clock in your brain called the suprachiasmatic nucleus, which tracks how long your day/night cycle is and sends signals for you to be awake. &nbsp;It takes the majority of its cues from ipRGCs, a type of cell in your eyes that controls pupillary reflex but contributes little to color vision. &nbsp;Blue and green light activate them very strongly, and red light activates them almost not at all.\"Process S\" (for Sleepiness), the driver of sleepiness: &nbsp;This sends signals for you to go to sleep, based on the buildup of various metabolites in your brain. &nbsp;They accumulate steadily over the day and are what causes...</p>"
    },
    {
      "id": "f9262ffc9ef7",
      "title": "An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics",
      "content": "arXiv:2601.15830v1 Announce Type: new  Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.",
      "url": "http://arxiv.org/abs/2601.15830",
      "author": "Abdul Hasib, A. S. M. Ahsanul Sarkar Akib",
      "published": "2026-01-23T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents IoT-based smart plant monitoring system integrating environmental sensors with ESP32 microcontroller, automated irrigation, OLED display, and cloud analytics.",
      "importance_score": 22,
      "reasoning": "Hardware/IoT systems paper with no AI/ML novelty. Basic sensor integration work.",
      "themes": [
        "IoT",
        "Agriculture",
        "Embedded Systems"
      ],
      "continuation": null,
      "summary_html": "<p>Presents IoT-based smart plant monitoring system integrating environmental sensors with ESP32 microcontroller, automated irrigation, OLED display, and cloud analytics.</p>",
      "content_html": "<p>arXiv:2601.15830v1 Announce Type: new  Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.</p>"
    },
    {
      "id": "e3e75938e0fe",
      "title": "How Could I Have Learned That Faster?",
      "content": "This is a question, but also a linkpost from my (new) Substack.How could I have thought that faster? is great advice I am embarrassed I did not think of myself. A subset of this, I would say, is not thinking original thoughts, but rather the ability to effectively learn new things. What prompted this is my continual inability to find all the papers relevant to a question I am researching in a timely manner.I have been reading about biological learning rules for four years, albeit not continuously, yet still manage to find papers that are both old and have all the keywords that I routinely search. I wonder how I missed them, if I found them before and ignored them for some reason. But the big fear isnt about any paper in particular, its that I wont be able to fill in huge knowledge gaps when the answer is out there.There is an especially humorous, or disappointing, example of this happening to someone else: that being a biologist rediscovering calculus in 1994.I think that this poses a real problem to academics in general and will only worsen as more and more research is published that could be applicable to your problem.I also think it is worse for autodidacts or people otherwise learning alone, having structure and people around you lets you leverage existing knowledge much easier.Maybe this is just me being bad at using search engines/the new wave of LLMs but it is a big frustration. I dont think tools like semantic scholar fully solve the problem either, at least not for me.I think this generalizes past finding papers to read. It is more of a failure to know what you should be spending your time doing during the skill/information acquisition stage of a project. When you start a project, you usually arent sure how you are going to solve all the problems that pop up along the way. If you dont already have a huge bag of problem solving techniques that are applicable to that domain, it is hard to know where you should be allocating your time to learn what wil...",
      "url": "https://www.lesswrong.com/posts/rixnRGGtCfgZsPB26/how-could-i-have-learned-that-faster",
      "author": "Dom Polsinelli",
      "published": "2026-01-22T12:35:03.641000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Meta-discussion on how to efficiently discover relevant research papers, noting the challenge of knowledge gaps even after years of studying a field.",
      "importance_score": 22,
      "reasoning": "Personal reflection on research methodology, not technical content.",
      "themes": [
        "Research Methodology",
        "Meta"
      ],
      "continuation": null,
      "summary_html": "<p>Meta-discussion on how to efficiently discover relevant research papers, noting the challenge of knowledge gaps even after years of studying a field.</p>",
      "content_html": "<p>This is a question, but also a linkpost from my (new) Substack.How could I have thought that faster? is great advice I am embarrassed I did not think of myself. A subset of this, I would say, is not thinking original thoughts, but rather the ability to effectively learn new things. What prompted this is my continual inability to find all the papers relevant to a question I am researching in a timely manner.I have been reading about biological learning rules for four years, albeit not continuously, yet still manage to find papers that are both old and have all the keywords that I routinely search. I wonder how I missed them, if I found them before and ignored them for some reason. But the big fear isnt about any paper in particular, its that I wont be able to fill in huge knowledge gaps when the answer is out there.There is an especially humorous, or disappointing, example of this happening to someone else: that being a biologist rediscovering calculus in 1994.I think that this poses a real problem to academics in general and will only worsen as more and more research is published that could be applicable to your problem.I also think it is worse for autodidacts or people otherwise learning alone, having structure and people around you lets you leverage existing knowledge much easier.Maybe this is just me being bad at using search engines/the new wave of LLMs but it is a big frustration. I dont think tools like semantic scholar fully solve the problem either, at least not for me.I think this generalizes past finding papers to read. It is more of a failure to know what you should be spending your time doing during the skill/information acquisition stage of a project. When you start a project, you usually arent sure how you are going to solve all the problems that pop up along the way. If you dont already have a huge bag of problem solving techniques that are applicable to that domain, it is hard to know where you should be allocating your time to learn what wil...</p>"
    },
    {
      "id": "e4971b55baf7",
      "title": "Resisting Reality",
      "content": "Sometimes updating on evidence opens roads we do not want to take: roads that we do not like as we know where they inevitably lead. We sometimes prefer to stay in homeostasis, in our current lane, suboptimal.One evocative example is the sort of paradoxical blend of invective mania and social apathy within trading circles. A lot of finance people dont take the political environment or the meanderings of the presidency seriously, they dont take whats happening seriously  they just go with the flow and they just trade. Their only goal is to forecast, to predict, and to capitalize on the arbitrage of that prediction unto reality; not to judge or countervail. This is rational and market-participant optimal, yet something is lost. Something somewhat ineffable and un-liminal - cannot be limned - which is hard to pin down and point out. If you point at the ineffability you get a scissor statement: the traders react I dont know what youre talking about? and the rest of the world yells YES! in unison of chorus, of course.Sometimes the mathematics points you in a direction that you prefer not to take. The answer is not to protest the theorem or stay within suboptimality - irrationality / pre-rationality. It is to resist the mathematics. This a companion post on how to defy and resist the mathematics you prefer were False: how to resist reality by shifting the assumptions you presumptively assumed.False TheoremsI once told my supervisor: \"I cannot set a theorem to false.\" Tasked with the coupling of two distributed systems within a proposed task that paradoxically, upon further examination, translated into solving what was effectively equivalent to securing a violation of the CAP theorem, I laid out my concerns and offered alternatives, such as \"I can try to implement a sidecar that conducts a two-phase commit protocol between this system that we own, and this other system that a different team owns, that I think shall accomplish the underlying intent behind your prop...",
      "url": "https://www.lesswrong.com/posts/JLk8Rwbw2zqMM59Kv/resisting-reality",
      "author": "robertzk",
      "published": "2026-01-22T08:50:27.558000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Philosophical reflection on rational response to unwanted information and updating beliefs when evidence points in uncomfortable directions.",
      "importance_score": 18,
      "reasoning": "Philosophy/trading psychology, not AI research.",
      "themes": [
        "Rationality",
        "Philosophy"
      ],
      "continuation": null,
      "summary_html": "<p>Philosophical reflection on rational response to unwanted information and updating beliefs when evidence points in uncomfortable directions.</p>",
      "content_html": "<p>Sometimes updating on evidence opens roads we do not want to take: roads that we do not like as we know where they inevitably lead. We sometimes prefer to stay in homeostasis, in our current lane, suboptimal.One evocative example is the sort of paradoxical blend of invective mania and social apathy within trading circles. A lot of finance people dont take the political environment or the meanderings of the presidency seriously, they dont take whats happening seriously  they just go with the flow and they just trade. Their only goal is to forecast, to predict, and to capitalize on the arbitrage of that prediction unto reality; not to judge or countervail. This is rational and market-participant optimal, yet something is lost. Something somewhat ineffable and un-liminal - cannot be limned - which is hard to pin down and point out. If you point at the ineffability you get a scissor statement: the traders react I dont know what youre talking about? and the rest of the world yells YES! in unison of chorus, of course.Sometimes the mathematics points you in a direction that you prefer not to take. The answer is not to protest the theorem or stay within suboptimality - irrationality / pre-rationality. It is to resist the mathematics. This a companion post on how to defy and resist the mathematics you prefer were False: how to resist reality by shifting the assumptions you presumptively assumed.False TheoremsI once told my supervisor: \"I cannot set a theorem to false.\" Tasked with the coupling of two distributed systems within a proposed task that paradoxically, upon further examination, translated into solving what was effectively equivalent to securing a violation of the CAP theorem, I laid out my concerns and offered alternatives, such as \"I can try to implement a sidecar that conducts a two-phase commit protocol between this system that we own, and this other system that a different team owns, that I think shall accomplish the underlying intent behind your prop...</p>"
    }
  ]
}