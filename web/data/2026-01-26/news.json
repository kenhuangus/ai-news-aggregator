{
  "category": "news",
  "date": "2026-01-26",
  "category_summary": "**AI Policy & Geopolitics** dominates this news cycle, with the **Trump administration's 'Pax Silica'** initiative [formalizing US strategy](/?date=2026-01-26&category=news#item-558516bbf6ef) to secure critical minerals for AI dominance, including the pursuit of **Greenland**.\n\n**Model Releases & Infrastructure:**\n- **StepFun** [launched **Step-DeepResearch**](/?date=2026-01-26&category=news#item-621b454d0cc6), a 32B parameter research agent built on **Qwen2.5**\n- **OpenAI** continues [pursuing **$1 trillion**](/?date=2026-01-26&category=news#item-f610fa6b72d7) in datacenter investments under **Sam Altman's** leadership\n\n**AI Ethics & Societal Impact:**\n- **Akido Labs** [deploying AI-assisted diagnostics](/?date=2026-01-26&category=news#item-e17e8afbf9ef) for low-income patients raises healthcare equity concerns\n- AI-generated [far-right persona **'Amelia'**](/?date=2026-01-26&category=news#item-4ab361d146f7) highlights growing synthetic media manipulation risks",
  "category_summary_html": "<p><strong>AI Policy & Geopolitics</strong> dominates this news cycle, with the <strong>Trump administration's 'Pax Silica'</strong> initiative <a href=\"/?date=2026-01-26&category=news#item-558516bbf6ef\" class=\"internal-link\" rel=\"noopener noreferrer\">formalizing US strategy</a> to secure critical minerals for AI dominance, including the pursuit of <strong>Greenland</strong>.</p>\n<p><strong>Model Releases & Infrastructure:</strong></p>\n<ul>\n<li><strong>StepFun</strong> <a href=\"/?date=2026-01-26&category=news#item-621b454d0cc6\" class=\"internal-link\" rel=\"noopener noreferrer\">launched <strong>Step-DeepResearch</strong></a>, a 32B parameter research agent built on <strong>Qwen2.5</strong></li>\n<li><strong>OpenAI</strong> continues <a href=\"/?date=2026-01-26&category=news#item-f610fa6b72d7\" class=\"internal-link\" rel=\"noopener noreferrer\">pursuing <strong>$1 trillion</strong></a> in datacenter investments under <strong>Sam Altman's</strong> leadership</li>\n</ul>\n<p><strong>AI Ethics & Societal Impact:</strong></p>\n<ul>\n<li><strong>Akido Labs</strong> <a href=\"/?date=2026-01-26&category=news#item-e17e8afbf9ef\" class=\"internal-link\" rel=\"noopener noreferrer\">deploying AI-assisted diagnostics</a> for low-income patients raises healthcare equity concerns</li>\n<li>AI-generated <a href=\"/?date=2026-01-26&category=news#item-4ab361d146f7\" class=\"internal-link\" rel=\"noopener noreferrer\">far-right persona <strong>'Amelia'</strong></a> highlights growing synthetic media manipulation risks</li>\n</ul>",
  "themes": [
    {
      "name": "AI Policy & Geopolitics",
      "description": "Government initiatives and strategic competition for AI resources and dominance",
      "item_count": 1,
      "example_items": [],
      "importance": 75.0
    },
    {
      "name": "AI Agents & Research Tools",
      "description": "New agent models and tools designed for complex research and reasoning tasks",
      "item_count": 1,
      "example_items": [],
      "importance": 67.0
    },
    {
      "name": "AI Infrastructure & Investment",
      "description": "Major datacenter investments and corporate strategy from leading AI companies",
      "item_count": 1,
      "example_items": [],
      "importance": 62.0
    },
    {
      "name": "AI Ethics & Healthcare",
      "description": "Concerns about AI deployment in sensitive domains and equity implications",
      "item_count": 1,
      "example_items": [],
      "importance": 58.0
    },
    {
      "name": "AI Misuse & Manipulation",
      "description": "Synthetic media and AI-generated content used for propaganda and misinformation",
      "item_count": 1,
      "example_items": [],
      "importance": 54.0
    }
  ],
  "total_items": 6,
  "items": [
    {
      "id": "558516bbf6ef",
      "title": "‘Pax Silica’ and Pursuit of Greenland Give Shape to Trump’s Imperial AI Ambitions | TechPolicy.Press",
      "content": "Analysis ‘Pax Silica’ and Pursuit of Greenland Give Shape to Trump’s Imperial AI Ambitions Justin Hendrix / Jan 20, 2026 President Donald Trump speaks with members of the media before boarding Marine One on the South Lawn of the White House en route to Joint Base Andrews, Maryland, Friday, January 9, 2026. (Official White House photo by Molly Riley) From the ascension of emperor Augustus in 27 BC through the death of Marcus Aurelius in 180 AD, historians say the Roman empire enjoyed a period of relative peace and prosperity known as Pax Romana. Now, the Trump administration says it intends to organize the AI age the way Rome ‘organized’ the ancient world. Access to critical minerals underlies these efforts. In December, the United States Department of State launched “ Pax Silica ,” a diplomatic initiative to organize a coalition of countries in order to establish a “new economic paradigm” built on “secure supply chains, trusted technology, and strategic infrastructure,” including such things as rare earths, semiconductors, and data centers. And now, President Donald Trump is ratcheting up threats to take control of Greenland, in part on the strategic rationale that it holds substantial critical minerals deposits. The growing urgency to secure critical minerals is consistent with Trump’s stated intention “to achieve and maintain unquestioned and unchallenged global technological dominance,” which the administration and its patrons believe is crucial to securing American security and economic prosperity. The carrot: Pax Silica and the AI supply chain The State Department says Pax Silica is “American AI diplomacy at its best: building coalitions, shaping markets, and advancing our national interests.” Under Secretary of State for Economic Affairs Jacob Helberg, who leads the initiative, calls Pax Silica a “prerequisite for national survival.” He says the aim is \"to create a competitive edge so steep, so insurmountable that no adversary or competitor can scale it” in order to “make America the arsenal of AI in this century.” The initiative kicked off with the Pax Silica Summit on December 12, at which participating countries including Australia, Japan, South Korea, the United Kingdom, Singapore, and Israel joined the US in signing the “ Pax Silica Declaration .” The signatories aim to collaborate to realize growth “across all levels of the global AI supply chain, driving historic opportunity and demand for energy, critical minerals, manufacturing, technological hardware, infrastructure, and new markets not yet invented.” Signatories of the “Pax Silica Declaration” join US Undersecretary of State for Economic Affairs Jacob Helberg (far left) at a signing ceremony at the recently renamed Donald J. Trump US Institute of Peace. (Source: State Department ) In short order, the State Department has recruited multiple additional signatories to the declaration, including Qatar and the United Arab Emirates , which joined last Wednesday. India is slated to join next month . The European Union, Canada, and Taiwan are also involved in the discussions, according to the State Department. The declaration is not a formal treaty with enforcement mechanisms—it's more like a “ statement of shared principles ” and intentions. For the US, the goal is straightforward. Right now, China controls about 90% of rare earth processing, most of the world's advanced manufacturing capacity, and has been aggressively investing in AI. The administration sees this as an unacceptable threat, so it is trying to organize America's allies into a parallel system that excludes China and creates supply chains that run through its partners instead. The framework involves the US providing advanced technology, security guarantees, and market access, while allies provide capital, manufacturing capacity, and natural resources. As noted by Rest of World , Qatar and the UAE bring sovereign wealth funds, energy, and strategic location. Japan and South Korea bring semiconductor and advanced manufacturing expertise. Australia brings minerals and technology. Israel and Singapore bring tech innovation. In this paradigm, the US sits at the top, setting standards and controlling access to the most critical AI technologies. While the rhetoric around Pax Silica is new, the pursuit of “adequate, stable, and reliable supply of materials for US national security, economic well-being, and industrial production” has long been the policy of the federal government, as evidenced in recent years by both the Biden administration and the first Trump administration. Our Content delivered to your inbox. Join our newsletter on issues and ideas at the intersection of tech & democracy Subscribe Loading... Thank you! You have successfully joined our subscriber list. For instance, President Joe Biden used the Defense Production Act to bolster the critical minerals supply, and his administration also developed partnerships with multiple countries to finance minerals projects. During the ",
      "url": "https://www.techpolicy.press/pax-silica-and-pursuit-of-greenland-give-shape-to-trumps-imperial-ai-ambitions/",
      "author": "",
      "published": "2026-01-26T02:32:13.000277",
      "source": "www.techpolicy.press",
      "source_type": "linked_article",
      "tags": [],
      "summary": "The Trump administration launched 'Pax Silica,' a State Department diplomatic initiative to secure critical minerals for AI, with Greenland acquisition as a key strategic goal. The policy frames US AI dominance in imperial terms, comparing it to how Rome organized the ancient world.",
      "importance_score": 75.0,
      "reasoning": "Significant AI policy development revealing US government's strategic approach to AI resource competition. The formal 'Pax Silica' initiative represents a notable escalation in AI geopolitics and resource access strategy.",
      "themes": [
        "AI policy",
        "geopolitics",
        "critical minerals",
        "US AI strategy",
        "government initiative"
      ],
      "continuation": null,
      "summary_html": "<p>The Trump administration launched 'Pax Silica,' a State Department diplomatic initiative to secure critical minerals for AI, with Greenland acquisition as a key strategic goal. The policy frames US AI dominance in imperial terms, comparing it to how Rome organized the ancient world.</p>",
      "content_html": "<p>Analysis ‘Pax Silica’ and Pursuit of Greenland Give Shape to Trump’s Imperial AI Ambitions Justin Hendrix / Jan 20, 2026 President Donald Trump speaks with members of the media before boarding Marine One on the South Lawn of the White House en route to Joint Base Andrews, Maryland, Friday, January 9, 2026. (Official White House photo by Molly Riley) From the ascension of emperor Augustus in 27 BC through the death of Marcus Aurelius in 180 AD, historians say the Roman empire enjoyed a period of relative peace and prosperity known as Pax Romana. Now, the Trump administration says it intends to organize the AI age the way Rome ‘organized’ the ancient world. Access to critical minerals underlies these efforts. In December, the United States Department of State launched “ Pax Silica ,” a diplomatic initiative to organize a coalition of countries in order to establish a “new economic paradigm” built on “secure supply chains, trusted technology, and strategic infrastructure,” including such things as rare earths, semiconductors, and data centers. And now, President Donald Trump is ratcheting up threats to take control of Greenland, in part on the strategic rationale that it holds substantial critical minerals deposits. The growing urgency to secure critical minerals is consistent with Trump’s stated intention “to achieve and maintain unquestioned and unchallenged global technological dominance,” which the administration and its patrons believe is crucial to securing American security and economic prosperity. The carrot: Pax Silica and the AI supply chain The State Department says Pax Silica is “American AI diplomacy at its best: building coalitions, shaping markets, and advancing our national interests.” Under Secretary of State for Economic Affairs Jacob Helberg, who leads the initiative, calls Pax Silica a “prerequisite for national survival.” He says the aim is \"to create a competitive edge so steep, so insurmountable that no adversary or competitor can scale it” in order to “make America the arsenal of AI in this century.” The initiative kicked off with the Pax Silica Summit on December 12, at which participating countries including Australia, Japan, South Korea, the United Kingdom, Singapore, and Israel joined the US in signing the “ Pax Silica Declaration .” The signatories aim to collaborate to realize growth “across all levels of the global AI supply chain, driving historic opportunity and demand for energy, critical minerals, manufacturing, technological hardware, infrastructure, and new markets not yet invented.” Signatories of the “Pax Silica Declaration” join US Undersecretary of State for Economic Affairs Jacob Helberg (far left) at a signing ceremony at the recently renamed Donald J. Trump US Institute of Peace. (Source: State Department ) In short order, the State Department has recruited multiple additional signatories to the declaration, including Qatar and the United Arab Emirates , which joined last Wednesday. India is slated to join next month . The European Union, Canada, and Taiwan are also involved in the discussions, according to the State Department. The declaration is not a formal treaty with enforcement mechanisms—it's more like a “ statement of shared principles ” and intentions. For the US, the goal is straightforward. Right now, China controls about 90% of rare earth processing, most of the world's advanced manufacturing capacity, and has been aggressively investing in AI. The administration sees this as an unacceptable threat, so it is trying to organize America's allies into a parallel system that excludes China and creates supply chains that run through its partners instead. The framework involves the US providing advanced technology, security guarantees, and market access, while allies provide capital, manufacturing capacity, and natural resources. As noted by Rest of World , Qatar and the UAE bring sovereign wealth funds, energy, and strategic location. Japan and South Korea bring semiconductor and advanced manufacturing expertise. Australia brings minerals and technology. Israel and Singapore bring tech innovation. In this paradigm, the US sits at the top, setting standards and controlling access to the most critical AI technologies. While the rhetoric around Pax Silica is new, the pursuit of “adequate, stable, and reliable supply of materials for US national security, economic well-being, and industrial production” has long been the policy of the federal government, as evidenced in recent years by both the Biden administration and the first Trump administration. Our Content delivered to your inbox. Join our newsletter on issues and ideas at the intersection of tech &amp; democracy Subscribe Loading... Thank you! You have successfully joined our subscriber list. For instance, President Joe Biden used the Defense Production Act to bolster the critical minerals supply, and his administration also developed partnerships with multiple countries to finance minerals projects. During the</p>"
    },
    {
      "id": "621b454d0cc6",
      "title": "StepFun AI Introduce Step-DeepResearch: A Cost-Effective Deep Research Agent Model Built Around Atomic Capabilities",
      "content": "StepFun has introduced Step-DeepResearch, a 32B parameter end to end deep research agent that aims to turn web search into actual research workflows with long horizon reasoning, tool use and structured reporting. The model is built on Qwen2.5 32B-Base and is trained to act as a single agent that plans, explores sources, verifies evidence and writes reports with citations, while keeping inference cost low.\n\n\n\nFrom Search to Deep Research\n\n\n\nMost existing web agents are tuned for multi-hop question-answering benchmarks. They try to match ground truth answers for short questions. This is closer to targeted retrieval than to real research. Deep research tasks are different. They involve latent intent recognition, long horizon decision making, multi-turn tool use, structured-reasoning and cross-source verification under uncertainty.\n\n\n\nStep-DeepResearch reframes this as sequential decision making over a compact set of atomic capabilities. The research team defines 4 atomic capabilities, planning and task decomposition, deep-information seeking, reflection and verification, and professional report generation. Instead of orchestrating many external agents, the system internalizes this loop into a single model that decides the next action at each step.\n\n\n\nData Synthesis around Atomic Capabilities\n\n\n\nTo teach these atomic capabilities, the research team builds separate data pipelines for each skill. For planning, they start from high quality technical reports, survey papers and financial analysis documents. They reverse-engineer realistic research plans and task trees from titles, abstracts and structure, then generate trajectories that follow these plans. This exposes the model to long horizon project structures, not only short question templates.\n\n\n\nFor deep information seeking, they construct graph based queries over knowledge graphs such as Wikidata5m and CN-DBpedia. They sample subgraphs, expand them using search, and synthesize questions that require multi hop reasoning across entities and documents. A separate pipeline uses a Wiki style hyperlink index to force cross document retrieval and combination of evidence. Easy questions that a strong model can already solve with a simple ReAct style strategy are filtered out, so training focuses on hard search problems.\n\n\n\nReflection and verification data is generated through self-correction loops and multi-agent teacher traces. Teacher agents extract claims, plan checks, verify facts, replan if inconsistencies appear and only then write reports. The resulting trajectories are cleaned and used as supervision for a single student agent. Report generation is trained in 2 phases, mid training for domain style and depth using query report pairs, then supervised fine-tuning with strict formatting and plan consistency constraints.\n\n\n\nProgressive Training on Qwen2.5-32B-Base\n\n\n\nThe training pipeline has 3 stages, agentic mid-training, supervised fine-tuning and reinforcement learning. In mid training stage-1, the team injects atomic capabilities without tools, using context length up to 32k tokens. The data covers active reading, synthetic reasoning traces, summarization and reflection. The research team show steady gains on SimpleQA, TriviaQA and FRAMES as training scales up to about 150B tokens, with the largest gains on FRAMES, which stresses structured reasoning.\n\n\n\nIn stage-2, the context extends to 128k tokens and explicit tool calls are introduced. The model learns tasks such as URL based question-answering, deep web search, long document summarization and long dialogue reasoning. This stage aligns the model with real research scenarios where search, browsing and analysis must be mixed in one trajectory.\n\n\n\nDuring supervised fine-tuning, the 4 atomic capabilities are composed into full deep search and deep research traces. Data cleaning keeps trajectories that are correct and short in terms of steps and tool calls. The pipeline injects controlled tool errors followed by correction to improve robustness, and enforces citation formats so that reports stay grounded in the retrieved sources.\n\n\n\nReinforcement learning then optimizes the agent in a real tool environment. The research team builds tasks and checklists through reverse synthesis, and trains a checklist style Rubrics Judge to score reports along fine grained dimensions. The reward design converts ternary rubric labels into asymmetric binary rewards that capture both positive targets and violations. The policy is trained with PPO and a learned critic, using generalized advantage estimation with near zero discount so that long trajectories are not truncated.\n\n\n\nSingle Agent ReAct Architecture and Search Stack\n\n\n\nAt inference time, Step-DeepResearch runs as a single ReAct style agent that alternates thinking, tool calls and observations until it decides to output a report. The tool set includes batch web search, a todo manager, shell commands and file operations. Execution runs in a sandbox with terminal persistence through tmux. A perception oriented browser reduces redundant page captures by using perceptual hash distance. Tools for document parsing, audio transcription and image analysis support multimodal inputs.\n\n\n\nInformation acquisition uses 2 related resources. StepFun team states that its Search API is grounded in more than 20M high quality papers and 600 premium indices. The research team then describes a curated authority indexing strategy that isolates more than 600 trusted domains, including government, academic and institutional sites. Retrieval operates at paragraph level and uses authority aware ranking so that high trust domains are preferred when relevance is similar.\n\n\n\nThe file tools support patch based editing, so the agent can update only modified sections of a report. A summary aware storage scheme writes full tool outputs to local files and injects only compact summaries into the context. This acts as external memory and avoids context overflow for long projects.\n\n\n\nEvaluation, Cost and Access\n\n\n\nTo measure deep research behavior, the team introduce ADR-Bench, a Chinese benchmark with 110 open ended tasks across 9 domains. 70 tasks cover general domains such as education, science and engineering and social life, evaluated by expert side by side comparison. 40 tasks in finance and law are scored with explicit rubrics that follow atomicity and verifiability constraints.\n\n\n\nOn Scale AI Research Rubrics, Step-DeepResearch reaches 61.42 percent rubric compliance, which is comparable to OpenAI-DeepResearch and Gemini-DeepResearch, and clearly ahead of multiple open and proprietary baselines. On ADR-Bench, expert-based Elo ratings show that the 32B model outperforms larger open-models such as MiniMax-M2, GLM-4.6 and DeepSeek-V3.2, and is competitive with systems like Kimi-Researcher and MiniMax-Agent-Pro.\n\n\n\nKey Takeaways\n\n\n\n\nSingle agent, atomic capability design: Step-DeepResearch is a 32B parameter single agent built on Qwen2.-32B-Base, it internalizes 4 atomic capabilities, planning, deep information seeking, reflection and verification, and professional report generation, instead of relying on many external agents.\n\n\n\nTargeted data synthesis for each skill: The research team builds separate data pipelines for planning, deep information seeking, reflection and report writing, using reverse-engineered plans from real reports, graph-based queries over Wikidata5m and CN-DBpedia, multi-agent teacher traces and strict report formatting data.\n\n\n\nThree stage training with long context and RL: Training uses mid training, supervised fine-tuning and reinforcement learning, with mid training up to 150B tokens at 32k and then 128k context, SFT composes full deep research trajectories, and PPO based RL with a Rubrics Judge optimizes reports against fine grained checklists.\n\n\n\nReAct architecture with curated search and external memory: At inference time the model runs a ReAct loop that calls tools for batch web search, todo, shell and file operations, uses a Search API grounded in more than 20M papers and 600 premium indices along with 600+trusted domains, and relies on patch editing and summary aware storage to act as external memory.\n\n\n\nCompetitive quality with lower cost: On Scale AI Research Rubrics the model reaches 61.42 percent rubric compliance and is competitive with OpenAI-DeepResearch and Gemini-DeepResearch, on ADR Bench it achieves 67.1 percent win or tie rate against strong baselines.\n\n\n\n\n\n\n\n\nCheck out the Paper and Repo. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post StepFun AI Introduce Step-DeepResearch: A Cost-Effective Deep Research Agent Model Built Around Atomic Capabilities appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/25/stepfun-ai-introduce-step-deepresearch-a-cost-effective-deep-research-agent-model-built-around-atomic-capabilities/",
      "author": "Asif Razzaq",
      "published": "2026-01-25T21:08:15",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "StepFun released Step-DeepResearch, a 32B parameter end-to-end research agent built on Qwen2.5 32B-Base. The model handles planning, source exploration, evidence verification, and report writing with citations while maintaining low inference costs.",
      "importance_score": 67.0,
      "reasoning": "Notable new AI agent model release with practical research capabilities. However, it's from a smaller lab, built on existing Qwen base, and represents incremental rather than breakthrough progress in agent development.",
      "themes": [
        "AI agents",
        "research AI",
        "new model release",
        "deep research",
        "Qwen ecosystem"
      ],
      "continuation": null,
      "summary_html": "<p>StepFun released Step-DeepResearch, a 32B parameter end-to-end research agent built on Qwen2.5 32B-Base. The model handles planning, source exploration, evidence verification, and report writing with citations while maintaining low inference costs.</p>",
      "content_html": "<p>StepFun has introduced Step-DeepResearch, a 32B parameter end to end deep research agent that aims to turn web search into actual research workflows with long horizon reasoning, tool use and structured reporting. The model is built on Qwen2.5 32B-Base and is trained to act as a single agent that plans, explores sources, verifies evidence and writes reports with citations, while keeping inference cost low.</p>\n<p>From Search to Deep Research</p>\n<p>Most existing web agents are tuned for multi-hop question-answering benchmarks. They try to match ground truth answers for short questions. This is closer to targeted retrieval than to real research. Deep research tasks are different. They involve latent intent recognition, long horizon decision making, multi-turn tool use, structured-reasoning and cross-source verification under uncertainty.</p>\n<p>Step-DeepResearch reframes this as sequential decision making over a compact set of atomic capabilities. The research team defines 4 atomic capabilities, planning and task decomposition, deep-information seeking, reflection and verification, and professional report generation. Instead of orchestrating many external agents, the system internalizes this loop into a single model that decides the next action at each step.</p>\n<p>Data Synthesis around Atomic Capabilities</p>\n<p>To teach these atomic capabilities, the research team builds separate data pipelines for each skill. For planning, they start from high quality technical reports, survey papers and financial analysis documents. They reverse-engineer realistic research plans and task trees from titles, abstracts and structure, then generate trajectories that follow these plans. This exposes the model to long horizon project structures, not only short question templates.</p>\n<p>For deep information seeking, they construct graph based queries over knowledge graphs such as Wikidata5m and CN-DBpedia. They sample subgraphs, expand them using search, and synthesize questions that require multi hop reasoning across entities and documents. A separate pipeline uses a Wiki style hyperlink index to force cross document retrieval and combination of evidence. Easy questions that a strong model can already solve with a simple ReAct style strategy are filtered out, so training focuses on hard search problems.</p>\n<p>Reflection and verification data is generated through self-correction loops and multi-agent teacher traces. Teacher agents extract claims, plan checks, verify facts, replan if inconsistencies appear and only then write reports. The resulting trajectories are cleaned and used as supervision for a single student agent. Report generation is trained in 2 phases, mid training for domain style and depth using query report pairs, then supervised fine-tuning with strict formatting and plan consistency constraints.</p>\n<p>Progressive Training on Qwen2.5-32B-Base</p>\n<p>The training pipeline has 3 stages, agentic mid-training, supervised fine-tuning and reinforcement learning. In mid training stage-1, the team injects atomic capabilities without tools, using context length up to 32k tokens. The data covers active reading, synthetic reasoning traces, summarization and reflection. The research team show steady gains on SimpleQA, TriviaQA and FRAMES as training scales up to about 150B tokens, with the largest gains on FRAMES, which stresses structured reasoning.</p>\n<p>In stage-2, the context extends to 128k tokens and explicit tool calls are introduced. The model learns tasks such as URL based question-answering, deep web search, long document summarization and long dialogue reasoning. This stage aligns the model with real research scenarios where search, browsing and analysis must be mixed in one trajectory.</p>\n<p>During supervised fine-tuning, the 4 atomic capabilities are composed into full deep search and deep research traces. Data cleaning keeps trajectories that are correct and short in terms of steps and tool calls. The pipeline injects controlled tool errors followed by correction to improve robustness, and enforces citation formats so that reports stay grounded in the retrieved sources.</p>\n<p>Reinforcement learning then optimizes the agent in a real tool environment. The research team builds tasks and checklists through reverse synthesis, and trains a checklist style Rubrics Judge to score reports along fine grained dimensions. The reward design converts ternary rubric labels into asymmetric binary rewards that capture both positive targets and violations. The policy is trained with PPO and a learned critic, using generalized advantage estimation with near zero discount so that long trajectories are not truncated.</p>\n<p>Single Agent ReAct Architecture and Search Stack</p>\n<p>At inference time, Step-DeepResearch runs as a single ReAct style agent that alternates thinking, tool calls and observations until it decides to output a report. The tool set includes batch web search, a todo manager, shell commands and file operations. Execution runs in a sandbox with terminal persistence through tmux. A perception oriented browser reduces redundant page captures by using perceptual hash distance. Tools for document parsing, audio transcription and image analysis support multimodal inputs.</p>\n<p>Information acquisition uses 2 related resources. StepFun team states that its Search API is grounded in more than 20M high quality papers and 600 premium indices. The research team then describes a curated authority indexing strategy that isolates more than 600 trusted domains, including government, academic and institutional sites. Retrieval operates at paragraph level and uses authority aware ranking so that high trust domains are preferred when relevance is similar.</p>\n<p>The file tools support patch based editing, so the agent can update only modified sections of a report. A summary aware storage scheme writes full tool outputs to local files and injects only compact summaries into the context. This acts as external memory and avoids context overflow for long projects.</p>\n<p>Evaluation, Cost and Access</p>\n<p>To measure deep research behavior, the team introduce ADR-Bench, a Chinese benchmark with 110 open ended tasks across 9 domains. 70 tasks cover general domains such as education, science and engineering and social life, evaluated by expert side by side comparison. 40 tasks in finance and law are scored with explicit rubrics that follow atomicity and verifiability constraints.</p>\n<p>On Scale AI Research Rubrics, Step-DeepResearch reaches 61.42 percent rubric compliance, which is comparable to OpenAI-DeepResearch and Gemini-DeepResearch, and clearly ahead of multiple open and proprietary baselines. On ADR-Bench, expert-based Elo ratings show that the 32B model outperforms larger open-models such as MiniMax-M2, GLM-4.6 and DeepSeek-V3.2, and is competitive with systems like Kimi-Researcher and MiniMax-Agent-Pro.</p>\n<p>Key Takeaways</p>\n<p>Single agent, atomic capability design: Step-DeepResearch is a 32B parameter single agent built on Qwen2.-32B-Base, it internalizes 4 atomic capabilities, planning, deep information seeking, reflection and verification, and professional report generation, instead of relying on many external agents.</p>\n<p>Targeted data synthesis for each skill: The research team builds separate data pipelines for planning, deep information seeking, reflection and report writing, using reverse-engineered plans from real reports, graph-based queries over Wikidata5m and CN-DBpedia, multi-agent teacher traces and strict report formatting data.</p>\n<p>Three stage training with long context and RL: Training uses mid training, supervised fine-tuning and reinforcement learning, with mid training up to 150B tokens at 32k and then 128k context, SFT composes full deep research trajectories, and PPO based RL with a Rubrics Judge optimizes reports against fine grained checklists.</p>\n<p>ReAct architecture with curated search and external memory: At inference time the model runs a ReAct loop that calls tools for batch web search, todo, shell and file operations, uses a Search API grounded in more than 20M papers and 600 premium indices along with 600+trusted domains, and relies on patch editing and summary aware storage to act as external memory.</p>\n<p>Competitive quality with lower cost: On Scale AI Research Rubrics the model reaches 61.42 percent rubric compliance and is competitive with OpenAI-DeepResearch and Gemini-DeepResearch, on ADR Bench it achieves 67.1 percent win or tie rate against strong baselines.</p>\n<p>Check out the&nbsp;Paper and Repo.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post StepFun AI Introduce Step-DeepResearch: A Cost-Effective Deep Research Agent Model Built Around Atomic Capabilities appeared first on MarkTechPost.</p>"
    },
    {
      "id": "f610fa6b72d7",
      "title": "Sam Altman’s make-or-break year: can the OpenAI CEO cash in his bet on the future?",
      "content": "Altman’s campaigning for his company coincides with its use of enormous present resources to serve an imagined futureSam Altman has claimed over the years that the advancement of AI could solve climate change, cure cancer, create a benevolent superintelligence beyond human comprehension, provide a tutor for every student, take over nearly half of the tasks in the economy and create what he calls “universal extreme wealth”.In order to bring about his utopian future, Altman is demanding enormous resources from the present. As CEO of OpenAI, the world’s most valuable privately owned company, he has in recent months announced plans for $1tn of investment into datacenters and struck multibillion-dollar deals with several chipmakers. If completed, the datacenters are expected to use more power than entire European nations. OpenAI is pushing an aggressive expansion – encroaching on industries like e-commerce, healthcare and entertainment – while increasingly integrating its products into government, universities, and the US military and making a play to turn ChatGPT into the new default homepage for millions. Continue reading...",
      "url": "https://www.theguardian.com/technology/ng-interactive/2026/jan/25/sam-altman-openai",
      "author": "Nick Robins-Early",
      "published": "2026-01-25T13:00:08",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Sam Altman",
        "OpenAI",
        "Technology",
        "Business",
        "AI (artificial intelligence)",
        "ChatGPT"
      ],
      "summary": "Feature analysis of Sam Altman and OpenAI's ambitious plans, including announced $1 trillion datacenter investments and multibillion-dollar chipmaker deals. The piece examines the tension between OpenAI's massive present resource demands and its utopian future promises.",
      "importance_score": 62.0,
      "reasoning": "Provides important context on OpenAI's strategic direction and infrastructure spending, but is primarily analysis/recap rather than breaking news. The $1tn investment figure is significant but previously reported.",
      "themes": [
        "OpenAI",
        "AI infrastructure",
        "AI investment",
        "Sam Altman",
        "datacenter expansion"
      ],
      "continuation": null,
      "summary_html": "<p>Feature analysis of Sam Altman and OpenAI's ambitious plans, including announced $1 trillion datacenter investments and multibillion-dollar chipmaker deals. The piece examines the tension between OpenAI's massive present resource demands and its utopian future promises.</p>",
      "content_html": "<p>Altman’s campaigning for his company coincides with its use of enormous present resources to serve an imagined futureSam Altman has claimed over the years that the advancement of AI could solve climate change, cure cancer, create a benevolent superintelligence beyond human comprehension, provide a tutor for every student, take over nearly half of the tasks in the economy and create what he calls “universal extreme wealth”.In order to bring about his utopian future, Altman is demanding enormous resources from the present. As CEO of OpenAI, the world’s most valuable privately owned company, he has in recent months announced plans for $1tn of investment into datacenters and struck multibillion-dollar deals with several chipmakers. If completed, the datacenters are expected to use more power than entire European nations. OpenAI is pushing an aggressive expansion – encroaching on industries like e-commerce, healthcare and entertainment – while increasingly integrating its products into government, universities, and the US military and making a play to turn ChatGPT into the new default homepage for millions. Continue reading...</p>"
    },
    {
      "id": "e17e8afbf9ef",
      "title": "We must not let AI ‘pull the doctor out of the visit’ for low-income patients | Leah Goodridge and Oni Blackstock",
      "content": "Generative AI is being pushed into healthcare – and diagnostic risks may deepen the class divideIn southern California, where rates of homelessness are among the highest in the nation, a private company, Akido Labs, is running clinics for unhoused patients and others with low incomes. The caveat? The patients are seen by medical assistants who use artificial intelligence (AI) to listen to the conversations, then spit out potential diagnoses and treatment plans, which are then reviewed by a doctor. The company’s goal, its chief technology officer told the MIT Technology Review, is to “pull the doctor out of the visit”.This is dangerous. Yet it’s part of a larger trend where generative AI is being pushed into healthcare for medical professionals. In 2025, a survey by the American Medical Association reported that two out of three physicians used AI to assist with their daily work, including diagnosing patients. One AI startup raised $200m to provide medical professionals with an app dubbed “ChatGPT for doctors”. US lawmakers are considering a bill that would recognize AI as able to prescribe medication. While this trend of AI in healthcare affects almost all patients, it has a deeper impact on people with low incomes who already face substantial barriers to care and higher rates of mistreatment in healthcare settings. People who are unhoused and have low incomes should not be testing grounds for AI in healthcare. Instead, their voices and priorities should drive if, how, and when AI is implemented in their care.Leah Goodridge is a lawyer who worked in homeless prevention litigation for 12 yearsOni Blackstock, MD, MHS, is a physician, founder and executive director of health justice, and a Public Voices Fellow on technology in the public interest with The OpEd Project Continue reading...",
      "url": "https://www.theguardian.com/commentisfree/2026/jan/25/ai-healthcare-risks-low-income-people",
      "author": "Leah Goodridge and Oni Blackstock",
      "published": "2026-01-25T14:00:13",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "US healthcare",
        "AI (artificial intelligence)",
        "Health",
        "Society",
        "US news",
        "Technology"
      ],
      "summary": "Akido Labs operates clinics in Southern California where medical assistants use AI for diagnoses on unhoused and low-income patients, with doctor review afterward. The company aims to 'pull the doctor out of the visit,' raising concerns about healthcare equity.",
      "importance_score": 58.0,
      "reasoning": "Highlights important real-world AI deployment with significant ethical implications for healthcare equity. However, it's primarily an opinion piece rather than news of a breakthrough or major policy change.",
      "themes": [
        "AI healthcare",
        "AI ethics",
        "healthcare equity",
        "AI deployment",
        "diagnostic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Akido Labs operates clinics in Southern California where medical assistants use AI for diagnoses on unhoused and low-income patients, with doctor review afterward. The company aims to 'pull the doctor out of the visit,' raising concerns about healthcare equity.</p>",
      "content_html": "<p>Generative AI is being pushed into healthcare – and diagnostic risks may deepen the class divideIn southern California, where rates of homelessness are among the highest in the nation, a private company, Akido Labs, is running clinics for unhoused patients and others with low incomes. The caveat? The patients are seen by medical assistants who use artificial intelligence (AI) to listen to the conversations, then spit out potential diagnoses and treatment plans, which are then reviewed by a doctor. The company’s goal, its chief technology officer told the MIT Technology Review, is to “pull the doctor out of the visit”.This is dangerous. Yet it’s part of a larger trend where generative AI is being pushed into healthcare for medical professionals. In 2025, a survey by the American Medical Association reported that two out of three physicians used AI to assist with their daily work, including diagnosing patients. One AI startup raised $200m to provide medical professionals with an app dubbed “ChatGPT for doctors”. US lawmakers are considering a bill that would recognize AI as able to prescribe medication. While this trend of AI in healthcare affects almost all patients, it has a deeper impact on people with low incomes who already face substantial barriers to care and higher rates of mistreatment in healthcare settings. People who are unhoused and have low incomes should not be testing grounds for AI in healthcare. Instead, their voices and priorities should drive if, how, and when AI is implemented in their care.Leah Goodridge is a lawyer who worked in homeless prevention litigation for 12 yearsOni Blackstock, MD, MHS, is a physician, founder and executive director of health justice, and a Public Voices Fellow on technology in the public interest with The OpEd Project Continue reading...</p>"
    },
    {
      "id": "4ab361d146f7",
      "title": "Meet ‘Amelia’: the AI-generated British schoolgirl who is a far-right social media star",
      "content": "Warning: this image has been manipulatedOne of the AI-generated Amelias that have exploded across social media channels.",
      "url": "https://www.theguardian.com/politics/2026/jan/25/ai-generated-british-schoolgirl-becomes-far-right-social-media-meme",
      "author": "Ben Quinn Political correspondent",
      "published": "2026-01-25T09:00:03",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Far right",
        "Social media",
        "AI (artificial intelligence)",
        "UK news",
        "Politics",
        "Technology"
      ],
      "summary": "An AI-generated persona named 'Amelia,' depicting a British schoolgirl, has become a viral far-right social media phenomenon. The synthetic character demonstrates how generative AI is being weaponized for political propaganda.",
      "importance_score": 54.0,
      "reasoning": "Significant example of AI misuse for political manipulation with real societal impact. However, it's more about AI application/misuse than frontier AI development itself.",
      "themes": [
        "AI misinformation",
        "synthetic media",
        "political manipulation",
        "social media",
        "AI safety"
      ],
      "continuation": null,
      "summary_html": "<p>An AI-generated persona named 'Amelia,' depicting a British schoolgirl, has become a viral far-right social media phenomenon. The synthetic character demonstrates how generative AI is being weaponized for political propaganda.</p>",
      "content_html": "<p>Warning: this image has been manipulatedOne of the AI-generated Amelias that have exploded across social media channels.</p>"
    },
    {
      "id": "3026e55bb1b3",
      "title": "A Coding Implementation to Automating LLM Quality Assurance with DeepEval, Custom Retrievers, and LLM-as-a-Judge Metrics",
      "content": "We initiate this tutorial by configuring a high-performance evaluation environment, specifically focused on integrating the DeepEval framework to bring unit-testing rigor to our LLM applications. By bridging the gap between raw retrieval and final generation, we implement a system that treats model outputs as testable code and uses LLM-as-a-judge metrics to quantify performance. We move beyond manual inspection by building a structured pipeline in which every query, retrieved context, and generated response is validated against rigorous academic-standard metrics. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserimport sys, os, textwrap, json, math, re\nfrom getpass import getpass\n\n\nprint(\" Hardening environment (prevents common Colab/py3.12 numpy corruption)...\")\n\n\n!pip -q uninstall -y numpy || true\n!pip -q install --no-cache-dir --force-reinstall \"numpy==1.26.4\"\n\n\n!pip -q install -U deepeval openai scikit-learn pandas tqdm\n\n\nprint(\" Packages installed.\")\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nfrom deepeval import evaluate\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\nfrom deepeval.metrics import (\n   AnswerRelevancyMetric,\n   FaithfulnessMetric,\n   ContextualRelevancyMetric,\n   ContextualPrecisionMetric,\n   ContextualRecallMetric,\n   GEval,\n)\n\n\nprint(\" Imports loaded successfully.\")\n\n\n\n\nOPENAI_API_KEY = getpass(\" Enter OPENAI_API_KEY (leave empty to run without OpenAI): \").strip()\nopenai_enabled = bool(OPENAI_API_KEY)\n\n\nif openai_enabled:\n   os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nprint(f\" OpenAI enabled: {openai_enabled}\")\n\n\n\nWe initialize our environment by stabilizing core dependencies and installing the deepeval framework to ensure a robust testing pipeline. Next, we import specialized metrics like Faithfulness and Contextual Recall while configuring our API credentials to enable automated, high-fidelity evaluation of our LLM responses. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different BrowserDOCS = [\n   {\n       \"id\": \"doc_01\",\n       \"title\": \"DeepEval Overview\",\n       \"text\": (\n           \"DeepEval is an open-source LLM evaluation framework for unit testing LLM apps. \"\n           \"It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics \"\n           \"such as contextual precision and faithfulness.\"\n       ),\n   },\n   {\n       \"id\": \"doc_02\",\n       \"title\": \"RAG Evaluation: Why Faithfulness Matters\",\n       \"text\": (\n           \"Faithfulness checks whether the answer is supported by retrieved context. \"\n           \"In RAG, hallucinations occur when the model states claims not grounded in context.\"\n       ),\n   },\n   {\n       \"id\": \"doc_03\",\n       \"title\": \"Contextual Precision\",\n       \"text\": (\n           \"Contextual precision evaluates how well retrieved chunks are ranked by relevance \"\n           \"to a query. High precision means relevant chunks appear earlier in the ranked list.\"\n       ),\n   },\n   {\n       \"id\": \"doc_04\",\n       \"title\": \"Contextual Recall\",\n       \"text\": (\n           \"Contextual recall measures whether the retriever returns enough relevant context \"\n           \"to answer the query. Low recall means key information was missed in retrieval.\"\n       ),\n   },\n   {\n       \"id\": \"doc_05\",\n       \"title\": \"Answer Relevancy\",\n       \"text\": (\n           \"Answer relevancy measures whether the generated answer addresses the user's query. \"\n           \"Even grounded answers can be irrelevant if they don't respond to the question.\"\n       ),\n   },\n   {\n       \"id\": \"doc_06\",\n       \"title\": \"G-Eval (GEval) Custom Rubrics\",\n       \"text\": (\n           \"G-Eval lets you define evaluation criteria in natural language. \"\n           \"It uses an LLM judge to score outputs against your rubric (e.g., correctness, tone, policy).\"\n       ),\n   },\n   {\n       \"id\": \"doc_07\",\n       \"title\": \"What a DeepEval Test Case Contains\",\n       \"text\": (\n           \"A test case typically includes input (query), actual_output (model answer), \"\n           \"expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.\"\n       ),\n   },\n   {\n       \"id\": \"doc_08\",\n       \"title\": \"Common Pitfall: Missing expected_output\",\n       \"text\": (\n           \"Some RAG metrics require expected_output in addition to input and retrieval_context. \"\n           \"If expected_output is None, evaluation fails for metrics like contextual precision/recall.\"\n       ),\n   },\n]\n\n\n\n\nEVAL_QUERIES = [\n   {\n       \"query\": \"What is DeepEval used for?\",\n       \"expected\": \"DeepEval is used to evaluate and unit test LLM applications using metrics like LLM-as-a-judge, G-Eval, and RAG metrics.\",\n   },\n   {\n       \"query\": \"What does faithfulness measure in a RAG system?\",\n       \"expected\": \"Faithfulness measures whether the generated answer is supported by the retrieved context and avoids hallucinations not grounded in that context.\",\n   },\n   {\n       \"query\": \"What does contextual precision mean?\",\n       \"expected\": \"Contextual precision evaluates whether relevant retrieved chunks are ranked higher than irrelevant ones for a given query.\",\n   },\n   {\n       \"query\": \"What does contextual recall mean in retrieval?\",\n       \"expected\": \"Contextual recall measures whether the retriever returns enough relevant context to answer the query, capturing key missing information issues.\",\n   },\n   {\n       \"query\": \"Why might an answer be relevant but still low quality in RAG?\",\n       \"expected\": \"An answer can address the question (relevant) but still be low quality if it is not grounded in retrieved context or misses important details.\",\n   },\n]\n\n\n\n\nWe define a structured knowledge base consisting of documentation snippets that serve as our ground-truth context for the RAG system. We also establish a set of evaluation queries and corresponding expected outputs to create a &#8220;gold dataset,&#8221; enabling us to assess how accurately our model retrieves information and generates grounded responses. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass TfidfRetriever:\n   def __init__(self, docs):\n       self.docs = docs\n       self.texts = [f\"{d['title']}\\n{d['text']}\" for d in docs]\n       self.vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n       self.matrix = self.vectorizer.fit_transform(self.texts)\n\n\n   def retrieve(self, query, k=4):\n       qv = self.vectorizer.transform([query])\n       sims = cosine_similarity(qv, self.matrix).flatten()\n       top_idx = np.argsort(-sims)[:k]\n       results = []\n       for i in top_idx:\n           results.append(\n               {\n                   \"id\": self.docs[i][\"id\"],\n                   \"score\": float(sims[i]),\n                   \"text\": self.texts[i],\n               }\n           )\n       return results\n\n\nretriever = TfidfRetriever(DOCS)\n\n\n\nWe implement a custom TF-IDF Retriever class that transforms our documentation into a searchable vector space using bigram-aware TF-IDF vectorization. This allows us to perform cosine similarity searches against the knowledge base, ensuring we can programmatically fetch the top-k most relevant text chunks for any given query. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef extractive_baseline_answer(query, retrieved_contexts):\n   \"\"\"\n   Offline fallback: we create a short answer by extracting the most relevant sentences.\n   This keeps the notebook runnable even without OpenAI.\n   \"\"\"\n   joined = \"\\n\".join(retrieved_contexts)\n   sents = re.split(r\"(?&lt;=[.!?])\\s+\", joined)\n   keywords = [w.lower() for w in re.findall(r\"[a-zA-Z]{4,}\", query)]\n   scored = []\n   for s in sents:\n       s_l = s.lower()\n       score = sum(1 for k in keywords if k in s_l)\n       if len(s.strip()) > 20:\n           scored.append((score, s.strip()))\n   scored.sort(key=lambda x: (-x[0], -len(x[1])))\n   best = [s for sc, s in scored[:3] if sc > 0]\n   if not best:\n       best = [s.strip() for s in sents[:2] if len(s.strip()) > 20]\n   ans = \" \".join(best).strip()\n   if not ans:\n       ans = \"I could not find enough context to answer confidently.\"\n   return ans\n\n\ndef openai_answer(query, retrieved_contexts, model=\"gpt-4.1-mini\"):\n   \"\"\"\n   Simple RAG prompt for demonstration. DeepEval metrics can still evaluate even if\n   your generation prompt differs; the key is we store retrieval_context separately.\n   \"\"\"\n   from openai import OpenAI\n   client = OpenAI()\n\n\n   context_block = \"\\n\\n\".join([f\"[CTX {i+1}]\\n{c}\" for i, c in enumerate(retrieved_contexts)])\n   prompt = f\"\"\"You are a concise technical assistant.\nUse ONLY the provided context to answer the query. If the answer is not in context, say you don't know.\n\n\nQuery:\n{query}\n\n\nContext:\n{context_block}\n\n\nAnswer:\"\"\"\n   resp = client.chat.completions.create(\n       model=model,\n       messages=[{\"role\": \"user\", \"content\": prompt}],\n       temperature=0.2,\n   )\n   return resp.choices[0].message.content.strip()\n\n\ndef rag_answer(query, retrieved_contexts):\n   if openai_enabled:\n       try:\n           return openai_answer(query, retrieved_contexts)\n       except Exception as e:\n           print(f\" OpenAI generation failed, falling back to extractive baseline. Error: {e}\")\n           return extractive_baseline_answer(query, retrieved_contexts)\n   else:\n       return extractive_baseline_answer(query, retrieved_contexts)\n\n\n\nWe implement a hybrid answering mechanism that prioritizes high-fidelity generation via OpenAI while maintaining a keyword-based extractive baseline as a reliable fallback. By isolating the retrieval context from the final generation, we ensure our DeepEval test cases remain consistent regardless of whether the answer is synthesized by an LLM or extracted programmatically. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserprint(\"\\n Running RAG to create test cases...\")\n\n\ntest_cases = []\nK = 4\n\n\nfor item in tqdm(EVAL_QUERIES):\n   q = item[\"query\"]\n   expected = item[\"expected\"]\n\n\n   retrieved = retriever.retrieve(q, k=K)\n   retrieval_context = [r[\"text\"] for r in retrieved] \n\n\n   actual = rag_answer(q, retrieval_context)\n\n\n   tc = LLMTestCase(\n       input=q,\n       actual_output=actual,\n       expected_output=expected,\n       retrieval_context=retrieval_context,\n   )\n   test_cases.append(tc)\n\n\nprint(f\" Built {len(test_cases)} LLMTestCase objects.\")\n\n\nprint(\"\\n Metrics configured.\")\n\n\nmetrics = [\n   AnswerRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n   FaithfulnessMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n   ContextualRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n   ContextualPrecisionMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n   ContextualRecallMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n\n\n   GEval(\n       name=\"RAG Correctness Rubric (GEval)\",\n       criteria=(\n           \"Score the answer for correctness and usefulness. \"\n           \"The answer must directly address the query, must not invent facts not supported by context, \"\n           \"and should be concise but complete.\"\n       ),\n       evaluation_params=[\n           LLMTestCaseParams.INPUT,\n           LLMTestCaseParams.ACTUAL_OUTPUT,\n           LLMTestCaseParams.EXPECTED_OUTPUT,\n           LLMTestCaseParams.RETRIEVAL_CONTEXT,\n       ],\n       model=\"gpt-4.1\",\n       threshold=0.5,\n       async_mode=True,\n   ),\n]\n\n\nif not openai_enabled:\n   print(\"\\n You did NOT provide an OpenAI API key.\")\n   print(\"DeepEval's LLM-as-a-judge metrics (AnswerRelevancy/Faithfulness/Contextual* and GEval) require an LLM judge.\")\n   print(\"Re-run this cell and provide OPENAI_API_KEY to run DeepEval metrics.\")\n   print(\"\\n However, your RAG pipeline + test case construction succeeded end-to-end.\")\n   rows = []\n   for i, tc in enumerate(test_cases):\n       rows.append({\n           \"id\": i,\n           \"query\": tc.input,\n           \"actual_output\": tc.actual_output[:220] + (\"...\" if len(tc.actual_output) > 220 else \"\"),\n           \"expected_output\": tc.expected_output[:220] + (\"...\" if len(tc.expected_output) > 220 else \"\"),\n           \"contexts\": len(tc.retrieval_context or []),\n       })\n   display(pd.DataFrame(rows))\n   raise SystemExit(\"Stopped before evaluation (no OpenAI key).\")\n\n\n\nWe execute the RAG pipeline to generate LLMTestCase objects by pairing our retrieved context with model-generated answers and ground-truth expectations. We then configure a comprehensive suite of DeepEval metrics, including G-Eval and specialized RAG indicators, to evaluate the system&#8217;s performance using an LLM-as-a-judge approach. Check out the FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserprint(\"\\n Running DeepEval evaluate(...) ...\")\n\n\nresults = evaluate(test_cases=test_cases, metrics=metrics)\n\n\nsummary_rows = []\nfor idx, tc in enumerate(test_cases):\n   row = {\n       \"case_id\": idx,\n       \"query\": tc.input,\n       \"actual_output\": tc.actual_output[:200] + (\"...\" if len(tc.actual_output) > 200 else \"\"),\n   }\n   for m in metrics:\n       row[m.__class__.__name__ if hasattr(m, \"__class__\") else str(m)] = None\n\n\n   summary_rows.append(row)\n\n\ndef try_extract_case_metrics(results_obj):\n   extracted = []\n   candidates = []\n   for attr in [\"test_results\", \"results\", \"evaluations\"]:\n       if hasattr(results_obj, attr):\n           candidates = getattr(results_obj, attr)\n           break\n   if not candidates and isinstance(results_obj, list):\n       candidates = results_obj\n\n\n   for case_i, case_result in enumerate(candidates or []):\n       item = {\"case_id\": case_i}\n       metrics_list = None\n       for attr in [\"metrics_data\", \"metrics\", \"metric_results\"]:\n           if hasattr(case_result, attr):\n               metrics_list = getattr(case_result, attr)\n               break\n       if isinstance(metrics_list, dict):\n           for k, v in metrics_list.items():\n               item[f\"{k}_score\"] = getattr(v, \"score\", None) if v is not None else None\n               item[f\"{k}_reason\"] = getattr(v, \"reason\", None) if v is not None else None\n       else:\n           for mr in metrics_list or []:\n               name = getattr(mr, \"name\", None) or getattr(getattr(mr, \"metric\", None), \"name\", None)\n               if not name:\n                   name = mr.__class__.__name__\n               item[f\"{name}_score\"] = getattr(mr, \"score\", None)\n               item[f\"{name}_reason\"] = getattr(mr, \"reason\", None)\n       extracted.append(item)\n   return extracted\n\n\ncase_metrics = try_extract_case_metrics(results)\n\n\ndf_base = pd.DataFrame([{\n   \"case_id\": i,\n   \"query\": tc.input,\n   \"actual_output\": tc.actual_output,\n   \"expected_output\": tc.expected_output,\n} for i, tc in enumerate(test_cases)])\n\n\ndf_metrics = pd.DataFrame(case_metrics) if case_metrics else pd.DataFrame([])\ndf = df_base.merge(df_metrics, on=\"case_id\", how=\"left\")\n\n\nscore_cols = [c for c in df.columns if c.endswith(\"_score\")]\ncompact = df[[\"case_id\", \"query\"] + score_cols].copy()\n\n\nprint(\"\\n Compact score table:\")\ndisplay(compact)\n\n\nprint(\"\\n Full details (includes reasons):\")\ndisplay(df)\n\n\nprint(\"\\n Done. Tip: if contextual precision/recall are low, improve retriever ranking/coverage; if faithfulness is low, tighten generation to only use context.\")\n\n\n\nWe finalize the workflow by executing the evaluate function, which triggers the LLM-as-a-judge process to score each test case against our defined metrics. We then aggregate these scores and their corresponding qualitative reasoning into a centralized DataFrame, providing a granular view of where the RAG pipeline excels or requires further optimization in retrieval and generation.\n\n\n\nAt last, we conclude by running our comprehensive evaluation suite, in which DeepEval transforms complex linguistic outputs into actionable data using metrics such as Faithfulness, Contextual Precision, and the G-Eval rubric. This systematic approach allows us to diagnose &#8220;silent failures&#8221; in retrieval and hallucinations in generation with surgical precision, providing the reasoning necessary to justify architectural changes. With these results, we move forward from experimental prototyping to a production-ready RAG system backed by a verifiable, metric-driven safety net.\n\n\n\n\n\n\n\nCheck out the FULL CODES here. Also, feel free to follow us on Twitter and don’t forget to join our 100k+ ML SubReddit and Subscribe to our Newsletter. Wait! are you on telegram? now you can join us on telegram as well.\nThe post A Coding Implementation to Automating LLM Quality Assurance with DeepEval, Custom Retrievers, and LLM-as-a-Judge Metrics appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/25/a-coding-implementation-to-automating-llm-quality-assurance-with-deepeval-custom-retrievers-and-llm-as-a-judge-metrics/",
      "author": "Asif Razzaq",
      "published": "2026-01-25T20:40:11",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Language Model",
        "Large Language Model",
        "Machine Learning",
        "Staff",
        "Technology",
        "Tutorials"
      ],
      "summary": "Technical tutorial demonstrating how to implement automated LLM quality assurance using the DeepEval framework with custom retrievers and LLM-as-a-judge metrics. Provides a structured pipeline for validating model outputs.",
      "importance_score": 38.0,
      "reasoning": "Educational technical content with practical utility for developers, but it's a tutorial rather than news. No new product, research, or announcement involved.",
      "themes": [
        "LLM evaluation",
        "AI quality assurance",
        "developer tools",
        "tutorials"
      ],
      "continuation": null,
      "summary_html": "<p>Technical tutorial demonstrating how to implement automated LLM quality assurance using the DeepEval framework with custom retrievers and LLM-as-a-judge metrics. Provides a structured pipeline for validating model outputs.</p>",
      "content_html": "<p>We initiate this tutorial by configuring a high-performance evaluation environment, specifically focused on integrating the DeepEval framework to bring unit-testing rigor to our LLM applications. By bridging the gap between raw retrieval and final generation, we implement a system that treats model outputs as testable code and uses LLM-as-a-judge metrics to quantify performance. We move beyond manual inspection by building a structured pipeline in which every query, retrieved context, and generated response is validated against rigorous academic-standard metrics. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserimport sys, os, textwrap, json, math, re</p>\n<p>from getpass import getpass</p>\n<p>print(\" Hardening environment (prevents common Colab/py3.12 numpy corruption)...\")</p>\n<p>!pip -q uninstall -y numpy || true</p>\n<p>!pip -q install --no-cache-dir --force-reinstall \"numpy==1.26.4\"</p>\n<p>!pip -q install -U deepeval openai scikit-learn pandas tqdm</p>\n<p>print(\" Packages installed.\")</p>\n<p>import numpy as np</p>\n<p>import pandas as pd</p>\n<p>from tqdm.auto import tqdm</p>\n<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>\n<p>from sklearn.metrics.pairwise import cosine_similarity</p>\n<p>from deepeval import evaluate</p>\n<p>from deepeval.test_case import LLMTestCase, LLMTestCaseParams</p>\n<p>from deepeval.metrics import (</p>\n<p>AnswerRelevancyMetric,</p>\n<p>FaithfulnessMetric,</p>\n<p>ContextualRelevancyMetric,</p>\n<p>ContextualPrecisionMetric,</p>\n<p>ContextualRecallMetric,</p>\n<p>GEval,</p>\n<p>)</p>\n<p>print(\" Imports loaded successfully.\")</p>\n<p>OPENAI_API_KEY = getpass(\" Enter OPENAI_API_KEY (leave empty to run without OpenAI): \").strip()</p>\n<p>openai_enabled = bool(OPENAI_API_KEY)</p>\n<p>if openai_enabled:</p>\n<p>os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY</p>\n<p>print(f\" OpenAI enabled: {openai_enabled}\")</p>\n<p>We initialize our environment by stabilizing core dependencies and installing the deepeval framework to ensure a robust testing pipeline. Next, we import specialized metrics like Faithfulness and Contextual Recall while configuring our API credentials to enable automated, high-fidelity evaluation of our LLM responses. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different BrowserDOCS = [</p>\n<p>{</p>\n<p>\"id\": \"doc_01\",</p>\n<p>\"title\": \"DeepEval Overview\",</p>\n<p>\"text\": (</p>\n<p>\"DeepEval is an open-source LLM evaluation framework for unit testing LLM apps. \"</p>\n<p>\"It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics \"</p>\n<p>\"such as contextual precision and faithfulness.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_02\",</p>\n<p>\"title\": \"RAG Evaluation: Why Faithfulness Matters\",</p>\n<p>\"text\": (</p>\n<p>\"Faithfulness checks whether the answer is supported by retrieved context. \"</p>\n<p>\"In RAG, hallucinations occur when the model states claims not grounded in context.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_03\",</p>\n<p>\"title\": \"Contextual Precision\",</p>\n<p>\"text\": (</p>\n<p>\"Contextual precision evaluates how well retrieved chunks are ranked by relevance \"</p>\n<p>\"to a query. High precision means relevant chunks appear earlier in the ranked list.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_04\",</p>\n<p>\"title\": \"Contextual Recall\",</p>\n<p>\"text\": (</p>\n<p>\"Contextual recall measures whether the retriever returns enough relevant context \"</p>\n<p>\"to answer the query. Low recall means key information was missed in retrieval.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_05\",</p>\n<p>\"title\": \"Answer Relevancy\",</p>\n<p>\"text\": (</p>\n<p>\"Answer relevancy measures whether the generated answer addresses the user's query. \"</p>\n<p>\"Even grounded answers can be irrelevant if they don't respond to the question.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_06\",</p>\n<p>\"title\": \"G-Eval (GEval) Custom Rubrics\",</p>\n<p>\"text\": (</p>\n<p>\"G-Eval lets you define evaluation criteria in natural language. \"</p>\n<p>\"It uses an LLM judge to score outputs against your rubric (e.g., correctness, tone, policy).\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_07\",</p>\n<p>\"title\": \"What a DeepEval Test Case Contains\",</p>\n<p>\"text\": (</p>\n<p>\"A test case typically includes input (query), actual_output (model answer), \"</p>\n<p>\"expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.\"</p>\n<p>),</p>\n<p>},</p>\n<p>{</p>\n<p>\"id\": \"doc_08\",</p>\n<p>\"title\": \"Common Pitfall: Missing expected_output\",</p>\n<p>\"text\": (</p>\n<p>\"Some RAG metrics require expected_output in addition to input and retrieval_context. \"</p>\n<p>\"If expected_output is None, evaluation fails for metrics like contextual precision/recall.\"</p>\n<p>),</p>\n<p>},</p>\n<p>]</p>\n<p>EVAL_QUERIES = [</p>\n<p>{</p>\n<p>\"query\": \"What is DeepEval used for?\",</p>\n<p>\"expected\": \"DeepEval is used to evaluate and unit test LLM applications using metrics like LLM-as-a-judge, G-Eval, and RAG metrics.\",</p>\n<p>},</p>\n<p>{</p>\n<p>\"query\": \"What does faithfulness measure in a RAG system?\",</p>\n<p>\"expected\": \"Faithfulness measures whether the generated answer is supported by the retrieved context and avoids hallucinations not grounded in that context.\",</p>\n<p>},</p>\n<p>{</p>\n<p>\"query\": \"What does contextual precision mean?\",</p>\n<p>\"expected\": \"Contextual precision evaluates whether relevant retrieved chunks are ranked higher than irrelevant ones for a given query.\",</p>\n<p>},</p>\n<p>{</p>\n<p>\"query\": \"What does contextual recall mean in retrieval?\",</p>\n<p>\"expected\": \"Contextual recall measures whether the retriever returns enough relevant context to answer the query, capturing key missing information issues.\",</p>\n<p>},</p>\n<p>{</p>\n<p>\"query\": \"Why might an answer be relevant but still low quality in RAG?\",</p>\n<p>\"expected\": \"An answer can address the question (relevant) but still be low quality if it is not grounded in retrieved context or misses important details.\",</p>\n<p>},</p>\n<p>]</p>\n<p>We define a structured knowledge base consisting of documentation snippets that serve as our ground-truth context for the RAG system. We also establish a set of evaluation queries and corresponding expected outputs to create a “gold dataset,” enabling us to assess how accurately our model retrieves information and generates grounded responses. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass TfidfRetriever:</p>\n<p>def __init__(self, docs):</p>\n<p>self.docs = docs</p>\n<p>self.texts = [f\"{d['title']}\\n{d['text']}\" for d in docs]</p>\n<p>self.vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))</p>\n<p>self.matrix = self.vectorizer.fit_transform(self.texts)</p>\n<p>def retrieve(self, query, k=4):</p>\n<p>qv = self.vectorizer.transform([query])</p>\n<p>sims = cosine_similarity(qv, self.matrix).flatten()</p>\n<p>top_idx = np.argsort(-sims)[:k]</p>\n<p>results = []</p>\n<p>for i in top_idx:</p>\n<p>results.append(</p>\n<p>{</p>\n<p>\"id\": self.docs[i][\"id\"],</p>\n<p>\"score\": float(sims[i]),</p>\n<p>\"text\": self.texts[i],</p>\n<p>}</p>\n<p>)</p>\n<p>return results</p>\n<p>retriever = TfidfRetriever(DOCS)</p>\n<p>We implement a custom TF-IDF Retriever class that transforms our documentation into a searchable vector space using bigram-aware TF-IDF vectorization. This allows us to perform cosine similarity searches against the knowledge base, ensuring we can programmatically fetch the top-k most relevant text chunks for any given query. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef extractive_baseline_answer(query, retrieved_contexts):</p>\n<p>\"\"\"</p>\n<p>Offline fallback: we create a short answer by extracting the most relevant sentences.</p>\n<p>This keeps the notebook runnable even without OpenAI.</p>\n<p>\"\"\"</p>\n<p>joined = \"\\n\".join(retrieved_contexts)</p>\n<p>sents = re.split(r\"(?&lt;=[.!?])\\s+\", joined)</p>\n<p>keywords = [w.lower() for w in re.findall(r\"[a-zA-Z]{4,}\", query)]</p>\n<p>scored = []</p>\n<p>for s in sents:</p>\n<p>s_l = s.lower()</p>\n<p>score = sum(1 for k in keywords if k in s_l)</p>\n<p>if len(s.strip()) &gt; 20:</p>\n<p>scored.append((score, s.strip()))</p>\n<p>scored.sort(key=lambda x: (-x[0], -len(x[1])))</p>\n<p>best = [s for sc, s in scored[:3] if sc &gt; 0]</p>\n<p>if not best:</p>\n<p>best = [s.strip() for s in sents[:2] if len(s.strip()) &gt; 20]</p>\n<p>ans = \" \".join(best).strip()</p>\n<p>if not ans:</p>\n<p>ans = \"I could not find enough context to answer confidently.\"</p>\n<p>return ans</p>\n<p>def openai_answer(query, retrieved_contexts, model=\"gpt-4.1-mini\"):</p>\n<p>\"\"\"</p>\n<p>Simple RAG prompt for demonstration. DeepEval metrics can still evaluate even if</p>\n<p>your generation prompt differs; the key is we store retrieval_context separately.</p>\n<p>\"\"\"</p>\n<p>from openai import OpenAI</p>\n<p>client = OpenAI()</p>\n<p>context_block = \"\\n\\n\".join([f\"[CTX {i+1}]\\n{c}\" for i, c in enumerate(retrieved_contexts)])</p>\n<p>prompt = f\"\"\"You are a concise technical assistant.</p>\n<p>Use ONLY the provided context to answer the query. If the answer is not in context, say you don't know.</p>\n<p>Query:</p>\n<p>{query}</p>\n<p>Context:</p>\n<p>{context_block}</p>\n<p>Answer:\"\"\"</p>\n<p>resp = client.chat.completions.create(</p>\n<p>model=model,</p>\n<p>messages=[{\"role\": \"user\", \"content\": prompt}],</p>\n<p>temperature=0.2,</p>\n<p>)</p>\n<p>return resp.choices[0].message.content.strip()</p>\n<p>def rag_answer(query, retrieved_contexts):</p>\n<p>if openai_enabled:</p>\n<p>try:</p>\n<p>return openai_answer(query, retrieved_contexts)</p>\n<p>except Exception as e:</p>\n<p>print(f\" OpenAI generation failed, falling back to extractive baseline. Error: {e}\")</p>\n<p>return extractive_baseline_answer(query, retrieved_contexts)</p>\n<p>else:</p>\n<p>return extractive_baseline_answer(query, retrieved_contexts)</p>\n<p>We implement a hybrid answering mechanism that prioritizes high-fidelity generation via OpenAI while maintaining a keyword-based extractive baseline as a reliable fallback. By isolating the retrieval context from the final generation, we ensure our DeepEval test cases remain consistent regardless of whether the answer is synthesized by an LLM or extracted programmatically. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserprint(\"\\n Running RAG to create test cases...\")</p>\n<p>test_cases = []</p>\n<p>K = 4</p>\n<p>for item in tqdm(EVAL_QUERIES):</p>\n<p>q = item[\"query\"]</p>\n<p>expected = item[\"expected\"]</p>\n<p>retrieved = retriever.retrieve(q, k=K)</p>\n<p>retrieval_context = [r[\"text\"] for r in retrieved]</p>\n<p>actual = rag_answer(q, retrieval_context)</p>\n<p>tc = LLMTestCase(</p>\n<p>input=q,</p>\n<p>actual_output=actual,</p>\n<p>expected_output=expected,</p>\n<p>retrieval_context=retrieval_context,</p>\n<p>)</p>\n<p>test_cases.append(tc)</p>\n<p>print(f\" Built {len(test_cases)} LLMTestCase objects.\")</p>\n<p>print(\"\\n Metrics configured.\")</p>\n<p>metrics = [</p>\n<p>AnswerRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),</p>\n<p>FaithfulnessMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),</p>\n<p>ContextualRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),</p>\n<p>ContextualPrecisionMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),</p>\n<p>ContextualRecallMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),</p>\n<p>GEval(</p>\n<p>name=\"RAG Correctness Rubric (GEval)\",</p>\n<p>criteria=(</p>\n<p>\"Score the answer for correctness and usefulness. \"</p>\n<p>\"The answer must directly address the query, must not invent facts not supported by context, \"</p>\n<p>\"and should be concise but complete.\"</p>\n<p>),</p>\n<p>evaluation_params=[</p>\n<p>LLMTestCaseParams.INPUT,</p>\n<p>LLMTestCaseParams.ACTUAL_OUTPUT,</p>\n<p>LLMTestCaseParams.EXPECTED_OUTPUT,</p>\n<p>LLMTestCaseParams.RETRIEVAL_CONTEXT,</p>\n<p>],</p>\n<p>model=\"gpt-4.1\",</p>\n<p>threshold=0.5,</p>\n<p>async_mode=True,</p>\n<p>),</p>\n<p>]</p>\n<p>if not openai_enabled:</p>\n<p>print(\"\\n You did NOT provide an OpenAI API key.\")</p>\n<p>print(\"DeepEval's LLM-as-a-judge metrics (AnswerRelevancy/Faithfulness/Contextual* and GEval) require an LLM judge.\")</p>\n<p>print(\"Re-run this cell and provide OPENAI_API_KEY to run DeepEval metrics.\")</p>\n<p>print(\"\\n However, your RAG pipeline + test case construction succeeded end-to-end.\")</p>\n<p>rows = []</p>\n<p>for i, tc in enumerate(test_cases):</p>\n<p>rows.append({</p>\n<p>\"id\": i,</p>\n<p>\"query\": tc.input,</p>\n<p>\"actual_output\": tc.actual_output[:220] + (\"...\" if len(tc.actual_output) &gt; 220 else \"\"),</p>\n<p>\"expected_output\": tc.expected_output[:220] + (\"...\" if len(tc.expected_output) &gt; 220 else \"\"),</p>\n<p>\"contexts\": len(tc.retrieval_context or []),</p>\n<p>})</p>\n<p>display(pd.DataFrame(rows))</p>\n<p>raise SystemExit(\"Stopped before evaluation (no OpenAI key).\")</p>\n<p>We execute the RAG pipeline to generate LLMTestCase objects by pairing our retrieved context with model-generated answers and ground-truth expectations. We then configure a comprehensive suite of DeepEval metrics, including G-Eval and specialized RAG indicators, to evaluate the system’s performance using an LLM-as-a-judge approach. Check out the&nbsp;FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserprint(\"\\n Running DeepEval evaluate(...) ...\")</p>\n<p>results = evaluate(test_cases=test_cases, metrics=metrics)</p>\n<p>summary_rows = []</p>\n<p>for idx, tc in enumerate(test_cases):</p>\n<p>row = {</p>\n<p>\"case_id\": idx,</p>\n<p>\"query\": tc.input,</p>\n<p>\"actual_output\": tc.actual_output[:200] + (\"...\" if len(tc.actual_output) &gt; 200 else \"\"),</p>\n<p>}</p>\n<p>for m in metrics:</p>\n<p>row[m.__class__.__name__ if hasattr(m, \"__class__\") else str(m)] = None</p>\n<p>summary_rows.append(row)</p>\n<p>def try_extract_case_metrics(results_obj):</p>\n<p>extracted = []</p>\n<p>candidates = []</p>\n<p>for attr in [\"test_results\", \"results\", \"evaluations\"]:</p>\n<p>if hasattr(results_obj, attr):</p>\n<p>candidates = getattr(results_obj, attr)</p>\n<p>break</p>\n<p>if not candidates and isinstance(results_obj, list):</p>\n<p>candidates = results_obj</p>\n<p>for case_i, case_result in enumerate(candidates or []):</p>\n<p>item = {\"case_id\": case_i}</p>\n<p>metrics_list = None</p>\n<p>for attr in [\"metrics_data\", \"metrics\", \"metric_results\"]:</p>\n<p>if hasattr(case_result, attr):</p>\n<p>metrics_list = getattr(case_result, attr)</p>\n<p>break</p>\n<p>if isinstance(metrics_list, dict):</p>\n<p>for k, v in metrics_list.items():</p>\n<p>item[f\"{k}_score\"] = getattr(v, \"score\", None) if v is not None else None</p>\n<p>item[f\"{k}_reason\"] = getattr(v, \"reason\", None) if v is not None else None</p>\n<p>else:</p>\n<p>for mr in metrics_list or []:</p>\n<p>name = getattr(mr, \"name\", None) or getattr(getattr(mr, \"metric\", None), \"name\", None)</p>\n<p>if not name:</p>\n<p>name = mr.__class__.__name__</p>\n<p>item[f\"{name}_score\"] = getattr(mr, \"score\", None)</p>\n<p>item[f\"{name}_reason\"] = getattr(mr, \"reason\", None)</p>\n<p>extracted.append(item)</p>\n<p>return extracted</p>\n<p>case_metrics = try_extract_case_metrics(results)</p>\n<p>df_base = pd.DataFrame([{</p>\n<p>\"case_id\": i,</p>\n<p>\"query\": tc.input,</p>\n<p>\"actual_output\": tc.actual_output,</p>\n<p>\"expected_output\": tc.expected_output,</p>\n<p>} for i, tc in enumerate(test_cases)])</p>\n<p>df_metrics = pd.DataFrame(case_metrics) if case_metrics else pd.DataFrame([])</p>\n<p>df = df_base.merge(df_metrics, on=\"case_id\", how=\"left\")</p>\n<p>score_cols = [c for c in df.columns if c.endswith(\"_score\")]</p>\n<p>compact = df[[\"case_id\", \"query\"] + score_cols].copy()</p>\n<p>print(\"\\n Compact score table:\")</p>\n<p>display(compact)</p>\n<p>print(\"\\n Full details (includes reasons):\")</p>\n<p>display(df)</p>\n<p>print(\"\\n Done. Tip: if contextual precision/recall are low, improve retriever ranking/coverage; if faithfulness is low, tighten generation to only use context.\")</p>\n<p>We finalize the workflow by executing the evaluate function, which triggers the LLM-as-a-judge process to score each test case against our defined metrics. We then aggregate these scores and their corresponding qualitative reasoning into a centralized DataFrame, providing a granular view of where the RAG pipeline excels or requires further optimization in retrieval and generation.</p>\n<p>At last, we conclude by running our comprehensive evaluation suite, in which DeepEval transforms complex linguistic outputs into actionable data using metrics such as Faithfulness, Contextual Precision, and the G-Eval rubric. This systematic approach allows us to diagnose “silent failures” in retrieval and hallucinations in generation with surgical precision, providing the reasoning necessary to justify architectural changes. With these results, we move forward from experimental prototyping to a production-ready RAG system backed by a verifiable, metric-driven safety net.</p>\n<p>Check out the&nbsp;FULL CODES here.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don’t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>The post A Coding Implementation to Automating LLM Quality Assurance with DeepEval, Custom Retrievers, and LLM-as-a-Judge Metrics appeared first on MarkTechPost.</p>"
    }
  ]
}